[
    {
        "title": "GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning through Bayesian Optimization",
        "authors": [
            "Bojana Rankovi\u0107",
            "Philippe Schwaller"
        ],
        "summary": "Large Language Models (LLMs) can encode complex relationships in their latent\nspaces, yet harnessing them for optimization under uncertainty remains\nchallenging. We address this gap with a novel architecture that reframes LLM\nfinetuning as Gaussian process (GP) marginal likelihood optimization via deep\nkernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs\nto preserve the benefits of both - LLMs to provide a rich and flexible input\nspace for Bayesian optimization and - GPs to model this space with predictive\nuncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction\noptimization, our method nearly doubles the discovery rate of high-performing\nreactions compared to static LLM embeddings (from 24% to 43% coverage of the\ntop 5% reactions in just 50 optimization iterations). We also observe a 14%\nimprovement over domain-specific representations without requiring specialized\nfeatures. Extensive empirical evaluation across 19 benchmarks - ranging from\ngeneral chemistry to reaction and molecular property optimization -\ndemonstrates our method's robustness, generality, and consistent improvements\nacross: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder),\n(3) pretraining domains (chemistry-related or general-purpose) and (4)\nhyperparameter settings (tuned once on a single dataset). Finally, we explain\nthese improvements: joint LLM-GP optimization through marginal likelihood\nimplicitly performs contrastive learning, aligning representations to produce\n(1) better-structured embedding spaces, (2) improved uncertainty calibration,\nand (3) more efficient sampling - without requiring any external loss. This\nwork provides both practical advances in sample-efficient optimization and\ninsights into what makes effective Bayesian optimization.",
        "published": "2025-04-08T17:59:57+00:00"
    },
    {
        "title": "PainNet: Statistical Relation Network with Episode-Based Training for Pain Estimation",
        "authors": [
            "Mina Bishay",
            "Graham Page",
            "Mohammad Mavadati"
        ],
        "summary": "Despite the span in estimating pain from facial expressions, limited works\nhave focused on estimating the sequence-level pain, which is reported by\npatients and used commonly in clinics. In this paper, we introduce a novel\nStatistical Relation Network, referred to as PainNet, designed for the\nestimation of the sequence-level pain. PainNet employs two key modules, the\nembedding and the relation modules, for comparing pairs of pain videos, and\nproducing relation scores indicating if each pair belongs to the same pain\ncategory or not. At the core of the embedding module is a statistical layer\nmounted on the top of a RNN for extracting compact video-level features. The\nstatistical layer is implemented as part of the deep architecture. Doing so,\nallows combining multiple training stages used in previous research, into a\nsingle end-to-end training stage. PainNet is trained using the episode-based\ntraining scheme, which involves comparing a query video with a set of videos\nrepresenting the different pain categories. Experimental results show the\nbenefit of using the statistical layer and the episode-based training in the\nproposed model. Furthermore, PainNet outperforms the state-of-the-art results\non self-reported pain estimation.",
        "published": "2025-04-08T17:58:52+00:00"
    },
    {
        "title": "Transfer between Modalities with MetaQueries",
        "authors": [
            "Xichen Pan",
            "Satya Narayan Shukla",
            "Aashu Singh",
            "Zhuokai Zhao",
            "Shlok Kumar Mishra",
            "Jialiang Wang",
            "Zhiyang Xu",
            "Jiuhai Chen",
            "Kunpeng Li",
            "Felix Juefei-Xu",
            "Ji Hou",
            "Saining Xie"
        ],
        "summary": "Unified multimodal models aim to integrate understanding (text output) and\ngeneration (pixel output), but aligning these different modalities within a\nsingle architecture often demands complex training recipes and careful data\nbalancing. We introduce MetaQueries, a set of learnable queries that act as an\nefficient interface between autoregressive multimodal LLMs (MLLMs) and\ndiffusion models. MetaQueries connects the MLLM's latents to the diffusion\ndecoder, enabling knowledge-augmented image generation by leveraging the MLLM's\ndeep understanding and reasoning capabilities. Our method simplifies training,\nrequiring only paired image-caption data and standard diffusion objectives.\nNotably, this transfer is effective even when the MLLM backbone remains frozen,\nthereby preserving its state-of-the-art multimodal understanding capabilities\nwhile achieving strong generative performance. Additionally, our method is\nflexible and can be easily instruction-tuned for advanced applications such as\nimage editing and subject-driven generation.",
        "published": "2025-04-08T17:58:47+00:00"
    },
    {
        "title": "Diagrammatic expansion for the mutual-information rate in the realm of limited statistics",
        "authors": [
            "Tobias K\u00fchn",
            "Gabriel Mahuas",
            "Ulisse Ferrari"
        ],
        "summary": "Neurons in sensory systems encode stimulus information into their stochastic\nspiking response. The Mutual information has been broadly applied to these\nsystems to quantify the neurons' capacity of transmitting such information.\nYet, while for discrete stimuli, like flashed images or single tones, its\ncomputation is straightforward, for dynamical stimuli it is necessary to\ncompute a (mutual) information rate (MIR), therefore integrating over the\nmultiple temporal correlations which characterize sensory systems. Previous\nmethods are based on extensive sampling of the neuronal response, require large\namounts of data, and are therefore prone to biases and inaccuracy. Here, we\ndevelop Moba-MIRA (moment-based mutual-information-rate approximation), a\ncomputational method to estimate the mutual information rate. To derive\nMoba-MIRA, we use Feynman diagrams to expand the mutual information in\narbitrary orders in the correlations around the corresponding value for the\nempirical spike count distributions of single binss. As a result, only the\nempirical estimation of the pairwise correlations between time bins and the\nsingle-bin entropies are required, without the need for the whole joint\nprobability distributions. We tested Moba-MIRA on synthetic data generated with\ngeneralized linear models, and showed that it requires only a few tens of\nstimulus repetitions to provide an accurate estimate of the information rate.\nFinally, we applied it to ex-vivo electrophysiological recordings of rats\nretina, obtaining rates ranging between 5 to 20 bits per second, consistent\nwith earlier estimates.",
        "published": "2025-04-08T17:57:27+00:00"
    },
    {
        "title": "Fractal and Regular Geometry of Deep Neural Networks",
        "authors": [
            "Simmaco Di Lillo",
            "Domenico Marinucci",
            "Michele Salvi",
            "Stefano Vigogna"
        ],
        "summary": "We study the geometric properties of random neural networks by investigating\nthe boundary volumes of their excursion sets for different activation\nfunctions, as the depth increases. More specifically, we show that, for\nactivations which are not very regular (e.g., the Heaviside step function), the\nboundary volumes exhibit fractal behavior, with their Hausdorff dimension\nmonotonically increasing with the depth. On the other hand, for activations\nwhich are more regular (e.g., ReLU, logistic and $\\tanh$), as the depth\nincreases, the expected boundary volumes can either converge to zero, remain\nconstant or diverge exponentially, depending on a single spectral parameter\nwhich can be easily computed. Our theoretical results are confirmed in some\nnumerical experiments based on Monte Carlo simulations.",
        "published": "2025-04-08T17:56:05+00:00"
    },
    {
        "title": "Kuramoto meets Koopman: Constants of motion, symmetries, and network motifs",
        "authors": [
            "Vincent Thibeault",
            "Benjamin Claveau",
            "Antoine Allard",
            "Patrick Desrosiers"
        ],
        "summary": "The partial integrability of the Kuramoto model is often thought to be\nrestricted to identically connected oscillators or groups thereof. Yet, the\nexact connectivity prerequisites for having constants of motion on more general\ngraphs have remained elusive. Using spectral properties of the Koopman\ngenerator, we derive necessary and sufficient conditions for the existence of\ndistinct constants of motion in the Kuramoto model with heterogeneous phase\nlags on any weighted, directed, signed graph. This reveals a broad class of\nnetwork motifs that support conserved quantities. Furthermore, we identify Lie\nsymmetries that generate new constants of motion. Our results provide a\nrigorous theoretical application of Koopman's framework to nonlinear dynamics\non complex networks.",
        "published": "2025-04-08T17:53:08+00:00"
    },
    {
        "title": "Underwater Robotic Simulators Review for Autonomous System Development",
        "authors": [
            "Sara Aldhaheri",
            "Yang Hu",
            "Yongchang Xie",
            "Peng Wu",
            "Dimitrios Kanoulas",
            "Yuanchang Liu"
        ],
        "summary": "The increasing complexity of underwater robotic systems has led to a surge in\nsimulation platforms designed to support perception, planning, and control\ntasks in marine environments. However, selecting the most appropriate\nunderwater robotic simulator (URS) remains a challenge due to wide variations\nin fidelity, extensibility, and task suitability. This paper presents a\ncomprehensive review and comparative analysis of five state-of-the-art,\nROS-compatible, open-source URSs: Stonefish, DAVE, HoloOcean, MARUS, and\nUNav-Sim. Each simulator is evaluated across multiple criteria including sensor\nfidelity, environmental realism, sim-to-real capabilities, and research impact.\nWe evaluate them across architectural design, sensor and physics modeling, task\ncapabilities, and research impact. Additionally, we discuss ongoing challenges\nin sim-to-real transfer and highlight the need for standardization and\nbenchmarking in the field. Our findings aim to guide practitioners in selecting\neffective simulation environments and inform future development of more robust\nand transferable URSs.",
        "published": "2025-04-08T17:43:48+00:00"
    },
    {
        "title": "Addressing Relative Degree Issues in Control Barrier Function Synthesis with Physics-Informed Neural Networks",
        "authors": [
            "Lukas Brunke",
            "Siqi Zhou",
            "Francesco D'Orazio",
            "Angela P. Schoellig"
        ],
        "summary": "In robotics, control barrier function (CBF)-based safety filters are commonly\nused to enforce state constraints. A critical challenge arises when the\nrelative degree of the CBF varies across the state space. This variability can\ncreate regions within the safe set where the control input becomes\nunconstrained. When implemented as a safety filter, this may result in\nchattering near the safety boundary and ultimately compromise system safety. To\naddress this issue, we propose a novel approach for CBF synthesis by\nformulating it as solving a set of boundary value problems. The solutions to\nthe boundary value problems are determined using physics-informed neural\nnetworks (PINNs). Our approach ensures that the synthesized CBFs maintain a\nconstant relative degree across the set of admissible states, thereby\npreventing unconstrained control scenarios. We illustrate the approach in\nsimulation and further verify it through real-world quadrotor experiments,\ndemonstrating its effectiveness in preserving desired system safety properties.",
        "published": "2025-04-08T17:41:43+00:00"
    },
    {
        "title": "A Case for Network-wide Orchestration of Host-based Intrusion Detection and Response",
        "authors": [
            "Mark Timmons",
            "Daniel Lukaszewski",
            "Geoffrey Xie"
        ],
        "summary": "Recent cyber incidents and the push for zero trust security underscore the\nnecessity of monitoring host-level events. However, current host-level\nintrusion detection systems (IDS) lack the ability to correlate alerts and\ncoordinate a network-wide response in real time. Motivated by advances in\nsystem-level extensions free of rebooting and network-wide orchestration of\nhost actions, we propose using a central IDS orchestrator to remotely program\nthe logic of each host IDS and collect the alerts generated in real time. In\nthis paper, we make arguments for such a system concept and provide a high\nlevel design of the main system components. Furthermore, we have developed a\nsystem prototype and evaluated it using two experimental scenarios rooted from\nreal-world attacks. The evaluation results show that the host-based IDS\norchestration system is able to defend against the attacks effectively.",
        "published": "2025-04-08T17:41:04+00:00"
    },
    {
        "title": "Monitoring Viewer Attention During Online Ads",
        "authors": [
            "Mina Bishay",
            "Graham Page",
            "Waleed Emad",
            "Mohammad Mavadati"
        ],
        "summary": "Nowadays, video ads spread through numerous online platforms, and are being\nwatched by millions of viewers worldwide. Big brands gauge the liking and\npurchase intent of their new ads, by analyzing the facial responses of viewers\nrecruited online to watch the ads from home or work. Although this approach\ncaptures naturalistic responses, it is susceptible to distractions inherent in\nthe participants' environments, such as a movie playing on TV, a colleague\nspeaking, or mobile notifications. Inattentive participants should get flagged\nand eliminated to avoid skewing the ad-testing process. In this paper we\nintroduce an architecture for monitoring viewer attention during online ads.\nLeveraging two behavior analysis toolkits; AFFDEX 2.0 and SmartEye SDK, we\nextract low-level facial features encompassing facial expressions, head pose,\nand gaze direction. These features are then combined to extract high-level\nfeatures that include estimated gaze on the screen plane, yawning, speaking,\netc -- this enables the identification of four primary distractors; off-screen\ngaze, drowsiness, speaking, and unattended screen. Our architecture tailors the\ngaze settings according to the device type (desktop or mobile). We validate our\narchitecture first on datasets annotated for specific distractors, and then on\na real-world ad testing dataset with various distractors. The proposed\narchitecture shows promising results in detecting distraction across both\ndesktop and mobile devices.",
        "published": "2025-04-08T17:34:02+00:00"
    },
    {
        "title": "Decentralized Federated Domain Generalization with Style Sharing: A Formal Modeling and Convergence Analysis",
        "authors": [
            "Shahryar Zehtabi",
            "Dong-Jun Han",
            "Seyyedali Hosseinalipour",
            "Christopher G. Brinton"
        ],
        "summary": "Much of the federated learning (FL) literature focuses on settings where\nlocal dataset statistics remain the same between training and testing time.\nRecent advances in domain generalization (DG) aim to use data from source\n(training) domains to train a model that generalizes well to data from unseen\ntarget (testing) domains. In this paper, we are motivated by two major gaps in\nexisting work on FL and DG: (1) the lack of formal mathematical analysis of DG\nobjectives and training processes; and (2) DG research in FL being limited to\nthe conventional star-topology architecture. Addressing the second gap, we\ndevelop $\\textit{Decentralized Federated Domain Generalization with Style\nSharing}$ ($\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to\nallow devices in a peer-to-peer network to achieve DG based on sharing style\ninformation inferred from their datasets. Additionally, we fill the first gap\nby providing the first systematic approach to mathematically analyzing\nstyle-based DG training optimization. We cast existing centralized DG\nalgorithms within our framework, and employ their formalisms to model\n$\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which\na sub-linear convergence rate of $\\texttt{StyleDDG}$ can be obtained. Through\nexperiments on two popular DG datasets, we demonstrate that $\\texttt{StyleDDG}$\ncan obtain significant improvements in accuracy across target domains with\nminimal added communication overhead compared to decentralized gradient methods\nthat do not employ style sharing.",
        "published": "2025-04-08T17:32:56+00:00"
    },
    {
        "title": "Orb-v3: atomistic simulation at scale",
        "authors": [
            "Benjamin Rhodes",
            "Sander Vandenhaute",
            "Vaidotas \u0160imkus",
            "James Gin",
            "Jonathan Godwin",
            "Tim Duignan",
            "Mark Neumann"
        ],
        "summary": "We introduce Orb-v3, the next generation of the Orb family of universal\ninteratomic potentials. Models in this family expand the\nperformance-speed-memory Pareto frontier, offering near SoTA performance across\na range of evaluations with a >10x reduction in latency and > 8x reduction in\nmemory. Our experiments systematically traverse this frontier, charting the\ntrade-off induced by roto-equivariance, conservatism and graph sparsity.\nContrary to recent literature, we find that non-equivariant, non-conservative\narchitectures can accurately model physical properties, including those which\nrequire higher-order derivatives of the potential energy surface.\n  This model release is guided by the principle that the most valuable\nfoundation models for atomic simulation will excel on all fronts: accuracy,\nlatency and system size scalability. The reward for doing so is a new era of\ncomputational chemistry driven by high-throughput and mesoscale all-atom\nsimulations.",
        "published": "2025-04-08T17:27:34+00:00"
    },
    {
        "title": "Continuous-variable spatio-spectral quantum networks in nonlinear photonic lattices",
        "authors": [
            "Natalia Costas",
            "Nadia Belabas",
            "David Barral"
        ],
        "summary": "Multiplexing information in different degrees of freedom and use of\nintegrated and fiber-optic components are natural solutions to the scalability\nbottleneck in optical quantum communications and computing. However, for\nbulk-optics systems, where size, cost, stability, and reliability are factors,\nthis remains either impractical or highly challenging to implement. In this\npaper we present a framework to engineer continuous-variable entanglement\nproduced through nondegenerate spontaneous parametric down-conversion in\n\\chi^(2) nonlinear photonic lattices in spatial and spectral degrees of freedom\nthat can solve the scalability challenge. We show how spatio-spectral pump\nshaping produce cluster states that are naturally distributable in quantum\ncommunication networks and a resource for measurement-based quantum computing.",
        "published": "2025-04-08T17:24:00+00:00"
    },
    {
        "title": "NNN: Next-Generation Neural Networks for Marketing Mix Modeling",
        "authors": [
            "Thomas Mulc",
            "Mike Anderson",
            "Paul Cubre",
            "Huikun Zhang",
            "Ivy Liu",
            "Saket Kumar"
        ],
        "summary": "We present NNN, a Transformer-based neural network approach to Marketing Mix\nModeling (MMM) designed to address key limitations of traditional methods.\nUnlike conventional MMMs which rely on scalar inputs and parametric decay\nfunctions, NNN uses rich embeddings to capture both quantitative and\nqualitative aspects of marketing and organic channels (e.g., search queries, ad\ncreatives). This, combined with its attention mechanism, enables NNN to model\ncomplex interactions, capture long-term effects, and potentially improve sales\nattribution accuracy. We show that L1 regularization permits the use of such\nexpressive models in typical data-constrained settings. Evaluating NNN on\nsimulated and real-world data demonstrates its efficacy, particularly through\nconsiderable improvement in predictive power. Beyond attribution, NNN provides\nvaluable, complementary insights through model probing, such as evaluating\nkeyword or creative effectiveness, enhancing model interpretability.",
        "published": "2025-04-08T16:57:11+00:00"
    },
    {
        "title": "Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs",
        "authors": [
            "Alhad Daftardar",
            "Jianqiao Mo",
            "Joey Ah-kiow",
            "Benedikt B\u00fcnz",
            "Ramesh Karri",
            "Siddharth Garg",
            "Brandon Reagen"
        ],
        "summary": "Zero-Knowledge Proofs (ZKPs) are rapidly gaining importance in\nprivacy-preserving and verifiable computing. ZKPs enable a proving party to\nprove the truth of a statement to a verifying party without revealing anything\nelse. ZKPs have applications in blockchain technologies, verifiable machine\nlearning, and electronic voting, but have yet to see widespread adoption due to\nthe computational complexity of the proving process. Recent works have\naccelerated the key primitives of state-of-the-art ZKP protocols on GPU and\nASIC. However, the protocols accelerated thus far face one of two challenges:\nthey either require a trusted setup for each application, or they generate\nlarger proof sizes with higher verification costs, limiting their applicability\nin scenarios with numerous verifiers or strict verification time constraints.\nThis work presents an accelerator, zkSpeed, for HyperPlonk, a state-of-the-art\nZKP protocol that supports both one-time, universal setup and small proof sizes\nfor typical ZKP applications in publicly verifiable, consensus-based systems.\nWe accelerate the entire protocol, including two major primitives: SumCheck and\nMulti-scalar Multiplications (MSMs). We develop a full-chip architecture using\n366.46 mm$^2$ and 2 TB/s of bandwidth to accelerate the entire proof generation\nprocess, achieving geometric mean speedups of 801$\\times$ over CPU baselines.",
        "published": "2025-04-08T16:56:10+00:00"
    },
    {
        "title": "Quantum Annealing for Combinatorial Optimization: A Benchmarking Study",
        "authors": [
            "Seongmin Kim",
            "Sang-Woo Ahn",
            "In-Saeng Suh",
            "Alexander W. Dowling",
            "Eungkyu Lee",
            "Tengfei Luo"
        ],
        "summary": "Quantum annealing (QA) has the potential to significantly improve solution\nquality and reduce time complexity in solving combinatorial optimization\nproblems compared to classical optimization methods. However, due to the\nlimited number of qubits and their connectivity, the QA hardware did not show\nsuch an advantage over classical methods in past benchmarking studies. Recent\nadvancements in QA with more than 5,000 qubits, enhanced qubit connectivity,\nand the hybrid architecture promise to realize the quantum advantage. Here, we\nuse a quantum annealer with state-of-the-art techniques and benchmark its\nperformance against classical solvers. To compare their performance, we solve\nover 50 optimization problem instances represented by large and dense\nHamiltonian matrices using quantum and classical solvers. The results\ndemonstrate that a state-of-the-art quantum solver has higher accuracy\n(~0.013%) and a significantly faster problem-solving time (~6,561x) than the\nbest classical solver. Our results highlight the advantages of leveraging QA\nover classical counterparts, particularly in hybrid configurations, for\nachieving high accuracy and substantially reduced problem solving time in\nlarge-scale real-world optimization problems.",
        "published": "2025-04-08T16:43:24+00:00"
    },
    {
        "title": "Resting State Functional Connectivity Patterns Associate with Alcohol Use Disorder Characteristics: Insights from the Triple Network Model",
        "authors": [
            "Daniel Guerrero",
            "Mario Dzemidzic",
            "Mahdi Moghaddam",
            "Mintao Liu",
            "Andrea Avena-Koenigsberger",
            "Jaroslaw Harezlak",
            "David A. Kareken",
            "Martin H. Plawecki",
            "Melissa A. Cyders",
            "Joaqu\u00edn Go\u00f1i"
        ],
        "summary": "Prolonged alcohol use results in neuroadaptations that mark more severe and\ntreatment-resistant alcohol use. The goal of this study was to identify\nfunctional connectivity brain patterns underlying Alcohol Use Disorder\n(AUD)-related characteristics in fifty-five adults (31 female) who endorsed\nheavy alcohol use. We hypothesized that resting-state functional connectivity\n(rsFC) of the Salience (SN), Frontoparietal (FPN), and Default Mode (DMN)\nnetworks would reflect self reported recent and lifetime alcohol use,\nlaboratory-based alcohol seeking, urgency, and sociodemographic characteristics\nrelated to AUD. To test our hypothesis, we combined the triple network model\n(TNM) of psychopathology with a multivariate data-driven approach, regularized\npartial least squares (rPLS), to unfold concurrent functional connectivity (FC)\npatterns and their association with AUD characteristics. We observed three\nconcurrent associations of interest: i) drinking and age-related cross\ncommunication between the SN and both the FPN and DMN; ii) family history\ndensity of AUD and urgency anticorrelations between the SN and FPN; and iii)\nalcohol seeking and sex-associated SN and DMN interactions. These findings\ndemonstrate the utility of combining theory- and data-driven approaches to\nuncover associations between resting-state functional substrates and\nAUD-related characteristics that could aid in the identification, development,\nand testing of novel treatment targets across preclinical and clinical models.",
        "published": "2025-04-08T16:42:45+00:00"
    },
    {
        "title": "Heuristic Methods are Good Teachers to Distill MLPs for Graph Link Prediction",
        "authors": [
            "Zongyue Qin",
            "Shichang Zhang",
            "Mingxuan Ju",
            "Tong Zhao",
            "Neil Shah",
            "Yizhou Sun"
        ],
        "summary": "Link prediction is a crucial graph-learning task with applications including\ncitation prediction and product recommendation. Distilling Graph Neural\nNetworks (GNNs) teachers into Multi-Layer Perceptrons (MLPs) students has\nemerged as an effective approach to achieve strong performance and reducing\ncomputational cost by removing graph dependency. However, existing distillation\nmethods only use standard GNNs and overlook alternative teachers such as\nspecialized model for link prediction (GNN4LP) and heuristic methods (e.g.,\ncommon neighbors). This paper first explores the impact of different teachers\nin GNN-to-MLP distillation. Surprisingly, we find that stronger teachers do not\nalways produce stronger students: MLPs distilled from GNN4LP can underperform\nthose distilled from simpler GNNs, while weaker heuristic methods can teach\nMLPs to near-GNN performance with drastically reduced training costs. Building\non these insights, we propose Ensemble Heuristic-Distilled MLPs (EHDM), which\neliminates graph dependencies while effectively integrating complementary\nsignals via a gating mechanism. Experiments on ten datasets show an average\n7.93% improvement over previous GNN-to-MLP approaches with 1.95-3.32 times less\ntraining time, indicating EHDM is an efficient and effective link prediction\nmethod.",
        "published": "2025-04-08T16:35:11+00:00"
    },
    {
        "title": "Plug and Play Distributed Control of Clustered Energy Hub Networks",
        "authors": [
            "Varsha Behrunani",
            "Cara Koepele",
            "Jared Miller",
            "Ahmed Aboudonia",
            "Philipp Heer",
            "Roy S. Smith",
            "John Lygeros"
        ],
        "summary": "The transition to renewable energy is driving the rise of distributed\nmulti-energy systems, in which individual energy hubs and prosumers (e.g.,\nhomes, industrial campuses) generate, store, and trade energy. Economic Model\nPredictive Control (MPC) schemes are widely used to optimize operation of\nenergy hubs by efficiently dispatching resources and minimizing costs while\nensuring operational constraints are met. Peer-to-peer (P2P) energy trading\namong hubs enhances network efficiency and reduces costs but also increases\ncomputational and privacy challenges, especially as the network scales.\nAdditionally, current distributed control techniques require global\nrecomputation whenever the network topology changes, limiting scalability. To\naddress these challenges, we propose a clustering-based P2P trading framework\nthat enables plug-and-play operation, allowing energy hubs to seamlessly join\nor leave without requiring network-wide controller updates. The impact is\nrestricted to the hubs within the affected cluster. The energy trading problem\nis formulated as a bi-level bargaining game, where inter-cluster trading\ncommitments are determined at the cluster level, while energy dispatch and\ncost-sharing among hubs within a cluster are refined at the hub level. Both\nlevels are solved in a distributed manner using ADMM, ensuring computational\nfeasibility and privacy preservation. Moreover, we develop plug-and-play\nprocedures to handle dynamic topology changes at both the hub and cluster\nlevels, minimizing disruptions across the network. Simulation results\ndemonstrate that the proposed bi-level framework reduces operational costs, and\nenables scalable energy management under plug-and-play operation.",
        "published": "2025-04-08T16:21:23+00:00"
    },
    {
        "title": "Hydroxide Mobility in Aqueous Systems: Ab Initio Accuracy with Millisecond Timescales",
        "authors": [
            "Jonas H\u00e4nseroth",
            "Daniel Sebastiani",
            "Jakob Scholl",
            "Karl Skadell",
            "Christian Dre\u00dfler"
        ],
        "summary": "We present a multiscale simulation approach for hydroxide transport in\naqueous solutions of potassium hydroxide, combining ab initio molecular\ndynamics (AIMD) simulations with force field ensemble averaging and lattice\nMonte Carlo techniques. This method achieves near ab initio accuracy by\ncapturing the femtosecond scale dielectric relaxation dynamics of the aqueous\nhydrogen bonding network, while extending the simulation capability to\nmillisecond diffusion timescales. This extraordinary extension of the available\nlength and time scales enables future studies of hydroxide mobility in\nfunctional materials such as nanostructured anion-exchange membranes, where\nhydroxide ions migrate through nanometer-sized channels. Remarkably, our\napproach demonstrates that a single AIMD trajectory is sufficient to predict\nhydroxide conductivity over a range of concentrations, underscoring its\ncomputational efficiency and relevance to the design of advanced energy\nmaterials.",
        "published": "2025-04-08T16:19:46+00:00"
    },
    {
        "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
        "authors": [
            "Ian Groves",
            "Andrew Campbell",
            "James Fernandes",
            "Diego Rodriguez",
            "Paul Murray",
            "Massimiliano Vasile",
            "Victoria Nockles"
        ],
        "summary": "Foundation Models, pre-trained on large unlabelled datasets before\ntask-specific fine-tuning, are increasingly being applied to specialised\ndomains. Recent examples include ClimaX for climate and Clay for satellite\nEarth observation, but a Foundation Model for Space Object Behavioural Analysis\nhas not yet been developed. As orbital populations grow, automated methods for\ncharacterising space object behaviour are crucial for space safety. We present\na Space Safety and Sustainability Foundation Model focusing on space object\nbehavioural analysis using light curves (LCs). We implemented a\nPerceiver-Variational Autoencoder (VAE) architecture, pre-trained with\nself-supervised reconstruction and masked reconstruction on 227,000 LCs from\nthe MMT-9 observatory. The VAE enables anomaly detection, motion prediction,\nand LC generation. We fine-tuned the model for anomaly detection & motion\nprediction using two independent LC simulators (CASSANDRA and GRIAL\nrespectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink\nplatforms. Our pre-trained model achieved a reconstruction error of 0.01%,\nidentifying potentially anomalous light curves through reconstruction\ndifficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90\nand 0.95 ROC AUC scores respectively in both anomaly detection and motion mode\nprediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly\npredictions on real data revealed distinct patterns including characteristic\nobject profiles and satellite glinting. Here, we demonstrate how\nself-supervised learning can simultaneously enable anomaly detection, motion\nprediction, and synthetic data generation from rich representations learned in\npre-training. Our work therefore supports space safety and sustainability\nthrough automated monitoring and simulation capabilities.",
        "published": "2025-04-08T16:19:19+00:00"
    },
    {
        "title": "Multi-Modality Sensing in mmWave Beamforming for Connected Vehicles Using Deep Learning",
        "authors": [
            "Muhammad Baqer Mollah",
            "Honggang Wang",
            "Mohammad Ataul Karim",
            "Hua Fang"
        ],
        "summary": "Beamforming techniques are considered as essential parts to compensate severe\npath losses in millimeter-wave (mmWave) communications. In particular, these\ntechniques adopt large antenna arrays and formulate narrow beams to obtain\nsatisfactory received powers. However, performing accurate beam alignment over\nnarrow beams for efficient link configuration by traditional standard defined\nbeam selection approaches, which mainly rely on channel state information and\nbeam sweeping through exhaustive searching, imposes computational and\ncommunications overheads. And, such resulting overheads limit their potential\nuse in vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V)\ncommunications involving highly dynamic scenarios. In comparison, utilizing\nout-of-band contextual information, such as sensing data obtained from sensor\ndevices, provides a better alternative to reduce overheads. This paper presents\na deep learning-based solution for utilizing the multi-modality sensing data\nfor predicting the optimal beams having sufficient mmWave received powers so\nthat the best V2I and V2V line-of-sight links can be ensured proactively. The\nproposed solution has been tested on real-world measured mmWave sensing and\ncommunication data, and the results show that it can achieve up to 98.19%\naccuracies while predicting top-13 beams. Correspondingly, when compared to\nexisting been sweeping approach, the beam sweeping searching space and time\noverheads are greatly shortened roughly by 79.67% and 91.89%, respectively\nwhich confirm a promising solution for beamforming in mmWave enabled\ncommunications.",
        "published": "2025-04-08T16:18:00+00:00"
    },
    {
        "title": "Real-Time Pitch/F0 Detection Using Spectrogram Images and Convolutional Neural Networks",
        "authors": [
            "Xufang Zhao",
            "Omer Tsimhoni"
        ],
        "summary": "This paper presents a novel approach to detect F0 through Convolutional\nNeural Networks and image processing techniques to directly estimate pitch from\nspectrogram images. Our new approach demonstrates a very good detection\naccuracy; a total of 92% of predicted pitch contours have strong or moderate\ncorrelations to the true pitch contours. Furthermore, the experimental\ncomparison between our new approach and other state-of-the-art CNN methods\nreveals that our approach can enhance the detection rate by approximately 5%\nacross various Signal-to-Noise Ratio conditions.",
        "published": "2025-04-08T16:01:25+00:00"
    },
    {
        "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
        "authors": [
            "Rijul Magu",
            "Arka Dutta",
            "Sean Kim",
            "Ashiqur R. KhudaBukhsh",
            "Munmun De Choudhury"
        ],
        "summary": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation.",
        "published": "2025-04-08T15:56:57+00:00"
    },
    {
        "title": "Rethinking the Nested U-Net Approach: Enhancing Biomarker Segmentation with Attention Mechanisms and Multiscale Feature Fusion",
        "authors": [
            "Saad Wazir",
            "Daeyoung Kim"
        ],
        "summary": "Identifying biomarkers in medical images is vital for a wide range of biotech\napplications. However, recent Transformer and CNN based methods often struggle\nwith variations in morphology and staining, which limits their feature\nextraction capabilities. In medical image segmentation, where data samples are\noften limited, state-of-the-art (SOTA) methods improve accuracy by using\npre-trained encoders, while end-to-end approaches typically fall short due to\ndifficulties in transferring multiscale features effectively between encoders\nand decoders. To handle these challenges, we introduce a nested UNet\narchitecture that captures both local and global context through Multiscale\nFeature Fusion and Attention Mechanisms. This design improves feature\nintegration from encoders, highlights key channels and regions, and restores\nspatial details to enhance segmentation performance. Our method surpasses SOTA\napproaches, as evidenced by experiments across four datasets and detailed\nablation studies. Code: https://github.com/saadwazir/ReN-UNet",
        "published": "2025-04-08T15:53:46+00:00"
    },
    {
        "title": "Hall Effect Thruster Forecasting using a Topological Approach for Data Assimilation",
        "authors": [
            "Max M. Chumley",
            "Firas A. Khasawneh"
        ],
        "summary": "Hall Effect Thrusters (HETs) are electric thrusters that eject heavy ionized\ngas particles from the spacecraft to generate thrust. Although traditionally\nthey were used for station keeping, recently They have been used for\ninterplanetary space missions due to their high delta-V potential and their\noperational longevity in contrast to other thrusters, e.g., chemical. However,\nthe operation of HETs involves complex processes such as ionization of gases,\nstrong magnetic fields, and complicated solar panel power supply interactions.\nTherefore, their operation is extremely difficult to model thus necessitating\nData Assimilation (DA) approaches for estimating and predicting their\noperational states. Because HET's operating environment is often noisy with\nnon-Gaussian sources, this significantly limits applicable DA tools. We\ndescribe a topological approach for data assimilation that bypasses these\nlimitations that does not depend on the noise model, and utilize it to forecast\nspatiotemporal plume field states of HETs. Our approach is a generalization of\nthe Topological Approach for Data Assimilation (TADA) method that allows\nincluding different forecast functions. We show how TADA can be combined with\nthe Long Short-Term Memory network for accurate forecasting. We then apply our\napproach to high-fidelity Hall Effect Thruster (HET) simulation data from the\nAir Force Research Laboratory (AFRL) rocket propulsion division where we\ndemonstrate the forecast resiliency of TADA on noise contaminated,\nhigh-dimensional data.",
        "published": "2025-04-08T15:52:50+00:00"
    },
    {
        "title": "A Large-Scale Analysis on Contextual Self-Supervised Video Representation Learning",
        "authors": [
            "Akash Kumar",
            "Ashlesha Kumar",
            "Vibhav Vineet",
            "Yogesh S Rawat"
        ],
        "summary": "Self-supervised learning has emerged as a powerful paradigm for label-free\nmodel pretraining, particularly in the video domain, where manual annotation is\ncostly and time-intensive. However, existing self-supervised approaches employ\ndiverse experimental setups, making direct comparisons challenging due to the\nabsence of a standardized benchmark. In this work, we establish a unified\nbenchmark that enables fair comparisons across different methods. Additionally,\nwe systematically investigate five critical aspects of self-supervised learning\nin videos: (1) dataset size, (2) model complexity, (3) data distribution, (4)\ndata noise, and (5) feature representations. To facilitate this study, we\nevaluate six self-supervised learning methods across six network architectures,\nconducting extensive experiments on five benchmark datasets and assessing\nperformance on two distinct downstream tasks. Our analysis reveals key insights\ninto the interplay between pretraining strategies, dataset characteristics,\npretext tasks, and model architectures. Furthermore, we extend these findings\nto Video Foundation Models (ViFMs), demonstrating their relevance in\nlarge-scale video representation learning. Finally, leveraging these insights,\nwe propose a novel approach that significantly reduces training data\nrequirements while surpassing state-of-the-art methods that rely on 10% more\npretraining data. We believe this work will guide future research toward a\ndeeper understanding of self-supervised video representation learning and its\nbroader implications.",
        "published": "2025-04-08T15:47:58+00:00"
    },
    {
        "title": "ARLO: A Tailorable Approach for Transforming Natural Language Software Requirements into Architecture using LLMs",
        "authors": [
            "Tooraj Helmi"
        ],
        "summary": "Software requirements expressed in natural language (NL) frequently suffer\nfrom verbosity, ambiguity, and inconsistency. This creates a range of\nchallenges, including selecting an appropriate architecture for a system and\nassessing different architectural alternatives. Relying on human expertise to\naccomplish the task of mapping NL requirements to architecture is\ntime-consuming and error-prone. This paper proposes ARLO, an approach that\nautomates this task by leveraging (1) a set of NL requirements for a system,\n(2) an existing standard that specifies architecturally relevant software\nquality attributes, and (3) a readily available Large Language Model (LLM).\nSpecifically, ARLO determines the subset of NL requirements for a given system\nthat is architecturally relevant and maps that subset to a tailorable matrix of\narchitectural choices. ARLO applies integer linear programming on the\narchitectural-choice matrix to determine the optimal architecture for the\ncurrent requirements. We demonstrate ARLO's efficacy using a set of real-world\nexamples. We highlight ARLO's ability (1) to trace the selected architectural\nchoices to the requirements and (2) to isolate NL requirements that exert a\nparticular influence on a system's architecture. This allows the\nidentification, comparative assessment, and exploration of alternative\narchitectural choices based on the requirements and constraints expressed\ntherein.",
        "published": "2025-04-08T15:38:42+00:00"
    },
    {
        "title": "Decentralizing AI Memory: SHIMI, a Semantic Hierarchical Memory Index for Scalable Agent Reasoning",
        "authors": [
            "Tooraj Helmi"
        ],
        "summary": "Retrieval-Augmented Generation (RAG) and vector-based search have become\nfoundational tools for memory in AI systems, yet they struggle with\nabstraction, scalability, and semantic precision - especially in decentralized\nenvironments. We present SHIMI (Semantic Hierarchical Memory Index), a unified\narchitecture that models knowledge as a dynamically structured hierarchy of\nconcepts, enabling agents to retrieve information based on meaning rather than\nsurface similarity. SHIMI organizes memory into layered semantic nodes and\nsupports top-down traversal from abstract intent to specific entities, offering\nmore precise and explainable retrieval. Critically, SHIMI is natively designed\nfor decentralized ecosystems, where agents maintain local memory trees and\nsynchronize them asynchronously across networks. We introduce a lightweight\nsync protocol that leverages Merkle-DAG summaries, Bloom filters, and\nCRDT-style conflict resolution to enable partial synchronization with minimal\noverhead. Through benchmark experiments and use cases involving decentralized\nagent collaboration, we demonstrate SHIMI's advantages in retrieval accuracy,\nsemantic fidelity, and scalability - positioning it as a core infrastructure\nlayer for decentralized cognitive systems.",
        "published": "2025-04-08T15:31:00+00:00"
    },
    {
        "title": "SpikeStream: Accelerating Spiking Neural Network Inference on RISC-V Clusters with Sparse Computation Extensions",
        "authors": [
            "Simone Manoni",
            "Paul Scheffler",
            "Luca Zanatta",
            "Andrea Acquaviva",
            "Luca Benini",
            "Andrea Bartolini"
        ],
        "summary": "Spiking Neural Network (SNN) inference has a clear potential for high energy\nefficiency as computation is triggered by events. However, the inherent\nsparsity of events poses challenges for conventional computing systems, driving\nthe development of specialized neuromorphic processors, which come with high\nsilicon area costs and lack the flexibility needed for running other\ncomputational kernels, limiting widespread adoption. In this paper, we explore\nthe low-level software design, parallelization, and acceleration of SNNs on\ngeneral-purpose multicore clusters with a low-overhead RISC-V ISA extension for\nstreaming sparse computations. We propose SpikeStream, an optimization\ntechnique that maps weights accesses to affine and indirect register-mapped\nmemory streams to enhance performance, utilization, and efficiency. Our results\non the end-to-end Spiking-VGG11 model demonstrate a significant 4.39x speedup\nand an increase in utilization from 9.28% to 52.3% compared to a non-streaming\nparallel baseline. Additionally, we achieve an energy efficiency gain of 3.46x\nover LSMCore and a performance gain of 2.38x over Loihi.",
        "published": "2025-04-08T15:28:44+00:00"
    },
    {
        "title": "FaceCloak: Learning to Protect Face Templates",
        "authors": [
            "Sudipta Banerjee",
            "Anubhav Jain",
            "Chinmay Hegde",
            "Nasir Memon"
        ],
        "summary": "Generative models can reconstruct face images from encoded representations\n(templates) bearing remarkable likeness to the original face raising security\nand privacy concerns. We present FaceCloak, a neural network framework that\nprotects face templates by generating smart, renewable binary cloaks. Our\nmethod proactively thwarts inversion attacks by cloaking face templates with\nunique disruptors synthesized from a single face template on the fly while\nprovably retaining biometric utility and unlinkability. Our cloaked templates\ncan suppress sensitive attributes while generalizing to novel feature\nextraction schemes and outperforms leading baselines in terms of biometric\nmatching and resiliency to reconstruction attacks. FaceCloak-based matching is\nextremely fast (inference time cost=0.28ms) and light-weight (0.57MB).",
        "published": "2025-04-08T15:23:21+00:00"
    },
    {
        "title": "Accelerating Vehicle Routing via AI-Initialized Genetic Algorithms",
        "authors": [
            "Ido Greenberg",
            "Piotr Sielski",
            "Hugo Linsenmaier",
            "Rajesh Gandham",
            "Shie Mannor",
            "Alex Fender",
            "Gal Chechik",
            "Eli Meirom"
        ],
        "summary": "Vehicle Routing Problems (VRP) are an extension of the Traveling Salesperson\nProblem and are a fundamental NP-hard challenge in combinatorial optimization.\nSolving VRP in real-time at large scale has become critical in numerous\napplications, from growing markets like last-mile delivery to emerging\nuse-cases like interactive logistics planning. Such applications involve\nsolving similar problem instances repeatedly, yet current state-of-the-art\nsolvers treat each instance on its own without leveraging previous examples. We\nintroduce a novel optimization framework that uses a reinforcement learning\nagent - trained on prior instances - to quickly generate initial solutions,\nwhich are then further optimized by genetic algorithms. Our framework,\nEvolutionary Algorithm with Reinforcement Learning Initialization (EARLI),\nconsistently outperforms current state-of-the-art solvers across various time\nscales. For example, EARLI handles vehicle routing with 500 locations within\n1s, 10x faster than current solvers for the same solution quality, enabling\napplications like real-time and interactive routing. EARLI can generalize to\nnew data, as demonstrated on real e-commerce delivery data of a previously\nunseen city. Our hybrid framework presents a new way to combine reinforcement\nlearning and genetic algorithms, paving the road for closer interdisciplinary\ncollaboration between AI and optimization communities towards real-time\noptimization in diverse domains.",
        "published": "2025-04-08T15:21:01+00:00"
    },
    {
        "title": "Robo-taxi Fleet Coordination at Scale via Reinforcement Learning",
        "authors": [
            "Luigi Tresca",
            "Carolin Schmidt",
            "James Harrison",
            "Filipe Rodrigues",
            "Gioele Zardini",
            "Daniele Gammelli",
            "Marco Pavone"
        ],
        "summary": "Fleets of robo-taxis offering on-demand transportation services, commonly\nknown as Autonomous Mobility-on-Demand (AMoD) systems, hold significant promise\nfor societal benefits, such as reducing pollution, energy consumption, and\nurban congestion. However, orchestrating these systems at scale remains a\ncritical challenge, with existing coordination algorithms often failing to\nexploit the systems' full potential. This work introduces a novel\ndecision-making framework that unites mathematical modeling with data-driven\ntechniques. In particular, we present the AMoD coordination problem through the\nlens of reinforcement learning and propose a graph network-based framework that\nexploits the main strengths of graph representation learning, reinforcement\nlearning, and classical operations research tools. Extensive evaluations across\ndiverse simulation fidelities and scenarios demonstrate the flexibility of our\napproach, achieving superior system performance, computational efficiency, and\ngeneralizability compared to prior methods. Finally, motivated by the need to\ndemocratize research efforts in this area, we release publicly available\nbenchmarks, datasets, and simulators for network-level coordination alongside\nan open-source codebase designed to provide accessible simulation platforms and\nestablish a standardized validation process for comparing methodologies. Code\navailable at: https://github.com/StanfordASL/RL4AMOD",
        "published": "2025-04-08T15:19:41+00:00"
    },
    {
        "title": "A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions",
        "authors": [
            "Ronghui Zhang",
            "Yuhang Ma",
            "Tengfei Li",
            "Ziyu Lin",
            "Yueying Wu",
            "Junzhou Chen",
            "Lin Zhang",
            "Jia Hu",
            "Tony Z. Qiu",
            "Konghui Guo"
        ],
        "summary": "Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments.",
        "published": "2025-04-08T15:13:01+00:00"
    },
    {
        "title": "Hyperbolic Category Discovery",
        "authors": [
            "Yuanpei Liu",
            "Zhenqi He",
            "Kai Han"
        ],
        "summary": "Generalized Category Discovery (GCD) is an intriguing open-world problem that\nhas garnered increasing attention. Given a dataset that includes both labelled\nand unlabelled images, GCD aims to categorize all images in the unlabelled\nsubset, regardless of whether they belong to known or unknown classes. In GCD,\nthe common practice typically involves applying a spherical projection operator\nat the end of the self-supervised pretrained backbone, operating within\nEuclidean or spherical space. However, both of these spaces have been shown to\nbe suboptimal for encoding samples that possesses hierarchical structures. In\ncontrast, hyperbolic space exhibits exponential volume growth relative to\nradius, making it inherently strong at capturing the hierarchical structure of\nsamples from both seen and unseen categories. Therefore, we propose to tackle\nthe category discovery challenge in the hyperbolic space. We introduce HypCD, a\nsimple \\underline{Hyp}erbolic framework for learning hierarchy-aware\nrepresentations and classifiers for generalized \\underline{C}ategory\n\\underline{D}iscovery. HypCD first transforms the Euclidean embedding space of\nthe backbone network into hyperbolic space, facilitating subsequent\nrepresentation and classification learning by considering both hyperbolic\ndistance and the angle between samples. This approach is particularly helpful\nfor knowledge transfer from known to unknown categories in GCD. We thoroughly\nevaluate HypCD on public GCD benchmarks, by applying it to various baseline and\nstate-of-the-art methods, consistently achieving significant improvements.",
        "published": "2025-04-08T15:12:33+00:00"
    },
    {
        "title": "Co-evolution of cooperation and resource allocation in the advantageous environment-based spatial multi-game using adaptive control",
        "authors": [
            "Chengbin Sun",
            "Alfonso de Miguel-Arribas",
            "Chaoqian Wang",
            "Haoxiang Xia",
            "Yamir Moreno"
        ],
        "summary": "In real-life complex systems, individuals often encounter multiple social\ndilemmas that cannot be effectively captured using a single-game model.\nFurthermore, the environment and limited resources both play a crucial role in\nshaping individuals' decision-making behaviors. In this study, we employ an\nadaptive control mechanism by which agents may benefit from their environment,\nthus redefining their individual fitness. Under this setting, a detailed\nexamination of the co-evolution of individual strategies and resource\nallocation is carried. Through extensive simulations, we find that the\nadvantageous environment mechanism not only significantly increases the\nproportion of cooperators in the system but also influences the resource\ndistribution among individuals. Additionally, limited resources reinforce\ncooperative behaviors within the system while shaping the evolutionary dynamics\nand strategic interactions across different dilemmas. Once the system reaches\nequilibrium, resource distribution becomes highly imbalanced. To promote fairer\nresource allocation, we introduce a minimum resource guarantee mechanism. Our\nresults show that this mechanism not only reduces disparities in resource\ndistribution across the entire system and among individuals in different\ndilemmas but also significantly enhances cooperative behavior in higher\nresource intervals. Finally, to assess the robustness of our model, we further\nexamine the influence of the advantageous environment on system-wide\ncooperation in small-world and random graph network models.",
        "published": "2025-04-08T15:00:31+00:00"
    },
    {
        "title": "On the Dynamics of Mating Preferences in Genetic Programming",
        "authors": [
            "Jos\u00e9 Maria Sim\u00f5es",
            "Nuno Louren\u00e7o",
            "Penousal Machado"
        ],
        "summary": "Several mating restriction techniques have been implemented in Evolutionary\nAlgorithms to promote diversity. From similarity-based selection to niche\npreservation, the general goal is to avoid premature convergence by not having\nfitness pressure as the single evolutionary force. In a way, such methods can\nresemble the mechanisms involved in Sexual Selection, although generally\nassuming a simplified approach. Recently, a selection method called mating\nPreferences as Ideal Mating Partners (PIMP) has been applied to GP, providing\npromising results both in performance and diversity maintenance. The method\nmimics Mate Choice through the unbounded evolution of personal preferences\nrather than having a single set of rules to shape parent selection. As such,\nPIMP allows ideal mate representations to evolve freely, thus potentially\ntaking advantage of Sexual Selection as a dynamic secondary force to fitness\npressure. However, it is still unclear how mating preferences affect the\noverall population and how dependent they are on set-up choices. In this work,\nwe tracked the evolution of individual preferences through different mutation\ntypes, searching for patterns and evidence of self-reinforcement. Results\nsuggest that mating preferences do not stand on their own, relying on subtree\nmutation to avoid convergence to single-node trees. Nevertheless, they\nconsistently promote smaller and more balanced solutions depth-wise than a\nstandard tournament selection, reducing the impact of bloat. Furthermore, when\ncoupled with subtree mutation it also results in more solution diversity with\nstatistically significant results.",
        "published": "2025-04-08T14:56:46+00:00"
    },
    {
        "title": "Characterizing direct and indirect causal effects when outcomes are dependent due to treatment spillover and outcome spillover",
        "authors": [
            "Subhankar Bhadra",
            "Michael Schweinberger"
        ],
        "summary": "We provide novel insight into causal inference when both treatment spillover\nand outcome spillover occur in connected populations, by taking advantage of\nrecent advances in statistical network analysis. Scenarios with treatment\nspillover and outcome spillover are challenging, because both forms of\nspillover affect outcomes and therefore treatment spillover and outcome\nspillover are intertwined, and outcomes are dependent conditional on treatments\nby virtue of outcome spillover. As a result, the direct and indirect causal\neffects arising from spillover have remained black boxes: While the direct and\nindirect causal effects can be identified, it is unknown how these causal\neffects explicitly depend on the effects of treatment, treatment spillover, and\noutcome spillover. We make three contributions, facilitated by low-rank random\ninterference graphs. First, we provide novel insight into direct and indirect\ncausal effects by disentangling the contributions of treatment, treatment\nspillover, and outcome spillover. Second, we provide scalable estimators of\ndirect and indirect causal effects. Third, we establish rates of convergence\nfor estimators of direct and indirect causal effects. These are the first\nconvergence rates in scenarios in which treatment spillover and outcome\nspillover are intertwined and outcomes are dependent conditional on treatments,\nand the interference graph is sparse or dense.",
        "published": "2025-04-08T14:55:34+00:00"
    },
    {
        "title": "Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation",
        "authors": [
            "Abinav Kalyanasundaram",
            "Karthikeyan Chandra Sekaran",
            "Philipp Stauber",
            "Michael Lange",
            "Wolfgang Utschick",
            "Michael Botsch"
        ],
        "summary": "Precise vehicle state estimation is crucial for safe and reliable autonomous\ndriving. The number of measurable states and their precision offered by the\nonboard vehicle sensor system are often constrained by cost. For instance,\nmeasuring critical quantities such as the Vehicle Sideslip Angle (VSA) poses\nsignificant commercial challenges using current optical sensors. This paper\naddresses these limitations by focusing on the development of high-performance\nvirtual sensors to enhance vehicle state estimation for active safety. The\nproposed Uncertainty-Aware Hybrid Learning (UAHL) architecture integrates a\nmachine learning model with vehicle motion models to estimate VSA directly from\nonboard sensor data. A key aspect of the UAHL architecture is its focus on\nuncertainty quantification for individual model estimates and hybrid fusion.\nThese mechanisms enable the dynamic weighting of uncertainty-aware predictions\nfrom machine learning and vehicle motion models to produce accurate and\nreliable hybrid VSA estimates. This work also presents a novel dataset named\nReal-world Vehicle State Estimation Dataset (ReV-StED), comprising synchronized\nmeasurements from advanced vehicle dynamic sensors. The experimental results\ndemonstrate the superior performance of the proposed method for VSA estimation,\nhighlighting UAHL as a promising architecture for advancing virtual sensors and\nenhancing active safety in autonomous vehicles.",
        "published": "2025-04-08T14:49:58+00:00"
    },
    {
        "title": "Sherlock: A Dataset for Process-aware Intrusion Detection Research on Power Grid Networks",
        "authors": [
            "Eric Wagner",
            "Lennart Bader",
            "Konrad Wolsing",
            "Martin Serror"
        ],
        "summary": "Physically distributed components and legacy protocols make the protection of\npower grids against increasing cyberattack threats challenging. Infamously, the\n2015 and 2016 blackouts in Ukraine were caused by cyberattacks, and the German\nFederal Office for Information Security (BSI) recorded over 200 cyber incidents\nagainst the German energy sector between 2023 and 2024. Intrusion detection\npromises to quickly detect such attacks and mitigate the worst consequences.\nHowever, public datasets of realistic scenarios are vital to evaluate these\nsystems. This paper introduces Sherlock, a dataset generated with the\nco-simulator Wattson. In total, Sherlock covers three scenarios with various\nattacks manipulating the process state by injecting malicious commands or\nmanipulating measurement values. We additionally test five recently-published\nintrusion detection systems on Sherlock, highlighting specific challenges for\nintrusion detection in power grids. Dataset and documentation are available at\nhttps://sherlock.wattson.it/.",
        "published": "2025-04-08T14:46:35+00:00"
    },
    {
        "title": "Towards Varroa destructor mite detection using a narrow spectra illumination",
        "authors": [
            "Samuel Bielik",
            "Simon Bilik"
        ],
        "summary": "This paper focuses on the development and modification of a beehive\nmonitoring device and Varroa destructor detection on the bees with the help of\nhyperspectral imagery while utilizing a U-net, semantic segmentation\narchitecture, and conventional computer vision methods. The main objectives\nwere to collect a dataset of bees and mites, and propose the computer vision\nmodel which can achieve the detection between bees and mites.",
        "published": "2025-04-08T14:41:42+00:00"
    },
    {
        "title": "Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for Scaled-up LLM Training",
        "authors": [
            "Daiyaan Arfeen",
            "Dheevatsa Mudigere",
            "Ankit More",
            "Bhargava Gopireddy",
            "Ahmet Inci",
            "Gregory R. Ganger"
        ],
        "summary": "LLM training is scaled up to 10Ks of GPUs by a mix of data-(DP) and\nmodel-parallel (MP) execution. Critical to achieving efficiency is\ntensor-parallel (TP; a form of MP) execution within tightly-coupled subsets of\nGPUs, referred to as a scale-up domain, and the larger the scale-up domain the\nbetter the performance. New datacenter architectures are emerging with more\nGPUs able to be tightly-coupled in a scale-up domain, such as moving from 8\nGPUs to 72 GPUs connected via NVLink. Unfortunately, larger scale-up domains\nincrease the blast-radius of failures, with a failure of single GPU potentially\nimpacting TP execution on the full scale-up domain, which can degrade overall\nLLM training throughput dramatically. With as few as 0.1% of GPUs being in a\nfailed state, a high TP-degree job can experience nearly 10% reduction in LLM\ntraining throughput. We propose nonuniform-tensor-parallelism (NTP) to mitigate\nthis amplified impact of GPU failures. In NTP, a DP replica that experiences\nGPU failures operates at a reduced TP degree, contributing throughput equal to\nthe percentage of still-functional GPUs. We also propose a rack-design with\nimproved electrical and thermal capabilities in order to sustain power-boosting\nof scale-up domains that have experienced failures; combined with NTP, this can\nallow the DP replica with the reduced TP degree (i.e., with failed GPUs) to\nkeep up with the others, thereby achieving near-zero throughput loss for\nlarge-scale LLM training.",
        "published": "2025-04-08T14:35:40+00:00"
    },
    {
        "title": "Low-Complexity SDP-ADMM for Physical-Layer Multicasting in Massive MIMO Systems",
        "authors": [
            "Mahmoud Zaher",
            "Emil Bj\u00f6rnson"
        ],
        "summary": "There is a demand for the same data content from several user equipments\n(UEs) in many wireless communication applications. Physical-layer multicasting\ncombines the beamforming capability of massive MIMO (multiple-input\nmultiple-output) and the broadcast nature of the wireless channel to\nefficiently deliver the same data to a group of UEs using a single\ntransmission. This paper tackles the max-min fair (MMF) multicast beamforming\noptimization, which is an NP-hard problem. We develop an efficient semidefinite\nprogram-alternating direction method of multipliers (SDP-ADMM) algorithm to\nfind the near-global optimal rank-1 solution to the MMF multicast problem in a\nmassive MIMO system. Numerical results show that the proposed SDP-ADMM\nalgorithm exhibits similar spectral efficiency performance to state-of-the-art\nalgorithms running on standard SDP solvers at a vastly reduced computational\ncomplexity. We highlight that the proposed ADMM elimination procedure can be\nemployed as an effective low-complexity rank reduction method for other\nproblems utilizing semidefinite relaxation.",
        "published": "2025-04-08T14:30:44+00:00"
    },
    {
        "title": "Accurate Ab-initio Neural-network Solutions to Large-Scale Electronic Structure Problems",
        "authors": [
            "Michael Scherbela",
            "Nicholas Gao",
            "Philipp Grohs",
            "Stephan G\u00fcnnemann"
        ],
        "summary": "We present finite-range embeddings (FiRE), a novel wave function ansatz for\naccurate large-scale ab-initio electronic structure calculations. Compared to\ncontemporary neural-network wave functions, FiRE reduces the asymptotic\ncomplexity of neural-network variational Monte Carlo (NN-VMC) by $\\sim\nn_\\text{el}$, the number of electrons. By restricting electron-electron\ninteractions within the neural network, FiRE accelerates all key operations --\nsampling, pseudopotentials, and Laplacian computations -- resulting in a\nreal-world $10\\times$ acceleration in now-feasible 180-electron calculations.\nWe validate our method's accuracy on various challenging systems, including\nbiochemical compounds, conjugated hydrocarbons, and organometallic compounds.\nOn these systems, FiRE's energies are consistently within chemical accuracy of\nthe most reliable data, including experiments, even in cases where\nhigh-accuracy methods such as CCSD(T), AFQMC, or contemporary NN-VMC fall\nshort. With these improvements in both runtime and accuracy, FiRE represents a\nnew `gold-standard' method for fast and accurate large-scale ab-initio\ncalculations, potentially enabling new computational studies in fields like\nquantum chemistry, solid-state physics, and material design.",
        "published": "2025-04-08T14:28:54+00:00"
    },
    {
        "title": "PINP: Physics-Informed Neural Predictor with latent estimation of fluid flows",
        "authors": [
            "Huaguan Chen",
            "Yang Liu",
            "Hao Sun"
        ],
        "summary": "Accurately predicting fluid dynamics and evolution has been a long-standing\nchallenge in physical sciences. Conventional deep learning methods often rely\non the nonlinear modeling capabilities of neural networks to establish mappings\nbetween past and future states, overlooking the fluid dynamics, or only\nmodeling the velocity field, neglecting the coupling of multiple physical\nquantities. In this paper, we propose a new physics-informed learning approach\nthat incorporates coupled physical quantities into the prediction process to\nassist with forecasting. Central to our method lies in the discretization of\nphysical equations, which are directly integrated into the model architecture\nand loss function. This integration enables the model to provide robust,\nlong-term future predictions. By incorporating physical equations, our model\ndemonstrates temporal extrapolation and spatial generalization capabilities.\nExperimental results show that our approach achieves the state-of-the-art\nperformance in spatiotemporal prediction across both numerical simulations and\nreal-world extreme-precipitation nowcasting benchmarks.",
        "published": "2025-04-08T14:11:01+00:00"
    },
    {
        "title": "Physics-Constrained Neural Network for Metasurface Optical Response Prediction",
        "authors": [
            "Hanieh Masoudian Saadabad",
            "Lingraj Kumar",
            "Reza Masoudian Saadabad",
            "Maja Colautti"
        ],
        "summary": "A physics-constrained neural network is presented for predicting the optical\nresponse of metasurfaces. Our approach incorporates physical laws directly into\nthe neural network architecture and loss function, addressing critical\nchallenges in the modeling of metasurfaces. Unlike methods that require\nspecialized weighting strategies or separate architectural branches to handle\ndifferent data regimes and phase wrapping discontinuities, this unified\napproach effectively addresses phase discontinuities, energy conservation\nconstraints, and complex gap-dependent behavior. We implement sine-cosine phase\nrepresentation with Euclidean normalization as a non-trainable layer within the\nnetwork, enabling the model to account for the periodic nature of phase while\nenforcing the mathematical constraint $\\sin^2 \\phi + \\cos^2 \\phi = 1$. A\nEuclidean distance-based loss function in the sine-cosine space ensures a\nphysically meaningful error metric while preventing discontinuity issues. The\nmodel achieves good, consistent performance with small, imbalanced datasets of\n580 and 1075 data points, compared to several thousand typically required by\nalternative approaches. This physics-informed approach preserves physical\ninterpretability while reducing reliance on large datasets and could be\nextended to other photonic structures by incorporating additional physical\nconstraints tailored to specific applications.",
        "published": "2025-04-08T14:10:28+00:00"
    },
    {
        "title": "GPU-accelerated Evolutionary Many-objective Optimization Using Tensorized NSGA-III",
        "authors": [
            "Hao Li",
            "Zhenyu Liang",
            "Ran Cheng"
        ],
        "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
        "published": "2025-04-08T14:09:23+00:00"
    },
    {
        "title": "New designs of linear optical interferometers with minimal depth and component count",
        "authors": [
            "Timoth\u00e9e Goubault de Brugi\u00e8re",
            "Rawad Mezher",
            "Sebastian Currie",
            "Shane Mansfield"
        ],
        "summary": "We adapt an algorithm for CNOT circuits synthesis based on the Bruhat\ndecomposition to the design of linear optical circuits with Mach-Zehnder\ninterferometers (MZI). The synthesis algorithm reduces to designing sorting\nnetworks with nearest neighbor swapping operations as elementary gates. We\nrecover previous designs from the literature but with additional theoretical\nproperties regarding the compiler that implements unitaries on the\ninterferometer. Notably the compiler can always decide whether a unitary can be\nimplemented on a given interferometer and, if so, returns the shallowest\npossible implementation. We also show natural extensions of our framework for\nboson sampling experiments and for the coupling of multiple integrated\ninterferometers to design larger linear optical systems. In both cases, the\ndesigns are optimal in terms of number of optical components. Finally, we\npropose a greedy design which exploits the arbritrary-but-fixed coupling of\nseparate integrated interferometers to perform shallow boson sampling. We\ndiscuss the optimal interferometer dimensions to maximize the transmission.\nBeyond boson sampling, our developed framework allows a resource-favourable\nimplemention of any non-adaptive linear optical quantum algorithm, by providing\nthe shallowest possible interferometer for implementing this algorithm.",
        "published": "2025-04-08T14:03:04+00:00"
    },
    {
        "title": "$L_\\textrm{dT}$: An ionospheric activity index based on distributions in GNSS-derived TEC rates of change",
        "authors": [
            "Paul Kinsler",
            "Biagio Forte"
        ],
        "summary": "Many aspects of our societies now depend upon satellite telecommunications,\nsuch as those requiring Global Navigation Satellite Systems (GNSS). GNSS is\nbased on radio waves that propagate through the ionosphere and experience\ncomplicated propagation effects caused by inhomogeneities in its electron\ndensity. The Earth's ionosphere forms part of the solar-terrestrial\nenvironment, and its state is determined by the spatial distribution and\ntemporal evolution of its electron density. It varies in response to the \"space\nweather\" combination of solar activity and geomagnetic conditions. Notably, the\nradio waves used in satellite telecommunications suffer due to the dispersive\nnature of the ionospheric plasma.\n  Scales and indices that summarise the state of the solar-terrestrial\nenvironment due to solar activity and geomagnetic conditions already exist.\nHowever, the response of the ionosphere to active geomagnetic conditions, its\ngeoeffectiveness, and its likely impact on systems and services are not\nencapsulated by these. This is due to the ionosphere's intrinsic day-to-day\nvariability, persistent seasonal patterns, and because radio wave measurements\nof the ionosphere depend upon many factors.\n  Here we develop a novel index that describes the state of the ionosphere\nduring specific space weather conditions. It is based on propagation\ndisturbances in GNSS signals, and is able to characterise the spatio-temporal\nevolution of ionospheric disturbances in near real time. This new scale\nencapsulates day-to-day variability, seasonal patterns, and the geo-effective\nresponse of the ionosphere to disturbed space weather conditions; and can be\napplied to data from any GNSS network. It is intended that this new scale will\nbe utilised by agencies providing space weather services, as well as by service\noperators to appreciate the current conditions in the ionosphere, thus\ninforming their operations.",
        "published": "2025-04-08T14:00:14+00:00"
    },
    {
        "title": "Explainable AI for building energy retrofitting under data scarcity",
        "authors": [
            "Panagiota Rempi",
            "Sotiris Pelekis",
            "Alexandros Menelaos Tzortzis",
            "Evangelos Karakolis",
            "Christos Ntanos",
            "Dimitris Askounis"
        ],
        "summary": "Enhancing energy efficiency in residential buildings is a crucial step toward\nmitigating climate change and reducing greenhouse gas emissions. Retrofitting\nexisting buildings, which account for a significant portion of energy\nconsumption, is critical particularly in regions with outdated and inefficient\nbuilding stocks. This study presents an Artificial Intelligence (AI) and\nMachine Learning (ML)-based framework to recommend energy efficiency measures\nfor residential buildings, leveraging accessible building characteristics to\nachieve energy class targets. Using Latvia as a case study, the methodology\naddresses challenges associated with limited datasets, class imbalance and data\nscarcity. The proposed approach integrates Conditional Tabular Generative\nAdversarial Networks (CTGAN) to generate synthetic data, enriching and\nbalancing the dataset. A Multi-Layer Perceptron (MLP) model serves as the\npredictive model performing multi-label classification to predict appropriate\nretrofit strategies. Explainable Artificial Intelligence (XAI), specifically\nSHapley Additive exPlanations (SHAP), ensures transparency and trust by\nidentifying key features that influence recommendations and guiding feature\nengineering choices for improved reliability and performance. The evaluation of\nthe approach shows that it notably overcomes data limitations, achieving\nimprovements up to 54% in precision, recall and F1 score. Although this study\nfocuses on Latvia, the methodology is adaptable to other regions, underscoring\nthe potential of AI in reducing the complexity and cost of building energy\nretrofitting overcoming data limitations. By facilitating decision-making\nprocesses and promoting stakeholders engagement, this work supports the global\ntransition toward sustainable energy use in the residential building sector.",
        "published": "2025-04-08T14:00:08+00:00"
    },
    {
        "title": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning Strategies",
        "authors": [
            "Julia Werner",
            "Christoph Gerum",
            "Jorg Nick",
            "Maxime Le Floch",
            "Franz Brinkmann",
            "Jochen Hampe",
            "Oliver Bringmann"
        ],
        "summary": "Capsule endoscopy is a method to capture images of the gastrointestinal tract\nand screen for diseases which might remain hidden if investigated with standard\nendoscopes. Due to the limited size of a video capsule, embedding AI models\ndirectly into the capsule demands careful consideration of the model size and\nthus complicates anomaly detection in this field. Furthermore, the scarcity of\navailable data in this domain poses an ongoing challenge to achieving effective\nanomaly detection. Thus, this work introduces an ensemble strategy to address\nthis challenge in anomaly detection tasks in video capsule endoscopies,\nrequiring only a small number of individual neural networks during both the\ntraining and inference phases. Ensemble learning combines the predictions of\nmultiple independently trained neural networks. This has shown to be highly\neffective in enhancing both the accuracy and robustness of machine learning\nmodels. However, this comes at the cost of higher memory usage and increased\ncomputational effort, which quickly becomes prohibitive in many real-world\napplications. Instead of applying the same training algorithm to each\nindividual network, we propose using various loss functions, drawn from the\nanomaly detection field, to train each network. The methods are validated on\nthe two largest publicly available datasets for video capsule endoscopy images,\nthe Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on\nthe Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our\napproach outperforms current baselines with significantly fewer parameters\nacross all models, which is a crucial step towards incorporating artificial\nintelligence into capsule endoscopies.",
        "published": "2025-04-08T13:39:39+00:00"
    },
    {
        "title": "3D evolution of protein networks and lipid globules in heat-treated egg yolk",
        "authors": [
            "Felix Wittwer",
            "Nimmi Das Anthuparambil",
            "Frederik Unger",
            "Randeer Pratap Gautam",
            "Silja Flenner",
            "Imke Greving",
            "Christian Gutt",
            "Peter Modregger"
        ],
        "summary": "Upon heating, egg yolk transforms from a liquid to a gel due to protein\ndenaturation. This process can serve as a useful model to better understand\nprotein denaturation in general. Using x-ray holographic tomography, we\ninvestigated the structural changes in egg yolk during boiling without the need\nfor complex sample fixation or drying. Our results reveal a developing\nseparation between proteins and lipids, with fatty components rapidly\naggregating into large globules that subsequently evolve into bubbles.",
        "published": "2025-04-08T13:35:25+00:00"
    },
    {
        "title": "OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model",
        "authors": [
            "Xiaochen Wei",
            "Weiwei Guo",
            "Wenxian Yu",
            "Feiming Wei",
            "Dongying Li"
        ],
        "summary": "Multimodal remote sensing image registration aligns images from different\nsensors for data fusion and analysis. However, current methods often fail to\nextract modality-invariant features when aligning image pairs with large\nnonlinear radiometric differences. To address this issues, we propose\nOSDM-MReg, a novel multimodal image registration framework based image-to-image\ntranslation to eliminate the gap of multimodal images. Firstly, we propose a\nnovel one-step unaligned target-guided conditional denoising diffusion\nprobabilistic models(UTGOS-CDDPM)to translate multimodal images into a unified\ndomain. In the inference stage, traditional conditional DDPM generate\ntranslated source image by a large number of iterations, which severely slows\ndown the image registration task. To address this issues, we use the unaligned\ntraget image as a condition to promote the generation of low-frequency features\nof the translated source image. Furthermore, during the training stage, we add\nthe inverse process of directly predicting the translated image to ensure that\nthe translated source image can be generated in one step during the testing\nstage. Additionally, to supervised the detail features of translated source\nimage, we propose a new perceptual loss that focuses on the high-frequency\nfeature differences between the translated and ground-truth images. Finally, a\nmultimodal multiscale image registration network (MM-Reg) fuse the multimodal\nfeature of the unimodal images and multimodal images by proposed multimodal\nfeature fusion strategy. Experiments demonstrate superior accuracy and\nefficiency across various multimodal registration tasks, particularly for\nSAR-optical image pairs.",
        "published": "2025-04-08T13:32:56+00:00"
    },
    {
        "title": "Optimizing Data-driven Weights In Multidimensional Indexes",
        "authors": [
            "Lidia Ceriani",
            "Chiara Gigliarano",
            "Paolo Verme"
        ],
        "summary": "Multidimensional indexes are ubiquitous, and popular, but present\nnon-negligible normative choices when it comes to attributing weights to their\ndimensions. This paper provides a more rigorous approach to the choice of\nweights by defining a set of desirable properties that weighting models should\nmeet. It shows that Bayesian Networks is the only model across statistical,\neconometric, and machine learning computational models that meets these\nproperties. An example with EU-SILC data illustrates this new approach\nhighlighting its potential for policies.",
        "published": "2025-04-08T13:18:00+00:00"
    },
    {
        "title": "Latent Multimodal Reconstruction for Misinformation Detection",
        "authors": [
            "Stefanos-Iordanis Papadopoulos",
            "Christos Koutlis",
            "Symeon Papadopoulos",
            "Panagiotis C. Petrantonakis"
        ],
        "summary": "Multimodal misinformation, such as miscaptioned images, where captions\nmisrepresent an image's origin, context, or meaning, poses a growing challenge\nin the digital age. To support fact-checkers, researchers have been focusing on\ncreating datasets and developing methods for multimodal misinformation\ndetection (MMD). Due to the scarcity of large-scale annotated MMD datasets,\nrecent studies leverage synthetic training data via out-of-context\nimage-caption pairs or named entity manipulations; altering names, dates, and\nlocations. However, these approaches often produce simplistic misinformation\nthat fails to reflect real-world complexity, limiting the robustness of\ndetection models trained on them. Meanwhile, despite recent advancements, Large\nVision-Language Models (LVLMs) remain underutilized for generating diverse,\nrealistic synthetic training data for MMD. To address this gap, we introduce\n\"MisCaption This!\", a training dataset comprising LVLM-generated miscaptioned\nimages. Additionally, we introduce \"Latent Multimodal Reconstruction\" (LAMAR),\na network trained to reconstruct the embeddings of truthful captions, providing\na strong auxiliary signal to the detection process. To optimize LAMAR, we\nexplore different training strategies (end-to-end training and large-scale\npre-training) and integration approaches (direct, mask, gate, and attention).\nExtensive experiments show that models trained on \"MisCaption This!\" generalize\nbetter on real-world misinformation, while LAMAR sets new state-of-the-art on\nboth NewsCLIPpings and VERITE benchmarks; highlighting the potential of\nLVLM-generated data and reconstruction-based approaches for advancing MMD. We\nrelease our code at:\nhttps://github.com/stevejpapad/miscaptioned-image-reconstruction",
        "published": "2025-04-08T13:16:48+00:00"
    },
    {
        "title": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?",
        "authors": [
            "Roman Kochnev",
            "Arash Torabi Goodarzi",
            "Zofia Antonina Bentyn",
            "Dmitry Ignatov",
            "Radu Timofte"
        ],
        "summary": "Optimal hyperparameter selection is critical for maximizing neural network\nperformance, especially as models grow in complexity. This work investigates\nthe viability of using large language models (LLMs) for hyperparameter\noptimization by employing a fine-tuned version of Code Llama. Through\nparameter-efficient fine-tuning using LoRA, we adapt the LLM to generate\naccurate and efficient hyperparameter recommendations tailored to diverse\nneural network architectures. Unlike traditional methods such as Optuna, which\nrely on exhaustive trials, the proposed approach achieves competitive or\nsuperior results in terms of Root Mean Square Error (RMSE) while significantly\nreducing computational overhead. Our approach highlights that LLM-based\noptimization not only matches state-of-the-art methods like Tree-structured\nParzen Estimators but also accelerates the tuning process. This positions LLMs\nas a promising alternative to conventional optimization techniques,\nparticularly for rapid experimentation. Furthermore, the ability to generate\nhyperparameters in a single inference step makes this method particularly\nwell-suited for resource-constrained environments such as edge devices and\nmobile applications, where computational efficiency is paramount. The results\nconfirm that LLMs, beyond their efficiency, offer substantial time savings and\ncomparable stability, underscoring their value in advancing machine learning\nworkflows. All generated hyperparameters are included in the LEMUR Neural\nNetwork (NN) Dataset, which is publicly available and serves as an open-source\nbenchmark for hyperparameter optimization research.",
        "published": "2025-04-08T13:15:47+00:00"
    },
    {
        "title": "econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians",
        "authors": [
            "Can Zhang",
            "Gim Hee Lee"
        ],
        "summary": "The primary focus of most recent works on open-vocabulary neural fields is\nextracting precise semantic features from the VLMs and then consolidating them\nefficiently into a multi-view consistent 3D neural fields representation.\nHowever, most existing works over-trusted SAM to regularize image-level CLIP\nwithout any further refinement. Moreover, several existing works improved\nefficiency by dimensionality reduction of semantic features from 2D VLMs before\nfusing with 3DGS semantic fields, which inevitably leads to multi-view\ninconsistency. In this work, we propose econSG for open-vocabulary semantic\nsegmentation with 3DGS. Our econSG consists of: 1) A Confidence-region Guided\nRegularization (CRR) that mutually refines SAM and CLIP to get the best of both\nworlds for precise semantic features with complete and precise boundaries. 2) A\nlow dimensional contextual space to enforce 3D multi-view consistency while\nimproving computational efficiency by fusing backprojected multi-view 2D\nfeatures and follow by dimensional reduction directly on the fused 3D features\ninstead of operating on each 2D view separately. Our econSG shows\nstate-of-the-art performance on four benchmark datasets compared to the\nexisting methods. Furthermore, we are also the most efficient training among\nall the methods.",
        "published": "2025-04-08T13:12:31+00:00"
    },
    {
        "title": "Under-Sampled High-Dimensional Data Recovery via Symbiotic Multi-Prior Tensor Reconstruction",
        "authors": [
            "Jie Yang",
            "Chang Su",
            "Yuhan Zhang",
            "Jianjun Zhu",
            "Jianli Wang"
        ],
        "summary": "The advancement of sensing technology has driven the widespread application\nof high-dimensional data. However, issues such as missing entries during\nacquisition and transmission negatively impact the accuracy of subsequent\ntasks. Tensor reconstruction aims to recover the underlying complete data from\nunder-sampled observed data by exploring prior information in high-dimensional\ndata. However, due to insufficient exploration, reconstruction methods still\nface challenges when sampling rate is extremely low. This work proposes a\ntensor reconstruction method integrating multiple priors to comprehensively\nexploit the inherent structure of the data. Specifically, the method combines\nlearnable tensor decomposition to enforce low-rank constraints of the\nreconstructed data, a pre-trained convolutional neural network for smoothing\nand denoising, and block-matching and 3D filtering regularization to enhance\nthe non-local similarity in the reconstructed data. An alternating direction\nmethod of the multipliers algorithm is designed to decompose the resulting\noptimization problem into three subproblems for efficient resolution. Extensive\nexperiments on color images, hyperspectral images, and grayscale videos\ndatasets demonstrate the superiority of our method in extreme cases as compared\nwith state-of-the-art methods.",
        "published": "2025-04-08T12:55:18+00:00"
    },
    {
        "title": "Comparative Analysis of Classical and Quantum-Inspired Solvers: A Preliminary Study on the Weighted Max-Cut Problem",
        "authors": [
            "Aitor Morais",
            "Eneko Osaba",
            "Iker Pastor",
            "Izaskun Oregui"
        ],
        "summary": "Combinatorial optimization is essential across numerous disciplines.\nTraditional metaheuristics excel at exploring complex solution spaces\nefficiently, yet they often struggle with scalability. Deep learning has become\na viable alternative for quickly generating high-quality solutions,\nparticularly when metaheuristics underperform. In recent years,\nquantum-inspired approaches such as tensor networks have shown promise in\naddressing these challenges. Despite these advancements, a thorough comparison\nof the different paradigms is missing. This study evaluates eight algorithms on\nWeighted Max-Cut graphs ranging from 10 to 250 nodes. Specifically, we compare\na Genetic Algorithm representing metaheuristics, a Graph Neural Network for\ndeep learning, and the Density Matrix Renormalization Group as a tensor network\napproach. Our analysis focuses on solution quality and computational efficiency\n(i.e., time and memory usage). Numerical results show that the Genetic\nAlgorithm achieves near-optimal results for small graphs, although its\ncomputation time grows significantly with problem size. The Graph Neural\nNetwork offers a balanced solution for medium-sized instances with low memory\ndemands and rapid inference, yet it exhibits more significant variability on\nlarger graphs. Meanwhile, the Tensor Network approach consistently yields high\napproximation ratios and efficient execution on larger graphs, albeit with\nincreased memory consumption.",
        "published": "2025-04-08T12:51:49+00:00"
    },
    {
        "title": "Adaptive RISE Control for Dual-Arm Unmanned Aerial Manipulator Systems with Deep Neural Networks",
        "authors": [
            "Yang Wang",
            "Hai Yu",
            "Shizhen Wu",
            "Zhichao Yang",
            "Jianda Han",
            "Yongchun Fang",
            "Xiao Liang"
        ],
        "summary": "The unmanned aerial manipulator system, consisting of a multirotor UAV\n(unmanned aerial vehicle) and a manipulator, has attracted considerable\ninterest from researchers. Nevertheless, the operation of a dual-arm\nmanipulator poses a dynamic challenge, as the CoM (center of mass) of the\nsystem changes with manipulator movement, potentially impacting the multirotor\nUAV. Additionally, unmodeled effects, parameter uncertainties, and external\ndisturbances can significantly degrade control performance, leading to\nunforeseen dangers. To tackle these issues, this paper proposes a nonlinear\nadaptive RISE (robust integral of the sign of the error) controller based on\nDNN (deep neural network). The first step involves establishing the kinematic\nand dynamic model of the dual-arm aerial manipulator. Subsequently, the\nadaptive RISE controller is proposed with a DNN feedforward term to effectively\naddress both internal and external challenges. By employing Lyapunov\ntechniques, the asymptotic convergence of the tracking error signals are\nguaranteed rigorously. Notably, this paper marks a pioneering effort by\npresenting the first DNN-based adaptive RISE controller design accompanied by a\ncomprehensive stability analysis. To validate the practicality and robustness\nof the proposed control approach, several groups of actual hardware experiments\nare conducted. The results confirm the efficacy of the developed methodology in\nhandling real-world scenarios, thereby offering valuable insights into the\nperformance of the dual-arm aerial manipulator system.",
        "published": "2025-04-08T12:43:45+00:00"
    },
    {
        "title": "Modular Soft Wearable Glove for Real-Time Gesture Recognition and Dynamic 3D Shape Reconstruction",
        "authors": [
            "Huazhi Dong",
            "Chunpeng Wang",
            "Mingyuan Jiang",
            "Francesco Giorgio-Serchi",
            "Yunjie Yang"
        ],
        "summary": "With the increasing demand for human-computer interaction (HCI), flexible\nwearable gloves have emerged as a promising solution in virtual reality,\nmedical rehabilitation, and industrial automation. However, the current\ntechnology still has problems like insufficient sensitivity and limited\ndurability, which hinder its wide application. This paper presents a highly\nsensitive, modular, and flexible capacitive sensor based on line-shaped\nelectrodes and liquid metal (EGaIn), integrated into a sensor module tailored\nto the human hand's anatomy. The proposed system independently captures bending\ninformation from each finger joint, while additional measurements between\nadjacent fingers enable the recording of subtle variations in inter-finger\nspacing. This design enables accurate gesture recognition and dynamic hand\nmorphological reconstruction of complex movements using point clouds.\nExperimental results demonstrate that our classifier based on Convolution\nNeural Network (CNN) and Multilayer Perceptron (MLP) achieves an accuracy of\n99.15% across 30 gestures. Meanwhile, a transformer-based Deep Neural Network\n(DNN) accurately reconstructs dynamic hand shapes with an Average Distance (AD)\nof 2.076\\pm3.231 mm, with the reconstruction accuracy at individual key points\nsurpassing SOTA benchmarks by 9.7% to 64.9%. The proposed glove shows excellent\naccuracy, robustness and scalability in gesture recognition and hand\nreconstruction, making it a promising solution for next-generation HCI systems.",
        "published": "2025-04-08T12:39:59+00:00"
    },
    {
        "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
        "authors": [
            "Sixiang Chen",
            "Jinbin Bai",
            "Zhuoran Zhao",
            "Tian Ye",
            "Qingyu Shi",
            "Donghao Zhou",
            "Wenhao Chai",
            "Xin Lin",
            "Jianzong Wu",
            "Chao Tang",
            "Shilin Xu",
            "Tao Zhang",
            "Haobo Yuan",
            "Yikang Zhou",
            "Wei Chow",
            "Linfeng Li",
            "Xiangtai Li",
            "Lei Zhu",
            "Lu Qi"
        ],
        "summary": "The landscape of image generation has rapidly evolved, from early GAN-based\napproaches to diffusion models and, most recently, to unified generative\narchitectures that seek to bridge understanding and generation tasks. Recent\nadvances, especially the GPT-4o, have demonstrated the feasibility of\nhigh-fidelity multimodal generation, their architectural design remains\nmysterious and unpublished. This prompts the question of whether image and text\ngeneration have already been successfully integrated into a unified framework\nfor those methods. In this work, we conduct an empirical study of GPT-4o's\nimage generation capabilities, benchmarking it against leading open-source and\ncommercial models. Our evaluation covers four main categories, including\ntext-to-image, image-to-image, image-to-3D, and image-to-X generation, with\nmore than 20 tasks. Our analysis highlights the strengths and limitations of\nGPT-4o under various settings, and situates it within the broader evolution of\ngenerative modeling. Through this investigation, we identify promising\ndirections for future unified generative models, emphasizing the role of\narchitectural design and data scaling.",
        "published": "2025-04-08T12:34:36+00:00"
    },
    {
        "title": "Security Vulnerabilities in Ethereum Smart Contracts: A Systematic Analysis",
        "authors": [
            "Jixuan Wu",
            "Lei Xie",
            "Xiaoqi Li"
        ],
        "summary": "Smart contracts are a secure and trustworthy application that plays a vital\nrole in decentralized applications in various fields such as insurance,the\ninternet, and gaming. However, in recent years, smart contract security\nbreaches have occurred frequently, and due to their financial properties, they\nhave caused huge economic losses, such as the most famous security incident\n\"The DAO\" which caused a loss of over \\$60 million in Ethereum. This has drawn\na lot of attention from all sides. Writing a secure smart contract is now a\ncritical issue.This paper focuses on Ether smart contracts and explains the\nmain components of Ether, smart contract architecture and mechanism.The\nenvironment used in this paper is the Ethernet environment, using remix online\ncompilation platform and Solidity language, according to the four security\nevents of American Chain, The DAO, Parity and KotET, the principles of integer\noverflow attack, reentrant attack, access control attack and denial of service\nattack are studied and analyzed accordingly, and the scenarios of these\nvulnerabilities are reproduced, and the measures to prevent them are given.\nFinally, preventive measures are given. In addition, the principles of short\naddress attack, early transaction attack and privileged function exposure\nattack are also introduced in detail, and security measures are proposed.As\nvulnerabilities continue to emerge, their classification will also evolve. The\nanalysis and research of the current vulnerabilities are also to lay a solid\nfoundation for avoiding more vulnerabilities.",
        "published": "2025-04-08T12:25:34+00:00"
    },
    {
        "title": "AVP-AP: Self-supervised Automatic View Positioning in 3D cardiac CT via Atlas Prompting",
        "authors": [
            "Xiaolin Fan",
            "Yan Wang",
            "Yingying Zhang",
            "Mingkun Bao",
            "Bosen Jia",
            "Dong Lu",
            "Yifan Gu",
            "Jian Cheng",
            "Haogang Zhu"
        ],
        "summary": "Automatic view positioning is crucial for cardiac computed tomography (CT)\nexaminations, including disease diagnosis and surgical planning. However, it is\nhighly challenging due to individual variability and large 3D search space.\nExisting work needs labor-intensive and time-consuming manual annotations to\ntrain view-specific models, which are limited to predicting only a fixed set of\nplanes. However, in real clinical scenarios, the challenge of positioning\nsemantic 2D slices with any orientation into varying coordinate space in\narbitrary 3D volume remains unsolved. We thus introduce a novel framework,\nAVP-AP, the first to use Atlas Prompting for self-supervised Automatic View\nPositioning in the 3D CT volume. Specifically, this paper first proposes an\natlas prompting method, which generates a 3D canonical atlas and trains a\nnetwork to map slices into their corresponding positions in the atlas space via\na self-supervised manner. Then, guided by atlas prompts corresponding to the\ngiven query images in a reference CT, we identify the coarse positions of\nslices in the target CT volume using rigid transformation between the 3D atlas\nand target CT volume, effectively reducing the search space. Finally, we refine\nthe coarse positions by maximizing the similarity between the predicted slices\nand the query images in the feature space of a given foundation model. Our\nframework is flexible and efficient compared to other methods, outperforming\nother methods by 19.8% average structural similarity (SSIM) in arbitrary view\npositioning and achieving 9% SSIM in two-chamber view compared to four\nradiologists. Meanwhile, experiments on a public dataset validate our\nframework's generalizability.",
        "published": "2025-04-08T12:24:37+00:00"
    },
    {
        "title": "Context-aware Rate Adaptation for Predictive Flying Networks using Contextual Bandits",
        "authors": [
            "Ruben Queiros",
            "Megumi Kaneko",
            "Helder Fontes",
            "Rui Campos"
        ],
        "summary": "The increasing complexity of wireless technologies, such as Wi-Fi, presents\nsignificant challenges for Rate Adaptation (RA) due to the large configuration\nspace of transmission parameters. While extensive research has been conducted\non RA for low-mobility networks, existing solutions fail to adapt in flying\nnetworks, where high mobility and dynamic wireless conditions introduce\nadditional uncertainty.\n  We propose Linear Upper Confidence Bound for RA (LinRA), a novel Contextual\nBandit-based approach that leverages real-time link context to optimize\ntransmission rates. Designed for predictive flying networks, where future\ntrajectories are known, LinRA proactively adapts to obstacles affecting channel\nquality. Simulation results demonstrate that LinRA converges\n$\\mathbf{5.2\\times}$ faster than state-of-the-art benchmarks and improves\nthroughput by 80\\% in Non Line-of-Sight (NLoS) conditions, matching the\nperformance of ideal algorithms. With low time complexity, LinRA is a scalable\nand efficient RA solution for predictive flying networks.",
        "published": "2025-04-08T12:23:20+00:00"
    },
    {
        "title": "Drought forecasting using a hybrid neural architecture for integrating time series and static data",
        "authors": [
            "Julian Agudelo",
            "Vincent Guigue",
            "Cristina Manfredotti",
            "Hadrien Piot"
        ],
        "summary": "Reliable forecasting is critical for early warning systems and adaptive\ndrought management. Most previous deep learning approaches focus solely on\nhomogeneous regions and rely on single-structured data. This paper presents a\nhybrid neural architecture that integrates time series and static data,\nachieving state-of-the-art performance on the DroughtED dataset. Our results\nillustrate the potential of designing neural models for the treatment of\nheterogeneous data in climate related tasks and present reliable prediction of\nUSDM categories, an expert-informed drought metric. Furthermore, this work\nvalidates the potential of DroughtED for enabling location-agnostic training of\ndeep learning models.",
        "published": "2025-04-08T12:11:34+00:00"
    },
    {
        "title": "CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics",
        "authors": [
            "Kuntian Zhang",
            "Simin Yu",
            "Yaoshu Wang",
            "Makoto Onizuka",
            "Chuan Xiao"
        ],
        "summary": "In this paper, we propose CKGAN, a novel generative adversarial network (GAN)\nvariant based on an integral probability metrics framework with characteristic\nkernel (CKIPM). CKIPM, as a distance between two probability distributions, is\ndesigned to optimize the lowerbound of the maximum mean discrepancy (MMD) in a\nreproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN\nmitigates the notorious problem of mode collapse by mapping the generated\nimages back to random noise. To save the effort of selecting the kernel\nfunction manually, we propose a soft selection method to automatically learn a\ncharacteristic kernel function. The experimental evaluation conducted on a set\nof synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that\nCKGAN generally outperforms other MMD-based GANs. The results also show that at\nthe cost of moderately more training time, the automatically selected kernel\nfunction delivers very close performance to the best of manually fine-tuned one\non real image benchmarks and is able to improve the performances of other\nMMD-based GANs.",
        "published": "2025-04-08T11:58:56+00:00"
    },
    {
        "title": "The Interconnection Tensor Rank and the Neural Network Storage Capacity",
        "authors": [
            "Boris V. Kryzhanovsky"
        ],
        "summary": "Neural network properties are considered in the case of the interconnection\ntensor rank being higher than two. This sort of interconnection tensor occurs\nin realization of crossbar-based neural networks. It is intrinsic for a\ncrossbar design to suffer from parasitic currents. It is shown that the\ninterconnection tensor of a certain form makes the neural network much more\nefficient: the storage capacity and basin of attraction of the network increase\nconsiderably. A network like the Hopfield one is used in the study.",
        "published": "2025-04-08T11:32:19+00:00"
    },
    {
        "title": "Local Thermal Non-Equilibrium Models in Porous Media: A Comparative Study of Conduction Effects",
        "authors": [
            "Anna Mareike Kostelecky",
            "Ivar Stefansson",
            "Carina Bringedal",
            "Tufan Ghosh",
            "Helge K. Dahle",
            "Rainer Helmig"
        ],
        "summary": "Instantaneous heat transfer between different phases is a common assumption\nfor modeling heat transfer in porous media, known as Local Thermal Equilibrium\n(LTE). This assumption may not hold in certain technical and environmental\napplications, especially in systems with large temperature gradients, large\ndifferences in thermal properties, or high velocities. Local Thermal\nNon-Equilibrium (LTNE) models aim to describe heat transfer processes when the\nLTE assumption may fail. In this work, we compare three continuum-scale models\nfrom the pore to the representative elementary volume (REV) scale.\nSpecifically, dual-network and REV-scale models are evaluated against a\npore-resolved model, which we perceive as a reference in the absence of\nexperimental results. Different effective models are used to obtain upscaled\nproperties on the REV scale and to compare resulting temperature profiles. The\nsystems investigated are fully saturated, consisting of one fluid and one solid\nphase. This study focuses on purely conductive systems without significant\ndifferences in thermal properties. Results show that LTE holds then for low\ninterfacial resistances. However, for large interfacial resistances, solid and\nfluid temperatures differ. The REV-scale model with effective parameters\nobtained by homogenization leads to similar results as the pore-resolved model,\nwhereas the dual-network model shows greater deviation due to its fixed spatial\nresolution. Among the evaluated effective parameter formulations for the\nREV-scale model, only the homogenization-based approach captures the LTNE\nbehavior, as it incorporates the interfacial heat transfer coefficient.\nConvection is relevant for most practical applications, and its impact will be\naddressed in a follow-up article.",
        "published": "2025-04-08T11:19:02+00:00"
    },
    {
        "title": "Deep RL-based Autonomous Navigation of Micro Aerial Vehicles (MAVs) in a complex GPS-denied Indoor Environment",
        "authors": [
            "Amit Kumar Singh",
            "Prasanth Kumar Duba",
            "P. Rajalakshmi"
        ],
        "summary": "The Autonomy of Unmanned Aerial Vehicles (UAVs) in indoor environments poses\nsignificant challenges due to the lack of reliable GPS signals in enclosed\nspaces such as warehouses, factories, and indoor facilities. Micro Aerial\nVehicles (MAVs) are preferred for navigating in these complex, GPS-denied\nscenarios because of their agility, low power consumption, and limited\ncomputational capabilities. In this paper, we propose a Reinforcement Learning\nbased Deep-Proximal Policy Optimization (D-PPO) algorithm to enhance realtime\nnavigation through improving the computation efficiency. The end-to-end network\nis trained in 3D realistic meta-environments created using the Unreal Engine.\nWith these trained meta-weights, the MAV system underwent extensive\nexperimental trials in real-world indoor environments. The results indicate\nthat the proposed method reduces computational latency by 91\\% during training\nperiod without significant degradation in performance. The algorithm was tested\non a DJI Tello drone, yielding similar results.",
        "published": "2025-04-08T11:14:37+00:00"
    },
    {
        "title": "Balancing long- and short-term dynamics for the modeling of saliency in videos",
        "authors": [
            "Theodor Wulff",
            "Fares Abawi",
            "Philipp Allgeuer",
            "Stefan Wermter"
        ],
        "summary": "The role of long- and short-term dynamics towards salient object detection in\nvideos is under-researched. We present a Transformer-based approach to learn a\njoint representation of video frames and past saliency information. Our model\nembeds long- and short-term information to detect dynamically shifting saliency\nin video. We provide our model with a stream of video frames and past saliency\nmaps, which acts as a prior for the next prediction, and extract spatiotemporal\ntokens from both modalities. The decomposition of the frame sequence into\ntokens lets the model incorporate short-term information from within the token,\nwhile being able to make long-term connections between tokens throughout the\nsequence. The core of the system consists of a dual-stream Transformer\narchitecture to process the extracted sequences independently before fusing the\ntwo modalities. Additionally, we apply a saliency-based masking scheme to the\ninput frames to learn an embedding that facilitates the recognition of\ndeviations from previous outputs. We observe that the additional prior\ninformation aids in the first detection of the salient location. Our findings\nindicate that the ratio of spatiotemporal long- and short-term features\ndirectly impacts the model's performance. While increasing the short-term\ncontext is beneficial up to a certain threshold, the model's performance\ngreatly benefits from an expansion of the long-term context.",
        "published": "2025-04-08T11:09:37+00:00"
    },
    {
        "title": "PRIMEDrive-CoT: A Precognitive Chain-of-Thought Framework for Uncertainty-Aware Object Interaction in Driving Scene Scenario",
        "authors": [
            "Sriram Mandalika",
            "Lalitha V",
            "Athira Nambiar"
        ],
        "summary": "Driving scene understanding is a critical real-world problem that involves\ninterpreting and associating various elements of a driving environment, such as\nvehicles, pedestrians, and traffic signals. Despite advancements in autonomous\ndriving, traditional pipelines rely on deterministic models that fail to\ncapture the probabilistic nature and inherent uncertainty of real-world\ndriving. To address this, we propose PRIMEDrive-CoT, a novel uncertainty-aware\nmodel for object interaction and Chain-of-Thought (CoT) reasoning in driving\nscenarios. In particular, our approach combines LiDAR-based 3D object detection\nwith multi-view RGB references to ensure interpretable and reliable scene\nunderstanding. Uncertainty and risk assessment, along with object interactions,\nare modelled using Bayesian Graph Neural Networks (BGNNs) for probabilistic\nreasoning under ambiguous conditions. Interpretable decisions are facilitated\nthrough CoT reasoning, leveraging object dynamics and contextual cues, while\nGrad-CAM visualizations highlight attention regions. Extensive evaluations on\nthe DriveCoT dataset demonstrate that PRIMEDrive-CoT outperforms\nstate-of-the-art CoT and risk-aware models.",
        "published": "2025-04-08T11:06:02+00:00"
    },
    {
        "title": "Intrinsic Saliency Guided Trunk-Collateral Network for Unsupervised Video Object Segmentation",
        "authors": [
            "Xiangyu Zheng",
            "Wanyun Li",
            "Songcheng He",
            "Xiaoqiang Li",
            "We Zhang"
        ],
        "summary": "Recent unsupervised video object segmentation (UVOS) methods predominantly\nadopt the motion-appearance paradigm. Mainstream motion-appearance approaches\nuse either the two-encoder structure to separately encode motion and appearance\nfeatures, or the single-encoder structure for joint encoding. However, these\nmethods fail to properly balance the motion-appearance relationship.\nConsequently, even with complex fusion modules for motion-appearance\nintegration, the extracted suboptimal features degrade the models' overall\nperformance. Moreover, the quality of optical flow varies across scenarios,\nmaking it insufficient to rely solely on optical flow to achieve high-quality\nsegmentation results. To address these challenges, we propose the Intrinsic\nSaliency guided Trunk-Collateral Net}work (ISTC-Net), which better balances the\nmotion-appearance relationship and incorporates model's intrinsic saliency\ninformation to enhance segmentation performance. Specifically, considering that\noptical flow maps are derived from RGB images, they share both commonalities\nand differences. We propose a novel Trunk-Collateral structure. The shared\ntrunk backbone captures the motion-appearance commonality, while the collateral\nbranch learns the uniqueness of motion features. Furthermore, an Intrinsic\nSaliency guided Refinement Module (ISRM) is devised to efficiently leverage the\nmodel's intrinsic saliency information to refine high-level features, and\nprovide pixel-level guidance for motion-appearance fusion, thereby enhancing\nperformance without additional input. Experimental results show that ISTC-Net\nachieved state-of-the-art performance on three UVOS datasets (89.2% J&F on\nDAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS) and four standard video\nsalient object detection (VSOD) benchmarks with the notable increase,\ndemonstrating its effectiveness and superiority over previous methods.",
        "published": "2025-04-08T11:02:14+00:00"
    },
    {
        "title": "Defending Deep Neural Networks against Backdoor Attacks via Module Switching",
        "authors": [
            "Weijun Li",
            "Ansh Arora",
            "Xuanli He",
            "Mark Dras",
            "Qiongkai Xu"
        ],
        "summary": "The exponential increase in the parameters of Deep Neural Networks (DNNs) has\nsignificantly raised the cost of independent training, particularly for\nresource-constrained entities. As a result, there is a growing reliance on\nopen-source models. However, the opacity of training processes exacerbates\nsecurity risks, making these models more vulnerable to malicious threats, such\nas backdoor attacks, while simultaneously complicating defense mechanisms.\nMerging homogeneous models has gained attention as a cost-effective\npost-training defense. However, we notice that existing strategies, such as\nweight averaging, only partially mitigate the influence of poisoned parameters\nand remain ineffective in disrupting the pervasive spurious correlations\nembedded across model parameters. We propose a novel module-switching strategy\nto break such spurious correlations within the model's propagation path. By\nleveraging evolutionary algorithms to optimize fusion strategies, we validate\nour approach against backdoor attacks targeting text and vision domains. Our\nmethod achieves effective backdoor mitigation even when incorporating a couple\nof compromised models, e.g., reducing the average attack success rate (ASR) to\n22% compared to 31.9% with the best-performing baseline on SST-2.",
        "published": "2025-04-08T11:01:07+00:00"
    },
    {
        "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference",
        "authors": [
            "Shuzhang Zhong",
            "Yanfan Sun",
            "Ling Liang",
            "Runsheng Wang",
            "Ru Huang",
            "Meng Li"
        ],
        "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
        "published": "2025-04-08T10:47:37+00:00"
    },
    {
        "title": "Matched Topological Subspace Detector",
        "authors": [
            "Chengen Liu",
            "Victor M. Tenorio",
            "Antonio G. Marques",
            "Elvin Isufi"
        ],
        "summary": "Topological spaces, represented by simplicial complexes, capture richer\nrelationships than graphs by modeling interactions not only between nodes but\nalso among higher-order entities, such as edges or triangles. This motivates\nthe representation of information defined in irregular domains as topological\nsignals. By leveraging the spectral dualities of Hodge and Dirac theory,\npractical topological signals often concentrate in specific spectral subspaces\n(e.g., gradient or curl). For instance, in a foreign currency exchange network,\nthe exchange flow signals typically satisfy the arbitrage-free condition and\nhence are curl-free. However, the presence of anomalies can disrupt these\nconditions, causing the signals to deviate from such subspaces. In this work,\nwe formulate a hypothesis testing framework to detect whether simplicial\ncomplex signals lie in specific subspaces in a principled and tractable manner.\nConcretely, we propose Neyman-Pearson matched topological subspace detectors\nfor signals defined at a single simplicial level (such as edges) or jointly\nacross all levels of a simplicial complex. The (energy-based projection)\nproposed detectors handle missing values, provide closed-form performance\nanalysis, and effectively capture the unique topological properties of the\ndata. We demonstrate the effectiveness of the proposed topological detectors on\nvarious real-world data, including foreign currency exchange networks.",
        "published": "2025-04-08T10:38:30+00:00"
    },
    {
        "title": "Actuarial Learning for Pension Fund Mortality Forecasting",
        "authors": [
            "Eduardo Fraga L. de Melo",
            "Helton Graziadei",
            "Rodrigo Targino"
        ],
        "summary": "For the assessment of the financial soundness of a pension fund, it is\nnecessary to take into account mortality forecasting so that longevity risk is\nconsistently incorporated into future cash flows. In this article, we employ\nmachine learning models applied to actuarial science ({\\it actuarial learning})\nto make mortality predictions for a relevant sample of pension funds'\nparticipants. Actuarial learning represents an emerging field that involves the\napplication of machine learning (ML) and artificial intelligence (AI)\ntechniques in actuarial science. This encompasses the use of algorithms and\ncomputational models to analyze large sets of actuarial data, such as\nregression trees, random forest, boosting, XGBoost, CatBoost, and neural\nnetworks (eg. FNN, LSTM, and MHA). Our results indicate that some ML/AI\nalgorithms present competitive out-of-sample performance when compared to the\nclassical Lee-Carter model. This may indicate interesting alternatives for\nconsistent liability evaluation and effective pension fund risk management.",
        "published": "2025-04-08T10:09:41+00:00"
    },
    {
        "title": "KAN-SAM: Kolmogorov-Arnold Network Guided Segment Anything Model for RGB-T Salient Object Detection",
        "authors": [
            "Xingyuan Li",
            "Ruichao Hou",
            "Tongwei Ren",
            "Gangshan Wu"
        ],
        "summary": "Existing RGB-thermal salient object detection (RGB-T SOD) methods aim to\nidentify visually significant objects by leveraging both RGB and thermal\nmodalities to enable robust performance in complex scenarios, but they often\nsuffer from limited generalization due to the constrained diversity of\navailable datasets and the inefficiencies in constructing multi-modal\nrepresentations. In this paper, we propose a novel prompt learning-based RGB-T\nSOD method, named KAN-SAM, which reveals the potential of visual foundational\nmodels for RGB-T SOD tasks. Specifically, we extend Segment Anything Model 2\n(SAM2) for RGB-T SOD by introducing thermal features as guiding prompts through\nefficient and accurate Kolmogorov-Arnold Network (KAN) adapters, which\neffectively enhance RGB representations and improve robustness. Furthermore, we\nintroduce a mutually exclusive random masking strategy to reduce reliance on\nRGB data and improve generalization. Experimental results on benchmarks\ndemonstrate superior performance over the state-of-the-art methods.",
        "published": "2025-04-08T10:07:02+00:00"
    },
    {
        "title": "Energy-Conserving Neural Network Closure Model for Long-Time Accurate and Stable LES",
        "authors": [
            "Toby van Gastelen",
            "Wouter Edeling",
            "Benjamin Sanderse"
        ],
        "summary": "Machine learning-based closure models for LES have shown promise in capturing\ncomplex turbulence dynamics but often suffer from instabilities and physical\ninconsistencies. In this work, we develop a novel skew-symmetric neural\narchitecture as closure model that enforces stability while preserving key\nphysical conservation laws. Our approach leverages a discretization that\nensures mass, momentum, and energy conservation, along with a face-averaging\nfilter to maintain mass conservation in coarse-grained velocity fields. We\ncompare our model against several conventional data-driven closures (including\nunconstrained convolutional neural networks), and the physics-based Smagorinsky\nmodel. Performance is evaluated on decaying turbulence and Kolmogorov flow for\nmultiple coarse-graining factors. In these test cases we observe that\nunconstrained machine learning models suffer from numerical instabilities. In\ncontrast, our skew-symmetric model remains stable across all tests, though at\nthe cost of increased dissipation. Despite this trade-off, we demonstrate that\nour model still outperforms the Smagorinsky model in unseen scenarios. These\nfindings highlight the potential of structure-preserving machine learning\nclosures for reliable long-time LES.",
        "published": "2025-04-08T09:49:18+00:00"
    },
    {
        "title": "Adaptive Substructure-Aware Expert Model for Molecular Property Prediction",
        "authors": [
            "Tianyi Jiang",
            "Zeyu Wang",
            "Shanqing Yu",
            "Qi Xuan"
        ],
        "summary": "Molecular property prediction is essential for applications such as drug\ndiscovery and toxicity assessment. While Graph Neural Networks (GNNs) have\nshown promising results by modeling molecules as molecular graphs, their\nreliance on data-driven learning limits their ability to generalize,\nparticularly in the presence of data imbalance and diverse molecular\nsubstructures. Existing methods often overlook the varying contributions of\ndifferent substructures to molecular properties, treating them uniformly. To\naddress these challenges, we propose ASE-Mol, a novel GNN-based framework that\nleverages a Mixture-of-Experts (MoE) approach for molecular property\nprediction. ASE-Mol incorporates BRICS decomposition and significant\nsubstructure awareness to dynamically identify positive and negative\nsubstructures. By integrating a MoE architecture, it reduces the adverse impact\nof negative motifs while improving adaptability to positive motifs.\nExperimental results on eight benchmark datasets demonstrate that ASE-Mol\nachieves state-of-the-art performance, with significant improvements in both\naccuracy and interpretability.",
        "published": "2025-04-08T09:25:03+00:00"
    },
    {
        "title": "Exact results for spin glass criticality",
        "authors": [
            "Gesualdo Delfino"
        ],
        "summary": "In recent years scale invariant scattering theory provided the first exact\naccess to the magnetic critical properties of two-dimensional statistical\nsystems with quenched disorder. We show how the theory extends to the overlap\nvariables entering the characterization of spin glass properties. The resulting\nexact fixed point equations yield both the magnetic and, for the first time,\nthe spin glass renormalization group fixed points. For the case of the random\nbond Ising model, on which we focus, the spin glass subspace of solutions is\nfound to contain a line of fixed points. We discuss the implications of the\nresults for Ising spin glass criticality and compare with the available\nnumerical results.",
        "published": "2025-04-08T09:22:05+00:00"
    },
    {
        "title": "Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments",
        "authors": [
            "Dolton Fernandes",
            "Pramod Kaushik",
            "Harsh Shukla",
            "Bapi Raju Surampudi"
        ],
        "summary": "Traditional Reinforcement Learning (RL) algorithms assume the distribution of\nthe data to be uniform or mostly uniform. However, this is not the case with\nmost real-world applications like autonomous driving or in nature where animals\nroam. Some experiences are encountered frequently, and most of the remaining\nexperiences occur rarely; the resulting distribution is called Zipfian. Taking\ninspiration from the theory of complementary learning systems, an architecture\nfor learning from Zipfian distributions is proposed where important long tail\ntrajectories are discovered in an unsupervised manner. The proposal comprises\nan episodic memory buffer containing a prioritised memory module to ensure\nimportant rare trajectories are kept longer to address the Zipfian problem,\nwhich needs credit assignment to happen in a sample efficient manner. The\nexperiences are then reinstated from episodic memory and given weighted\nimportance forming the trajectory to be executed. Notably, the proposed\narchitecture is modular, can be incorporated in any RL architecture and yields\nimproved performance in multiple Zipfian tasks over traditional architectures.\nOur method outperforms IMPALA by a significant margin on all three tasks and\nall three evaluation metrics (Zipfian, Uniform, and Rare Accuracy) and also\ngives improvements on most Atari environments that are considered challenging",
        "published": "2025-04-08T09:21:39+00:00"
    },
    {
        "title": "Channel State Information Analysis for Jamming Attack Detection in Static and Dynamic UAV Networks -- An Experimental Study",
        "authors": [
            "Pavlo Mykytyn",
            "Ronald Chitauro",
            "Zoya Dyka",
            "Peter Langendoerfer"
        ],
        "summary": "Networks built on the IEEE 802.11 standard have experienced rapid growth in\nthe last decade. Their field of application is vast, including smart home\napplications, Internet of Things (IoT), and short-range high throughput static\nand dynamic inter-vehicular communication networks. Within such networks,\nChannel State Information (CSI) provides a detailed view of the state of the\ncommunication channel and represents the combined effects of multipath\npropagation, scattering, phase shift, fading, and power decay. In this work, we\ninvestigate the problem of jamming attack detection in static and dynamic\nvehicular networks. We utilize ESP32-S3 modules to set up a communication\nnetwork between an Unmanned Aerial Vehicle (UAV) and a Ground Control Station\n(GCS), to experimentally test the combined effects of a constant jammer on\nrecorded CSI parameters, and the feasibility of jamming detection through CSI\nanalysis in static and dynamic communication scenarios.",
        "published": "2025-04-08T09:15:53+00:00"
    },
    {
        "title": "Human Activity Recognition using RGB-Event based Sensors: A Multi-modal Heat Conduction Model and A Benchmark Dataset",
        "authors": [
            "Shiao Wang",
            "Xiao Wang",
            "Bo Jiang",
            "Lin Zhu",
            "Guoqi Li",
            "Yaowei Wang",
            "Yonghong Tian",
            "Jin Tang"
        ],
        "summary": "Human Activity Recognition (HAR) primarily relied on traditional RGB cameras\nto achieve high-performance activity recognition. However, the challenging\nfactors in real-world scenarios, such as insufficient lighting and rapid\nmovements, inevitably degrade the performance of RGB cameras. To address these\nchallenges, biologically inspired event cameras offer a promising solution to\novercome the limitations of traditional RGB cameras. In this work, we rethink\nhuman activity recognition by combining the RGB and event cameras. The first\ncontribution is the proposed large-scale multi-modal RGB-Event human activity\nrecognition benchmark dataset, termed HARDVS 2.0, which bridges the dataset\ngaps. It contains 300 categories of everyday real-world actions with a total of\n107,646 paired videos covering various challenging scenarios. Inspired by the\nphysics-informed heat conduction model, we propose a novel multi-modal heat\nconduction operation framework for effective activity recognition, termed\nMMHCO-HAR. More in detail, given the RGB frames and event streams, we first\nextract the feature embeddings using a stem network. Then, multi-modal Heat\nConduction blocks are designed to fuse the dual features, the key module of\nwhich is the multi-modal Heat Conduction Operation layer. We integrate RGB and\nevent embeddings through a multi-modal DCT-IDCT layer while adaptively\nincorporating the thermal conductivity coefficient via FVEs into this module.\nAfter that, we propose an adaptive fusion module based on a policy routing\nstrategy for high-performance classification. Comprehensive experiments\ndemonstrate that our method consistently performs well, validating its\neffectiveness and robustness. The source code and benchmark dataset will be\nreleased on https://github.com/Event-AHU/HARDVS/tree/HARDVSv2",
        "published": "2025-04-08T09:14:24+00:00"
    },
    {
        "title": "Unimodular Waveform Design for Integrated Sensing and Communication MIMO System via Manifold Optimization",
        "authors": [
            "Jiangtao Wang",
            "Xuyang Zhao",
            "Muyu Mei",
            "Yongchao Wang"
        ],
        "summary": "Integrated sensing and communication (ISAC) has been widely recognized as one\nof the key technologies for 6G wireless networks. In this paper, we focus on\nthe waveform design of ISAC system, which can realize radar sensing while also\nfacilitate information transmission. The main content is as follows: first, we\nformulate the waveform design problem as a nonconvex and non-smooth model with\na unimodulus constraint based on the measurement metric of the radar and\ncommunication system. Second, we transform the model into an unconstrained\nproblem on the Riemannian manifold and construct the corresponding operators by\nanalyzing the unimodulus constraint. Third, to achieve the solution\nefficiently, we propose a low-complexity non-smooth unimodulus manifold\ngradient descent (N-UMGD) algorithm with theoretical convergence guarantee. The\nsimulation results show that the proposed algorithm can concentrate the energy\nof the sensing signal in the desired direction and realize information\ntransmission with a low bit error rate.",
        "published": "2025-04-08T09:08:50+00:00"
    },
    {
        "title": "End-to-End Dialog Neural Coreference Resolution: Balancing Efficiency and Accuracy in Large-Scale Systems",
        "authors": [
            "Zhang Dong",
            "Songhang deng",
            "Mingbang Wang",
            "Le Dai",
            "Jiyuan Li",
            "Xingzu Liu",
            "Ruilin Nong"
        ],
        "summary": "Large-scale coreference resolution presents a significant challenge in\nnatural language processing, necessitating a balance between efficiency and\naccuracy. In response to this challenge, we introduce an End-to-End Neural\nCoreference Resolution system tailored for large-scale applications. Our system\nefficiently identifies and resolves coreference links in text, ensuring minimal\ncomputational overhead without compromising on performance. By utilizing\nadvanced neural network architectures, we incorporate various contextual\nembeddings and attention mechanisms, which enhance the quality of predictions\nfor coreference pairs. Furthermore, we apply optimization strategies to\naccelerate processing speeds, making the system suitable for real-world\ndeployment. Extensive evaluations conducted on benchmark datasets demonstrate\nthat our model achieves improved accuracy compared to existing approaches,\nwhile effectively maintaining rapid inference times. Rigorous testing confirms\nthe ability of our system to deliver precise coreference resolutions\nefficiently, thereby establishing a benchmark for future advancements in this\nfield.",
        "published": "2025-04-08T09:06:52+00:00"
    },
    {
        "title": "Federated Unlearning Made Practical: Seamless Integration via Negated Pseudo-Gradients",
        "authors": [
            "Alessio Mora",
            "Carlo Mazzocca",
            "Rebecca Montanari",
            "Paolo Bellavista"
        ],
        "summary": "The right to be forgotten is a fundamental principle of privacy-preserving\nregulations and extends to Machine Learning (ML) paradigms such as Federated\nLearning (FL). While FL enhances privacy by enabling collaborative model\ntraining without sharing private data, trained models still retain the\ninfluence of training data. Federated Unlearning (FU) methods recently proposed\noften rely on impractical assumptions for real-world FL deployments, such as\nstoring client update histories or requiring access to a publicly available\ndataset. To address these constraints, this paper introduces a novel method\nthat leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).\nOur approach only uses standard client model updates, anyway employed during\nregular FL rounds, and interprets them as pseudo-gradients. When a client needs\nto be forgotten, we apply the negated of their pseudo-gradients, appropriately\nscaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly\nintegrates with FL workflows, incurs no additional computational and\ncommunication overhead beyond standard FL rounds, and supports concurrent\nunlearning requests. We extensively evaluated the proposed method on two\nwell-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and\na real-world medical imaging dataset for segmentation (ProstateMRI), using\nthree different neural architectures: two residual networks and a vision\ntransformer. The experimental results across various settings demonstrate that\nPUF achieves state-of-the-art forgetting effectiveness and recovery time,\nwithout relying on any additional assumptions, thus underscoring its practical\napplicability.",
        "published": "2025-04-08T09:05:33+00:00"
    },
    {
        "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery State Information in Energy Harvesting IoT Networks",
        "authors": [
            "Hao Wu",
            "Shengtian Yang",
            "Jun Chen",
            "Chao Chen",
            "Anding Wang"
        ],
        "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
        "published": "2025-04-08T08:40:36+00:00"
    },
    {
        "title": "Meta-Continual Learning of Neural Fields",
        "authors": [
            "Seungyoon Woo",
            "Junhyeog Yun",
            "Gunhee Kim"
        ],
        "summary": "Neural Fields (NF) have gained prominence as a versatile framework for\ncomplex data representation. This work unveils a new problem setting termed\n\\emph{Meta-Continual Learning of Neural Fields} (MCL-NF) and introduces a novel\nstrategy that employs a modular architecture combined with optimization-based\nmeta-learning. Focused on overcoming the limitations of existing methods for\ncontinual learning of neural fields, such as catastrophic forgetting and slow\nconvergence, our strategy achieves high-quality reconstruction with\nsignificantly improved learning speed. We further introduce Fisher Information\nMaximization loss for neural radiance fields (FIM-NeRF), which maximizes\ninformation gains at the sample level to enhance learning generalization, with\nproved convergence guarantee and generalization bound. We perform extensive\nevaluations across image, audio, video reconstruction, and view synthesis tasks\non six diverse datasets, demonstrating our method's superiority in\nreconstruction quality and speed over existing MCL and CL-NF approaches.\nNotably, our approach attains rapid adaptation of neural fields for city-scale\nNeRF rendering with reduced parameter requirement.",
        "published": "2025-04-08T08:38:37+00:00"
    },
    {
        "title": "Why is Normalization Necessary for Linear Recommenders?",
        "authors": [
            "Seongmin Park",
            "Mincheol Yoon",
            "Hye-young Kim",
            "Jongwuk Lee"
        ],
        "summary": "Despite their simplicity, linear autoencoder (LAE)-based models have shown\ncomparable or even better performance with faster inference speed than neural\nrecommender models. However, LAEs face two critical challenges: (i) popularity\nbias, which tends to recommend popular items, and (ii) neighborhood bias, which\noverly focuses on capturing local item correlations. To address these issues,\nthis paper first analyzes the effect of two existing normalization methods for\nLAEs, i.e., random-walk and symmetric normalization. Our theoretical analysis\nreveals that normalization highly affects the degree of popularity and\nneighborhood biases among items. Inspired by this analysis, we propose a\nversatile normalization solution, called Data-Adaptive Normalization (DAN),\nwhich flexibly controls the popularity and neighborhood biases by adjusting\nitem- and user-side normalization to align with unique dataset characteristics.\nOwing to its model-agnostic property, DAN can be easily applied to various\nLAE-based models. Experimental results show that DAN-equipped LAEs consistently\nimprove existing LAE-based models across six benchmark datasets, with\nsignificant gains of up to 128.57% and 12.36% for long-tail items and unbiased\nevaluations, respectively. Refer to our code in https://github.com/psm1206/DAN.",
        "published": "2025-04-08T08:37:32+00:00"
    },
    {
        "title": "Mass-Spring Models for Passive Keyword Spotting: A Springtronics Approach",
        "authors": [
            "Finn Bohte",
            "Theophile Louvet",
            "Vincent Maillou",
            "Marc Serra Garcia"
        ],
        "summary": "Mechanical systems played a foundational role in computing history, and have\nregained interest due to their unique properties, such as low damping and the\nability to process mechanical signals without transduction. However, recent\nefforts have primarily focused on elementary computations, implemented in\nsystems based on pre-defined reservoirs, or in periodic systems such as arrays\nof buckling beams. Here, we numerically demonstrate a passive mechanical system\n-- in the form of a nonlinear mass-spring model -- that tackles a real-world\nbenchmark for keyword spotting in speech signals. The model is organized in a\nhierarchical architecture combining feature extraction and continuous-time\nconvolution, with each individual stage tailored to the physics of the\nconsidered mass-spring systems. For each step in the computation, a subsystem\nis designed by combining a small set of low-order polynomial potentials. These\npotentials act as fundamental components that interconnect a network of masses.\nIn analogy to electronic circuit design, where complex functional circuits are\nconstructed by combining basic components into hierarchical designs, we refer\nto this framework as springtronics. We introduce springtronic systems with\nhundreds of degrees of freedom, achieving speech classification accuracy\ncomparable to existing sub-mW electronic systems.",
        "published": "2025-04-08T08:31:19+00:00"
    },
    {
        "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions",
        "authors": [
            "Hao Zhang",
            "Yanping Zha",
            "Qingwei Zhuang",
            "Zhenfeng Shao",
            "Jiayi Ma"
        ],
        "summary": "Current image fusion methods struggle to adapt to real-world environments\nencompassing diverse degradations with spatially varying characteristics. To\naddress this challenge, we propose a robust fusion controller (RFC) capable of\nachieving degradation-aware image fusion through fine-grained language\ninstructions, ensuring its reliable application in adverse environments.\nSpecifically, RFC first parses language instructions to innovatively derive the\nfunctional condition and the spatial condition, where the former specifies the\ndegradation type to remove, while the latter defines its spatial coverage.\nThen, a composite control priori is generated through a multi-condition\ncoupling network, achieving a seamless transition from abstract language\ninstructions to latent control variables. Subsequently, we design a hybrid\nattention-based fusion network to aggregate multi-modal information, in which\nthe obtained composite control priori is deeply embedded to linearly modulate\nthe intermediate fused features. To ensure the alignment between language\ninstructions and control outcomes, we introduce a novel language-feature\nalignment loss, which constrains the consistency between feature-level gains\nand the composite control priori. Extensive experiments on publicly available\ndatasets demonstrate that our RFC is robust against various composite\ndegradations, particularly in highly challenging flare scenarios.",
        "published": "2025-04-08T08:22:55+00:00"
    },
    {
        "title": "Negotiating Strict Latency Limits for Dynamic Real-Time Services in Vehicular Time-Sensitive Networks",
        "authors": [
            "Timo H\u00e4ckel",
            "Lisa Maile",
            "Philipp Meyer",
            "Franz Korf",
            "Thomas C. Schmidt"
        ],
        "summary": "Future vehicles are expected to dynamically deploy in-vehicle applications\nwithin a Service-Oriented Architecture (SOA). Critical services operate under\nhard real-time constraints, which Time-Sensitive Networking (TSN) complements\non the in-vehicle Ethernet layer. TSN ensures deterministic communication\nbetween critical services and its Credit-Based Shaper (CBS) supports dynamic\nresource reservations. However, the dynamic nature of service deployment\nchallenges network resource configuration, since any new reservation may change\nthe latency of already validated flows. In addition, standard methods of\nworst-case latency analysis for CBS have been found incorrect, and current TSN\nstream reservation procedures lack mechanisms to signal application layer\nQuality-of-Service (QoS) requirements or verify deadlines. In this paper, we\npropose a QoS negotiation scheme within the automotive SOA that interacts with\nthe TSN network controller to reserve resources while ensuring latency bounds.\nWe comparatively evaluate reservation schemes using worst-case analysis and\nsimulations of a realistic In-Vehicle Network (IVN) for demonstrating their\nimpact on QoS guarantees, resource utilization, and setup times. We find that\nonly a reservation scheme utilizing per-queue delay budgets and network\ncalculus provides valid configurations and guarantees acceptable latency bounds\nthroughout the IVN. The proposed service negotiation mechanism efficiently\nestablishes 450 vehicular network reservations in just 11ms.",
        "published": "2025-04-08T08:22:32+00:00"
    },
    {
        "title": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM",
        "authors": [
            "Jirong Zha",
            "Yuxuan Fan",
            "Xiao Yang",
            "Chen Gao",
            "Xinlei Chen"
        ],
        "summary": "3D spatial understanding is essential in real-world applications such as\nrobotics, autonomous vehicles, virtual reality, and medical imaging. Recently,\nLarge Language Models (LLMs), having demonstrated remarkable success across\nvarious domains, have been leveraged to enhance 3D understanding tasks, showing\npotential to surpass traditional computer vision methods. In this survey, we\npresent a comprehensive review of methods integrating LLMs with 3D spatial\nunderstanding. We propose a taxonomy that categorizes existing methods into\nthree branches: image-based methods deriving 3D understanding from 2D visual\ndata, point cloud-based methods working directly with 3D representations, and\nhybrid modality-based methods combining multiple data streams. We\nsystematically review representative methods along these categories, covering\ndata representations, architectural modifications, and training strategies that\nbridge textual and 3D modalities. Finally, we discuss current limitations,\nincluding dataset scarcity and computational challenges, while highlighting\npromising research directions in spatial perception, multi-modal fusion, and\nreal-world applications.",
        "published": "2025-04-08T08:11:39+00:00"
    },
    {
        "title": "Video Flow as Time Series: Discovering Temporal Consistency and Variability for VideoQA",
        "authors": [
            "Zijie Song",
            "Zhenzhen Hu",
            "Yixiao Ma",
            "Jia Li",
            "Richang Hong"
        ],
        "summary": "Video Question Answering (VideoQA) is a complex video-language task that\ndemands a sophisticated understanding of both visual content and temporal\ndynamics. Traditional Transformer-style architectures, while effective in\nintegrating multimodal data, often simplify temporal dynamics through\npositional encoding and fail to capture non-linear interactions within video\nsequences. In this paper, we introduce the Temporal Trio Transformer (T3T), a\nnovel architecture that models time consistency and time variability. The T3T\nintegrates three key components: Temporal Smoothing (TS), Temporal Difference\n(TD), and Temporal Fusion (TF). The TS module employs Brownian Bridge for\ncapturing smooth, continuous temporal transitions, while the TD module\nidentifies and encodes significant temporal variations and abrupt changes\nwithin the video content. Subsequently, the TF module synthesizes these\ntemporal features with textual cues, facilitating a deeper contextual\nunderstanding and response accuracy. The efficacy of the T3T is demonstrated\nthrough extensive testing on multiple VideoQA benchmark datasets. Our results\nunderscore the importance of a nuanced approach to temporal modeling in\nimproving the accuracy and depth of video-based question answering.",
        "published": "2025-04-08T08:08:03+00:00"
    },
    {
        "title": "FASR-Net: Unsupervised Shadow Removal Leveraging Inherent Frequency Priors",
        "authors": [
            "Tao Lin",
            "Qingwang Wang",
            "Qiwei Liang",
            "Minghua Tang",
            "Yuxuan Sun"
        ],
        "summary": "Shadow removal is challenging due to the complex interaction of geometry,\nlighting, and environmental factors. Existing unsupervised methods often\noverlook shadow-specific priors, leading to incomplete shadow recovery. To\naddress this issue, we propose a novel unsupervised Frequency Aware Shadow\nRemoval Network (FASR-Net), which leverages the inherent frequency\ncharacteristics of shadow regions. Specifically, the proposed Wavelet Attention\nDownsampling Module (WADM) integrates wavelet-based image decomposition and\ndeformable attention, effectively breaking down the image into frequency\ncomponents to enhance shadow details within specific frequency bands. We also\nintroduce several new loss functions for precise shadow-free image\nreproduction: a frequency loss to capture image component details, a\nbrightness-chromaticity loss that references the chromaticity of shadow-free\nregions, and an alignment loss to ensure smooth transitions between shadowed\nand shadow-free regions. Experimental results on the AISTD and SRD datasets\ndemonstrate that our method achieves superior shadow removal performance.",
        "published": "2025-04-08T08:00:58+00:00"
    },
    {
        "title": "Residual U-Net for accurate and efficient prediction of hemodynamics in two-dimensional asymmetric stenosis",
        "authors": [
            "Xintong Zou",
            "Suiyang Tong",
            "Wenhui Peng",
            "Qiuxiang Huang",
            "Jianchun Wang"
        ],
        "summary": "This study presents residual U-Net (U-ResNet), a deep learning surrogate\nmodel for predicting hemodynamic fields in two-dimensional asymmetric stenotic\nchannels at Reynolds numbers ranging from 200 to 800. By integrating residual\nconnections with multi-scale feature extraction, U-ResNet achieves exceptional\naccuracy while significantly reducing computational costs compared to\ncomputational fluid dynamics (CFD) approaches. Comprehensive evaluation against\nU-Net, Fourier Neural Operator (FNO), and U-Net enhanced Fourier Neural\nOperator demonstrates U-ResNet's superior performance in capturing sharp\nhemodynamic gradients and complex flow features. Notably, U-ResNet demonstrates\nrobust generalization to interpolated Reynolds numbers without retraining - a\ncapability rarely achieved in existing models. The model's non-dimensional\nformulation ensures scalability across vessel sizes and anatomical locations,\nenhancing its applicability to diverse clinical scenarios. Statistical analysis\nof prediction errors reveals that U-ResNet maintains substantially narrower\nerror distributions compared to spectral methods, confirming its reliability\nfor critical hemodynamic assessment. These advances position U-ResNet as a\npromising tool for real-time clinical decision support, treatment planning, and\nmedical device optimization, with future work focusing on extension to\nthree-dimensional geometries and integration with patient-specific data.",
        "published": "2025-04-08T07:59:19+00:00"
    },
    {
        "title": "A Lightweight Multi-Module Fusion Approach for Korean Character Recognition",
        "authors": [
            "Inho Jake Park",
            "Jaehoon Jay Jeong",
            "Ho-Sang Jo"
        ],
        "summary": "Optical Character Recognition (OCR) is essential in applications such as\ndocument processing, license plate recognition, and intelligent surveillance.\nHowever, existing OCR models often underperform in real-world scenarios due to\nirregular text layouts, poor image quality, character variability, and high\ncomputational costs.\n  This paper introduces SDA-Net (Stroke-Sensitive Attention and Dynamic Context\nEncoding Network), a lightweight and efficient architecture designed for robust\nsingle-character recognition. SDA-Net incorporates: (1) a Dual Attention\nMechanism to enhance stroke-level and spatial feature extraction; (2) a Dynamic\nContext Encoding module that adaptively refines semantic information using a\nlearnable gating mechanism; (3) a U-Net-inspired Feature Fusion Strategy for\ncombining low-level and high-level features; and (4) a highly optimized\nlightweight backbone that reduces memory and computational demands.\n  Experimental results show that SDA-Net achieves state-of-the-art accuracy on\nchallenging OCR benchmarks, with significantly faster inference, making it\nwell-suited for deployment in real-time and edge-based OCR systems.",
        "published": "2025-04-08T07:50:19+00:00"
    },
    {
        "title": "Temporal Dynamic Embedding for Irregularly Sampled Time Series",
        "authors": [
            "Mincheol Kim",
            "Soo-Yong Shin"
        ],
        "summary": "In several practical applications, particularly healthcare, clinical data of\neach patient is individually recorded in a database at irregular intervals as\nrequired. This causes a sparse and irregularly sampled time series, which makes\nit difficult to handle as a structured representation of the prerequisites of\nneural network models. We therefore propose temporal dynamic embedding (TDE),\nwhich enables neural network models to receive data that change the number of\nvariables over time. TDE regards each time series variable as an embedding\nvector evolving over time, instead of a conventional fixed structured\nrepresentation, which causes a critical missing problem. For each time step,\nTDE allows for the selective adoption and aggregation of only observed variable\nsubsets and represents the current status of patient based on current\nobservations. The experiment was conducted on three clinical datasets:\nPhysioNet 2012, MIMIC-III, and PhysioNet 2019. The TDE model performed\ncompetitively or better than the imputation-based baseline and several recent\nstate-of-the-art methods with reduced training runtime.",
        "published": "2025-04-08T07:49:22+00:00"
    },
    {
        "title": "AiGAS-dEVL-RC: An Adaptive Growing Neural Gas Model for Recurrently Drifting Unsupervised Data Streams",
        "authors": [
            "Maria Arostegi",
            "Miren Nekane Bilbao",
            "Jesus L. Lobo",
            "Javier Del Ser"
        ],
        "summary": "Concept drift and extreme verification latency pose significant challenges in\ndata stream learning, particularly when dealing with recurring concept changes\nin dynamic environments. This work introduces a novel method based on the\nGrowing Neural Gas (GNG) algorithm, designed to effectively handle abrupt\nrecurrent drifts while adapting to incrementally evolving data distributions\n(incremental drifts). Leveraging the self-organizing and topological\nadaptability of GNG, the proposed approach maintains a compact yet informative\nmemory structure, allowing it to efficiently store and retrieve knowledge of\npast or recurring concepts, even under conditions of delayed or sparse stream\nsupervision. Our experiments highlight the superiority of our approach over\nexisting data stream learning methods designed to cope with incremental\nnon-stationarities and verification latency, demonstrating its ability to\nquickly adapt to new drifts, robustly manage recurring patterns, and maintain\nhigh predictive accuracy with a minimal memory footprint. Unlike other\ntechniques that fail to leverage recurring knowledge, our proposed approach is\nproven to be a robust and efficient online learning solution for unsupervised\ndrifting data flows.",
        "published": "2025-04-08T07:42:50+00:00"
    },
    {
        "title": "RETROcode: Leveraging a Code Database for Improved Natural Language to Code Generation",
        "authors": [
            "Nathana\u00ebl Beau",
            "Beno\u00eet Crabb\u00e9"
        ],
        "summary": "As text and code resources have expanded, large-scale pre-trained models have\nshown promising capabilities in code generation tasks, typically employing\nsupervised fine-tuning with problem statement-program pairs. However,\nincreasing model size and data volume for performance gains also raises\ncomputational demands and risks of overfitting. Addressing these challenges, we\npresent RETROcode, a novel adaptation of the RETRO architecture \\cite{RETRO}\nfor sequence-to-sequence models, utilizing a large code database as an\nauxiliary scaling method. This approach, diverging from simply enlarging model\nand dataset sizes, allows RETROcode to leverage a vast code database for\nprediction, enhancing the model's efficiency by integrating extensive memory.\nOur findings indicate that RETROcode not only outperforms similar-sized\ntraditional architectures on test sets but also approaches the effectiveness of\nthe much larger Codex model, despite being trained from scratch on a\nsubstantially smaller dataset.",
        "published": "2025-04-08T07:41:13+00:00"
    },
    {
        "title": "A Douglas-Rachford Splitting Method for Solving Monotone Variational Inequalities in Linear-quadratic Dynamic Games",
        "authors": [
            "Reza Rahimi Baghbadorani",
            "Emilio Benenati",
            "Sergio Grammatico"
        ],
        "summary": "This paper considers constrained linear dynamic games with quadratic\nobjective functions, which can be cast as affine variational inequalities. By\nleveraging the problem structure, we apply the Douglas-Rachford splitting,\nwhich generates a solution algorithm with linear convergence rate. The fast\nconvergence of the method enables receding-horizon control architectures.\nFurthermore, we demonstrate that the associated VI admits a closed-form\nsolution within a neighborhood of the attractor, thus allowing for a further\nreduction in computation time. Finally, we benchmark the proposed method via\nnumerical experiments in an automated driving application.",
        "published": "2025-04-08T07:38:27+00:00"
    },
    {
        "title": "Interpretable Non-linear Survival Analysis with Evolutionary Symbolic Regression",
        "authors": [
            "Luigi Rovito",
            "Marco Virgolin"
        ],
        "summary": "Survival Regression (SuR) is a key technique for modeling time to event in\nimportant applications such as clinical trials and semiconductor manufacturing.\nCurrently, SuR algorithms belong to one of three classes: non-linear black-box\n-- allowing adaptability to many datasets but offering limited interpretability\n(e.g., tree ensembles); linear glass-box -- being easier to interpret but\nlimited to modeling only linear interactions (e.g., Cox proportional hazards);\nand non-linear glass-box -- allowing adaptability and interpretability, but\nempirically found to have several limitations (e.g., explainable boosting\nmachines, survival trees). In this work, we investigate whether Symbolic\nRegression (SR), i.e., the automated search of mathematical expressions from\ndata, can lead to non-linear glass-box survival models that are interpretable\nand accurate. We propose an evolutionary, multi-objective, and multi-expression\nimplementation of SR adapted to SuR. Our empirical results on five real-world\ndatasets show that SR consistently outperforms traditional glass-box methods\nfor SuR in terms of accuracy per number of dimensions in the model, while\nexhibiting comparable accuracy with black-box methods. Furthermore, we offer\nqualitative examples to assess the interpretability potential of SR models for\nSuR. Code at: https://github.com/lurovi/SurvivalMultiTree-pyNSGP.",
        "published": "2025-04-08T07:37:37+00:00"
    },
    {
        "title": "Dispersion-corrected Machine Learning Potentials for 2D van der Waals Materials",
        "authors": [
            "Mikkel Ohm Sauer",
            "Peder Meisner Lyngby",
            "Kristian Sommer Thygesen"
        ],
        "summary": "Machine-learned interatomic potentials (MLIPs) based on message passing\nneural networks hold promise to enable large-scale atomistic simulations of\ncomplex materials with ab initio accuracy. A number of MLIPs trained on\nenergies and forces from density functional theory (DFT) calculations employing\nsemi-local exchange-correlation (xc) functionals have recently been introduced.\nHere, we benchmark the performance of six dispersion-corrected MLIPs on a\ndataset of van der Waals heterobilayers containing between 4 and 300 atoms in\nthe moir\\'e cell. Using various structure similarity metrics, we compare the\nrelaxed heterostructures to the ground truth DFT results. With some notable\nexceptions, the model precisions are comparable to the uncertainty on the DFT\nresults stemming from the choice of xc-functional. We further explore how the\nstructural inaccuracies propagate to the electronic properties, and find\nexcellent performance with average errors on band energies as low as 35 meV.\nOur results demonstrate that recent MLIPs after dispersion corrections are on\npar with DFT for general vdW heterostructures, and thus justify their\napplication to complex and experimentally relevant 2D materials.",
        "published": "2025-04-08T07:35:53+00:00"
    },
    {
        "title": "InvNeRF-Seg: Fine-Tuning a Pre-Trained NeRF for 3D Object Segmentation",
        "authors": [
            "Jiangsan Zhao",
            "Jakob Geipel",
            "Krzysztof Kusnierek",
            "Xuean Cui"
        ],
        "summary": "Neural Radiance Fields (NeRF) have been widely adopted for reconstructing\nhigh quality 3D point clouds from 2D RGB images. However, the segmentation of\nthese reconstructed 3D scenes is more essential for downstream tasks such as\nobject counting, size estimation, and scene understanding. While segmentation\non raw 3D point clouds using deep learning requires labor intensive and\ntime-consuming manual annotation, directly training NeRF on binary masks also\nfails due to the absence of color and shading cues essential for geometry\nlearning. We propose Invariant NeRF for Segmentation (InvNeRFSeg), a two step,\nzero change fine tuning strategy for 3D segmentation. We first train a standard\nNeRF on RGB images and then fine tune it using 2D segmentation masks without\naltering either the model architecture or loss function. This approach produces\nhigher quality, cleaner segmented point clouds directly from the refined\nradiance field with minimal computational overhead or complexity. Field density\nanalysis reveals consistent semantic refinement: densities of object regions\nincrease while background densities are suppressed, ensuring clean and\ninterpretable segmentations. We demonstrate InvNeRFSegs superior performance\nover both SA3D and FruitNeRF on both synthetic fruit and real world soybean\ndatasets. This approach effectively extends 2D segmentation to high quality 3D\nsegmentation.",
        "published": "2025-04-08T07:31:01+00:00"
    },
    {
        "title": "SEA-LION: Southeast Asian Languages in One Network",
        "authors": [
            "Raymond Ng",
            "Thanh Ngan Nguyen",
            "Yuli Huang",
            "Ngee Chia Tai",
            "Wai Yi Leong",
            "Wei Qi Leong",
            "Xianbin Yong",
            "Jian Gang Ngui",
            "Yosephine Susanto",
            "Nicholas Cheng",
            "Hamsawardhini Rengarajan",
            "Peerat Limkonchotiwat",
            "Adithya Venkatadri Hulagadri",
            "Kok Wai Teng",
            "Yeo Yeow Tong",
            "Bryan Siow",
            "Wei Yi Teo",
            "Wayne Lau",
            "Choon Meng Tan",
            "Brandon Ong",
            "Zhi Hao Ong",
            "Jann Railey Montalan",
            "Adwin Chan",
            "Sajeban Antonyrex",
            "Ren Lee",
            "Esther Choa",
            "David Ong Tat-Wee",
            "Bing Jie Darius Liu",
            "William Chandra Tjhi",
            "Erik Cambria",
            "Leslie Teo"
        ],
        "summary": "Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity.",
        "published": "2025-04-08T07:24:51+00:00"
    },
    {
        "title": "Causal Portfolio Optimization: Principles and Sensitivity-Based Solutions",
        "authors": [
            "Alejandro Rodriguez Dominguez"
        ],
        "summary": "Fundamental and necessary principles for achieving efficient portfolio\noptimization based on asset and diversification dynamics are presented. The\nCommonality Principle is a necessary and sufficient condition for identifying\noptimal drivers of a portfolio in terms of its diversification dynamics. The\nproof relies on the Reichenbach Common Cause Principle, along with the fact\nthat the sensitivities of portfolio constituents with respect to the common\ncausal drivers are themselves causal. A conformal map preserves idiosyncratic\ndiversification from the unconditional setting while optimizing systematic\ndiversification on an embedded space of these sensitivities. Causal\nmethodologies for combinatorial driver selection are presented, such as the use\nof Bayesian networks and correlation-based algorithms from Reichenbach's\nprinciple. Limitations of linear models in capturing causality are discussed,\nand included for completeness alongside more advanced models such as neural\nnetworks. Portfolio optimization methods are presented that map risk from the\nsensitivity space to other risk measures of interest. Finally, the work\nintroduces a novel risk management framework based on Common Causal Manifolds,\nincluding both theoretical development and experimental validation. The\nsensitivity space is predicted along the common causal manifold, which is\nmodeled as a causal time system. Sensitivities are forecasted using SDEs\ncalibrated to data previously extracted from neural networks to move along the\nmanifold via its tangent bundles. An optimization method is then proposed that\naccumulates information across future predicted tangent bundles on the common\ncausal time system manifold. It aggregates sensitivity-based distance metrics\nalong the trajectory to build a comprehensive sensitivity distance matrix. This\nmatrix enables trajectory-wide optimal diversification, taking into account\nfuture dynamics.",
        "published": "2025-04-08T07:21:40+00:00"
    },
    {
        "title": "DDT: Decoupled Diffusion Transformer",
        "authors": [
            "Shuai Wang",
            "Zhi Tian",
            "Weilin Huang",
            "Limin Wang"
        ],
        "summary": "Diffusion transformers have demonstrated remarkable generation quality,\nalbeit requiring longer training iterations and numerous inference steps. In\neach denoising step, diffusion transformers encode the noisy inputs to extract\nthe lower-frequency semantic component and then decode the higher frequency\nwith identical modules. This scheme creates an inherent optimization dilemma:\nencoding low-frequency semantics necessitates reducing high-frequency\ncomponents, creating tension between semantic encoding and high-frequency\ndecoding. To resolve this challenge, we propose a new\n\\textbf{\\color{ddt}D}ecoupled \\textbf{\\color{ddt}D}iffusion\n\\textbf{\\color{ddt}T}ransformer~(\\textbf{\\color{ddt}DDT}), with a decoupled\ndesign of a dedicated condition encoder for semantic extraction alongside a\nspecialized velocity decoder. Our experiments reveal that a more substantial\nencoder yields performance improvements as model size increases. For ImageNet\n$256\\times256$, Our DDT-XL/2 achieves a new state-of-the-art performance of\n{1.31 FID}~(nearly $4\\times$ faster training convergence compared to previous\ndiffusion transformers). For ImageNet $512\\times512$, Our DDT-XL/2 achieves a\nnew state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our\ndecoupled architecture enhances inference speed by enabling the sharing\nself-condition between adjacent denoising steps. To minimize performance\ndegradation, we propose a novel statistical dynamic programming approach to\nidentify optimal sharing strategies.",
        "published": "2025-04-08T07:17:45+00:00"
    },
    {
        "title": "PRACH Preamble Detection as a Multi-Class Classification Problem: A Machine Learning Approach Using SVM",
        "authors": [
            "F. Ferenc",
            "M. Szczachor"
        ],
        "summary": "This study addresses the preamble detection problem in the Random Access\nprocedure of LTE/5G networks by formulating it as a multi-class classification\ntask and evaluating the effectiveness of machine learning techniques. A Support\nVector Machine (SVM) model is implemented and compared against conventional\ndetection methods. The proposed approach improves preamble index assignment,\nenhancing detection efficiency for User Equipment (UE) accessing the network.\nPerformance analysis demonstrates that the SVM-based solution increases\ndetection accuracy while reducing missed detections. These findings underscore\nthe potential of machine learning in optimizing the Random Access procedure and\nimproving network accessibility.",
        "published": "2025-04-08T07:15:50+00:00"
    },
    {
        "title": "LLM$\\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling for Generating Long-Form Articles from Extremely Long Resources",
        "authors": [
            "Haoyu Wang",
            "Yujia Fu",
            "Zhu Zhang",
            "Shuo Wang",
            "Zirui Ren",
            "Xiaorong Wang",
            "Zhili Li",
            "Chaoqun He",
            "Bo An",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "summary": "Long-form generation is crucial for a wide range of practical applications,\ntypically categorized into short-to-long and long-to-long generation. While\nshort-to-long generations have received considerable attention, generating long\ntexts from extremely long resources remains relatively underexplored. The\nprimary challenge in long-to-long generation lies in effectively integrating\nand analyzing relevant information from extensive inputs, which remains\ndifficult for current large language models (LLMs). In this paper, we propose\nLLM$\\times$MapReduce-V2, a novel test-time scaling strategy designed to enhance\nthe ability of LLMs to process extremely long inputs. Drawing inspiration from\nconvolutional neural networks, which iteratively integrate local features into\nhigher-level global representations, LLM$\\times$MapReduce-V2 utilizes stacked\nconvolutional scaling layers to progressively expand the understanding of input\nmaterials. Both quantitative and qualitative experimental results demonstrate\nthat our approach substantially enhances the ability of LLMs to process long\ninputs and generate coherent, informative long-form articles, outperforming\nseveral representative baselines.",
        "published": "2025-04-08T07:03:48+00:00"
    },
    {
        "title": "Robust and Efficient Average Consensus with Non-Coherent Over-the-Air Aggregation",
        "authors": [
            "Yuhang Deng",
            "Zheng Chen",
            "Erik G. Larsson"
        ],
        "summary": "Non-coherent over-the-air (OTA) computation has garnered increasing attention\nfor its advantages in facilitating information aggregation among distributed\nagents in resource-constrained networks without requiring precise channel\nestimation. A promising application scenario of this method is distributed\naverage consensus in wireless multi-agent systems. However, in such scenario,\nnon-coherent interference from concurrent OTA transmissions can introduce bias\nin the consensus value. To address this issue, we develop a robust distributed\naverage consensus algorithm by formulating the consensus problem as a\ndistributed optimization problem. Using decentralized projected gradient\ndescent (D-PGD), our proposed algorithm can achieve unbiased mean square\naverage consensus even in the presence of non-coherent interference and noise.\nAdditionally, we implement transmit power control and receive scaling\nmechanisms to further accelerate convergence. Simulation results demonstrate\nthat our method can significantly enhance the convergence speed of the D-PGD\nalgorithm for OTA average consensus without compromising accuracy.",
        "published": "2025-04-08T07:03:02+00:00"
    },
    {
        "title": "QEMesh: Employing A Quadric Error Metrics-Based Representation for Mesh Generation",
        "authors": [
            "Jiaqi Li",
            "Ruowei Wang",
            "Yu Liu",
            "Qijun Zhao"
        ],
        "summary": "Mesh generation plays a crucial role in 3D content creation, as mesh is\nwidely used in various industrial applications. Recent works have achieved\nimpressive results but still face several issues, such as unrealistic patterns\nor pits on surfaces, thin parts missing, and incomplete structures. Most of\nthese problems stem from the choice of shape representation or the capabilities\nof the generative network. To alleviate these, we extend PoNQ, a Quadric Error\nMetrics (QEM)-based representation, and propose a novel model, QEMesh, for\nhigh-quality mesh generation. PoNQ divides the shape surface into tiny patches,\neach represented by a point with its normal and QEM matrix, which preserves\nfine local geometry information. In our QEMesh, we regard these elements as\ngenerable parameters and design a unique latent diffusion model containing a\nnovel multi-decoder VAE for PoNQ parameters generation. Given the latent code\ngenerated by the diffusion model, three parameter decoders produce several PoNQ\nparameters within each voxel cell, and an occupancy decoder predicts which\nvoxel cells containing parameters to form the final shape. Extensive\nevaluations demonstrate that our method generates results with watertight\nsurfaces and is comparable to state-of-the-art methods in several main metrics.",
        "published": "2025-04-08T06:40:56+00:00"
    },
    {
        "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in a 64-bit Application Class RISC-V Processor",
        "authors": [
            "Christopher Reinwardt",
            "Robert Balas",
            "Alessandro Ottaviano",
            "Angelo Garofalo",
            "Luca Benini"
        ],
        "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
        "published": "2025-04-08T06:38:27+00:00"
    },
    {
        "title": "SEVERE++: Evaluating Benchmark Sensitivity in Generalization of Video Representation Learning",
        "authors": [
            "Fida Mohammad Thoker",
            "Letian Jiang",
            "Chen Zhao",
            "Piyush Bagad",
            "Hazel Doughty",
            "Bernard Ghanem",
            "Cees G. M. Snoek"
        ],
        "summary": "Continued advances in self-supervised learning have led to significant\nprogress in video representation learning, offering a scalable alternative to\nsupervised approaches by removing the need for manual annotations. Despite\nstrong performance on standard action recognition benchmarks, video\nself-supervised learning methods are largely evaluated under narrow protocols,\ntypically pretraining on Kinetics-400 and fine-tuning on similar datasets,\nlimiting our understanding of their generalization in real world scenarios. In\nthis work, we present a comprehensive evaluation of modern video\nself-supervised models, focusing on generalization across four key downstream\nfactors: domain shift, sample efficiency, action granularity, and task\ndiversity. Building on our prior work analyzing benchmark sensitivity in\nCNN-based contrastive learning, we extend the study to cover state-of-the-art\ntransformer-based video-only and video-text models. Specifically, we benchmark\n12 transformer-based methods (7 video-only, 5 video-text) and compare them to\n10 CNN-based methods, totaling over 1100 experiments across 8 datasets and 7\ndownstream tasks. Our analysis shows that, despite architectural advances,\ntransformer-based models remain sensitive to downstream conditions. No method\ngeneralizes consistently across all factors, video-only transformers perform\nbetter under domain shifts, CNNs outperform for fine-grained tasks, and\nvideo-text models often underperform despite large scale pretraining. We also\nfind that recent transformer models do not consistently outperform earlier\napproaches. Our findings provide a detailed view of the strengths and\nlimitations of current video SSL methods and offer a unified benchmark for\nevaluating generalization in video representation learning.",
        "published": "2025-04-08T06:00:28+00:00"
    },
    {
        "title": "Diabetic Retinopathy Detection Based on Convolutional Neural Networks with SMOTE and CLAHE Techniques Applied to Fundus Images",
        "authors": [
            "Sidhiq Mardianta",
            "Affandy",
            "Catur Supriyanto",
            "Catur Supriyanto",
            "Adi Wijaya"
        ],
        "summary": "Diabetic retinopathy (DR) is one of the major complications in diabetic\npatients' eyes, potentially leading to permanent blindness if not detected\ntimely. This study aims to evaluate the accuracy of artificial intelligence\n(AI) in diagnosing DR. The method employed is the Synthetic Minority\nOver-sampling Technique (SMOTE) algorithm, applied to identify DR and its\nseverity stages from fundus images using the public dataset \"APTOS 2019\nBlindness Detection.\" Literature was reviewed via ScienceDirect, ResearchGate,\nGoogle Scholar, and IEEE Xplore. Classification results using Convolutional\nNeural Network (CNN) showed the best performance for the binary classes normal\n(0) and DR (1) with an accuracy of 99.55%, precision of 99.54%, recall of\n99.54%, and F1-score of 99.54%. For the multiclass classification No_DR (0),\nMild (1), Moderate (2), Severe (3), Proliferate_DR (4), the accuracy was\n95.26%, precision 95.26%, recall 95.17%, and F1-score 95.23%. Evaluation using\nthe confusion matrix yielded results of 99.68% for binary classification and\n96.65% for multiclass. This study highlights the significant potential in\nenhancing the accuracy of DR diagnosis compared to traditional human analysis",
        "published": "2025-04-08T05:38:53+00:00"
    },
    {
        "title": "Architecture independent generalization bounds for overparametrized deep ReLU networks",
        "authors": [
            "Thomas Chen",
            "Chun-Kai Kevin Chien",
            "Patricia Mu\u00f1oz Ewald",
            "Andrew G. Moore"
        ],
        "summary": "We prove that overparametrized neural networks are able to generalize with a\ntest error that is independent of the level of overparametrization, and\nindependent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds\nthat only depend on the metric geometry of the test and training sets, on the\nregularity properties of the activation function, and on the operator norms of\nthe weights and norms of biases. For overparametrized deep ReLU networks with a\ntraining sample size bounded by the input space dimension, we explicitly\nconstruct zero loss minimizers without use of gradient descent, and prove that\nthe generalization error is independent of the network architecture.",
        "published": "2025-04-08T05:37:38+00:00"
    },
    {
        "title": "StayLTC: A Cost-Effective Multimodal Framework for Hospital Length of Stay Forecasting",
        "authors": [
            "Sudeshna Jana",
            "Manjira Sinha",
            "Tirthankar Dasgupta"
        ],
        "summary": "Accurate prediction of Length of Stay (LOS) in hospitals is crucial for\nimproving healthcare services, resource management, and cost efficiency. This\npaper presents StayLTC, a multimodal deep learning framework developed to\nforecast real-time hospital LOS using Liquid Time-Constant Networks (LTCs).\nLTCs, with their continuous-time recurrent dynamics, are evaluated against\ntraditional models using structured data from Electronic Health Records (EHRs)\nand clinical notes. Our evaluation, conducted on the MIMIC-III dataset,\ndemonstrated that LTCs significantly outperform most of the other time series\nmodels, offering enhanced accuracy, robustness, and efficiency in resource\nutilization. Additionally, LTCs demonstrate a comparable performance in LOS\nprediction compared to time series large language models, while requiring\nsignificantly less computational power and memory, underscoring their potential\nto advance Natural Language Processing (NLP) tasks in healthcare.",
        "published": "2025-04-08T05:27:53+00:00"
    },
    {
        "title": "kNN-SVC: Robust Zero-Shot Singing Voice Conversion with Additive Synthesis and Concatenation Smoothness Optimization",
        "authors": [
            "Keren Shao",
            "Ke Chen",
            "Matthew Baas",
            "Shlomo Dubnov"
        ],
        "summary": "Robustness is critical in zero-shot singing voice conversion (SVC). This\npaper introduces two novel methods to strengthen the robustness of the kNN-VC\nframework for SVC. First, kNN-VC's core representation, WavLM, lacks harmonic\nemphasis, resulting in dull sounds and ringing artifacts. To address this, we\nleverage the bijection between WavLM, pitch contours, and spectrograms to\nperform additive synthesis, integrating the resulting waveform into the model\nto mitigate these issues. Second, kNN-VC overlooks concatenative smoothness, a\nkey perceptual factor in SVC. To enhance smoothness, we propose a new distance\nmetric that filters out unsuitable kNN candidates and optimize the summing\nweights of the candidates during inference. Although our techniques are built\non the kNN-VC framework for implementation convenience, they are broadly\napplicable to general concatenative neural synthesis models. Experimental\nresults validate the effectiveness of these modifications in achieving robust\nSVC. Demo: http://knnsvc.com Code: https://github.com/SmoothKen/knn-svc",
        "published": "2025-04-08T04:59:56+00:00"
    },
    {
        "title": "Covariance-Intersection-based Distributed Kalman Filtering: Stability Problems Revisited",
        "authors": [
            "Zhongyao Hu",
            "Bo Chen",
            "Chao Sun",
            "Li Yu"
        ],
        "summary": "This paper studies the stability of covariance-intersection (CI)-based\ndistributed Kalman filtering in time-varying systems. For the general\ntime-varying case, a relationship between the error covariance and the\nobservability Gramian is established. Utilizing this relationship, we\ndemonstrate an intuition that the stability of a node is only related to the\nobservability of those nodes that can reach it uniformly. For the periodic\ntime-varying case, it is proved by a monotonicity analysis method that CI-based\ndistributed Kalman filtering converges periodically for any initial condition.\nThe convergent point is shown to be the unique positive definite solution to a\nRiccati-like equation. Additionally, by constructing an intermediate difference\nequation, the closed-loop transition matrix of the estimation error system is\nproved to be Schur stable. Notably, all theoretical results are obtained\nwithout requiring network connectivity assumptions. Finally, simulations verify\nthe effectiveness of the stability results.",
        "published": "2025-04-08T04:44:50+00:00"
    },
    {
        "title": "Noisy Deep Ensemble: Accelerating Deep Ensemble Learning via Noise Injection",
        "authors": [
            "Shunsuke Sakai",
            "Shunsuke Tsuge",
            "Tatsuhito Hasegawa"
        ],
        "summary": "Neural network ensembles is a simple yet effective approach for enhancing\ngeneralization capabilities. The most common method involves independently\ntraining multiple neural networks initialized with different weights and then\naveraging their predictions during inference. However, this approach increases\ntraining time linearly with the number of ensemble members. To address this\nissue, we propose the novel ``\\textbf{Noisy Deep Ensemble}'' method,\nsignificantly reducing the training time required for neural network ensembles.\nIn this method, a \\textit{parent model} is trained until convergence, and then\nthe weights of the \\textit{parent model} are perturbed in various ways to\nconstruct multiple \\textit{child models}. This perturbation of the\n\\textit{parent model} weights facilitates the exploration of different local\nminima while significantly reducing the training time for each ensemble member.\nWe evaluated our method using diverse CNN architectures on CIFAR-10 and\nCIFAR-100 datasets, surpassing conventional efficient ensemble methods and\nachieving test accuracy comparable to standard ensembles. Code is available at\n\\href{https://github.com/TSTB-dev/NoisyDeepEnsemble}{https://github.com/TSTB-dev/NoisyDeepEnsemble}",
        "published": "2025-04-08T04:36:39+00:00"
    },
    {
        "title": "Dual Boost-Driven Graph-Level Clustering Network",
        "authors": [
            "John Smith",
            "Wenxuan Tu",
            "Junlong Wu",
            "Wenxin Zhang",
            "Jingxin Liu",
            "Haotian Wang",
            "Jieren Cheng",
            "Huajie Lei",
            "Guangzhen Yao",
            "Lingren Wang",
            "Mengfei Li",
            "Renda Han",
            "Yu Li"
        ],
        "summary": "Graph-level clustering remains a pivotal yet formidable challenge in graph\nlearning. Recently, the integration of deep learning with representation\nlearning has demonstrated notable advancements, yielding performance\nenhancements to a certain degree. However, existing methods suffer from at\nleast one of the following issues: 1. the original graph structure has noise,\nand 2. during feature propagation and pooling processes, noise is gradually\naggregated into the graph-level embeddings through information propagation.\nConsequently, these two limitations mask clustering-friendly information,\nleading to suboptimal graph-level clustering performance. To this end, we\npropose a novel Dual Boost-Driven Graph-Level Clustering Network (DBGCN) to\nalternately promote graph-level clustering and filtering out interference\ninformation in a unified framework. Specifically, in the pooling step, we\nevaluate the contribution of features at the global and optimize them using a\nlearnable transformation matrix to obtain high-quality graph-level\nrepresentation, such that the model's reasoning capability can be improved.\nMoreover, to enable reliable graph-level clustering, we first identify and\nsuppress information detrimental to clustering by evaluating similarities\nbetween graph-level representations, providing more accurate guidance for\nmulti-view fusion. Extensive experiments demonstrated that DBGCN outperforms\nthe state-of-the-art graph-level clustering methods on six benchmark datasets.",
        "published": "2025-04-08T04:32:46+00:00"
    },
    {
        "title": "Contraction and concentration of measures with applications to theoretical neuroscience",
        "authors": [
            "Simone Betteti",
            "Francesco Bullo"
        ],
        "summary": "We investigate the asymptotic behavior of probability measures associated\nwith stochastic dynamical systems featuring either globally contracting or\n$B_{r}$-contracting drift terms. While classical results often assume constant\ndiffusion and gradient-based drifts, we extend the analysis to spatially\ninhomogeneous diffusion and non-integrable vector fields. We establish\nsufficient conditions for the existence and uniqueness of stationary measures\nunder global contraction, showing that convergence is preserved when the\ncontraction rate dominates diffusion inhomogeneity. For systems contracting\nonly outside of a compact set and with constant diffusion, we demonstrate mass\nconcentration near the minima of an associated non-convex potential, like in\nmultistable regimes. The theoretical findings are illustrated through Hopfield\nnetworks, highlighting implications for memory retrieval dynamics in noisy\nenvironments.",
        "published": "2025-04-08T04:24:39+00:00"
    },
    {
        "title": "Reconstruction-Free Anomaly Detection with Diffusion Models via Direct Latent Likelihood Evaluation",
        "authors": [
            "Shunsuke Sakai",
            "Tatsuhito Hasegawa"
        ],
        "summary": "Diffusion models, with their robust distribution approximation capabilities,\nhave demonstrated excellent performance in anomaly detection. However,\nconventional reconstruction-based approaches rely on computing the\nreconstruction error between the original and denoised images, which requires\ncareful noise-strength tuning and over ten network evaluations per\ninput-leading to significantly slower detection speeds. To address these\nlimitations, we propose a novel diffusion-based anomaly detection method that\ncircumvents the need for resource-intensive reconstruction. Instead of\nreconstructing the input image, we directly infer its corresponding latent\nvariables and measure their density under the Gaussian prior distribution.\nRemarkably, the prior density proves effective as an anomaly score even when\nusing a short partial diffusion process of only 2-5 steps. We evaluate our\nmethod on the MVTecAD dataset, achieving an AUC of 0.991 at 15 FPS, thereby\nsetting a new state-of-the-art speed-AUC anomaly detection trade-off.",
        "published": "2025-04-08T04:23:43+00:00"
    },
    {
        "title": "Entangling quantum memories over 420 km in fiber",
        "authors": [
            "Xi-Yu Luo",
            "Chao-Yang Wang",
            "Ming-Yang Zheng",
            "Bin Wang",
            "Jian-Long Liu",
            "Bo-Feng Gao",
            "Jun Li",
            "Zi Yan",
            "Qiao-Mu Ke",
            "Da Teng",
            "Rui-Chun Wang",
            "Jun Wu",
            "Jia Huang",
            "Hao Li",
            "Li-Xing You",
            "Xiu-Ping Xie",
            "Feihu Xu",
            "Qiang Zhang",
            "Xiao-Hui Bao",
            "Jian-Wei Pan"
        ],
        "summary": "Long-distance entanglement is pivotal for quantum communication, distributed\nquantum computing and sensing. Significant progresses have been made in\nextending the distribution distance of entangled photons, either in free space\nor fiber. For future quantum network applications, matter-based entanglement is\nmore favorable since the capability of storage is essential for advanced\napplications. Extending entanglement distance for memory qubits was partially\nhindered by the mismatch of its photonic emission wavelength with the low-loss\ntransmission window of optical fiber. By incorporating quantum frequency\nconversion, memory-memory entanglement has been successfully extended to\nseveral tens of kilometers. Here, we make a significant step further by\nreporting the entanglement between two atomic ensemble quantum memories over\n420 km. We convert photons emitted from the memories to telecom S-band, which\nenable us to exploit the significantly low transmission loss in fiber (0.17\ndB/km). We employ the DLCZ scheme for remote entanglement generation, and\ndelicately stabilize the relative phase between the two memories by using\nfulltime far-off-resonant locking to reduce high-frequency noise and\nintermittent dual-band locking to compensate low-frequency drift jointly. We\ndemonstrate that the memory-memory entangling probability beats the\nrepeaterless channel capacity for direct entanglement distribution. Our\nexperiment provides a testbed of studying quantum network applications from\nmetropolitan scale to intercity scale.",
        "published": "2025-04-08T04:19:24+00:00"
    },
    {
        "title": "Nes2Net: A Lightweight Nested Architecture for Foundation Model Driven Speech Anti-spoofing",
        "authors": [
            "Tianchi Liu",
            "Duc-Tuan Truong",
            "Rohan Kumar Das",
            "Kong Aik Lee",
            "Haizhou Li"
        ],
        "summary": "Speech foundation models have significantly advanced various speech-related\ntasks by providing exceptional representation capabilities. However, their\nhigh-dimensional output features often create a mismatch with downstream task\nmodels, which typically require lower-dimensional inputs. A common solution is\nto apply a dimensionality reduction (DR) layer, but this approach increases\nparameter overhead, computational costs, and risks losing valuable information.\nTo address these issues, we propose Nested Res2Net (Nes2Net), a lightweight\nback-end architecture designed to directly process high-dimensional features\nwithout DR layers. The nested structure enhances multi-scale feature\nextraction, improves feature interaction, and preserves high-dimensional\ninformation. We first validate Nes2Net on CtrSVDD, a singing voice deepfake\ndetection dataset, and report a 22% performance improvement and an 87% back-end\ncomputational cost reduction over the state-of-the-art baseline. Additionally,\nextensive testing across four diverse datasets: ASVspoof 2021, ASVspoof 5,\nPartialSpoof, and In-the-Wild, covering fully spoofed speech, adversarial\nattacks, partial spoofing, and real-world scenarios, consistently highlights\nNes2Net's superior robustness and generalization capabilities. The code package\nand pre-trained models are available at https://github.com/Liu-Tianchi/Nes2Net.",
        "published": "2025-04-08T04:11:28+00:00"
    },
    {
        "title": "How communities shape epidemic spreading: A hierarchically structured metapopulation perspective",
        "authors": [
            "Haoyang Qian",
            "Malbor Asllani"
        ],
        "summary": "Recent outbreaks of COVID-19, Zika, Ebola, and influenza have renewed\ninterest in advancing epidemic models to better reflect the complexities of\ndisease spreading. Modern approaches incorporate social norms, mobility\npatterns, and heterogeneous community structures to capture the interplay\nbetween social and biological dynamics. This study examines epidemic\npropagation in hierarchically structured metapopulation networks, where\nindividuals interact within localized communities -- such as schools,\nworkplaces, and theaters -- and diffuse across them. Using mean-field\naveraging, we derive a scaling law linking contagion rates to the mean\nconnectivity degree, while stability analysis identifies thresholds for\ninfection surges. In networks with heterogeneous mean degrees, spectral\nperturbation theory reveals how structural variability accelerates and\namplifies disease spreading. We find that nodes with above-average degrees are\nnot only infected earlier but also act as key outbreak drivers. Framing\nepidemic dynamics as a continuous phase transition, we apply pattern formation\ntheory to show that the critical eigenvectors governing system stability are\nshaped by the network's degree distribution. Crucially, by analyzing Laplacian\neigenvector localization, we uncover a one-to-one correspondence between\ncommunity infection densities and the entries of the critical eigenvector --\nrevealing how internal community structure directly shapes global infection\npatterns. This work provides a systematic framework for understanding and\npredicting epidemic dynamics in structured populations, while highlighting the\nfundamental role of community organization.",
        "published": "2025-04-08T04:02:06+00:00"
    },
    {
        "title": "Lattice: Learning to Efficiently Compress the Memory",
        "authors": [
            "Mahdi Karami",
            "Vahab Mirrokni"
        ],
        "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
        "published": "2025-04-08T03:48:43+00:00"
    },
    {
        "title": "Improved Inference of Inverse Ising Problems under Missing Observations in Restricted Boltzmann Machines",
        "authors": [
            "Kaiji Sekimoto",
            "Muneki Yasuda"
        ],
        "summary": "Restricted Boltzmann machines (RBMs) are energy-based models analogous to the\nIsing model and are widely applied in statistical machine learning. The\nstandard inverse Ising problem with a complete dataset requires computing both\ndata and model expectations and is computationally challenging because model\nexpectations have a combinatorial explosion. Furthermore, in many applications,\nthe available datasets are partially incomplete, making it difficult to compute\neven data expectations. In this study, we propose a approximation framework for\nthese expectations in the practical inverse Ising problems that integrates\nmean-field approximation or persistent contrastive divergence to generate\nrefined initial points and spatial Monte Carlo integration to enhance estimator\naccuracy. We demonstrate that the proposed method effectively and accurately\ntunes the model parameters in comparison to the conventional method.",
        "published": "2025-04-08T03:39:56+00:00"
    },
    {
        "title": "CTI-Unet: Cascaded Threshold Integration for Improved U-Net Segmentation of Pathology Images",
        "authors": [
            "Mingyang Zhu",
            "Yuqiu Liang",
            "Jiacheng Wang"
        ],
        "summary": "Chronic kidney disease (CKD) is a growing global health concern,\nnecessitating precise and efficient image analysis to aid diagnosis and\ntreatment planning. Automated segmentation of kidney pathology images plays a\ncentral role in facilitating clinical workflows, yet conventional segmentation\nmodels often require delicate threshold tuning. This paper proposes a novel\n\\textit{Cascaded Threshold-Integrated U-Net (CTI-Unet)} to overcome the\nlimitations of single-threshold segmentation. By sequentially integrating\nmultiple thresholded outputs, our approach can reconcile noise suppression with\nthe preservation of finer structural details. Experiments on the challenging\nKPIs2024 dataset demonstrate that CTI-Unet outperforms state-of-the-art\narchitectures such as nnU-Net, Swin-Unet, and CE-Net, offering a robust and\nflexible framework for kidney pathology image segmentation.",
        "published": "2025-04-08T03:35:09+00:00"
    },
    {
        "title": "A Multi-Modal AI System for Screening Mammography: Integrating 2D and 3D Imaging to Improve Breast Cancer Detection in a Prospective Clinical Study",
        "authors": [
            "Jungkyu Park",
            "Jan Witowski",
            "Yanqi Xu",
            "Hari Trivedi",
            "Judy Gichoya",
            "Beatrice Brown-Mulry",
            "Malte Westerhoff",
            "Linda Moy",
            "Laura Heacock",
            "Alana Lewin",
            "Krzysztof J. Geras"
        ],
        "summary": "Although digital breast tomosynthesis (DBT) improves diagnostic performance\nover full-field digital mammography (FFDM), false-positive recalls remain a\nconcern in breast cancer screening. We developed a multi-modal artificial\nintelligence system integrating FFDM, synthetic mammography, and DBT to provide\nbreast-level predictions and bounding-box localizations of suspicious findings.\nOur AI system, trained on approximately 500,000 mammography exams, achieved\n0.945 AUROC on an internal test set. It demonstrated capacity to reduce recalls\nby 31.7% and radiologist workload by 43.8% while maintaining 100% sensitivity,\nunderscoring its potential to improve clinical workflows. External validation\nconfirmed strong generalizability, reducing the gap to a perfect AUROC by\n35.31%-69.14% relative to strong baselines. In prospective deployment across 18\nsites, the system reduced recall rates for low-risk cases. An improved version,\ntrained on over 750,000 exams with additional labels, further reduced the gap\nby 18.86%-56.62% across large external datasets. Overall, these results\nunderscore the importance of utilizing all available imaging modalities,\ndemonstrate the potential for clinical impact, and indicate feasibility of\nfurther reduction of the test error with increased training set when using\nlarge-capacity neural networks.",
        "published": "2025-04-08T03:29:40+00:00"
    },
    {
        "title": "PTRL: Prior Transfer Deep Reinforcement Learning for Legged Robots Locomotion",
        "authors": [
            "Haodong Huang",
            "Shilong Sun",
            "Zida Zhao",
            "Hailin Huang",
            "Changqing Shen",
            "Wenfu Xu"
        ],
        "summary": "In the field of legged robot motion control, reinforcement learning (RL)\nholds great promise but faces two major challenges: high computational cost for\ntraining individual robots and poor generalization of trained models. To\naddress these problems, this paper proposes a novel framework called Prior\nTransfer Reinforcement Learning (PTRL), which improves both training efficiency\nand model transferability across different robots. Drawing inspiration from\nmodel transfer techniques in deep learning, PTRL introduces a fine-tuning\nmechanism that selectively freezes layers of the policy network during\ntransfer, making it the first to apply such a method in RL. The framework\nconsists of three stages: pre-training on a source robot using the Proximal\nPolicy Optimization (PPO) algorithm, transferring the learned policy to a\ntarget robot, and fine-tuning with partial network freezing. Extensive\nexperiments on various robot platforms confirm that this approach significantly\nreduces training time while maintaining or even improving performance.\nMoreover, the study quantitatively analyzes how the ratio of frozen layers\naffects transfer results, providing valuable insights into optimizing the\nprocess. The experimental outcomes show that PTRL achieves better walking\ncontrol performance and demonstrates strong generalization and adaptability,\noffering a promising solution for efficient and scalable RL-based control of\nlegged robots.",
        "published": "2025-04-08T03:11:43+00:00"
    },
    {
        "title": "Model-Agnostic Policy Explanations with Large Language Models",
        "authors": [
            "Zhang Xi-Jia",
            "Yue Guo",
            "Shufei Chen",
            "Simon Stepputtis",
            "Matthew Gombolay",
            "Katia Sycara",
            "Joseph Campbell"
        ],
        "summary": "Intelligent agents, such as robots, are increasingly deployed in real-world,\nhuman-centric environments. To foster appropriate human trust and meet legal\nand ethical standards, these agents must be able to explain their behavior.\nHowever, state-of-the-art agents are typically driven by black-box models like\ndeep neural networks, limiting their interpretability. We propose a method for\ngenerating natural language explanations of agent behavior based only on\nobserved states and actions -- without access to the agent's underlying model.\nOur approach learns a locally interpretable surrogate model of the agent's\nbehavior from observations, which then guides a large language model to\ngenerate plausible explanations with minimal hallucination. Empirical results\nshow that our method produces explanations that are more comprehensible and\ncorrect than those from baselines, as judged by both language models and human\nevaluators. Furthermore, we find that participants in a user study more\naccurately predicted the agent's future actions when given our explanations,\nsuggesting improved understanding of agent behavior.",
        "published": "2025-04-08T02:56:02+00:00"
    },
    {
        "title": "Continual Learning of Multiple Cognitive Functions with Brain-inspired Temporal Development Mechanism",
        "authors": [
            "Bing Han",
            "Feifei Zhao",
            "Yinqian Sun",
            "Wenxuan Pan",
            "Yi Zeng"
        ],
        "summary": "Cognitive functions in current artificial intelligence networks are tied to\nthe exponential increase in network scale, whereas the human brain can\ncontinuously learn hundreds of cognitive functions with remarkably low energy\nconsumption. This advantage is in part due to the brain cross-regional temporal\ndevelopment mechanisms, where the progressive formation, reorganization, and\npruning of connections from basic to advanced regions, facilitate knowledge\ntransfer and prevent network redundancy. Inspired by these, we propose the\nContinual Learning of Multiple Cognitive Functions with Brain-inspired Temporal\nDevelopment Mechanism(TD-MCL), enabling cognitive enhancement from simple to\ncomplex in Perception-Motor-Interaction(PMI) multiple cognitive task scenarios.\nThe TD-MCL model proposes the sequential evolution of long-range connections\nbetween different cognitive modules to promote positive knowledge transfer,\nwhile using feedback-guided local connection inhibition and pruning to\neffectively eliminate redundancies in previous tasks, reducing energy\nconsumption while preserving acquired knowledge. Experiments show that the\nproposed method can achieve continual learning capabilities while reducing\nnetwork scale, without introducing regularization, replay, or freezing\nstrategies, and achieving superior accuracy on new tasks compared to direct\nlearning. The proposed method shows that the brain's developmental mechanisms\noffer a valuable reference for exploring biologically plausible, low-energy\nenhancements of general cognitive abilities.",
        "published": "2025-04-08T02:36:36+00:00"
    },
    {
        "title": "Assessing Teleportation of Logical Qubits in a Distributed Quantum Architecture under Error Correction",
        "authors": [
            "John Stack",
            "Ming Wang",
            "Frank Mueller"
        ],
        "summary": "Quantum computing is facing challenges in terms of scaling to thousands of\nqubits and implementing quantum error correction (QEC). Scaling efforts focus\non connecting multiple smaller quantum devices in a distributed manner while\nerror correction, as a means to overcome noisy physical qubits, is being\naddressed by developing denser codes with protocols for logical qubits and\nlogical quantum gates. Teleportation of quantum states becomes an important\noperation as it transfers states from one node to another node within a\ndistributed device. For physical qubits, today's high quantum network noise\nrates prevent the teleportation of states with useful accuracy. By employing\nQEC, we show that logical qubits can be teleported between nodes under Surface\nCode and qLDPC encodings with very low logical error rates, even with network\nnoise in near-term regimes. We use circuit-level simulations to assess physical\nand network noise regimes ranging from $10^{-1}$ to $10^{-6}$. This is a wider\nrange than typically studied in circuit level simulations and understanding the\nbehavior of QEC codes in these regimes is necessary for achieving accurate\ncomputation.",
        "published": "2025-04-08T01:56:19+00:00"
    },
    {
        "title": "Extended SQP Methods in Nonsmooth Difference Programming Applied to Problems with Variational Inequality Constraints",
        "authors": [
            "Boris S. Mordukhovich",
            "Yixia Song",
            "Shangzhi Zeng",
            "Jin Zhang"
        ],
        "summary": "This paper explores a new class of constrained difference programming\nproblems, where the objective and constraints are formulated as differences of\nfunctions, without requiring their convexity. To investigate such problems,\nnovel variants of the extended sequential quadratic method are introduced.\nThese algorithms iteratively solve strongly convex quadratic subproblems\nconstructed via linear approximations of the given data by using their\ngradients and subgradients. The convergence of the proposed methods is\nrigorously analyzed by employing, in particular, the Polyak-\\L\nojasiewicz-Kurdyka property that ensures global convergence for various classes\nof functions in the problem formulation, e.g., semialgebraic ones. The original\nframework is further extended to address difference programming problems with\nvariational inequality (VI) constraints. By reformulating VI constraints via\nregularized gap functions, such problems are naturally embedded into\nconstrained difference programming that leads us to direct applications of the\nproposed algorithms. Numerical experiments for the class of continuous network\ndesign problems demonstrate the efficiency of the new methods.",
        "published": "2025-04-08T01:50:09+00:00"
    },
    {
        "title": "Spectral Similarity Masks Structural Diversity at Hydrophobic Water Interfaces",
        "authors": [
            "Yong Wang",
            "Yifan Li",
            "Linhan Du",
            "Chunyi Zhang",
            "Lorenzo Agosta",
            "Marcos Calegari Andrade",
            "Annabella Selloni",
            "Roberto Car"
        ],
        "summary": "The air-water and graphene-water interfaces represent quintessential examples\nof the liquid-gas and liquid-solid boundaries, respectively. While the\nsum-frequency generation (SFG) spectra of these interfaces exhibit certain\nsimilarities, a consensus on their signals and interpretations has yet to be\nreached. Leveraging deep learning, we accessed fully first-principles SFG\nspectra for both systems, addressing recent experimental discrepancies. Despite\nboth interfaces exhibiting microscopically hydrophobic characteristics, our\nfindings reveal that similarities in SFG signals do not translate into\ncomparable interfacial microscopic properties. Instead, graphene-water and\nair-water interfaces exhibit fundamental differences in SFG-active thicknesses,\nhydrogen-bonding networks, and dynamic diffusion behavior. These distinctions\nunderscore the stronger confinements imposed by the solid-liquid interface\ncompared with the weaker constraints of the gas-liquid interface.",
        "published": "2025-04-08T01:00:15+00:00"
    },
    {
        "title": "Finding Fantastic Experts in MoEs: A Unified Study for Expert Dropping Strategies and Observations",
        "authors": [
            "Ajay Jaiswal",
            "Jianyu Wang",
            "Yixiao Li",
            "Pingzhi Li",
            "Tianlong Chen",
            "Zhangyang Wang",
            "Chong Wang",
            "Ruoming Pang",
            "Xianzhi Du"
        ],
        "summary": "Sparsely activated Mixture-of-Experts (SMoE) has shown promise in scaling up\nthe learning capacity of neural networks. However, vanilla SMoEs have issues\nsuch as expert redundancy and heavy memory requirements, making them\ninefficient and non-scalable, especially for resource-constrained scenarios.\nExpert-level sparsification of SMoEs involves pruning the least important\nexperts to address these limitations. In this work, we aim to address three\nquestions: (1) What is the best recipe to identify the least knowledgeable\nsubset of experts that can be dropped with minimal impact on performance? (2)\nHow should we perform expert dropping (one-shot or iterative), and what\ncorrection measures can we undertake to minimize its drastic impact on SMoE\nsubnetwork capabilities? (3) What capabilities of full-SMoEs are severely\nimpacted by the removal of the least dominant experts, and how can we recover\nthem? Firstly, we propose MoE Experts Compression Suite (MC-Suite), which is a\ncollection of some previously explored and multiple novel recipes to provide a\ncomprehensive benchmark for estimating expert importance from diverse\nperspectives, as well as unveil numerous valuable insights for SMoE experts.\nSecondly, unlike prior works with a one-shot expert pruning approach, we\nexplore the benefits of iterative pruning with the re-estimation of the\nMC-Suite criterion. Moreover, we introduce the benefits of task-agnostic\nfine-tuning as a correction mechanism during iterative expert dropping, which\nwe term MoE Lottery Subnetworks. Lastly, we present an experimentally validated\nconjecture that, during expert dropping, SMoEs' instruction-following\ncapabilities are predominantly hurt, which can be restored to a robust level\nsubject to external augmentation of instruction-following capabilities using\nk-shot examples and supervised fine-tuning.",
        "published": "2025-04-08T00:49:08+00:00"
    },
    {
        "title": "Gaze-Guided Learning: Avoiding Shortcut Bias in Visual Classification",
        "authors": [
            "Jiahang Li",
            "Shibo Xue",
            "Yong Su"
        ],
        "summary": "Inspired by human visual attention, deep neural networks have widely adopted\nattention mechanisms to learn locally discriminative attributes for challenging\nvisual classification tasks. However, existing approaches primarily emphasize\nthe representation of such features while neglecting their precise\nlocalization, which often leads to misclassification caused by shortcut biases.\nThis limitation becomes even more pronounced when models are evaluated on\ntransfer or out-of-distribution datasets. In contrast, humans are capable of\nleveraging prior object knowledge to quickly localize and compare fine-grained\nattributes, a capability that is especially crucial in complex and\nhigh-variance classification scenarios. Motivated by this, we introduce\nGaze-CIFAR-10, a human gaze time-series dataset, along with a dual-sequence\ngaze encoder that models the precise sequential localization of human attention\non distinct local attributes. In parallel, a Vision Transformer (ViT) is\nemployed to learn the sequential representation of image content. Through\ncross-modal fusion, our framework integrates human gaze priors with\nmachine-derived visual sequences, effectively correcting inaccurate\nlocalization in image feature representations. Extensive qualitative and\nquantitative experiments demonstrate that gaze-guided cognitive cues\nsignificantly enhance classification accuracy.",
        "published": "2025-04-08T00:40:46+00:00"
    },
    {
        "title": "Selective Filtering of Photonic Quantum Entanglement via Anti-Parity-Time Symmetry",
        "authors": [
            "Mahmoud A. Selim",
            "Max Ehrhardt",
            "Yuqiang Ding",
            "Hediyeh M. Dinani",
            "Qi Zhong",
            "Armando Perez Leija",
            "Sahin K. Ozdemir",
            "Matthias Heinrich",
            "Alexander Szameit",
            "Demetrios N. Christodoulides",
            "Mercedeh Khajavikhan"
        ],
        "summary": "Entanglement is a key resource for quantum computing, sensing, and\ncommunication, however it is highly susceptible to decoherence. To address\nthis, quantum optics has explored filtering techniques like photon ancillas and\nRydberg atom blockade to restore entangled states. Here, we introduce a an\nentirely new approach to entanglement retrieval exploiting non-Hermitian\nsystems. By employing an anti-parity-time two-state guiding configuration, we\ndemonstrate efficient extraction of entanglement from any input state. This\nfilter is implemented on a lossless waveguide network using Lanczos\ntransformations, consistent with Wigner-Weisskopf theory. This scheme achieves\nnear-unity fidelity under single- and two-photon excitation and is scalable to\nhigher photon levels while remaining robust against decoherence during\npropagation. Our work offers new insights into using non-Hermitian symmetries\nto address central challenges in quantum technologies.",
        "published": "2025-04-08T00:31:07+00:00"
    },
    {
        "title": "BoolE: Exact Symbolic Reasoning via Boolean Equality Saturation",
        "authors": [
            "Jiaqi Yin",
            "Zhan Song",
            "Chen Chen",
            "Qihao Hu",
            "Cunxi Yu"
        ],
        "summary": "Boolean symbolic reasoning for gate-level netlists is a critical step in\nverification, logic and datapath synthesis, and hardware security.\nSpecifically, reasoning datapath and adder tree in bit-blasted Boolean networks\nis particularly crucial for verification and synthesis, and challenging.\nConventional approaches either fail to accurately (exactly) identify the\nfunction blocks of the designs in gate-level netlist with structural hashing\nand symbolic propagation, or their reasoning performance is highly sensitive to\nstructure modifications caused by technology mapping or logic optimization.\nThis paper introduces BoolE, an exact symbolic reasoning framework for Boolean\nnetlists using equality saturation. BoolE optimizes scalability and performance\nby integrating domain-specific Boolean ruleset for term rewriting. We\nincorporate a novel extraction algorithm into BoolE to enhance its structural\ninsight and computational efficiency, which adeptly identifies and captures\nmulti-input, multi-output high-level structures (e.g., full adder) in the\nreconstructed e-graph.\n  Our experiments show that BoolE surpasses state-of-the-art symbolic reasoning\nbaselines, including the conventional functional approach (ABC) and machine\nlearning-based method (Gamora). Specifically, we evaluated its performance on\nvarious multiplier architecture with different configurations. Our results show\nthat BoolE identifies $3.53\\times$ and $3.01\\times$ more exact full adders than\nABC in carry-save array and Booth-encoded multipliers, respectively.\nAdditionally, we integrated BoolE into multiplier formal verification tasks,\nwhere it significantly accelerates the performance of traditional formal\nverification tools using computer algebra, demonstrated over four orders of\nmagnitude runtime improvements.",
        "published": "2025-04-08T00:25:25+00:00"
    },
    {
        "title": "A Lightweight Large Vision-language Model for Multimodal Medical Images",
        "authors": [
            "Belal Alsinglawi",
            "Chris McCarthy",
            "Sara Webb",
            "Christopher Fluke",
            "Navid Toosy Saidy"
        ],
        "summary": "Medical Visual Question Answering (VQA) enhances clinical decision-making by\nenabling systems to interpret medical images and answer clinical queries.\nHowever, developing efficient, high-performance VQA models is challenging due\nto the complexity of medical imagery and diverse modalities. In this paper, we\nintroduce a lightweight, multimodal VQA model integrating BiomedCLIP for image\nfeature extraction and LLaMA-3 for text processing. Designed for medical VQA\ntasks, our model achieves state-of-the-art performance on the OmniMedVQA\ndataset. With approximately 8 billion parameters, it requires only two NVIDIA\n40 GB A100 GPUs, demonstrating superior efficiency over larger models. Our\nresults show 73.4% accuracy for open-end questions, surpassing existing models\nand validating its potential for real-world medical applications. Key\ncontributions include a specialized multimodal VQA model, a resource-efficient\narchitecture, and strong performance in answering open-ended clinical\nquestions.",
        "published": "2025-04-08T00:19:48+00:00"
    },
    {
        "title": "Conditions for Inter-brain Synchronization in Remote Communication: Investigating the Role of Transmission Delay",
        "authors": [
            "Sinyu Lai",
            "Wanhui Li",
            "Kaoru Amano",
            "Jun Rekimoto"
        ],
        "summary": "Inter-brain synchronization (IBS), the alignment of neural activities between\nindividuals, is a fundamental mechanism underlying effective social\ninteractions and communication. Prior research has demonstrated that IBS can\noccur during collaborative tasks and is deeply connected to communication\neffectiveness. Building on these findings, recent investigations reveal that\nIBS happens during remote interactions, implying that brain activities between\nindividuals can synchronize despite latency and physical separation. However,\nthe conditions under which this synchronization occurs or is disrupted in\nremote settings, especially the effect of latency, are not fully understood.\nThis study investigates how varying transmission latency affects IBS, in order\nto identify thresholds where synchronization is disrupted. Using\nelectroencephalography measurements quantified through Phase Locking Value -- a\nmetric that captures synchronization between brainwave phases -- we first\nconfirm synchronization under face-to-face conditions and then observe changes\nin IBS across remote communication scenarios. Our findings reveal that IBS can\noccur during remote collaboration, but is critically dependent on transmission\ndelays, with delays exceeding 450 ms significantly disrupting synchronization.\nThese findings suggest that IBS may serve as a key indicator of communication\nquality in remote interactions, offering insights for improving remote\ncommunication systems and collaboration.",
        "published": "2025-04-07T23:48:11+00:00"
    },
    {
        "title": "Scalable MHz-Rate Entanglement Distribution in Low-Latency Quantum Networks Interconnecting Heterogeneous Quantum Processors",
        "authors": [
            "Jiapeng Zhao",
            "Yang Xu",
            "Xiyuan Lu",
            "Eneet Kaur",
            "Michael Kilzer",
            "Ramana Kompella",
            "Robert W. Boyd",
            "Reza Nejabati"
        ],
        "summary": "Practical distributed quantum computing and error correction require\nhigh-qubit-rate, high-fidelity, and low-reconfiguration-latency quantum\nnetworks between heterogeneous quantum information processors. Unfortunately,\nin a quantum network with homogeneous quantum processors, the theoretical\nentanglement distribution rate for a single channel is limited to the 100-kHz\nlevel with a millisecond-level reconfiguration latency, which is not sufficient\nfor error-corrected distributed quantum computing. Here, we propose a quantum\nnetwork architecture by introducing the concept of a reconfigurable quantum\ninterface. In our protocol, through tuning the frequency and temporal mode of\nthe photonic qubits to dense wavelength division multiplexing (DWDM) channels,\na 4.5 MHz Bell pair distribution rate, with a potential of more than 40 MHz\nBell pair rate, is achieved. Through the use of reconfigurable quantum\ninterfaces and wavelength-selective switches, a nanosecond network\nreconfiguration latency can be demonstrated with low-loss, low-infidelity and\nhigh-dimensional switches. To the best of our knowledge, our architecture is\nthe first practical solution that can accommodate the entanglement distribution\nbetween heterogeneous quantum nodes with a rate and latency that satisfy most\ndistributed quantum circuits and error correction requirements. The proposed\narchitecture is compatible with the industry-standard DWDM infrastructure,\noffering a scalable and cost-effective solution for distributed quantum\ncomputing.",
        "published": "2025-04-07T23:47:49+00:00"
    },
    {
        "title": "Improved Stochastic Texture Filtering Through Sample Reuse",
        "authors": [
            "Bartlomiej Wronski",
            "Matt Pharr",
            "Tomas Akenine-M\u00f6ller"
        ],
        "summary": "Stochastic texture filtering (STF) has re-emerged as a technique that can\nbring down the cost of texture filtering of advanced texture compression\nmethods, e.g., neural texture compression. However, during texture\nmagnification, the swapped order of filtering and shading with STF can result\nin aliasing. The inability to smoothly interpolate material properties stored\nin textures, such as surface normals, leads to potentially undesirable\nappearance changes. We present a novel method to improve the quality of\nstochastically-filtered magnified textures and reduce the image difference\ncompared to traditional texture filtering. When textures are magnified, nearby\npixels filter similar sets of texels and we introduce techniques for sharing\ntexel values among pixels with only a small increase in cost (0.04--0.14~ms per\nframe). We propose an improvement to weighted importance sampling that\nguarantees that our method never increases error beyond single-sample\nstochastic texture filtering. Under high magnification, our method has >10 dB\nhigher PSNR than single-sample STF. Our results show greatly improved image\nquality both with and without spatiotemporal denoising.",
        "published": "2025-04-07T23:28:52+00:00"
    },
    {
        "title": "Federated Hierarchical Reinforcement Learning for Adaptive Traffic Signal Control",
        "authors": [
            "Yongjie Fu",
            "Lingyun Zhong",
            "Zifan Li",
            "Xuan Di"
        ],
        "summary": "Multi-agent reinforcement learning (MARL) has shown promise for adaptive\ntraffic signal control (ATSC), enabling multiple intersections to coordinate\nsignal timings in real time. However, in large-scale settings, MARL faces\nconstraints due to extensive data sharing and communication requirements.\nFederated learning (FL) mitigates these challenges by training shared models\nwithout directly exchanging raw data, yet traditional FL methods such as FedAvg\nstruggle with highly heterogeneous intersections. Different intersections\nexhibit varying traffic patterns, demands, and road structures, so performing\nFedAvg across all agents is inefficient. To address this gap, we propose\nHierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs\nclustering-based or optimization-based techniques to dynamically group\nintersections and perform FedAvg independently within groups of intersections\nwith similar characteristics, enabling more effective coordination and\nscalability than standard FedAvg. Our experiments on synthetic and real-world\ntraffic networks demonstrate that HFRL not only outperforms both decentralized\nand standard federated RL approaches but also identifies suitable grouping\npatterns based on network structure or traffic demand, resulting in a more\nrobust framework for distributed, heterogeneous systems.",
        "published": "2025-04-07T23:02:59+00:00"
    },
    {
        "title": "Extended Sensitivity-Aware Reactive Power Dispatch Algorithm for Smart Inverters with Multiple Control Modes",
        "authors": [
            "Mohammad Almomani",
            "Ahmed Alkhonain",
            "Venkataramana Ajjarapu"
        ],
        "summary": "The increasing integration of Distributed Energy Resources (DERs) in\ndistribution networks presents new challenges for voltage regulation and\nreactive power support. This paper extends a sensitivity-aware reactive power\ndispatch algorithm tailored to manage smart inverters operating under different\ncontrol modes, including PQ, PV, and Volt-Var (VV). The proposed approach\ndynamically optimizes reactive power dispatch and voltage setpoints, enabling\neffective coordination among distribution systems as a virtual power plant\n(VPP) to support the transmission network. The algorithm is applied to the IEEE\n13-bus and IEEE-123 bus test systems, and its performance is validated by\ncomparing results with OpenDSS simulations across various operating scenarios.\nResults show that the maximum error in the voltages is less than 0.015 pu.",
        "published": "2025-04-07T22:44:12+00:00"
    },
    {
        "title": "Towards Efficient Real-Time Video Motion Transfer via Generative Time Series Modeling",
        "authors": [
            "Tasmiah Haque",
            "Md. Asif Bin Syed",
            "Byungheon Jeong",
            "Xue Bai",
            "Sumit Mohan",
            "Somdyuti Paul",
            "Imtiaz Ahmed",
            "Srinjoy Das"
        ],
        "summary": "We propose a deep learning framework designed to significantly optimize\nbandwidth for motion-transfer-enabled video applications, including video\nconferencing, virtual reality interactions, health monitoring systems, and\nvision-based real-time anomaly detection. To capture complex motion\neffectively, we utilize the First Order Motion Model (FOMM), which encodes\ndynamic objects by detecting keypoints and their associated local affine\ntransformations. These keypoints are identified using a self-supervised\nkeypoint detector and arranged into a time series corresponding to the\nsuccessive frames. Forecasting is performed on these keypoints by integrating\ntwo advanced generative time series models into the motion transfer pipeline,\nnamely the Variational Recurrent Neural Network (VRNN) and the Gated Recurrent\nUnit with Normalizing Flow (GRU-NF). The predicted keypoints are subsequently\nsynthesized into realistic video frames using an optical flow estimator paired\nwith a generator network, thereby facilitating accurate video forecasting and\nenabling efficient, low-frame-rate video transmission. We validate our results\nacross three datasets for video animation and reconstruction using the\nfollowing metrics: Mean Absolute Error, Joint Embedding Predictive Architecture\nEmbedding Distance, Structural Similarity Index, and Average Pair-wise\nDisplacement. Our results confirm that by utilizing the superior reconstruction\nproperty of the Variational Autoencoder, the VRNN integrated FOMM excels in\napplications involving multi-step ahead forecasts such as video conferencing.\nOn the other hand, by leveraging the Normalizing Flow architecture for exact\nlikelihood estimation, and enabling efficient latent space sampling, the GRU-NF\nbased FOMM exhibits superior capabilities for producing diverse future samples\nwhile maintaining high visual quality for tasks like real-time video-based\nanomaly detection.",
        "published": "2025-04-07T22:21:54+00:00"
    },
    {
        "title": "dpBento: Benchmarking DPUs for Data Processing",
        "authors": [
            "Jiasheng Hu",
            "Chihan Cui",
            "Anna Li",
            "Raahil Vora",
            "Yuanfan Chen",
            "Philip A. Bernstein",
            "Jialin Li",
            "Qizhen Zhang"
        ],
        "summary": "Data processing units (DPUs, SoC-based SmartNICs) are emerging data center\nhardware that provide opportunities to address cloud data processing\nchallenges. Their onboard compute, memory, network, and auxiliary storage can\nbe leveraged to offload a variety of data processing tasks. Although recent\nwork shows promising benefits of DPU offloading for specific operations, a\ncomprehensive view of the implications of DPUs for data processing is missing.\nBenchmarking can help, but existing benchmark tools lack the focus on data\nprocessing and are limited to specific DPUs. In this paper, we present dpBento,\na benchmark suite that aims to uncover the performance characteristics of\ndifferent DPU resources and different DPUs, and the performance implications of\noffloading a wide range of data processing operations and systems to DPUs. It\nprovides an abstraction for automated performance testing and reporting and is\neasily extensible. We use dpBento to measure recent DPUs, present our\nbenchmarking results, and highlight insights into the potential benefits of DPU\noffloading for data processing.",
        "published": "2025-04-07T22:19:22+00:00"
    },
    {
        "title": "Riemannian Geometry for the classification of brain states with intracortical brain-computer interfaces",
        "authors": [
            "Arnau Marin-Llobet",
            "Arnau Manasanch",
            "Sergio Sanchez-Manso",
            "Lluc Tresserras",
            "Xinhe Zhang",
            "Yining Hua",
            "Hao Zhao",
            "Melody Torao-Angosto",
            "Maria V Sanchez-Vives",
            "Leonardo Dalla Porta"
        ],
        "summary": "This study investigates the application of Riemannian geometry-based methods\nfor brain decoding using invasive electrophysiological recordings. Although\npreviously employed in non-invasive, the utility of Riemannian geometry for\ninvasive datasets, which are typically smaller and scarcer, remains less\nexplored. Here, we propose a Minimum Distance to Mean (MDM) classifier using a\nRiemannian geometry approach based on covariance matrices extracted from\nintracortical Local Field Potential (LFP) recordings across various regions\nduring different brain state dynamics. For benchmarking, we evaluated the\nperformance of our approach against Convolutional Neural Networks (CNNs) and\nEuclidean MDM classifiers. Our results indicate that the Riemannian\ngeometry-based classification not only achieves a superior mean F1\nmacro-averaged score across different channel configurations but also requires\nup to two orders of magnitude less computational training time. Additionally,\nthe geometric framework reveals distinct spatial contributions of brain regions\nacross varying brain states, suggesting a state-dependent organization that\ntraditional time series-based methods often fail to capture. Our findings align\nwith previous studies supporting the efficacy of geometry-based methods and\nextending their application to invasive brain recordings, highlighting their\npotential for broader clinical use, such as brain computer interface\napplications.",
        "published": "2025-04-07T22:11:59+00:00"
    },
    {
        "title": "FORCE: Feature-Oriented Representation with Clustering and Explanation",
        "authors": [
            "Rishav Mukherjee",
            "Jeffrey Ahearn Thompson"
        ],
        "summary": "Learning about underlying patterns in data using latent unobserved structures\nto improve the accuracy of predictive models has become an active avenue of\ndeep learning research. Most approaches cluster the original features to\ncapture certain latent structures. However, the information gained in the\nprocess can often be implicitly derived by sufficiently complex models. Thus,\nsuch approaches often provide minimal benefits. We propose a SHAP (Shapley\nAdditive exPlanations) based supervised deep learning framework FORCE which\nrelies on two-stage usage of SHAP values in the neural network architecture,\n(i) an additional latent feature to guide model training, based on clustering\nSHAP values, and (ii) initiating an attention mechanism within the architecture\nusing latent information. This approach gives a neural network an indication\nabout the effect of unobserved values that modify feature importance for an\nobservation. The proposed framework is evaluated on three real life datasets.\nOur results demonstrate that FORCE led to dramatic improvements in overall\nperformance as compared to networks that did not incorporate the latent feature\nand attention framework (e.g., F1 score for presence of heart disease 0.80 vs\n0.72). Using cluster assignments and attention based on SHAP values guides deep\nlearning, enhancing latent pattern learning and overall discriminative\ncapability.",
        "published": "2025-04-07T22:05:50+00:00"
    },
    {
        "title": "Bridging Industrial Expertise and XR with LLM-Powered Conversational Agents",
        "authors": [
            "Despina Tomkou",
            "George Fatouros",
            "Andreas Andreou",
            "Georgios Makridis",
            "Fotis Liarokapis",
            "Dimitrios Dardanis",
            "Athanasios Kiourtis",
            "John Soldatos",
            "Dimosthenis Kyriazis"
        ],
        "summary": "This paper introduces a novel integration of Retrieval-Augmented Generation\n(RAG) enhanced Large Language Models (LLMs) with Extended Reality (XR)\ntechnologies to address knowledge transfer challenges in industrial\nenvironments. The proposed system embeds domain-specific industrial knowledge\ninto XR environments through a natural language interface, enabling hands-free,\ncontext-aware expert guidance for workers. We present the architecture of the\nproposed system consisting of an LLM Chat Engine with dynamic tool\norchestration and an XR application featuring voice-driven interaction.\nPerformance evaluation of various chunking strategies, embedding models, and\nvector databases reveals that semantic chunking, balanced embedding models, and\nefficient vector stores deliver optimal performance for industrial knowledge\nretrieval. The system's potential is demonstrated through early implementation\nin multiple industrial use cases, including robotic assembly, smart\ninfrastructure maintenance, and aerospace component servicing. Results indicate\npotential for enhancing training efficiency, remote assistance capabilities,\nand operational guidance in alignment with Industry 5.0's human-centric and\nresilient approach to industrial development.",
        "published": "2025-04-07T22:02:19+00:00"
    },
    {
        "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning",
        "authors": [
            "Taiwei Shi",
            "Yiyang Wu",
            "Linxin Song",
            "Tianyi Zhou",
            "Jieyu Zhao"
        ],
        "summary": "Reinforcement finetuning (RFT) has shown great potential for enhancing the\nmathematical reasoning capabilities of large language models (LLMs), but it is\noften sample- and compute-inefficient, requiring extensive training. In this\nwork, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a\nmethod that significantly improves both the efficiency and final accuracy of\nRFT through adaptive curriculum learning. AdaRFT dynamically adjusts the\ndifficulty of training problems based on the model's recent reward signals,\nensuring that the model consistently trains on tasks that are challenging but\nsolvable. This adaptive sampling strategy accelerates learning by maintaining\nan optimal difficulty range, avoiding wasted computation on problems that are\ntoo easy or too hard. AdaRFT requires only a lightweight extension to standard\nRFT algorithms like Proximal Policy Optimization (PPO), without modifying the\nreward function or model architecture. Experiments on competition-level math\ndatasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT\nsignificantly improves both training efficiency and reasoning performance. We\nevaluate AdaRFT across multiple data distributions and model sizes, showing\nthat it reduces the number of training steps by up to 2x and improves accuracy\nby a considerable margin, offering a more scalable and effective RFT framework.",
        "published": "2025-04-07T21:31:31+00:00"
    },
    {
        "title": "L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery",
        "authors": [
            "Yi-Zhen Tsai",
            "Xuechen Zhang",
            "Zheng Li",
            "Jiasi Chen"
        ],
        "summary": "Traditional 3D content representations include dense point clouds that\nconsume large amounts of data and hence network bandwidth, while newer\nrepresentations such as neural radiance fields suffer from poor frame rates due\nto their non-standard volumetric rendering pipeline. 3D Gaussian splats (3DGS)\ncan be seen as a generalization of point clouds that meet the best of both\nworlds, with high visual quality and efficient rendering for real-time frame\nrates. However, delivering 3DGS scenes from a hosting server to client devices\nis still challenging due to high network data consumption (e.g., 1.5 GB for a\nsingle scene). The goal of this work is to create an efficient 3D content\ndelivery framework that allows users to view high quality 3D scenes with 3DGS\nas the underlying data representation. The main contributions of the paper are:\n(1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling\nalgorithms to choose what splats to download at what time, and (3) Trace-driven\nexperiments from users wearing virtual reality headsets to evaluate the visual\nquality and latency. Our system for Layered 3D Gaussian Splats delivery L3GS\ndemonstrates high visual quality, achieving 16.9% higher average SSIM compared\nto baselines, and also works with other compressed 3DGS representations.",
        "published": "2025-04-07T21:23:32+00:00"
    },
    {
        "title": "On-chip arbitrary dispersion engineering with deep photonic networks",
        "authors": [
            "Kazim Gorgulu",
            "Aycan Deniz Vit",
            "Ali Najjar Amiri",
            "Emir Salih Magden"
        ],
        "summary": "We demonstrate the design and optimization of on-chip arbitrary dispersion\nprofiles using deep photonic networks constructed from custom-designed\nMach-Zehnder interferometers. These photonic networks employ optimizable\nwaveguide tapers, enabling precise engineering of wavelength-dependent\ndispersion profiles. We experimentally demonstrate a proof-of-concept two-port\nphotonic network that exhibits a highly nonintuitive triangular dispersion\nprofile over the wavelength range of 1.54 $\\mu\\mathrm{m}$ to 1.58\n$\\mu\\mathrm{m}$ while simultaneously achieving a flat-band transmission with an\ninsertion loss of less than 0.7 dB. We also illustrate the potential of\nmulti-port photonic networks to enhance design freedom for more complex and\ncustomizable dispersion profiles, enabling new possibilities for on-chip\ndispersion engineering.",
        "published": "2025-04-07T21:03:46+00:00"
    },
    {
        "title": "Prism: Dynamic and Flexible Benchmarking of LLMs Code Generation with Monte Carlo Tree Search",
        "authors": [
            "Vahid Majdinasab",
            "Amin Nikanjam",
            "Foutse Khomh"
        ],
        "summary": "The rapid advancement of Large Language Models (LLMs) has outpaced\ntraditional evaluation methods. Static benchmarks fail to capture the depth and\nbreadth of LLM capabilities and eventually become obsolete, while most dynamic\napproaches either rely too heavily on LLM-based evaluation or remain\nconstrained by predefined test sets. We introduce Prism, a flexible, dynamic\nbenchmarking framework designed for comprehensive LLM assessment. Prism builds\non three key components: (1) a tree-based state representation that models\nevaluation as a Markov Decision Process, (2) a Monte Carlo Tree Search\nalgorithm adapted to uncover challenging evaluation scenarios, and (3) a\nmulti-agent evaluation pipeline that enables simultaneous assessment of diverse\ncapabilities. To ensure robust evaluation, Prism integrates structural\nmeasurements of tree exploration patterns with performance metrics across\ndifficulty levels, providing detailed diagnostics of error patterns, test\ncoverage, and solution approaches. Through extensive experiments on five\nstate-of-the-art LLMs, we analyze how model architecture and scale influence\ncode generation performance across varying task difficulties. Our results\ndemonstrate Prism's effectiveness as a dynamic benchmark that evolves with\nmodel advancements while offering deeper insights into their limitations.",
        "published": "2025-04-07T20:53:18+00:00"
    },
    {
        "title": "Few-shot Personalized Scanpath Prediction",
        "authors": [
            "Ruoyu Xue",
            "Jingyi Xu",
            "Sounak Mondal",
            "Hieu Le",
            "Gregory Zelinsky",
            "Minh Hoai",
            "Dimitris Samaras"
        ],
        "summary": "A personalized model for scanpath prediction provides insights into the\nvisual preferences and attention patterns of individual subjects. However,\nexisting methods for training scanpath prediction models are data-intensive and\ncannot be effectively personalized to new individuals with only a few available\nexamples. In this paper, we propose few-shot personalized scanpath prediction\ntask (FS-PSP) and a novel method to address it, which aims to predict scanpaths\nfor an unseen subject using minimal support data of that subject's scanpath\nbehavior. The key to our method's adaptability is the Subject-Embedding Network\n(SE-Net), specifically designed to capture unique, individualized\nrepresentations for each subject's scanpaths. SE-Net generates subject\nembeddings that effectively distinguish between subjects while minimizing\nvariability among scanpaths from the same individual. The personalized scanpath\nprediction model is then conditioned on these subject embeddings to produce\naccurate, personalized results. Experiments on multiple eye-tracking datasets\ndemonstrate that our method excels in FS-PSP settings and does not require any\nfine-tuning steps at test time. Code is available at:\nhttps://github.com/cvlab-stonybrook/few-shot-scanpath",
        "published": "2025-04-07T20:48:41+00:00"
    },
    {
        "title": "Neural network-enhanced integrators for simulating ordinary differential equations",
        "authors": [
            "Amine Othmane",
            "Kathrin Fla\u00dfkamp"
        ],
        "summary": "Numerous applications necessitate the computation of numerical solutions to\ndifferential equations across a wide range of initial conditions and system\nparameters, which feeds the demand for efficient yet accurate numerical\nintegration methods.This study proposes a neural network (NN) enhancement of\nclassical numerical integrators. NNs are trained to learn integration errors,\nwhich are then used as additive correction terms in numerical schemes. The\nperformance of these enhanced integrators is compared with well-established\nmethods through numerical studies, with a particular emphasis on computational\nefficiency. Analytical properties are examined in terms of local errors and\nbackward error analysis. Embedded Runge-Kutta schemes are then employed to\ndevelop enhanced integrators that mitigate generalization risk, ensuring that\nthe neural network's evaluation in previously unseen regions of the state space\ndoes not destabilize the integrator. It is guaranteed that the enhanced\nintegrators perform at least as well as the desired classical Runge-Kutta\nschemes. The effectiveness of the proposed approaches is demonstrated through\nextensive numerical studies using a realistic model of a wind turbine, with\nparameters derived from the established simulation framework OpenFast.",
        "published": "2025-04-07T20:38:35+00:00"
    },
    {
        "title": "REEF: Relevance-Aware and Efficient LLM Adapter for Video Understanding",
        "authors": [
            "Sakib Reza",
            "Xiyun Song",
            "Heather Yu",
            "Zongfang Lin",
            "Mohsen Moghaddam",
            "Octavia Camps"
        ],
        "summary": "Integrating vision models into large language models (LLMs) has sparked\nsignificant interest in creating vision-language foundation models, especially\nfor video understanding. Recent methods often utilize memory banks to handle\nuntrimmed videos for video-level understanding. However, they typically\ncompress visual memory using similarity-based greedy approaches, which can\noverlook the contextual importance of individual tokens. To address this, we\nintroduce an efficient LLM adapter designed for video-level understanding of\nuntrimmed videos that prioritizes the contextual relevance of spatio-temporal\ntokens. Our framework leverages scorer networks to selectively compress the\nvisual memory bank and filter spatial tokens based on relevance, using a\ndifferentiable Top-K operator for end-to-end training. Across three key\nvideo-level understanding tasks$\\unicode{x2013}$ untrimmed video\nclassification, video question answering, and video\ncaptioning$\\unicode{x2013}$our method achieves competitive or superior results\non four large-scale datasets while reducing computational overhead by up to\n34%. The code will be available soon on GitHub.",
        "published": "2025-04-07T20:36:34+00:00"
    },
    {
        "title": "Towards Zero Trust Security in Connected Vehicles: A Comprehensive Survey",
        "authors": [
            "Malak Annabi",
            "Abdelhafid Zeroual",
            "Nadhir Messai"
        ],
        "summary": "Zero Trust is the new cybersecurity model that challenges the traditional one\nby promoting continuous verification of users, devices, and applications,\nwhatever their position or origin. This model is critical for reducing the\nattack surface and preventing lateral movement without relying on implicit\ntrust. Adopting the zero trust principle in Intelligent Transportation Systems\n(ITS), especially in the context of connected vehicles (CVs), presents an\nadequate solution in the face of increasing cyber threats, thereby\nstrengthening the ITS environment. This paper offers an understanding of Zero\nTrust security through a comprehensive review of existing literature,\nprinciples, and challenges. It specifically examines its applications in\nemerging technologies, particularly within connected vehicles, addressing\npotential issues and cyber threats faced by CVs. Inclusion/exclusion criteria\nfor the systematic literature review were planned alongside a bibliometric\nanalysis. Moreover, keyword co-occurrence analysis was done, which indicates\ntrends and general themes for the Zero Trust model, Zero Trust implementation,\nand Zero Trust application. Furthermore, the paper explores various ZT models\nproposed in the literature for connected vehicles, shedding light on the\nchallenges associated with their integration into CV systems. Future directions\nof this research will focus on incorporating Zero Trust principles within\nVehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication\nparadigms. This initiative intends to enhance the security posture and safety\nprotocols within interconnected vehicular networks. The proposed research seeks\nto address the unique cybersecurity vulnerabilities inherent in the highly\ndynamic nature of vehicular communication systems.",
        "published": "2025-04-07T20:29:11+00:00"
    },
    {
        "title": "Secure Diagnostics: Adversarial Robustness Meets Clinical Interpretability",
        "authors": [
            "Mohammad Hossein Najafi",
            "Mohammad Morsali",
            "Mohammadreza Pashanejad",
            "Saman Soleimani Roudi",
            "Mohammad Norouzi",
            "Saeed Bagheri Shouraki"
        ],
        "summary": "Deep neural networks for medical image classification often fail to\ngeneralize consistently in clinical practice due to violations of the i.i.d.\nassumption and opaque decision-making. This paper examines interpretability in\ndeep neural networks fine-tuned for fracture detection by evaluating model\nperformance against adversarial attack and comparing interpretability methods\nto fracture regions annotated by an orthopedic surgeon. Our findings prove that\nrobust models yield explanations more aligned with clinically meaningful areas,\nindicating that robustness encourages anatomically relevant feature\nprioritization. We emphasize the value of interpretability for facilitating\nhuman-AI collaboration, in which models serve as assistants under a\nhuman-in-the-loop paradigm: clinically plausible explanations foster trust,\nenable error correction, and discourage reliance on AI for high-stakes\ndecisions. This paper investigates robustness and interpretability as\ncomplementary benchmarks for bridging the gap between benchmark performance and\nsafe, actionable clinical deployment.",
        "published": "2025-04-07T20:26:02+00:00"
    },
    {
        "title": "Randomly measured quantum particle",
        "authors": [
            "V. Gurarie"
        ],
        "summary": "We consider the motion of a quantum particle whose position is measured in\nrandom places at random moments in time. We show that a freely moving particle\nmeasured in this way undergoes superdiffusion, while a charged particle moving\nin a magnetic field confined to the lowest Landau level undergoes conventional\ndiffusion. We also look at a particle moving in one dimensional space in a\nrandom time-independent potential, so that it is Anderson localized, which is\nalso measured at random points in space and randomly in time. We find that\nrandom measurements break localization and this particle also undergoes\ndiffusion. To address these questions, we develop formalism similar to that\nemployed when studying classical and quantum problems with time-dependent\nnoise.",
        "published": "2025-04-07T20:18:30+00:00"
    },
    {
        "title": "Graph Neural Networks for Enhancing Ensemble Forecasts of Extreme Rainfall",
        "authors": [
            "Christopher B\u00fclte",
            "Sohir Maskey",
            "Philipp Scholl",
            "Jonas von Berg",
            "Gitta Kutyniok"
        ],
        "summary": "Climate change is increasing the occurrence of extreme precipitation events,\nthreatening infrastructure, agriculture, and public safety. Ensemble prediction\nsystems provide probabilistic forecasts but exhibit biases and difficulties in\ncapturing extreme weather. While post-processing techniques aim to enhance\nforecast accuracy, they rarely focus on precipitation, which exhibits complex\nspatial dependencies and tail behavior. Our novel framework leverages graph\nneural networks to post-process ensemble forecasts, specifically modeling the\nextremes of the underlying distribution. This allows to capture spatial\ndependencies and improves forecast accuracy for extreme events, thus leading to\nmore reliable forecasts and mitigating risks of extreme precipitation and\nflooding.",
        "published": "2025-04-07T20:01:55+00:00"
    },
    {
        "title": "REVEAL: Relation-based Video Representation Learning for Video-Question-Answering",
        "authors": [
            "Sofian Chaybouti",
            "Walid Bousselham",
            "Moritz Wolter",
            "Hilde Kuehne"
        ],
        "summary": "Video-Question-Answering (VideoQA) comprises the capturing of complex visual\nrelation changes over time, remaining a challenge even for advanced Video\nLanguage Models (VLM), i.a., because of the need to represent the visual\ncontent to a reasonably sized input for those models. To address this problem,\nwe propose\n  RElation-based Video rEpresentAtion Learning (REVEAL), a framework designed\nto capture visual relation information by encoding them into structured,\ndecomposed representations. Specifically, inspired by spatiotemporal scene\ngraphs, we propose to encode video sequences as sets of relation triplets in\nthe form of (\\textit{subject-predicate-object}) over time via their language\nembeddings. To this end, we extract explicit relations from video captions and\nintroduce a Many-to-Many Noise Contrastive Estimation (MM-NCE) together with a\nQ-Former architecture to align an unordered set of video-derived queries with\ncorresponding text-based relation descriptions. At inference, the resulting\nQ-former produces an efficient token representation that can serve as input to\na VLM for VideoQA.\n  We evaluate the proposed framework on five challenging benchmarks: NeXT-QA,\nIntent-QA, STAR, VLEP, and TVQA. It shows that the resulting query-based video\nrepresentation is able to outperform global alignment-based CLS or patch token\nrepresentations and achieves competitive results against state-of-the-art\nmodels, particularly on tasks requiring temporal reasoning and relation\ncomprehension. The code and models will be publicly released.",
        "published": "2025-04-07T19:54:04+00:00"
    },
    {
        "title": "Quantum Mechanics and Neural Networks",
        "authors": [
            "Christian Ferko",
            "James Halverson"
        ],
        "summary": "We demonstrate that any Euclidean-time quantum mechanical theory may be\nrepresented as a neural network, ensured by the Kosambi-Karhunen-Lo\\`eve\ntheorem, mean-square path continuity, and finite two-point functions. The\nadditional constraint of reflection positivity, which is related to unitarity,\nmay be achieved by a number of mechanisms, such as imposing neural network\nparameter space splitting or the Markov property. Non-differentiability of the\nnetworks is related to the appearance of non-trivial commutators. Neural\nnetworks acting on Markov processes are no longer Markov, but still reflection\npositive, which facilitates the definition of deep neural network quantum\nsystems. We illustrate these principles in several examples using numerical\nimplementations, recovering classic quantum mechanical results such as\nHeisenberg uncertainty, non-trivial commutators, and the spectrum.",
        "published": "2025-04-07T19:54:00+00:00"
    },
    {
        "title": "Intermediate Layer Classifiers for OOD generalization",
        "authors": [
            "Arnas Uselis",
            "Seong Joon Oh"
        ],
        "summary": "Deep classifiers are known to be sensitive to data distribution shifts,\nprimarily due to their reliance on spurious correlations in training data. It\nhas been suggested that these classifiers can still find useful features in the\nnetwork's last layer that hold up under such shifts. In this work, we question\nthe use of last-layer representations for out-of-distribution (OOD)\ngeneralisation and explore the utility of intermediate layers. To this end, we\nintroduce \\textit{Intermediate Layer Classifiers} (ILCs). We discover that\nintermediate layer representations frequently offer substantially better\ngeneralisation than those from the penultimate layer. In many cases, zero-shot\nOOD generalisation using earlier-layer representations approaches the few-shot\nperformance of retraining on penultimate layer representations. This is\nconfirmed across multiple datasets, architectures, and types of distribution\nshifts. Our analysis suggests that intermediate layers are less sensitive to\ndistribution shifts compared to the penultimate layer. These findings highlight\nthe importance of understanding how information is distributed across network\nlayers and its role in OOD generalisation, while also pointing to the limits of\npenultimate layer representation utility. Code is available at\nhttps://github.com/oshapio/intermediate-layer-generalization",
        "published": "2025-04-07T19:50:50+00:00"
    },
    {
        "title": "QARPET: A Crossbar Chip for Benchmarking Semiconductor Spin Qubits",
        "authors": [
            "Alberto Tosato",
            "Asser Elsayed",
            "Federico Poggiali",
            "Lucas Stehouwer",
            "Davide Costa",
            "Karina Hudson",
            "Davide Degli Esposti",
            "Giordano Scappucci"
        ],
        "summary": "Large-scale integration of semiconductor spin qubits into industrial quantum\nprocessors hinges on the ability to characterize the performance of quantum\ncomponents at scale. While the semiconductor industry has addressed scalable\ntesting for transistors using device matrix arrays, extending this approach to\nquantum dot spin qubits is challenged by their operation at sub-kelvin\ntemperatures, in the presence of magnetic fields, and by the use of\nradio-frequency signals. Here, we present QARPET (Qubit-Array Research Platform\nfor Engineering and Testing), a scalable architecture for characterizing spin\nqubits using a quantum dot crossbar array with sublinear scaling of\ninterconnects. The crossbar features tightly pitched (1 {\\mu}m), individually\naddressable spin qubit tiles and is implemented in planar germanium, by\nfabricating a large device with the potential to host 1058 hole spin qubits. We\nfocus our measurements on a patch of 40 tiles and demonstrate key device\nfunctionality at millikelvin temperature including unique tile addressability,\nthreshold voltage and charge noise statistics, and characterisation of hole\nspin qubits and their coherence times in a single tile. These demonstrations\npave the way for a new generation of quantum devices designed for the\nstatistical characterisation of spin qubits and for developing automated\nroutines for quantum dot tuning and spin qubit operation.",
        "published": "2025-04-07T19:49:44+00:00"
    },
    {
        "title": "Generative Adversarial Networks with Limited Data: A Survey and Benchmarking",
        "authors": [
            "Omar De Mitri",
            "Ruyu Wang",
            "Marco F. Huber"
        ],
        "summary": "Generative Adversarial Networks (GANs) have shown impressive results in\nvarious image synthesis tasks. Vast studies have demonstrated that GANs are\nmore powerful in feature and expression learning compared to other generative\nmodels and their latent space encodes rich semantic information. However, the\ntremendous performance of GANs heavily relies on the access to large-scale\ntraining data and deteriorates rapidly when the amount of data is limited. This\npaper aims to provide an overview of GANs, its variants and applications in\nvarious vision tasks, focusing on addressing the limited data issue. We analyze\nstate-of-the-art GANs in limited data regime with designed experiments, along\nwith presenting various methods attempt to tackle this problem from different\nperspectives. Finally, we further elaborate on remaining challenges and trends\nfor future research.",
        "published": "2025-04-07T19:46:56+00:00"
    },
    {
        "title": "Large-Scale Classification of Shortwave Communication Signals with Machine Learning",
        "authors": [
            "Stefan Scholl"
        ],
        "summary": "This paper presents a deep learning approach to the classification of 160\nshortwave radio signals. It addresses the typical challenges of the shortwave\nspectrum, which are the large number of different signal types, the presence of\nvarious analog modulations and ionospheric propagation. As a classifier a deep\nconvolutional neural network is used, that is trained to recognize 160 typical\nshortwave signal classes. The approach is blind and therefore does not require\npreknowledge or special preprocessing of the signal and no manual design of\ndiscriminative features for each signal class. The network is trained on a\nlarge number of synthetically generated signals and high quality recordings.\nFinally, the network is evaluated on real-world radio signals obtained from\nglobally deployed receiver hardware and achieves up to 90% accuracy for an\nobservation time of only 1 second.",
        "published": "2025-04-07T19:45:08+00:00"
    },
    {
        "title": "GraphPINE: Graph Importance Propagation for Interpretable Drug Response Prediction",
        "authors": [
            "Yoshitaka Inoue",
            "Tianfan Fu",
            "Augustin Luna"
        ],
        "summary": "Explainability is necessary for many tasks in biomedical research. Recent\nexplainability methods have focused on attention, gradient, and Shapley value.\nThese do not handle data with strong associated prior knowledge and fail to\nconstrain explainability results based on known relationships between\npredictive features.\n  We propose GraphPINE, a graph neural network (GNN) architecture leveraging\ndomain-specific prior knowledge to initialize node importance optimized during\ntraining for drug response prediction. Typically, a manual post-prediction step\nexamines literature (i.e., prior knowledge) to understand returned predictive\nfeatures. While node importance can be obtained for gradient and attention\nafter prediction, node importance from these methods lacks complementary prior\nknowledge; GraphPINE seeks to overcome this limitation. GraphPINE differs from\nother GNN gating methods by utilizing an LSTM-like sequential format. We\nintroduce an importance propagation layer that unifies 1) updates for feature\nmatrix and node importance and 2) uses GNN-based graph propagation of feature\nvalues. This initialization and updating mechanism allows for informed feature\nlearning and improved graph representation.\n  We apply GraphPINE to cancer drug response prediction using drug screening\nand gene data collected for over 5,000 gene nodes included in a gene-gene graph\nwith a drug-target interaction (DTI) graph for initial importance. The\ngene-gene graph and DTIs were obtained from curated sources and weighted by\narticle count discussing relationships between drugs and genes. GraphPINE\nachieves a PR-AUC of 0.894 and ROC-AUC of 0.796 across 952 drugs. Code is\navailable at https://anonymous.4open.science/r/GraphPINE-40DE.",
        "published": "2025-04-07T19:42:12+00:00"
    },
    {
        "title": "Probing the Visualization Literacy of Vision Language Models: the Good, the Bad, and the Ugly",
        "authors": [
            "Lianghan Dong",
            "Anamaria Crisan"
        ],
        "summary": "Vision Language Models (VLMs) demonstrate promising chart comprehension\ncapabilities. Yet, prior explorations of their visualization literacy have been\nlimited to assessing their response correctness and fail to explore their\ninternal reasoning. To address this gap, we adapted attention-guided class\nactivation maps (AG-CAM) for VLMs, to visualize the influence and importance of\ninput features (image and text) on model responses. Using this approach, we\nconducted an examination of four open-source (ChartGemma, Janus 1B and 7B, and\nLLaVA) and two closed-source (GPT-4o, Gemini) models comparing their\nperformance and, for the open-source models, their AG-CAM results. Overall, we\nfound that ChartGemma, a 3B parameter VLM fine-tuned for chart\nquestion-answering (QA), outperformed other open-source models and exhibited\nperformance on par with significantly larger closed-source VLMs. We also found\nthat VLMs exhibit spatial reasoning by accurately localizing key chart\nfeatures, and semantic reasoning by associating visual elements with\ncorresponding data values and query tokens. Our approach is the first to\ndemonstrate the use of AG-CAM on early fusion VLM architectures, which are\nwidely used, and for chart QA. We also show preliminary evidence that these\nresults can align with human reasoning. Our promising open-source VLMs results\npave the way for transparent and reproducible research in AI visualization\nliteracy.",
        "published": "2025-04-07T19:16:56+00:00"
    },
    {
        "title": "Biomechanical Constraints Assimilation in Deep-Learning Image Registration: Application to sliding and locally rigid deformations",
        "authors": [
            "Ziad Kheil",
            "Soleakhena Ken",
            "Laurent Risser"
        ],
        "summary": "Regularization strategies in medical image registration often take a\none-size-fits-all approach by imposing uniform constraints across the entire\nimage domain. Yet biological structures are anything but regular. Lacking\nstructural awareness, these strategies may fail to consider a panoply of\nspatially inhomogeneous deformation properties, which would faithfully account\nfor the biomechanics of soft and hard tissues, especially in poorly contrasted\nstructures.\n  To bridge this gap, we propose a learning-based image registration approach\nin which the inferred deformation properties can locally adapt themselves to\ntrained biomechanical characteristics. Specifically, we first enforce in the\ntraining process local rigid displacements, shearing motions or pseudo-elastic\ndeformations using regularization losses inspired from the field of\nsolid-mechanics. We then show on synthetic and real 3D thoracic and abdominal\nimages that these mechanical properties of different nature are well\ngeneralized when inferring the deformations between new image pairs. Our\napproach enables neural-networks to infer tissue-specific deformation patterns\ndirectly from input images, ensuring mechanically plausible motion. These\nnetworks preserve rigidity within hard tissues while allowing controlled\nsliding in regions where tissues naturally separate, more faithfully capturing\nphysiological motion. The code is publicly available at\nhttps://github.com/Kheil-Z/biomechanical_DLIR .",
        "published": "2025-04-07T19:12:51+00:00"
    },
    {
        "title": "Diffusion-based Models for Unpaired Super-resolution in Fluid Dynamics",
        "authors": [
            "Wuzhe Xu",
            "Yulong Lu",
            "Lian shen",
            "Anqing Xuan",
            "Ali Barzegari"
        ],
        "summary": "High-fidelity, high-resolution numerical simulations are crucial for studying\ncomplex multiscale phenomena in fluid dynamics, such as turbulent flows and\nocean waves. However, direct numerical simulations with high-resolution solvers\nare computationally prohibitive. As an alternative, super-resolution techniques\nenable the enhancement of low-fidelity, low-resolution simulations. However,\ntraditional super-resolution approaches rely on paired low-fidelity,\nlow-resolution and high-fidelity, high-resolution datasets for training, which\nare often impossible to acquire in complex flow systems. To address this\nchallenge, we propose a novel two-step approach that eliminates the need for\npaired datasets. First, we perform unpaired domain translation at the\nlow-resolution level using an Enhanced Denoising Diffusion Implicit Bridge.\nThis process transforms low-fidelity, low-resolution inputs into high-fidelity,\nlow-resolution outputs, and we provide a theoretical analysis to highlight the\nadvantages of this enhanced diffusion-based approach. Second, we employ the\ncascaded Super-Resolution via Repeated Refinement model to upscale the\nhigh-fidelity, low-resolution prediction to the high-resolution result. We\ndemonstrate the effectiveness of our approach across three fluid dynamics\nproblems. Moreover, by incorporating a neural operator to learn system\ndynamics, our method can be extended to improve evolutionary simulations of\nlow-fidelity, low-resolution data.",
        "published": "2025-04-07T19:08:28+00:00"
    },
    {
        "title": "Broadcast via Mobile Agents in a Dynamic Network: Interplay of Graph Properties & Agents",
        "authors": [
            "William K. Moses Jr.",
            "Amanda Redlich",
            "Frederick Stock"
        ],
        "summary": "In this paper, we revisit the problem of \\textsc{Broadcast}, introduced by\nDas, Giachoudis, Luccio, and Markou [OPODIS, 2020], where $k+1$ agents are\ninitially placed on an $n$ node dynamic graph, where $1$ agent has a message\nthat must be broadcast to the remaining $k$ ignorant agents. The original paper\nstudied the relationship between the number of agents needed to solve the\nproblem and the edge density of the graph. The paper presented strong evidence\nthat edge density of a graph, or the number of redundant edges within the\ngraph, may be the correct graph property to accurately differentiate whether\n$k= o(n)$ agents (low edge density) or $k = \\Omega(n)$ agents (high edge\ndensity) are needed to solve the problem.\n  In this paper, we show that surprisingly, edge density may not in fact be the\ncorrect differentiating property. The original paper presents graphs with edge\ndensity $1.1\\overline{6}$ that require $\\Omega(n)$ agents, however, we\nconstruct graphs with edge density $> 1.1\\overline{6}$ and develop an algorithm\nto solve the problem on those graphs using only $o(n)$ agents. We subsequently\nshow that the relationship between edge density and number of agents is fairly\nweak by first constructing graphs with edge density tending to $1$ from above\nthat require $\\Omega(n/f(n))$ agents to solve, for any function $f(n) \\to\n\\infty$ as $n \\to \\infty$. We then construct an infinite family of graphs with\nedge density $< \\rho$ requiring exactly $k$ ignorant agents to solve\n\\textsc{Broadcast}, for any $k>0$ and $\\rho>1$.",
        "published": "2025-04-07T19:07:24+00:00"
    },
    {
        "title": "Data-Driven Molecular Dynamics and TEM Analysis of Platinum Crystal Growth on Graphene and Reactive Hydrogen-Sensing Dynamics",
        "authors": [
            "Akram Ibrahim",
            "Ahmed M. Hafez",
            "Mahmooda Sultana",
            "Can Ataca"
        ],
        "summary": "Pt-functionalized graphene harnesses graphene's exceptional carrier mobility\nwith Pt's catalytic activity for hydrogen sensing, yet the mechanisms of Pt\ncrystal growth, its interaction with graphene, and the consequent impact on\nhydrogen sensitivity remain incompletely understood. We develop a high-fidelity\nequivariant machine-learned interatomic potential (MLIP) to perform large-scale\nmolecular dynamics (MD) simulations with near-density functional theory (DFT)\naccuracy. Our simulations capture key growth stages-including Pt nucleation,\ncoalescence, and the formation of either polycrystalline clusters or epitaxial\nthin films-under varying deposition loadings and rates. Transmission electron\nmicroscopy and Raman measurements validate the predicted morphologies,\nrevealing small approximately spherical clusters at lower Pt loadings that\nevolve into slightly thicker, more planar domains with increased loading.\nReactive MD shows hydrogen dissociates predominantly on Pt nanostructures at\nroom temperature, with minimal spillover onto pristine graphene. Furthermore,\nhydrogen uptake increases with Pt loading at a diminishing rate, while reaction\nkinetics are significantly faster at lower coverages and rapidly decline with\nincreasing loading. DFT calculations indicate undercoordinated Pt clusters\ninduce $n$-type doping in graphene, which is diminished when hydrogen\nadsorption depletes Pt electron density, thereby transducing the adsorption\nevents from Pt-surfaces to the Pt-graphene interface. By correlating deposition\nconditions, nanostructure morphology, and hydrogen sensing dynamics, our\nfindings suggest that moderate Pt loadings can effectively balance sufficient\ndoping with a pronounced Pt-mediated electronic response to graphene. These\ninsights underscore the importance of combining DFT and MLIP simulations with\nexperiments to guide next-generation chemiresistive gas sensor design.",
        "published": "2025-04-07T19:02:18+00:00"
    },
    {
        "title": "Survey on Algorithms for multi-index models",
        "authors": [
            "Joan Bruna",
            "Daniel Hsu"
        ],
        "summary": "We review the literature on algorithms for estimating the index space in a\nmulti-index model. The primary focus is on computationally efficient\n(polynomial-time) algorithms in Gaussian space, the assumptions under which\nconsistency is guaranteed by these methods, and their sample complexity. In\nmany cases, a gap is observed between the sample complexity of the best known\ncomputationally efficient methods and the information-theoretical minimum. We\nalso review algorithms based on estimating the span of gradients using\nnonparametric methods, and algorithms based on fitting neural networks using\ngradient descent",
        "published": "2025-04-07T18:50:11+00:00"
    },
    {
        "title": "Safe Automated Refactoring for Efficient Migration of Imperative Deep Learning Programs to Graph Execution",
        "authors": [
            "Raffi Khatchadourian",
            "Tatiana Castro V\u00e9lez",
            "Mehdi Bagherzadeh",
            "Nan Jia",
            "Anita Raja"
        ],
        "summary": "Efficiency is essential to support responsiveness w.r.t. ever-growing\ndatasets, especially for Deep Learning (DL) systems. DL frameworks have\ntraditionally embraced deferred execution-style DL code -- supporting symbolic,\ngraph-based Deep Neural Network (DNN) computation. While scalable, such\ndevelopment is error-prone, non-intuitive, and difficult to debug.\nConsequently, more natural, imperative DL frameworks encouraging eager\nexecution have emerged at the expense of run-time performance. Though hybrid\napproaches aim for the \"best of both worlds,\" using them effectively requires\nsubtle considerations to make code amenable to safe, accurate, and efficient\ngraph execution. We present an automated refactoring approach that assists\ndevelopers in specifying whether their otherwise eagerly-executed imperative DL\ncode could be reliably and efficiently executed as graphs while preserving\nsemantics. The approach, based on a novel imperative tensor analysis,\nautomatically determines when it is safe and potentially advantageous to\nmigrate imperative DL code to graph execution. The approach is implemented as a\nPyDev Eclipse IDE plug-in that integrates the WALA Ariadne analysis framework\nand evaluated on 19 Python projects consisting of 132.05 KLOC. We found that\n326 of 766 candidate functions (42.56%) were refactorable, and an average\nspeedup of 2.16 on performance tests was observed. The results indicate that\nthe approach is useful in optimizing imperative DL code to its full potential.",
        "published": "2025-04-07T18:48:43+00:00"
    },
    {
        "title": "Evolving Financial Trading Strategies with Vectorial Genetic Programming",
        "authors": [
            "Rui Menoita",
            "Sara Silva"
        ],
        "summary": "Establishing profitable trading strategies in financial markets is a\nchallenging task. While traditional methods like technical analysis have long\nserved as foundational tools for traders to recognize and act upon market\npatterns, the evolving landscape has called for more advanced techniques. We\nexplore the use of Vectorial Genetic Programming (VGP) for this task,\nintroducing two new variants of VGP, one that allows operations with complex\nnumbers and another that implements a strongly-typed version of VGP. We\nevaluate the different variants on three financial instruments, with datasets\nspanning more than seven years. Despite the inherent difficulty of this task,\nit was possible to evolve profitable trading strategies. A comparative analysis\nof the three VGP variants and standard GP revealed that standard GP is always\namong the worst whereas strongly-typed VGP is always among the best.",
        "published": "2025-04-07T18:41:31+00:00"
    },
    {
        "title": "Less but Better: Parameter-Efficient Fine-Tuning of Large Language Models for Personality Detection",
        "authors": [
            "Lingzhi Shen",
            "Yunfei Long",
            "Xiaohao Cai",
            "Guanming Chen",
            "Imran Razzak",
            "Shoaib Jameel"
        ],
        "summary": "Personality detection automatically identifies an individual's personality\nfrom various data sources, such as social media texts. However, as the\nparameter scale of language models continues to grow, the computational cost\nbecomes increasingly difficult to manage. Fine-tuning also grows more complex,\nmaking it harder to justify the effort and reliably predict outcomes. We\nintroduce a novel parameter-efficient fine-tuning framework, PersLLM, to\naddress these challenges. In PersLLM, a large language model (LLM) extracts\nhigh-dimensional representations from raw data and stores them in a dynamic\nmemory layer. PersLLM then updates the downstream layers with a replaceable\noutput network, enabling flexible adaptation to various personality detection\nscenarios. By storing the features in the memory layer, we eliminate the need\nfor repeated complex computations by the LLM. Meanwhile, the lightweight output\nnetwork serves as a proxy for evaluating the overall effectiveness of the\nframework, improving the predictability of results. Experimental results on key\nbenchmark datasets like Kaggle and Pandora show that PersLLM significantly\nreduces computational cost while maintaining competitive performance and strong\nadaptability.",
        "published": "2025-04-07T18:30:39+00:00"
    },
    {
        "title": "TRATSS: Transformer-Based Task Scheduling System for Autonomous Vehicles",
        "authors": [
            "Yazan Youssef",
            "Paulo Ricardo Marques de Araujo",
            "Aboelmagd Noureldin",
            "Sidney Givigi"
        ],
        "summary": "Efficient scheduling remains a critical challenge in various domains,\nrequiring solutions to complex NP-hard optimization problems to achieve optimal\nresource allocation and maximize productivity. In this paper, we introduce a\nframework called Transformer-Based Task Scheduling System (TRATSS), designed to\naddress the intricacies of single agent scheduling in graph-based environments.\nBy integrating the latest advancements in reinforcement learning and\ntransformer architecture, TRATSS provides a novel system that outputs optimized\ntask scheduling decisions while dynamically adapting to evolving task\nrequirements and resource availability. Leveraging the self-attention mechanism\nin transformers, TRATSS effectively captures complex task dependencies, thereby\nproviding solutions with enhanced resource utilization and task completion\nefficiency. Experimental evaluations on benchmark datasets demonstrate TRATSS's\neffectiveness in providing high-quality solutions to scheduling problems that\ninvolve multiple action profiles.",
        "published": "2025-04-07T18:23:13+00:00"
    },
    {
        "title": "A Novel Approach to Linking Histology Images with DNA Methylation",
        "authors": [
            "Manahil Raza",
            "Muhammad Dawood",
            "Talha Qaiser",
            "Nasir M. Rajpoot"
        ],
        "summary": "DNA methylation is an epigenetic mechanism that regulates gene expression by\nadding methyl groups to DNA. Abnormal methylation patterns can disrupt gene\nexpression and have been linked to cancer development. To quantify DNA\nmethylation, specialized assays are typically used. However, these assays are\noften costly and have lengthy processing times, which limits their widespread\navailability in routine clinical practice. In contrast, whole slide images\n(WSIs) for the majority of cancer patients can be more readily available. As\nsuch, given the ready availability of WSIs, there is a compelling need to\nexplore the potential relationship between WSIs and DNA methylation patterns.\nTo address this, we propose an end-to-end graph neural network based weakly\nsupervised learning framework to predict the methylation state of gene groups\nexhibiting coherent patterns across samples. Using data from three cohorts from\nThe Cancer Genome Atlas (TCGA) - TCGA-LGG (Brain Lower Grade Glioma), TCGA-GBM\n(Glioblastoma Multiforme) ($n$=729) and TCGA-KIRC (Kidney Renal Clear Cell\nCarcinoma) ($n$=511) - we demonstrate that the proposed approach achieves\nsignificantly higher AUROC scores than the state-of-the-art (SOTA) methods, by\nmore than $20\\%$. We conduct gene set enrichment analyses on the gene groups\nand show that majority of the gene groups are significantly enriched in\nimportant hallmarks and pathways. We also generate spatially enriched heatmaps\nto further investigate links between histological patterns and DNA methylation\nstates. To the best of our knowledge, this is the first study that explores\nassociation of spatially resolved histological patterns with gene group\nmethylation states across multiple cancer types using weakly supervised deep\nlearning.",
        "published": "2025-04-07T18:19:01+00:00"
    },
    {
        "title": "CRDT Emulation, Simulation, and Representation Independence",
        "authors": [
            "Nathan Liittschwager",
            "Jonathan Castello",
            "Stelios Tsampas",
            "Lindsey Kuper"
        ],
        "summary": "Conflict-free replicated data types (CRDTs) are distributed data structures\ndesigned for fault tolerance and high availability. CRDTs can be taxonomized\ninto state-based CRDTs, in which replicas apply updates locally and\nperiodically broadcast their local state to other replicas, and operation-based\n(op-based) CRDTs, in which every state-updating operation is individually\nbroadcast and applied at each replica. In the literature, state-based and\nop-based CRDTs are considered equivalent due to the existence of algorithms\nthat transform one kind of CRDT into the other. In particular, verification\ntechniques and results for one kind of CRDT are often said to be applicable to\nthe other kind, thanks to this equivalence. However, what it means for\nstate-based and op-based CRDTs to emulate each other has never been made fully\nprecise. In particular, emulation is nontrivial since state-based and op-based\nCRDTs place different requirements on the behavior of the underlying network\nwith regard to both the causal ordering of message delivery, and the\ngranularity of the messages themselves.\n  In this paper, we specify and formalize CRDT emulation in terms of simulation\nby modeling CRDTs and their interactions with the network as formal transition\nsystems. We show that emulation can be understood as weak simulations between\nthe transition systems of the original and emulating CRDT systems, thus closing\na gap in the CRDT literature. We precisely characterize which properties of\nCRDT systems are preserved by our weak simulations, and therefore which\nproperties can be said to be applicable to state-based CRDTs as long as they\nare applicable to op-based CRDTs and vice versa. Finally, we leverage our\nemulation results to obtain a general representation independence result for\nCRDTs: intuitively, clients of a CRDT cannot tell whether they are interacting\nwith a state-based or op-based CRDT in particular.",
        "published": "2025-04-07T18:11:53+00:00"
    },
    {
        "title": "Physics-informed Modularized Neural Network for Advanced Building Control by Deep Reinforcement Learning",
        "authors": [
            "Zixin Jiang",
            "Xuezheng Wang",
            "Bing Dong"
        ],
        "summary": "Physics-informed machine learning (PIML) provides a promising solution for\nbuilding energy modeling and can serve as a virtual environment to enable\nreinforcement learning (RL) agents to interact and learn. However, challenges\nremain in efficiently integrating physics priors, evaluating the effectiveness\nof physics constraints, balancing model accuracy and physics consistency, and\nenabling real-world implementation. To address these gaps, this study\nintroduces a Physics-Informed Modularized Neural Network (PI-ModNN), which\nincorporates physics priors through a physics-informed model structure, loss\nfunctions, and hard constraints. A new evaluation metric called \"temperature\nresponse violation\" is developed to quantify the physical consistency of\ndata-driven building dynamic models under varying control inputs and training\ndata sizes. Additionally, a physics prior evaluation framework based on rule\nimportance is proposed to assess the contribution of each individual physics\nprior, offering guidance on selecting appropriate PIML techniques. Results\nindicate that incorporating physical priors does not always improve model\nperformance; inappropriate priors may decrease model accuracy and consistency.\nHowever, hard constraints are effective in enforcing model consistency.\nFurthermore, we present a general workflow for developing control-oriented PIML\nmodels and integrating them with deep reinforcement learning (DRL). Following\nthis framework, a case study implementing DRL in an office space over three\nmonths demonstrates potential energy savings of 31.4%. Finally, we provide a\ngeneral guideline for integrating data-driven models with advanced building\ncontrol through a four-step evaluation framework, paving the way for reliable\nand scalable deployment of advanced building controls.",
        "published": "2025-04-07T18:07:54+00:00"
    },
    {
        "title": "Benchmarking vibrational spectra: 5000 accurate eigenstates of acetonitrile using tree tensor network states",
        "authors": [
            "Henrik R. Larsson"
        ],
        "summary": "Accurate vibrational spectra are essential for understanding how molecules\nbehave, yet their computation remains challenging and benchmark data to\nreliable compare different methods are sparse. Here, we present high-accuracy\neigenstate computations for the six-atom, 12-dimensional acetonitrile molecule,\na prototypical, strongly coupled, anharmonic system. Using a density matrix\nrenormalization group (DMRG) algorithm with a tree-tensor-network-state (TTNS)\nansatz, a refinement using TTNSs as basis set, and reliable procedures to\nestimate energy errors, we compute up to 5,000 vibrational states with error\nestimates below 0.0007 $\\mathrm{cm}^{-1}$. Our analysis reveals that previous\nworks underestimated the energy error by up to two orders of magnitude. Our\ndata serve as a benchmark for future vibrational spectroscopy methods and our\nnew method offers a path toward similarly precise computations of large,\ncomplex molecular systems.",
        "published": "2025-04-07T18:00:02+00:00"
    },
    {
        "title": "SmolVLM: Redefining small and efficient multimodal models",
        "authors": [
            "Andr\u00e9s Marafioti",
            "Orr Zohar",
            "Miquel Farr\u00e9",
            "Merve Noyan",
            "Elie Bakouch",
            "Pedro Cuenca",
            "Cyril Zakka",
            "Loubna Ben Allal",
            "Anton Lozhkov",
            "Nouamane Tazi",
            "Vaibhav Srivastav",
            "Joshua Lochner",
            "Hugo Larcher",
            "Mathieu Morlon",
            "Lewis Tunstall",
            "Leandro von Werra",
            "Thomas Wolf"
        ],
        "summary": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
        "published": "2025-04-07T17:58:57+00:00"
    },
    {
        "title": "One-Minute Video Generation with Test-Time Training",
        "authors": [
            "Karan Dalal",
            "Daniel Koceja",
            "Gashon Hussein",
            "Jiarui Xu",
            "Yue Zhao",
            "Youjin Song",
            "Shihao Han",
            "Ka Chun Cheung",
            "Jan Kautz",
            "Carlos Guestrin",
            "Tatsunori Hashimoto",
            "Sanmi Koyejo",
            "Yejin Choi",
            "Yu Sun",
            "Xiaolong Wang"
        ],
        "summary": "Transformers today still struggle to generate one-minute videos because\nself-attention layers are inefficient for long context. Alternatives such as\nMamba layers struggle with complex multi-scene stories because their hidden\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\nwhose hidden states themselves can be neural networks, therefore more\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\ngenerate one-minute videos from text storyboards. For proof of concept, we\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\npoints in a human evaluation of 100 videos per method. Although promising,\nresults still contain artifacts, likely due to the limited capability of the\npre-trained 5B model. The efficiency of our implementation can also be\nimproved. We have only experimented with one-minute videos due to resource\nconstraints, but the approach can be extended to longer videos and more complex\nstories. Sample videos, code and annotations are available at:\nhttps://test-time-training.github.io/video-dit",
        "published": "2025-04-07T17:56:31+00:00"
    },
    {
        "title": "Status Updating with Time Stamp Errors",
        "authors": [
            "Md Nurul Absar Siddiky",
            "Ahmed Arafa"
        ],
        "summary": "A status updating system is considered in which multiple processes are\nsampled and transmitted through a shared channel. Each process has its\ndedicated server that processes its samples before time stamping them for\ntransmission. Time stamps, however, are prone to errors, and hence the status\nupdates received may not be credible. Our setting models the time stamp error\nrate as a function of the servers' busy times. Hence, to reduce errors and\nenhance credibility, servers need to process samples on a relatively prolonged\nschedule. This, however, deteriorates timeliness, which is captured through the\nage of information (AoI) metric. An optimization problem is formulated whose\ngoal to characterize the optimal processes' schedule and sampling instances to\nachieve the optimal trade-off between timeliness and credibility. The problem\nis first solved for a single process setting, where it is shown that a\nthreshold-based sleep-wake schedule is optimal, in which the server wakes up\nand is allowed to process newly incoming samples only if the AoI surpasses a\ncertain threshold that depends on the required timeliness-credibility\ntrade-off. Such insights are then extended to the multi-process setting, where\ntwo main scheduling and sleep-wake policies, namely round-robin scheduling with\nthreshold-waiting and asymmetric scheduling with zero-waiting, are introduced\nand analyzed.",
        "published": "2025-04-07T17:52:52+00:00"
    },
    {
        "title": "UK APAP R-matrix electron-impact excitation cross-sections for modelling laboratory and astrophysical plasma",
        "authors": [
            "G. Del Zanna",
            "G. Liang",
            "J. Mao",
            "N. R. Badnell"
        ],
        "summary": "Systematic R-matrix calculations of electron-impact excitation for ions of\nastrophysical interest have been performed since 2007 for many iso-electronic\nsequences as part of the UK Atomic Process for Astrophysical Plasma (APAP)\nnetwork. Rate coefficients for Maxwellian electron distributions have been\nprovided and used extensively in the literature and many databases for\nastrophysics. Here, we provide averaged collision strengths to be used to model\nplasma where electrons are non-Maxwellian, which often occur in laboratory and\nastrophysical plasma. We also provide for many ions new Maxwellian-averaged\ncollision strengths which include important corrections to the published\nvalues. The H- and He-like atomic data were recently made available in\nMao+(2022). Here, we provide data for ions of the Li-, Be-, B-, C-, N-, O-,\nNe-, Na-, and Mg-like sequences.",
        "published": "2025-04-07T17:37:09+00:00"
    },
    {
        "title": "FERIVer: An FPGA-assisted Emulated Framework for RTL Verification of RISC-V Processors",
        "authors": [
            "Kun Qin",
            "Xiaorang Guo",
            "Martin Schulz",
            "Carsten Trinitis"
        ],
        "summary": "Processor design and verification require a synergistic approach that\ncombines instruction-level functional simulations with precise hardware\nemulations. The trade-off between speed and accuracy in the instruction set\nsimulation poses a significant challenge to the efficiency of processor\nverification. By tapping the potentials of Field Programmable Gate Arrays\n(FPGAs), we propose an FPGA-assisted System-on-Chip (SoC) platform that\nfacilitates cross-verification by the embedded CPU and the synthesized hardware\nin the programmable fabrics. This method accelerates the verification of the\nRISC-V Instruction Set Architecture (ISA) processor at a speed of 5 million\ninstructions per second (MIPS), which is 150x faster than the vendor-specific\ntool (Xilinx XSim) and a 35x boost to the state-of-the-art open-source\nverification setup (Verilator). With less than 7\\% hardware occupation on Zynq\n7000 FPGA, the proposed framework enables flexible verification with high time\nand cost efficiency for exploring RISC-V instruction set architectures.",
        "published": "2025-04-07T17:34:05+00:00"
    },
    {
        "title": "Estimation of Heat Transfer Coefficient in Heat Exchangers from closed-loop data using Neural Networks",
        "authors": [
            "Ramachandran Anantharaman",
            "Carlos Gonzalez Rojas",
            "Luna Artemis van Leeuwen",
            "Leyla \u00d6zkan"
        ],
        "summary": "Heat exchangers (HEXs) play a central role in process industries for thermal\nenergy transfer. Fouling, the gradual accumulation of solids on heat transfer\nsurfaces, causes a time-varying decrease in the overall heat transfer\ncoefficient (U(t)), significantly impacting the efficiency of heat transfer.\nGood estimation and modeling of fouling (the heat transfer coefficient) will\nlead to better fouling mitigation strategies. This study investigates the\nidentifiability of the time-varying $U(t)$ in HEXs from closed-loop operational\ndata, without external excitation of reference signals or knowledge of the\ncontroller parameters. We establish that while the complete system model cannot\nbe identified under these given constraints, the time-varying heat transfer\ncoefficient $U(t)$ remains identifiable. Further, we propose a neural network\nbased architecture, called (Per-PINN), for estimation and modeling the heat\ntransfer coefficient from the closed-loop system data. This Per-PINN model is\nshown to perform better than the existing Physics-Informed Neural Networks\n(PINN) based models for inverse parameter learning as it inherently fixes the\nunderlying physical equations and learns only the time-varying parameter U(t).",
        "published": "2025-04-07T17:31:46+00:00"
    },
    {
        "title": "AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point Detection for Accurate Characterization of Anomalous Diffusion in Video Data",
        "authors": [
            "Yusef Ahsini",
            "Marc Escoto",
            "J. Alberto Conejero"
        ],
        "summary": "Anomalous diffusion occurs in a wide range of systems, including protein\ntransport within cells, animal movement in complex habitats, pollutant\ndispersion in groundwater, and nanoparticle motion in synthetic materials.\nAccurately estimating the anomalous diffusion exponent and the diffusion\ncoefficient from the particle trajectories is essential to distinguish between\nsub-diffusive, super-diffusive, or normal diffusion regimes. These estimates\nprovide a deeper insight into the underlying dynamics of the system,\nfacilitating the identification of particle behaviors and the detection of\nchanges in diffusion states. However, analyzing short and noisy video data,\nwhich often yield incomplete and heterogeneous trajectories, poses a\nsignificant challenge for traditional statistical approaches. We introduce a\ndata-driven method that integrates particle tracking, an attention\n  U-Net architecture, and a change-point detection algorithm to address these\nissues. This approach not only infers the anomalous diffusion parameters with\nhigh accuracy but also identifies temporal transitions between different\nstates, even in the presence of noise and limited temporal resolution. Our\nmethodology demonstrated strong performance in the 2nd Anomalous Diffusion\n(AnDi) Challenge benchmark within the top submissions for video tasks.",
        "published": "2025-04-07T17:08:17+00:00"
    },
    {
        "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models",
        "authors": [
            "Yang Yan",
            "Yu Lu",
            "Renjun Xu",
            "Zhenzhong Lan"
        ],
        "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\n\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to $\\leq$7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.",
        "published": "2025-04-07T16:57:10+00:00"
    },
    {
        "title": "PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks",
        "authors": [
            "Marius Almanst\u00f6tter",
            "Roman Vetter",
            "Dagmar Iber"
        ],
        "summary": "Parameter estimation for differential equations from measured data is an\ninverse problem prevalent across quantitative sciences. Physics-Informed Neural\nNetworks (PINNs) have emerged as effective tools for solving such problems,\nespecially with sparse measurements and incomplete system information. However,\nPINNs face convergence issues, stability problems, overfitting, and complex\nloss function design. Here we introduce PINNverse, a training paradigm that\naddresses these limitations by reformulating the learning process as a\nconstrained differential optimization problem. This approach achieves a dynamic\nbalance between data loss and differential equation residual loss during\ntraining while preventing overfitting. PINNverse combines the advantages of\nPINNs with the Modified Differential Method of Multipliers to enable\nconvergence on any point on the Pareto front. We demonstrate robust and\naccurate parameter estimation from noisy data in four classical ODE and PDE\nmodels from physics and biology. Our method enables accurate parameter\ninference also when the forward problem is expensive to solve.",
        "published": "2025-04-07T16:34:57+00:00"
    },
    {
        "title": "Embedded Federated Feature Selection with Dynamic Sparse Training: Balancing Accuracy-Cost Tradeoffs",
        "authors": [
            "Afsaneh Mahanipour",
            "Hana Khamfroush"
        ],
        "summary": "Federated Learning (FL) enables multiple resource-constrained edge devices\nwith varying levels of heterogeneity to collaboratively train a global model.\nHowever, devices with limited capacity can create bottlenecks and slow down\nmodel convergence. One effective approach to addressing this issue is to use an\nefficient feature selection method, which reduces overall resource demands by\nminimizing communication and computation costs, thereby mitigating the impact\nof struggling nodes. Existing federated feature selection (FFS) methods are\neither considered as a separate step from FL or rely on a third party. These\napproaches increase computation and communication overhead, making them\nimpractical for real-world high-dimensional datasets. To address this, we\npresent \\textit{Dynamic Sparse Federated Feature Selection} (DSFFS), the first\ninnovative embedded FFS that is efficient in both communication and\ncomputation. In the proposed method, feature selection occurs simultaneously\nwith model training. During training, input-layer neurons, their connections,\nand hidden-layer connections are dynamically pruned and regrown, eliminating\nuninformative features. This process enhances computational efficiency on\ndevices, improves network communication efficiency, and boosts global model\nperformance. Several experiments are conducted on nine real-world datasets of\nvarying dimensionality from diverse domains, including biology, image, speech,\nand text. The results under a realistic non-iid data distribution setting show\nthat our approach achieves a better trade-off between accuracy, computation,\nand communication costs by selecting more informative features compared to\nother state-of-the-art FFS methods.",
        "published": "2025-04-07T16:33:05+00:00"
    },
    {
        "title": "Federated Learning for Medical Image Classification: A Comprehensive Benchmark",
        "authors": [
            "Zhekai Zhou",
            "Guibo Luo",
            "Mingzhi Chen",
            "Zhenyu Weng",
            "Yuesheng Zhu"
        ],
        "summary": "The federated learning paradigm is wellsuited for the field of medical image\nanalysis, as it can effectively cope with machine learning on isolated\nmulticenter data while protecting the privacy of participating parties.\nHowever, current research on optimization algorithms in federated learning\noften focuses on limited datasets and scenarios, primarily centered around\nnatural images, with insufficient comparative experiments in medical contexts.\nIn this work, we conduct a comprehensive evaluation of several state-of-the-art\nfederated learning algorithms in the context of medical imaging. We conduct a\nfair comparison of classification models trained using various federated\nlearning algorithms across multiple medical imaging datasets. Additionally, we\nevaluate system performance metrics, such as communication cost and\ncomputational efficiency, while considering different federated learning\narchitectures. Our findings show that medical imaging datasets pose substantial\nchallenges for current federated learning optimization algorithms. No single\nalgorithm consistently delivers optimal performance across all medical\nfederated learning scenarios, and many optimization algorithms may underperform\nwhen applied to these datasets. Our experiments provide a benchmark and\nguidance for future research and application of federated learning in medical\nimaging contexts. Furthermore, we propose an efficient and robust method that\ncombines generative techniques using denoising diffusion probabilistic models\nwith label smoothing to augment datasets, widely enhancing the performance of\nfederated learning on classification tasks across various medical imaging\ndatasets. Our code will be released on GitHub, offering a reliable and\ncomprehensive benchmark for future federated learning studies in medical\nimaging.",
        "published": "2025-04-07T16:22:18+00:00"
    },
    {
        "title": "IAEmu: Learning Galaxy Intrinsic Alignment Correlations",
        "authors": [
            "Sneh Pandya",
            "Yuanyuan Yang",
            "Nicholas Van Alfen",
            "Jonathan Blazek",
            "Robin Walters"
        ],
        "summary": "The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing\nanalyses, arise from correlations in galaxy shapes driven by tidal interactions\nand galaxy formation processes. Accurate IA modeling is essential for robust\ncosmological inference, but current approaches rely on perturbative methods\nthat break down on nonlinear scales or on expensive simulations. We introduce\nIAEmu, a neural network-based emulator that predicts the galaxy\nposition-position ($\\xi$), position-orientation ($\\omega$), and\norientation-orientation ($\\eta$) correlation functions and their uncertainties\nusing mock catalogs based on the halo occupation distribution (HOD) framework.\nCompared to simulations, IAEmu achieves ~3% average error for $\\xi$ and ~5% for\n$\\omega$, while capturing the stochasticity of $\\eta$ without overfitting. The\nemulator provides both aleatoric and epistemic uncertainties, helping identify\nregions where predictions may be less reliable. We also demonstrate\ngeneralization to non-HOD alignment signals by fitting to IllustrisTNG\nhydrodynamical simulation data. As a fully differentiable neural network, IAEmu\nenables $\\sim$10,000$\\times$ speed-ups in mapping HOD parameters to correlation\nfunctions on GPUs, compared to CPU-based simulations. This acceleration\nfacilitates inverse modeling via gradient-based sampling, making IAEmu a\npowerful surrogate model for galaxy bias and IA studies with direct\napplications to Stage IV weak lensing surveys.",
        "published": "2025-04-07T16:19:50+00:00"
    },
    {
        "title": "Physics-Informed Neural Networks for One-Dimensional Quantum Well Problems",
        "authors": [
            "Soumyadip Sarkar"
        ],
        "summary": "We implement physics-informed neural networks (PINNs) to solve the\ntime-independent Schr\\\"odinger equation for three canonical one-dimensional\nquantum potentials: an infinite square well, a finite square well, and a finite\nbarrier. The PINN models incorporate trial wavefunctions that exactly satisfy\nboundary conditions (Dirichlet zeros at domain boundaries), and they optimize a\nloss functional combining the PDE residual with a normalization constraint. For\nthe infinite well, the ground-state energy is known (E = pi^2 in dimensionless\nunits) and held fixed in training, whereas for the finite well and barrier, the\neigenenergy is treated as a trainable parameter. We use fully-connected neural\nnetworks with smooth activation functions to represent the wavefunction and\ndemonstrate that PINNs can learn the ground-state eigenfunctions and\neigenvalues for these quantum systems. The results show that the PINN-predicted\nwavefunctions closely match analytical solutions or expected behaviors, and the\nlearned eigenenergies converge to known values. We present training logs and\nconvergence of the energy parameter, as well as figures comparing the PINN\nsolutions to exact results. The discussion addresses the performance of PINNs\nrelative to traditional numerical methods, highlighting challenges such as\nconvergence to the correct eigenvalue, sensitivity to initialization, and the\ndifficulty of modeling discontinuous potentials. We also discuss the importance\nof the normalization term to resolve the scaling ambiguity of the wavefunction.\nFinally, we conclude that PINNs are a viable approach for quantum eigenvalue\nproblems, and we outline future directions including extensions to\nhigher-dimensional and time-dependent Schr\\\"odinger equations.",
        "published": "2025-04-07T16:18:26+00:00"
    },
    {
        "title": "Security Risks in Vision-Based Beam Prediction: From Spatial Proxy Attacks to Feature Refinement",
        "authors": [
            "Avi Deb Raha",
            "Kitae Kim",
            "Mrityunjoy Gain",
            "Apurba Adhikary",
            "Zhu Han",
            "Eui-Nam Huh",
            "Choong Seon Hong"
        ],
        "summary": "The rapid evolution towards the sixth-generation (6G) networks demands\nadvanced beamforming techniques to address challenges in dynamic, high-mobility\nscenarios, such as vehicular communications. Vision-based beam prediction\nutilizing RGB camera images emerges as a promising solution for accurate and\nresponsive beam selection. However, reliance on visual data introduces unique\nvulnerabilities, particularly susceptibility to adversarial attacks, thus\npotentially compromising beam accuracy and overall network reliability. In this\npaper, we conduct the first systematic exploration of adversarial threats\nspecifically targeting vision-based mmWave beam selection systems. Traditional\nwhite-box attacks are impractical in this context because ground-truth beam\nindices are inaccessible and spatial dynamics are complex. To address this, we\npropose a novel black-box adversarial attack strategy, termed Spatial Proxy\nAttack (SPA), which leverages spatial correlations between user positions and\nbeam indices to craft effective perturbations without requiring access to model\nparameters or labels. To counteract these adversarial vulnerabilities, we\nformulate an optimization framework aimed at simultaneously enhancing beam\nselection accuracy under clean conditions and robustness against adversarial\nperturbations. We introduce a hybrid deep learning architecture integrated with\na dedicated Feature Refinement Module (FRM), designed to systematically filter\nirrelevant, noisy and adversarially perturbed visual features. Evaluations\nusing standard backbone models such as ResNet-50 and MobileNetV2 demonstrate\nthat our proposed method significantly improves performance, achieving up to an\n+21.07\\% gain in Top-K accuracy under clean conditions and a 41.31\\% increase\nin Top-1 adversarial robustness compared to different baseline models.",
        "published": "2025-04-07T16:08:32+00:00"
    },
    {
        "title": "Post-Training Language Models for Continual Relation Extraction",
        "authors": [
            "Sefika Efeoglu",
            "Adrian Paschke",
            "Sonja Schimmler"
        ],
        "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction.",
        "published": "2025-04-07T16:01:22+00:00"
    },
    {
        "title": "Bayesian estimation of causal effects from observational categorical data",
        "authors": [
            "Vera Kvisgaard",
            "Johan Pensar"
        ],
        "summary": "We present a Bayesian procedure for estimation of pairwise intervention\neffects in a high-dimensional system of categorical variables. We assume that\nwe have observational data generated from an unknown causal Bayesian network\nfor which there are no latent confounders. Most of the existing methods\ndeveloped for this setting assume that the underlying model is linear Gaussian,\nincluding the Bayesian IDA (BIDA) method that we build upon in this work. By\ncombining a Bayesian backdoor estimator with model averaging, we obtain a\nposterior over the intervention distributions of a cause-effect pair that can\nbe expressed as a mixture over stochastic linear combinations of Dirichlet\ndistributions. Although there is no closed-form expression for the posterior\ndensity, it is straightforward to produce Monte Carlo approximations of target\nquantities through direct sampling, and we also derive closed-form expressions\nfor a few selected moments. To scale up the proposed procedure, we employ\nMarkov Chain Monte Carlo (MCMC), which also enables us to use more efficient\nadjustment sets compared to the current exact BIDA. Finally, we use\nJensen-Shannon divergence to define a novel causal effect based on a set of\nintervention distributions in the general categorical setting. We compare our\nmethod to the original IDA method and existing Bayesian approaches in numerical\nsimulations and show that categorical BIDA performs favorably against the\nexisting alternative methods in terms of producing point estimates and\ndiscovering strong effects.",
        "published": "2025-04-07T15:49:36+00:00"
    },
    {
        "title": "P2Mark: Plug-and-play Parameter-intrinsic Watermarking for Neural Speech Generation",
        "authors": [
            "Yong Ren",
            "Jiangyan Yi",
            "Tao Wang",
            "Jianhua Tao",
            "Zhengqi Wen",
            "Chenxing Li",
            "Zheng Lian",
            "Ruibo Fu",
            "Ye Bai",
            "Xiaohui Zhang"
        ],
        "summary": "Recently, a large number of advanced neural speech generation methods have\nemerged in the open-source community. Although this has facilitated the\napplication and development of technology, it has also increased the difficulty\nof preventing the abuse of generated speech and protecting copyrights. Audio\nwatermarking technology is an effective method for proactively protecting\ngenerated speech, but when the source codes and model weights of the neural\nspeech generation methods are open-sourced, audio watermarks based on previous\nwatermarking methods can be easily removed or manipulated. This paper proposes\na Plug-and-play Parameter-intrinsic WaterMarking (P2Mark) method for neural\nspeech generation system protection. The main advantage of P2Mark is that the\nwatermark information is flexibly integrated into the neural speech generation\nmodel in the form of parameters by training a watermark adapter rather than\ninjecting the watermark into the model in the form of features. After the\nwatermark adapter with the watermark embedding is merged with the pre-trained\ngeneration model, the watermark information cannot be easily removed or\nmanipulated. Therefore, P2Mark will be a reliable choice for proactively\ntracing and protecting the copyrights of neural speech generation models in\nopen-source white-box scenarios. We validated P2Mark on two main types of\ndecoders in neural speech generation: vocoder and codec. Experimental results\nshow that P2Mark achieves performance comparable to state-of-the-art audio\nwatermarking methods that cannot be used for open-source white-box protection\nscenarios in terms of watermark extraction accuracy, watermark\nimperceptibility, and robustness.",
        "published": "2025-04-07T15:47:09+00:00"
    },
    {
        "title": "Universal Lymph Node Detection in Multiparametric MRI with Selective Augmentation",
        "authors": [
            "Tejas Sudharshan Mathai",
            "Sungwon Lee",
            "Thomas C. Shen",
            "Zhiyong Lu",
            "Ronald M. Summers"
        ],
        "summary": "Robust localization of lymph nodes (LNs) in multiparametric MRI (mpMRI) is\ncritical for the assessment of lymphadenopathy. Radiologists routinely measure\nthe size of LN to distinguish benign from malignant nodes, which would require\nsubsequent cancer staging. Sizing is a cumbersome task compounded by the\ndiverse appearances of LNs in mpMRI, which renders their measurement difficult.\nFurthermore, smaller and potentially metastatic LNs could be missed during a\nbusy clinical day. To alleviate these imaging and workflow problems, we propose\na pipeline to universally detect both benign and metastatic nodes in the body\nfor their ensuing measurement. The recently proposed VFNet neural network was\nemployed to identify LN in T2 fat suppressed and diffusion weighted imaging\n(DWI) sequences acquired by various scanners with a variety of exam protocols.\nWe also use a selective augmentation technique known as Intra-Label LISA (ILL)\nto diversify the input data samples the model sees during training, such that\nit improves its robustness during the evaluation phase. We achieved a\nsensitivity of $\\sim$83\\% with ILL vs. $\\sim$80\\% without ILL at 4 FP/vol.\nCompared with current LN detection approaches evaluated on mpMRI, we show a\nsensitivity improvement of $\\sim$9\\% at 4 FP/vol.",
        "published": "2025-04-07T15:46:43+00:00"
    },
    {
        "title": "Maximum Shortest Path Interdiction Problem by Upgrading Nodes on Trees under Unit Cost",
        "authors": [
            "Qiao Zhang",
            "Xiao Li",
            "Xiucui Guan",
            "Panos M. Pardalos"
        ],
        "summary": "Network interdiction problems by deleting critical nodes have wide\napplications. However, node deletion is not always feasible in certain\npractical scenarios. We consider the maximum shortest path interdiction problem\nby upgrading nodes on trees under unit cost (MSPIT-UN$_u$). It aims to upgrade\na subset of nodes to maximize the length of the shortest root-leaf distance\nsuch that the total upgrade cost under unit cost is upper bounded by a given\nvalue. We develop a dynamic programming algorithm with a time complexity of\n$O(n^3)$ to solve this problem. Furthermore, we consider the related minimum\ncost problem of (MSPIT-UN$_u$) and propose an $O(n^3\\log n)$ binary search\nalgorithm, where a dynamic programming algorithm is exceeded in each iteration\nto solve its corresponding problem (MSPIT-UN$_u$). Finally, we design numerical\nexperiments to show the effectiveness of the algorithms.",
        "published": "2025-04-07T15:41:34+00:00"
    },
    {
        "title": "Resource-Efficient Beam Prediction in mmWave Communications with Multimodal Realistic Simulation Framework",
        "authors": [
            "Yu Min Park",
            "Yan Kyaw Tun",
            "Walid Saad",
            "Choong Seon Hong"
        ],
        "summary": "Beamforming is a key technology in millimeter-wave (mmWave) communications\nthat improves signal transmission by optimizing directionality and intensity.\nHowever, conventional channel estimation methods, such as pilot signals or beam\nsweeping, often fail to adapt to rapidly changing communication environments.\nTo address this limitation, multimodal sensing-aided beam prediction has gained\nsignificant attention, using various sensing data from devices such as LiDAR,\nradar, GPS, and RGB images to predict user locations or network conditions.\nDespite its promising potential, the adoption of multimodal sensing-aided beam\nprediction is hindered by high computational complexity, high costs, and\nlimited datasets. Thus, in this paper, a resource-efficient learning approach\nis proposed to transfer knowledge from a multimodal network to a monomodal\n(radar-only) network based on cross-modal relational knowledge distillation\n(CRKD), while reducing computational overhead and preserving predictive\naccuracy. To enable multimodal learning with realistic data, a novel multimodal\nsimulation framework is developed while integrating sensor data generated from\nthe autonomous driving simulator CARLA with MATLAB-based mmWave channel\nmodeling, and reflecting real-world conditions. The proposed CRKD achieves its\nobjective by distilling relational information across different feature spaces,\nwhich enhances beam prediction performance without relying on expensive sensor\ndata. Simulation results demonstrate that CRKD efficiently distills multimodal\nknowledge, allowing a radar-only model to achieve $94.62\\%$ of the teacher\nperformance. In particular, this is achieved with just $10\\%$ of the teacher\nnetwork's parameters, thereby significantly reducing computational complexity\nand dependence on multimodal sensor data.",
        "published": "2025-04-07T15:38:25+00:00"
    },
    {
        "title": "MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation",
        "authors": [
            "Rayan Merghani Ahmed",
            "Adnan Iltaf",
            "Bin Li",
            "Shoujun Zhou"
        ],
        "summary": "The accurate segmentation of coronary Digital Subtraction Angiography (DSA)\nimages is essential for diagnosing and treating coronary artery diseases.\nDespite advances in deep learning-based segmentation, challenges such as low\ncontrast, noise, overlapping structures, high intra-class variance, and class\nimbalance limit precise vessel delineation. To overcome these limitations, we\npropose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture\nfor coronary DSA image segmentation. The framework combined Multi-Scale Dilated\nBottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM),\nwhich not only enhances multi-scale feature extraction but also preserve\nfine-grained details, and improve contextual understanding. Furthermore, we\npropose a new Supervised Prototypical Contrastive Loss (SPCL), which combines\nsupervised and prototypical contrastive learning to minimize class imbalance\nand high intra-class variance by focusing on hard-to-classified background\nsamples. Experiments carried out on a private coronary DSA dataset demonstrate\nthat MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice\ncoefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average\nSurface Distance (ASD) and Average Contour Distance (ACD). The developed\nframework provides clinicians with precise vessel segmentation, enabling\naccurate identification of coronary stenosis and supporting informed diagnostic\nand therapeutic decisions. The code will be released at the following GitHub\nprofile link https://github.com/rayanmerghani/MSA-UNet3plus.",
        "published": "2025-04-07T15:35:30+00:00"
    },
    {
        "title": "Utility-aware Social Network Anonymization using Genetic Algorithms",
        "authors": [
            "Samuel Bonello",
            "Rachel G. de Jong",
            "Thomas H. W. B\u00e4ck",
            "Frank W. Takes"
        ],
        "summary": "Social networks may contain privacy-sensitive information about individuals.\nThe objective of the network anonymization problem is to alter a given social\nnetwork dataset such that the number of anonymous nodes in the social graph is\nmaximized. Here, a node is anonymous if it does not have a unique surrounding\nnetwork structure. At the same time, the aim is to ensure data utility, i.e.,\npreserve topological network properties and retain good performance on\ndownstream network analysis tasks. We propose two versions of a genetic\nalgorithm tailored to this problem: one generic GA and a uniqueness-aware GA\n(UGA). The latter aims to target edges more effectively during mutation by\navoiding edges connected to already anonymous nodes. After hyperparameter\ntuning, we compare the two GAs against two existing baseline algorithms on\nseveral real-world network datasets. Results show that the proposed genetic\nalgorithms manage to anonymize on average 14 times more nodes than the best\nbaseline algorithm. Additionally, data utility experiments demonstrate how the\nUGA requires fewer edge deletions, and how our GAs and the baselines retain\nperformance on downstream tasks equally well. Overall, our results suggest that\ngenetic algorithms are a promising approach for finding solutions to the\nnetwork anonymization problem.",
        "published": "2025-04-07T15:29:28+00:00"
    },
    {
        "title": "Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval",
        "authors": [
            "Kidist Amde Mekonnen",
            "Yubao Tang",
            "Maarten de Rijke"
        ],
        "summary": "Generative information retrieval (GenIR) is a promising neural retrieval\nparadigm that formulates document retrieval as a document identifier (docid)\ngeneration task, allowing for end-to-end optimization toward a unified global\nretrieval objective. However, existing GenIR models suffer from token-level\nmisalignment, where models trained to predict the next token often fail to\ncapture document-level relevance effectively. While reinforcement\nlearning-based methods, such as reinforcement learning from relevance feedback\n(RLRF), aim to address this misalignment through reward modeling, they\nintroduce significant complexity, requiring the optimization of an auxiliary\nreward function followed by reinforcement fine-tuning, which is computationally\nexpensive and often unstable. To address these challenges, we propose direct\ndocument relevance optimization (DDRO), which aligns token-level docid\ngeneration with document-level relevance estimation through direct optimization\nvia pairwise ranking, eliminating the need for explicit reward modeling and\nreinforcement learning. Experimental results on benchmark datasets, including\nMS MARCO document and Natural Questions, show that DDRO outperforms\nreinforcement learning-based methods, achieving a 7.4% improvement in MRR@10\nfor MS MARCO and a 19.9% improvement for Natural Questions. These findings\nhighlight DDRO's potential to enhance retrieval effectiveness with a simplified\noptimization approach. By framing alignment as a direct optimization problem,\nDDRO simplifies the ranking optimization pipeline of GenIR models while\noffering a viable alternative to reinforcement learning-based methods.",
        "published": "2025-04-07T15:27:37+00:00"
    },
    {
        "title": "BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks",
        "authors": [
            "Wei Li",
            "Yang Zou",
            "Christopher Ellis",
            "Ruben Purdy",
            "Shawn Blanton",
            "Jos\u00e9 M. F. Moura"
        ],
        "summary": "While many EDA tasks already involve graph-based data, existing LLMs in EDA\nprimarily either represent graphs as sequential text, or simply ignore\ngraph-structured data that might be beneficial like dataflow graphs of RTL\ncode. Recent studies have found that LLM performance suffers when graphs are\nrepresented as sequential text, and using additional graph information\nsignificantly boosts performance. To address these challenges, we introduce\nBRIDGES, a framework designed to incorporate graph modality into LLMs for EDA\ntasks. BRIDGES integrates an automated data generation workflow, a solution\nthat combines graph modality with LLM, and a comprehensive evaluation suite.\nFirst, we establish an LLM-driven workflow to generate RTL and netlist-level\ndata, converting them into dataflow and netlist graphs with function\ndescriptions. This workflow yields a large-scale dataset comprising over\n500,000 graph instances and more than 1.5 billion tokens. Second, we propose a\nlightweight cross-modal projector that encodes graph representations into\ntext-compatible prompts, enabling LLMs to effectively utilize graph data\nwithout architectural modifications. Experimental results demonstrate 2x to 10x\nimprovements across multiple tasks compared to text-only baselines, including\naccuracy in design retrieval, type prediction and perplexity in function\ndescription, with negligible computational overhead (<1% model weights increase\nand <30% additional runtime overhead). Even without additional LLM finetuning,\nour results outperform text-only by a large margin. We plan to release BRIDGES,\nincluding the dataset, models, and training flow.",
        "published": "2025-04-07T15:27:32+00:00"
    },
    {
        "title": "Cellular Network Design for UAV Corridors via Data-driven High-dimensional Bayesian Optimization",
        "authors": [
            "Mohamed Benzaghta",
            "Giovanni Geraci",
            "David L\u00f3pez-P\u00e9rez",
            "Alvaro Valcarce"
        ],
        "summary": "We address the challenge of designing cellular networks for uncrewed aerial\nvehicles (UAVs) corridors through a novel data-driven approach. We assess\nmultiple state-of-the-art high-dimensional Bayesian optimization (HD-BO)\ntechniques to jointly optimize the cell antenna tilts and half-power beamwidth\n(HPBW). We find that some of these approaches achieve over 20dB gains in median\nSINR along UAV corridors, with negligible degradation to ground user\nperformance. Furthermore, we explore the HD-BO's capabilities in terms of model\ngeneralization via transfer learning, where data from a previously observed\nscenario source is leveraged to predict the optimal solution for a new scenario\ntarget. We provide examples of scenarios where such transfer learning is\nsuccessful and others where it fails. Moreover, we demonstrate that HD-BO\nenables multi-objective optimization, identifying optimal design trade-offs\nbetween data rates on the ground versus UAV coverage reliability. We observe\nthat aiming to provide UAV coverage across the entire sky can lower the rates\nfor ground users compared to setups specifically optimized for UAV corridors.\nFinally, we validate our approach through a case study in a real-world cellular\nnetwork, where HD-BO identifies optimal and non-obvious antenna configurations\nthat result in more than double the rates along 3D UAV corridors with\nnegligible ground performance loss.",
        "published": "2025-04-07T15:20:14+00:00"
    },
    {
        "title": "PRDTs: Composable Knowledge-Based Consensus Protocols with Replicated Data Types",
        "authors": [
            "Julian Haas",
            "Ragnar Mogk",
            "Annette Bieniusa",
            "Mira Mezini"
        ],
        "summary": "Consensus protocols are fundamental in distributed systems as they enable\nsoftware with strong consistency properties. However, designing optimized\nprotocols for specific use-cases under certain system assumptions is typically\na laborious and error-prone process requiring expert knowledge. While most\nrecent optimized protocols are variations of well-known algorithms like Paxos\nor Raft, they often necessitate complete re-implementations, potentially\nintroducing new bugs and complicating the application of existing verification\nresults. This approach stands in the way of application-specific consistency\nprotocols that can easily be amended or swapped out, depending on the given\napplication and deployment scenario.\n  We propose Protocol Replicated Data Types (PRDTs), a novel programming model\nfor implementing consensus protocols using replicated data types (RDTs).\nInspired by the knowledge-based view of consensus, PRDTs employ RDTs to\nmonotonically accumulate knowledge until agreement is reached. This approach\nallows for implementations focusing on high-level protocol logic with minimal\nnetwork environment assumptions. Moreover, by applying existing algebraic\ncomposition techniques for RDTs in the PRDT context, we enable composable\nprotocol building-blocks for implementing complex protocols. We present a\nformal model of our approach, demonstrate its application in PRDT-based\nimplementations of existing protocols, and report empirical evaluation results.\nOur findings indicate that the PRDT approach offers enhanced flexibility and\ncomposability in protocol design, facilitates reasoning about correctness, and\ndoes not suffer from inherent performance limitations that would prevent its\nuse in real-world applications.",
        "published": "2025-04-07T15:17:35+00:00"
    },
    {
        "title": "Attention-Based Multi-Scale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes",
        "authors": [
            "Guangqiang Li",
            "M. Amine Atoui",
            "Xiangshun Li"
        ],
        "summary": "Fault diagnosis in multimode processes plays a critical role in ensuring the\nsafe operation of industrial systems across multiple modes. It faces a great\nchallenge yet to be addressed - that is, the significant distributional\ndifferences among monitoring data from multiple modes make it difficult for the\nmodels to extract shared feature representations related to system health\nconditions. In response to this problem, this paper introduces a novel method\ncalled attention-based multi-scale temporal fusion network. The multi-scale\ndepthwise convolution and gated recurrent unit are employed to extract\nmulti-scale contextual local features and long-short-term features. A temporal\nattention mechanism is designed to focus on critical time points with higher\ncross-mode shared information, thereby enhancing the accuracy of fault\ndiagnosis. The proposed model is applied to Tennessee Eastman process dataset\nand three-phase flow facility dataset. The experiments demonstrate that the\nproposed model achieves superior diagnostic performance and maintains a small\nmodel size.",
        "published": "2025-04-07T15:16:22+00:00"
    },
    {
        "title": "SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection",
        "authors": [
            "Bonan Ding",
            "Jin Xie",
            "Jing Nie",
            "Jiale Cao"
        ],
        "summary": "Multimodal 3D object detection based on deep neural networks has indeed made\nsignificant progress. However, it still faces challenges due to the\nmisalignment of scale and spatial information between features extracted from\n2D images and those derived from 3D point clouds. Existing methods usually\naggregate multimodal features at a single stage. However, leveraging\nmulti-stage cross-modal features is crucial for detecting objects of various\nscales. Therefore, these methods often struggle to integrate features across\ndifferent scales and modalities effectively, thereby restricting the accuracy\nof detection. Additionally, the time-consuming Query-Key-Value-based\n(QKV-based) cross-attention operations often utilized in existing methods aid\nin reasoning the location and existence of objects by capturing non-local\ncontexts. However, this approach tends to increase computational complexity. To\naddress these challenges, we present SSLFusion, a novel Scale & Space Aligned\nLatent Fusion Model, consisting of a scale-aligned fusion strategy (SAF), a\n3D-to-2D space alignment module (SAM), and a latent cross-modal fusion module\n(LFM). SAF mitigates scale misalignment between modalities by aggregating\nfeatures from both images and point clouds across multiple levels. SAM is\ndesigned to reduce the inter-modal gap between features from images and point\nclouds by incorporating 3D coordinate information into 2D image features.\nAdditionally, LFM captures cross-modal non-local contexts in the latent space\nwithout utilizing the QKV-based attention operations, thus mitigating\ncomputational complexity. Experiments on the KITTI and DENSE datasets\ndemonstrate that our SSLFusion outperforms state-of-the-art methods. Our\napproach obtains an absolute gain of 2.15% in 3D AP, compared with the\nstate-of-art method GraphAlign on the moderate level of the KITTI test set.",
        "published": "2025-04-07T15:15:06+00:00"
    },
    {
        "title": "Modeling Micro-Doppler Signature of Multi-Propeller Drones in Distributed ISAC",
        "authors": [
            "Heraldo Cesar Alves Costa",
            "Saw J. Myint",
            "Carsten Andrich",
            "Sebastian W. Giehl",
            "Maximilian Engelhardt",
            "Christian Schneider",
            "Reiner S. Thom\u00e4"
        ],
        "summary": "Integrated Sensing and Communication (ISAC) will be one key feature of future\n6G networks, enabling simultaneous communication and radar sensing. The radar\nsensing geometry of ISAC will be multistatic since that corresponds to the\ncommon distributed structure of a mobile communication network. Within this\nframework, micro-Doppler analysis plays a vital role in classifying targets\nbased on their micromotions, such as rotating propellers, vibration, or moving\nlimbs. However, research on bistatic micro-Doppler effects, particularly in\nISAC systems utilizing OFDM waveforms, remains limited. Existing methods,\nincluding electromagnetic simulations often lack scalability for generating the\nlarge datasets required to train machine learning algorithms. To address this\ngap, this work introduces an OFDM-based bistatic micro-Doppler model for\nmulti-propeller drones. The proposed model adapts the classic thin-wire model\nto include bistatic sensing configuration with an OFDM-like signal. Then, it\nextends further by incorporating multiple propellers and integrating the\nreflectivity of the drone's static parts. Measurements were performed to\ncollect ground truth data for verification of the proposed model. Validation\nresults show that the model generates micro-Doppler signatures closely\nresembling those obtained from measurements, demonstrating its potential as a\ntool for data generation. In addition, it offers a comprehensive approach to\nanalyzing bistatic micro-Doppler effects.",
        "published": "2025-04-07T15:12:24+00:00"
    },
    {
        "title": "RLBayes: a Bayesian Network Structure Learning Algorithm via Reinforcement Learning-Based Search Strategy",
        "authors": [
            "Mingcan Wang",
            "Junchang Xin",
            "Luxuan Qu",
            "Qi Chen",
            "Zhiqiong Wang"
        ],
        "summary": "The score-based structure learning of Bayesian network (BN) is an effective\nway to learn BN models, which are regarded as some of the most compelling\nprobabilistic graphical models in the field of representation and reasoning\nunder uncertainty. However, the search space of structure learning grows\nsuper-exponentially as the number of variables increases, which makes BN\nstructure learning an NP-hard problem, as well as a combination optimization\nproblem (COP). Despite the successes of many heuristic methods on it, the\nresults of the structure learning of BN are usually unsatisfactory. Inspired by\nQ-learning, in this paper, a Bayesian network structure learning algorithm via\nreinforcement learning-based (RL-based) search strategy is proposed, namely\nRLBayes. The method borrows the idea of RL and tends to record and guide the\nlearning process by a dynamically maintained Q-table. By creating and\nmaintaining the dynamic Q-table, RLBayes achieve storing the unlimited search\nspace within limited space, thereby achieving the structure learning of BN via\nQ-learning. Not only is it theoretically proved that RLBayes can converge to\nthe global optimal BN structure, but also it is experimentally proved that\nRLBayes has a better effect than almost all other heuristic search algorithms.",
        "published": "2025-04-07T15:11:51+00:00"
    },
    {
        "title": "SparsyFed: Sparse Adaptive Federated Training",
        "authors": [
            "Adriano Guastella",
            "Lorenzo Sani",
            "Alex Iacob",
            "Alessio Mora",
            "Paolo Bellavista",
            "Nicholas D. Lane"
        ],
        "summary": "Sparse training is often adopted in cross-device federated learning (FL)\nenvironments where constrained devices collaboratively train a machine learning\nmodel on private data by exchanging pseudo-gradients across heterogeneous\nnetworks. Although sparse training methods can reduce communication overhead\nand computational burden in FL, they are often not used in practice for the\nfollowing key reasons: (1) data heterogeneity makes it harder for clients to\nreach consensus on sparse models compared to dense ones, requiring longer\ntraining; (2) methods for obtaining sparse masks lack adaptivity to accommodate\nvery heterogeneous data distributions, crucial in cross-device FL; and (3)\nadditional hyperparameters are required, which are notably challenging to tune\nin FL. This paper presents SparsyFed, a practical federated sparse training\nmethod that critically addresses the problems above. Previous works have only\nsolved one or two of these challenges at the expense of introducing new\ntrade-offs, such as clients' consensus on masks versus sparsity pattern\nadaptivity. We show that SparsyFed simultaneously (1) can produce 95% sparse\nmodels, with negligible degradation in accuracy, while only needing a single\nhyperparameter, (2) achieves a per-round weight regrowth 200 times smaller than\nprevious methods, and (3) allows the sparse masks to adapt to highly\nheterogeneous data distributions and outperform all baselines under such\nconditions.",
        "published": "2025-04-07T14:57:02+00:00"
    },
    {
        "title": "A Reinforcement Learning Method for Environments with Stochastic Variables: Post-Decision Proximal Policy Optimization with Dual Critic Networks",
        "authors": [
            "Leonardo Kanashiro Felizardo",
            "Edoardo Fadda",
            "Paolo Brandimarte",
            "Emilio Del-Moral-Hernandez",
            "Mari\u00e1 Cristina Vasconcelos Nascimento"
        ],
        "summary": "This paper presents Post-Decision Proximal Policy Optimization (PDPPO), a\nnovel variation of the leading deep reinforcement learning method, Proximal\nPolicy Optimization (PPO). The PDPPO state transition process is divided into\ntwo steps: a deterministic step resulting in the post-decision state and a\nstochastic step leading to the next state. Our approach incorporates\npost-decision states and dual critics to reduce the problem's dimensionality\nand enhance the accuracy of value function estimation. Lot-sizing is a mixed\ninteger programming problem for which we exemplify such dynamics. The objective\nof lot-sizing is to optimize production, delivery fulfillment, and inventory\nlevels in uncertain demand and cost parameters. This paper evaluates the\nperformance of PDPPO across various environments and configurations. Notably,\nPDPPO with a dual critic architecture achieves nearly double the maximum reward\nof vanilla PPO in specific scenarios, requiring fewer episode iterations and\ndemonstrating faster and more consistent learning across different\ninitializations. On average, PDPPO outperforms PPO in environments with a\nstochastic component in the state transition. These results support the\nbenefits of using a post-decision state. Integrating this post-decision state\nin the value function approximation leads to more informed and efficient\nlearning in high-dimensional and stochastic environments.",
        "published": "2025-04-07T14:56:43+00:00"
    },
    {
        "title": "Taming Double-Spending in Offline Payments with Reputation-Weighted Loan Networks",
        "authors": [
            "Nektarios Evangelou",
            "Rowdy Chotkan",
            "Bulat Nasrulin",
            "J\u00e9r\u00e9mie Decouchant"
        ],
        "summary": "Blockchain solutions typically assume a synchronous network to ensure\nconsistency and achieve consensus. In contrast, offline transaction systems aim\nto enable users to agree on and execute transactions without assuming bounded\ncommunication delays when interacting with the blockchain. Most existing\noffline payment schemes depend on trusted hardware wallets that are assumed to\nbe secure and tamper-proof. While this work introduces Overdraft, a novel\noffline payment system that shifts the reliance from hardware to users\nthemselves. Overdraft allows potential payment receivers to assess the\nlikelihood of being paid, allowing them to accept transactions with confidence\nor deny them. Overdraft achieves this by maintaining a loan network that is\nweighted by online reputation. This loan network contains time-limited\nagreements where users pledge to cover another user's payment if necessary. For\nexample, when a payer lacks sufficient funds at the moment of commitment.\nOffline users rely on the last known view of the loan network -- which they had\naccess to when last online -- to determine whether to participate in an offline\ntransaction. This view is used to estimate the probability of eventual payment,\npossibly using multiple loans. Once online again, users commit their\ntransactions to the blockchain with any conflicts being resolved\ndeterministically. Overdraft incorporates incentives for users and is designed\nto be resilient against Sybil attacks. As a proof of concept, we implemented\nOverdraft as an Ethereum Solidity smart contract and deployed it on the Sepolia\ntestnet to evaluate its performance.",
        "published": "2025-04-07T14:48:19+00:00"
    },
    {
        "title": "EffOWT: Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively",
        "authors": [
            "Bingyang Wang",
            "Kaer Huang",
            "Bin Li",
            "Yiqiang Yan",
            "Lihe Zhang",
            "Huchuan Lu",
            "You He"
        ],
        "summary": "Open-World Tracking (OWT) aims to track every object of any category, which\nrequires the model to have strong generalization capabilities. Trackers can\nimprove their generalization ability by leveraging Visual Language Models\n(VLMs). However, challenges arise with the fine-tuning strategies when VLMs are\ntransferred to OWT: full fine-tuning results in excessive parameter and memory\ncosts, while the zero-shot strategy leads to sub-optimal performance. To solve\nthe problem, EffOWT is proposed for efficiently transferring VLMs to OWT.\nSpecifically, we build a small and independent learnable side network outside\nthe VLM backbone. By freezing the backbone and only executing backpropagation\non the side network, the model's efficiency requirements can be met. In\naddition, EffOWT enhances the side network by proposing a hybrid structure of\nTransformer and CNN to improve the model's performance in the OWT field.\nFinally, we implement sparse interactions on the MLP, thus reducing parameter\nupdates and memory costs significantly. Thanks to the proposed methods, EffOWT\nachieves an absolute gain of 5.5% on the tracking metric OWTA for unknown\ncategories, while only updating 1.3% of the parameters compared to full\nfine-tuning, with a 36.4% memory saving. Other metrics also demonstrate obvious\nimprovement.",
        "published": "2025-04-07T14:47:58+00:00"
    },
    {
        "title": "Unifying Physics- and Data-Driven Modeling via Novel Causal Spatiotemporal Graph Neural Network for Interpretable Epidemic Forecasting",
        "authors": [
            "Shuai Han",
            "Lukas Stelz",
            "Thomas R. Sokolowski",
            "Kai Zhou",
            "Horst St\u00f6cker"
        ],
        "summary": "Accurate epidemic forecasting is crucial for effective disease control and\nprevention. Traditional compartmental models often struggle to estimate\ntemporally and spatially varying epidemiological parameters, while deep\nlearning models typically overlook disease transmission dynamics and lack\ninterpretability in the epidemiological context. To address these limitations,\nwe propose a novel Causal Spatiotemporal Graph Neural Network (CSTGNN), a\nhybrid framework that integrates a Spatio-Contact SIR model with Graph Neural\nNetworks (GNNs) to capture the spatiotemporal propagation of epidemics.\nInter-regional human mobility exhibits continuous and smooth spatiotemporal\npatterns, leading to adjacent graph structures that share underlying mobility\ndynamics. To model these dynamics, we employ an adaptive static connectivity\ngraph to represent the stable components of human mobility and utilize a\ntemporal dynamics model to capture fluctuations within these patterns. By\nintegrating the adaptive static connectivity graph with the temporal dynamics\ngraph, we construct a dynamic graph that encapsulates the comprehensive\nproperties of human mobility networks. Additionally, to capture temporal trends\nand variations in infectious disease spread, we introduce a temporal\ndecomposition model to handle temporal dependence. This model is then\nintegrated with a dynamic graph convolutional network for epidemic forecasting.\nWe validate our model using real-world datasets at the provincial level in\nChina and the state level in Germany. Extensive studies demonstrate that our\nmethod effectively models the spatiotemporal dynamics of infectious diseases,\nproviding a valuable tool for forecasting and intervention strategies.\nFurthermore, analysis of the learned parameters offers insights into disease\ntransmission mechanisms, enhancing the interpretability and practical\napplicability of our model.",
        "published": "2025-04-07T14:46:11+00:00"
    },
    {
        "title": "Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection",
        "authors": [
            "Jon Guti\u00e9rrez Zaballa",
            "Koldo Basterretxea",
            "Javier Echanobe"
        ],
        "summary": "Machine learning-based embedded systems for safety-critical applications,\nsuch as aerospace and autonomous driving, must be robust to perturbations\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\nmodern electronic devices become more susceptible to background radiation,\nincreasing the concern about failures produced by soft errors. The resilience\nof deep neural networks (DNNs) to these errors depends not only on target\ndevice technology but also on model structure and the numerical representation\nand arithmetic precision of their parameters. Compression techniques like\npruning and quantization, used to reduce memory footprint and computational\ncomplexity, alter both model structure and representation, affecting soft error\nrobustness. In this regard, although often overlooked, the choice of activation\nfunctions (AFs) impacts not only accuracy and trainability but also\ncompressibility and error resilience. This paper explores the use of bounded\nAFs to enhance robustness against parameter perturbations, while evaluating\ntheir effects on model accuracy, compressibility, and computational load with a\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\ndeveloped for semantic segmentation of hyperspectral images with application to\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\nSoM.",
        "published": "2025-04-07T14:21:31+00:00"
    },
    {
        "title": "Inertia-induced scaling and criticality in martensites",
        "authors": [
            "O\u011fuz Umut Salman",
            "Alphonse Finel",
            "Lev Truskinovsky"
        ],
        "summary": "Martensites subjected to quasistatic deformation are known to exhibit power\nlaw distributed acoustic emission in a broad range of scales. However, the\norigin of the observed scaling behavior and the mechanism of self organization\ntowards criticality remains obscure. Here we argue that the power law structure\nof the fluctuations spectrum can be interpreted as an effect of inertia. The\ngeneral insight is that inertial dynamics can become a crucial player when the\nunderlying mechanical system is only marginally stable. We first illustrate the\npossibility of inertia-induced criticality using an elementary example of mass\npoints connected by bi-stable springs. We then explore the effects of inertia\nin the fully realistic two and three dimensional continuum models of specific\nelastic phase transitions in crystals.",
        "published": "2025-04-07T14:18:20+00:00"
    },
    {
        "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
        "authors": [
            "Anja Surina",
            "Amin Mansouri",
            "Lars Quaedvlieg",
            "Amal Seddas",
            "Maryna Viazovska",
            "Emmanuel Abbe",
            "Caglar Gulcehre"
        ],
        "summary": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non three combinatorial optimization tasks - bin packing, traveling salesman,\nand the flatpack problem - show that combining RL and evolutionary search\nimproves discovery efficiency of improved algorithms, showcasing the potential\nof RL-enhanced evolutionary strategies to assist computer scientists and\nmathematicians for more efficient algorithm design.",
        "published": "2025-04-07T14:14:15+00:00"
    },
    {
        "title": "Decentralized Semantic Federated Learning for Real-Time Public Safety Tasks: Challenges, Methods, and Directions",
        "authors": [
            "Baosheng Li",
            "Weifeng Gao",
            "Zehui Xiong",
            "Jin Xie",
            "Binquan Guo",
            "Miao Du"
        ],
        "summary": "Public safety tasks rely on the collaborative functioning of multiple edge\ndevices (MEDs) and base stations (BSs) in different regions, consuming\nsignificant communication energy and computational resources to execute\ncritical operations like fire monitoring and rescue missions. Traditional\nfederated edge computing (EC) methods require frequent central communication,\nconsuming substantial energy and struggling with resource heterogeneity across\ndevices, networks, and data. To this end, this paper introduces a decentralized\nsemantic federated learning (DSFL) framework tailored for large-scale wireless\ncommunication systems and heterogeneous MEDs. The framework incorporates a\nhierarchical semantic communication (SC) scheme to extend EC coverage and\nreduce communication overhead. Specifically, the lower layer optimizes intra-BS\ncommunication through task-specific encoding and selective transmission under\nconstrained networks, while the upper layer ensures robust inter-BS\ncommunication via semantic aggregation and distributed consensus across\ndifferent regions. To further balance communication costs and semantic\naccuracy, an energy-efficient aggregation scheme is developed for both intra-BS\nand inter-BS communication. The effectiveness of the DSFL framework is\ndemonstrated through a case study using the BoWFire dataset, showcasing its\npotential in real-time fire detection scenarios. Finally, we outlines open\nissues for edge intelligence and SC in public safety tasks.",
        "published": "2025-04-07T14:13:50+00:00"
    },
    {
        "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
        "authors": [
            "Liu Xiao",
            "Li Zhiyuan",
            "Lin Yueyu"
        ],
        "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
        "published": "2025-04-07T14:04:30+00:00"
    },
    {
        "title": "Hollow Victory: How Malicious Proposers Exploit Validator Incentives in Optimistic Rollup Dispute Games",
        "authors": [
            "Suhyeon Lee"
        ],
        "summary": "Blockchain systems, such as Ethereum, are increasingly adopting layer-2\nscaling solutions to improve transaction throughput and reduce fees. One\npopular layer-2 approach is the Optimistic Rollup, which relies on a mechanism\nknown as a dispute game for block proposals. In these systems, validators can\nchallenge blocks that they believe contain errors, and a successful challenge\nresults in the transfer of a portion of the proposer's deposit as a reward. In\nthis paper, we reveal a structural vulnerability in the mechanism: validators\nmay not be awarded a proper profit despite winning a dispute challenge. We\ndevelop a formal game-theoretic model of the dispute game and analyze several\nscenarios, including cases where the proposer controls some validators and\ncases where a secondary auction mechanism is deployed to induce additional\nparticipation. Our analysis demonstrates that under current designs, the\ncompetitive pressure from validators may be insufficient to deter malicious\nbehavior. We find that increased validator competition, paradoxically driven by\nhigher rewards or participation, can allow a malicious proposer to\nsignificantly lower their net loss by capturing value through mechanisms like\nauctions. To address this, we propose countermeasures such as an escrowed\nreward mechanism and a commit-reveal protocol. Our findings provide critical\ninsights into enhancing the economic security of layer-2 scaling solutions in\nblockchain networks.",
        "published": "2025-04-07T14:00:46+00:00"
    },
    {
        "title": "All-to-all connectivity of Rydberg-atom-based quantum processors with messenger qubits",
        "authors": [
            "Ivan V. Dudinets",
            "Stanislav S. Straupe",
            "Aleksey K. Fedorov",
            "Oleg V. Lychkovskiy"
        ],
        "summary": "Rydberg atom arrays is a promising platform for programmable quantum\nsimulators and universal quantum processors. A major challenge threatening the\nscalability of this platform is the limited qubit connectivity due to the\nfinite range of interactions between atoms. We discuss an approach to realize\ndynamical all-to-all connectivity between qubits with the use of moving atoms,\nwhich we referred to as messenger qubits, that interact with the computational\nqubits of the processor. We propose four specific architectures capitalizing on\nthis idea and compare them one to another, as well as to alternative\napproaches. We argue that the use of messenger qubits, while posing new\ntechnological challenges, promises further development of the\nRydberg-atom-based platform.",
        "published": "2025-04-07T13:57:24+00:00"
    },
    {
        "title": "Characterisation of conserved and reacting moieties in chemical reaction networks",
        "authors": [
            "Hadjar Rahou",
            "Hulda S. Haraldsdottir",
            "Filippo Martinelli",
            "Ines Thiele",
            "Ronan M. T. Fleming"
        ],
        "summary": "A detailed understanding of biochemical networks at the molecular level is\nessential for studying complex cellular processes. In this paper, we provide a\ncomprehensive description of biochemical networks by considering individual\natoms and chemical bonds. To address combinatorial complexity, we introduce a\nwell-established approach to group similar types of information within\nbiochemical networks. A conserved moiety is a set of atoms whose association is\ninvariant across all reactions in a network. A reacting moiety is a set of\nbonds that are either broken, formed, or undergo a change in bond order in at\nleast one reaction in the network. By mathematically identifying these\nmoieties, we establish the biological significance of conserved and reacting\nmoieties according to the mathematical properties of the stoichiometric matrix.\nWe also present a novel decomposition of the stoichiometric matrix based on\nconserved moieties. This approach bridges the gap between graph theory, linear\nalgebra, and biological interpretation, thus opening up new horizons in the\nstudy of chemical reaction networks.",
        "published": "2025-04-07T13:54:47+00:00"
    },
    {
        "title": "PvNeXt: Rethinking Network Design and Temporal Motion for Point Cloud Video Recognition",
        "authors": [
            "Jie Wang",
            "Tingfa Xu",
            "Lihe Ding",
            "Xinjie Zhang",
            "Long Bai",
            "Jianan Li"
        ],
        "summary": "Point cloud video perception has become an essential task for the realm of 3D\nvision. Current 4D representation learning techniques typically engage in\niterative processing coupled with dense query operations. Although effective in\ncapturing temporal features, this approach leads to substantial computational\nredundancy. In this work, we propose a framework, named as PvNeXt, for\neffective yet efficient point cloud video recognition, via personalized\none-shot query operation. Specially, PvNeXt consists of two key modules, the\nMotion Imitator and the Single-Step Motion Encoder. The former module, the\nMotion Imitator, is designed to capture the temporal dynamics inherent in\nsequences of point clouds, thus generating the virtual motion corresponding to\neach frame. The Single-Step Motion Encoder performs a one-step query operation,\nassociating point cloud of each frame with its corresponding virtual motion\nframe, thereby extracting motion cues from point cloud sequences and capturing\ntemporal dynamics across the entire sequence. Through the integration of these\ntwo modules, {PvNeXt} enables personalized one-shot queries for each frame,\neffectively eliminating the need for frame-specific looping and intensive query\nprocesses. Extensive experiments on multiple benchmarks demonstrate the\neffectiveness of our method.",
        "published": "2025-04-07T13:43:51+00:00"
    },
    {
        "title": "On the Performance of an Explainable Language Model on PubMedQA",
        "authors": [
            "Venkat Srinivasan",
            "Vishaal Jatav",
            "Anushka Chandrababu",
            "Geetika Sharma"
        ],
        "summary": "Large language models (LLMs) have shown significant abilities in retrieving\nmedical knowledge, reasoning over it and answering medical questions comparably\nto physicians. However, these models are not interpretable, hallucinate, are\ndifficult to maintain and require enormous compute resources for training and\ninference. In this paper, we report results from Gyan, an explainable language\nmodel based on an alternative architecture, on the PubmedQA data set. The Gyan\nLLM is a compositional language model and the model is decoupled from\nknowledge. Gyan is trustable, transparent, does not hallucinate and does not\nrequire significant training or compute resources. Gyan is easily transferable\nacross domains. Gyan-4.3 achieves SOTA results on PubmedQA with 87.1% accuracy\ncompared to 82% by MedPrompt based on GPT-4 and 81.8% by Med-PaLM 2 (Google and\nDeepMind). We will be reporting results for other medical data sets - MedQA,\nMedMCQA, MMLU - Medicine in the future.",
        "published": "2025-04-07T13:42:02+00:00"
    },
    {
        "title": "AI-Driven Tactical Communications and Networking for Defense: A Survey and Emerging Trends",
        "authors": [
            "Victor Monzon Baeza",
            "Ra\u00fal Parada",
            "Laura Concha Salor",
            "Carlos Monzo"
        ],
        "summary": "The integration of Artificial Intelligence (AI) in military communications\nand networking is reshaping modern defense strategies, enhancing secure data\nexchange, real-time situational awareness, and autonomous decision-making. This\nsurvey explores how AI-driven technologies improve tactical communication\nnetworks, radar-based data transmission, UAV-assisted relay systems, and\nelectronic warfare resilience. The study highlights AI applications in adaptive\nsignal processing, multi-agent coordination for network optimization,\nradar-assisted target tracking, and AI-driven electronic countermeasures. Our\nwork introduces a novel three-criteria evaluation methodology. It\nsystematically assesses AI applications based on general system objectives,\ncommunications constraints in the military domain, and critical tactical\nenvironmental factors. We analyze key AI techniques for different types of\nlearning applied to multi-domain network interoperability and distributed data\ninformation fusion in military operations. We also address challenges such as\nadversarial AI threats, the real-time adaptability of autonomous communication\nnetworks, and the limitations of current AI models under battlefield\nconditions. Finally, we discuss emerging trends in self-healing networks,\nAI-augmented decision support systems, and intelligent spectrum allocation. We\nprovide a structured roadmap for future AI-driven defense communications and\nnetworking research.",
        "published": "2025-04-07T13:38:32+00:00"
    },
    {
        "title": "LDGNet: A Lightweight Difference Guiding Network for Remote Sensing Change Detection",
        "authors": [
            "Chenfeng Xu"
        ],
        "summary": "With the rapid advancement of deep learning, the field of change detection\n(CD) in remote sensing imagery has achieved remarkable progress. Existing\nchange detection methods primarily focus on achieving higher accuracy with\nincreased computational costs and parameter sizes, leaving development of\nlightweight methods for rapid real-world processing an underexplored challenge.\nTo address this challenge, we propose a Lightweight Difference Guiding Network\n(LDGNet), leveraging absolute difference image to guide optical remote sensing\nchange detection. First, to enhance the feature representation capability of\nthe lightweight backbone network, we propose the Difference Guiding Module\n(DGM), which leverages multi-scale features extracted from the absolute\ndifference image to progressively influence the original image encoder at each\nlayer, thereby reinforcing feature extraction. Second, we propose the\nDifference-Aware Dynamic Fusion (DADF) module with Visual State Space Model\n(VSSM) for lightweight long-range dependency modeling. The module first uses\nfeature absolute differences to guide VSSM's global contextual modeling of\nchange regions, then employs difference attention to dynamically fuse these\nlong-range features with feature differences, enhancing change semantics while\nsuppressing noise and background. Extensive experiments on multiple datasets\ndemonstrate that our method achieves comparable or superior performance to\ncurrent state-of-the-art (SOTA) methods requiring several times more\ncomputation, while maintaining only 3.43M parameters and 1.12G FLOPs.",
        "published": "2025-04-07T13:33:54+00:00"
    },
    {
        "title": "MIAT: Maneuver-Intention-Aware Transformer for Spatio-Temporal Trajectory Prediction",
        "authors": [
            "Chandra Raskoti",
            "Iftekharul Islam",
            "Xuan Wang",
            "Weizi Li"
        ],
        "summary": "Accurate vehicle trajectory prediction is critical for safe and efficient\nautonomous driving, especially in mixed traffic environments with both\nhuman-driven and autonomous vehicles. However, uncertainties introduced by\ninherent driving behaviors -- such as acceleration, deceleration, and left and\nright maneuvers -- pose significant challenges for reliable trajectory\nprediction. We introduce a Maneuver-Intention-Aware Transformer (MIAT)\narchitecture, which integrates a maneuver intention awareness mechanism with\nspatiotemporal interaction modeling to enhance long-horizon trajectory\npredictions. We systematically investigate the impact of varying awareness of\nmaneuver intention on both short- and long-horizon trajectory predictions.\nEvaluated on the real-world NGSIM dataset and benchmarked against various\ntransformer- and LSTM-based methods, our approach achieves an improvement of up\nto 4.7% in short-horizon predictions and a 1.6% in long-horizon predictions\ncompared to other intention-aware benchmark methods. Moreover, by leveraging an\nintention awareness control mechanism, MIAT realizes an 11.1% performance boost\nin long-horizon predictions, with a modest drop in short-horizon performance.",
        "published": "2025-04-07T13:30:00+00:00"
    },
    {
        "title": "CMaP-SAM: Contraction Mapping Prior for SAM-driven Few-shot Segmentation",
        "authors": [
            "Shuai Chen",
            "Fanman Meng",
            "Haoran Wei",
            "Chenhao Wu",
            "Qingbo Wu",
            "Linfeng Xu",
            "Hongliang Li"
        ],
        "summary": "Few-shot segmentation (FSS) aims to segment new classes using few annotated\nimages. While recent FSS methods have shown considerable improvements by\nleveraging Segment Anything Model (SAM), they face two critical limitations:\ninsufficient utilization of structural correlations in query images, and\nsignificant information loss when converting continuous position priors to\ndiscrete point prompts. To address these challenges, we propose CMaP-SAM, a\nnovel framework that introduces contraction mapping theory to optimize position\npriors for SAM-driven few-shot segmentation. CMaP-SAM consists of three key\ncomponents: (1) a contraction mapping module that formulates position prior\noptimization as a Banach contraction mapping with convergence guarantees. This\nmodule iteratively refines position priors through pixel-wise structural\nsimilarity, generating a converged prior that preserves both semantic guidance\nfrom reference images and structural correlations in query images; (2) an\nadaptive distribution alignment module bridging continuous priors with SAM's\nbinary mask prompt encoder; and (3) a foreground-background decoupled\nrefinement architecture producing accurate final segmentation masks. Extensive\nexperiments demonstrate CMaP-SAM's effectiveness, achieving state-of-the-art\nperformance with 71.1 mIoU on PASCAL-$5^i$ and 56.1 on COCO-$20^i$ datasets.",
        "published": "2025-04-07T13:19:16+00:00"
    },
    {
        "title": "MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond",
        "authors": [
            "Shenghao Ren",
            "Yi Lu",
            "Jiayi Huang",
            "Jiayi Zhao",
            "He Zhang",
            "Tao Yu",
            "Qiu Shen",
            "Xun Cao"
        ],
        "summary": "Existing human Motion Capture (MoCap) methods mostly focus on the visual\nsimilarity while neglecting the physical plausibility. As a result, downstream\ntasks such as driving virtual human in 3D scene or humanoid robots in real\nworld suffer from issues such as timing drift and jitter, spatial problems like\nsliding and penetration, and poor global trajectory accuracy. In this paper, we\nrevisit human MoCap from the perspective of interaction between human body and\nphysical world by exploring the role of pressure. Firstly, we construct a\nlarge-scale human Motion capture dataset with Pressure, RGB and Optical sensors\n(named MotionPRO), which comprises 70 volunteers performing 400 types of\nmotion, encompassing a total of 12.4M pose frames. Secondly, we examine both\nthe necessity and effectiveness of the pressure signal through two challenging\ntasks: (1) pose and trajectory estimation based solely on pressure: We propose\na network that incorporates a small kernel decoder and a long-short-term\nattention module, and proof that pressure could provide accurate global\ntrajectory and plausible lower body pose. (2) pose and trajectory estimation by\nfusing pressure and RGB: We impose constraints on orthographic similarity along\nthe camera axis and whole-body contact along the vertical axis to enhance the\ncross-attention strategy to fuse pressure and RGB feature maps. Experiments\ndemonstrate that fusing pressure with RGB features not only significantly\nimproves performance in terms of objective metrics, but also plausibly drives\nvirtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that\nincorporating physical perception enables humanoid robots to perform more\nprecise and stable actions, which is highly beneficial for the development of\nembodied artificial intelligence. Project page is available at:\nhttps://nju-cite-mocaphumanoid.github.io/MotionPRO/",
        "published": "2025-04-07T13:17:24+00:00"
    },
    {
        "title": "Probabilistic Position-Aided Beam Selection for mmWave MIMO Systems",
        "authors": [
            "Joseph K. Chege",
            "Arie Yeredor",
            "Martin Haardt"
        ],
        "summary": "Millimeter-wave (mmWave) MIMO systems rely on highly directional beamforming\nto overcome severe path loss and ensure robust communication links. However,\nselecting the optimal beam pair efficiently remains a challenge due to the\nlarge search space and the overhead of conventional methods. This paper\nproposes a probabilistic position-aided beam selection approach that exploits\nthe statistical dependence between user equipment (UE) positions and optimal\nbeam indices. We model the underlying joint probability mass function (PMF) of\nthe positions and the beam indices as a low-rank tensor and estimate its\nparameters from training data using Bayesian inference. The estimated model is\nthen used to predict the best (or a list of the top) beam pair indices for new\nUE positions. The proposed method is evaluated using data generated from a\nstate-of-the-art ray tracing simulator and compared with neural network-based\nand fingerprinting approaches. The results show that our approach achieves a\nhigh data rate with fewer training samples and a significantly reduced beam\nsearch space. These advantages render it a promising solution for practical\nmmWave MIMO deployments, reducing the beam search overhead while maintaining a\nreliable connectivity.",
        "published": "2025-04-07T12:56:51+00:00"
    },
    {
        "title": "Analog phase-sensitive time-reversal of optically-carried radiofrequency signals",
        "authors": [
            "Thomas Llauze",
            "Anne Louchet-Chauvet"
        ],
        "summary": "Achieving low-latency time-reversal of broadband radiofrequency signals is\ncrucial for reliable communications in dynamic, uncontrolled environments.\nHowever, existing approaches are either digitally assisted -- making broadband\nextension challenging -- or limited to amplitude modulation. In this work, we\nreport the very first experimental realization of a fully analog,\nphase-preserving time-reversal architecture for optically-carried\nradiofrequency signals. The method exploits the exceptional coherence\nproperties of rare-earth ion-doped materials, and leverages the\nwell-established photon echo mechanism, widely used in quantum technologies.\nWhile our demonstration is conducted with a modest bandwidth, we identify the\nfundamental cause of this limitation and propose solutions for future\nscalability.",
        "published": "2025-04-07T12:52:41+00:00"
    },
    {
        "title": "AsyReC: A Multimodal Graph-based Framework for Spatio-Temporal Asymmetric Dyadic Relationship Classification",
        "authors": [
            "Wang Tang",
            "Fethiye Irmak Dogan",
            "Linbo Qing",
            "Hatice Gunes"
        ],
        "summary": "Dyadic social relationships, which refer to relationships between two\nindividuals who know each other through repeated interactions (or not), are\nshaped by shared spatial and temporal experiences. Current computational\nmethods for modeling these relationships face three major challenges: (1) the\nfailure to model asymmetric relationships, e.g., one individual may perceive\nthe other as a friend while the other perceives them as an acquaintance, (2)\nthe disruption of continuous interactions by discrete frame sampling, which\nsegments the temporal continuity of interaction in real-world scenarios, and\n(3) the limitation to consider periodic behavioral cues, such as rhythmic\nvocalizations or recurrent gestures, which are crucial for inferring the\nevolution of dyadic relationships. To address these challenges, we propose\nAsyReC, a multimodal graph-based framework for asymmetric dyadic relationship\nclassification, with three core innovations: (i) a triplet graph neural network\nwith node-edge dual attention that dynamically weights multimodal cues to\ncapture interaction asymmetries (addressing challenge 1); (ii) a clip-level\nrelationship learning architecture that preserves temporal continuity, enabling\nfine-grained modeling of real-world interaction dynamics (addressing challenge\n2); and (iii) a periodic temporal encoder that projects time indices onto\nsine/cosine waveforms to model recurrent behavioral patterns (addressing\nchallenge 3). Extensive experiments on two public datasets demonstrate\nstate-of-the-art performance, while ablation studies validate the critical role\nof asymmetric interaction modeling and periodic temporal encoding in improving\nthe robustness of dyadic relationship classification in real-world scenarios.\nOur code is publicly available at: https://github.com/tw-repository/AsyReC.",
        "published": "2025-04-07T12:52:23+00:00"
    },
    {
        "title": "Graph-based Diffusion Model for Collaborative Filtering",
        "authors": [
            "Xuan Zhang",
            "Xiang Deng",
            "Hongxing Yuan",
            "Chunyu Wei",
            "Yushun Fan"
        ],
        "summary": "Recently, diffusion-based recommendation methods have achieved impressive\nresults. However, existing approaches predominantly treat each user's\nhistorical interactions as independent training samples, overlooking the\npotential of higher-order collaborative signals between users and items. Such\nsignals, which encapsulate richer and more nuanced relationships, can be\nnaturally captured using graph-based data structures. To address this\nlimitation, we extend diffusion-based recommendation methods to the graph\ndomain by directly modeling user-item bipartite graphs with diffusion models.\nThis enables better modeling of the higher-order connectivity inherent in\ncomplex interaction dynamics. However, this extension introduces two primary\nchallenges: (1) Noise Heterogeneity, where interactions are influenced by\nvarious forms of continuous and discrete noise, and (2) Relation Explosion,\nreferring to the high computational costs of processing large-scale graphs. To\ntackle these challenges, we propose a Graph-based Diffusion Model for\nCollaborative Filtering (GDMCF). To address noise heterogeneity, we introduce a\nmulti-level noise corruption mechanism that integrates both continuous and\ndiscrete noise, effectively simulating real-world interaction complexities. To\nmitigate relation explosion, we design a user-active guided diffusion process\nthat selectively focuses on the most meaningful edges and active users,\nreducing inference costs while preserving the graph's topological integrity.\nExtensive experiments on three benchmark datasets demonstrate that GDMCF\nconsistently outperforms state-of-the-art methods, highlighting its\neffectiveness in capturing higher-order collaborative signals and improving\nrecommendation performance.",
        "published": "2025-04-07T12:51:18+00:00"
    },
    {
        "title": "Multi-level Neural Networks for high-dimensional parametric obstacle problems",
        "authors": [
            "Martin Eigel",
            "Cosmas Hei\u00df",
            "Janina E. Sch\u00fctte"
        ],
        "summary": "A new method to solve computationally challenging (random) parametric\nobstacle problems is developed and analyzed, where the parameters can influence\nthe related partial differential equation (PDE) and determine the position and\nsurface structure of the obstacle. As governing equation, a stationary elliptic\ndiffusion problem is assumed. The high-dimensional solution of the obstacle\nproblem is approximated by a specifically constructed convolutional neural\nnetwork (CNN). This novel algorithm is inspired by a finite element constrained\nmultigrid algorithm to represent the parameter to solution map. This has two\nbenefits: First, it allows for efficient practical computations since\nmulti-level data is used as an explicit output of the NN thanks to an\nappropriate data preprocessing. This improves the efficacy of the training\nprocess and subsequently leads to small errors in the natural energy norm.\nSecond, the comparison of the CNN to a multigrid algorithm provides means to\ncarry out a complete a priori convergence and complexity analysis of the\nproposed NN architecture. Numerical experiments illustrate a state-of-the-art\nperformance for this challenging problem.",
        "published": "2025-04-07T12:50:56+00:00"
    },
    {
        "title": "Concept Extraction for Time Series with ECLAD-ts",
        "authors": [
            "Antonia Holzapfel",
            "Andres Felipe Posada-Moreno",
            "Sebastian Trimpe"
        ],
        "summary": "Convolutional neural networks (CNNs) for time series classification (TSC) are\nbeing increasingly used in applications ranging from quality prediction to\nmedical diagnosis. The black box nature of these models makes understanding\ntheir prediction process difficult. This issue is crucial because CNNs are\nprone to learning shortcuts and biases, compromising their robustness and\nalignment with human expectations. To assess whether such mechanisms are being\nused and the associated risk, it is essential to provide model explanations\nthat reflect the inner workings of the model. Concept Extraction (CE) methods\noffer such explanations, but have mostly been developed for the image domain so\nfar, leaving a gap in the time series domain. In this work, we present a CE and\nlocalization method tailored to the time series domain, based on the ideas of\nCE methods for images. We propose the novel method ECLAD-ts, which provides\npost-hoc global explanations based on how the models encode subsets of the\ninput at different levels of abstraction. For this, concepts are produced by\nclustering timestep-wise aggregations of CNN activation maps, and their\nimportance is computed based on their impact on the prediction process. We\nevaluate our method on synthetic and natural datasets. Furthermore, we assess\nthe advantages and limitations of CE in time series through empirical results.\nOur results show that ECLAD-ts effectively explains models by leveraging their\ninternal representations, providing useful insights about their prediction\nprocess.",
        "published": "2025-04-07T12:49:20+00:00"
    },
    {
        "title": "Solving the fully nonlinear Monge-Amp\u00e8re equation using the Legendre-Kolmogorov-Arnold Network method",
        "authors": [
            "Bingcheng Hu",
            "Lixiang Jin",
            "Zhaoxiang Li"
        ],
        "summary": "In this paper, we propose a novel neural network framework, the\nLegendre-Kolmogorov-Arnold Network (Legendre-KAN) method, designed to solve\nfully nonlinear Monge-Amp\\`ere equations with Dirichlet boundary conditions.\nThe architecture leverages the orthogonality of Legendre polynomials as basis\nfunctions, significantly enhancing both convergence speed and solution accuracy\ncompared to traditional methods. Furthermore, the Kolmogorov-Arnold\nrepresentation theorem provides a strong theoretical foundation for the\ninterpretability and optimization of the network. We demonstrate the\neffectiveness of the proposed method through numerical examples, involving both\nsmooth and singular solutions in various dimensions. This work not only\naddresses the challenges of solving high-dimensional and singular\nMonge-Amp\\`ere equations but also highlights the potential of neural\nnetwork-based approaches for complex partial differential equations.\nAdditionally, the method is applied to the optimal transport problem in image\nmapping, showcasing its practical utility in geometric image transformation.\nThis approach is expected to pave the way for further enhancement of KAN-based\napplications and numerical solutions of PDEs across a wide range of scientific\nand engineering fields.",
        "published": "2025-04-07T12:47:47+00:00"
    },
    {
        "title": "Entangling two Rydberg Superatoms via Heralded Storage",
        "authors": [
            "Zi-Ye An",
            "Bo-Wei Lu",
            "Jun Li",
            "Chao-Wei Yang",
            "Li Li",
            "Xiao-Hui Bao",
            "Jian-Wei Pan"
        ],
        "summary": "Heralded storage of photons is crucial for advancing quantum networks.\nPrevious realizations have primarily relied on single atoms strongly coupled to\noptical cavities. In this work, we present the experimental realization of\nheralded storage using a Rydberg superatom, a mesoscopic atomic ensemble\noperating in the strong blockade regime. In our approach, an input photon is\ninitially stored in the superatom via electromagnetically induced transparency.\nSubsequently, a second photon is emitted conditioned on the success of the\nfirst photon's storage. Due to the collectively enhanced interaction, both the\nstorage and the emission of the herald photon can be rather efficient in\nprinciple. As a demonstration of this technique, we use it to entangle two\nremote Rydberg superatoms. This method obviates the need for an intermediate\nnode, which is commonly employed in traditional interference-based remote\nentanglement schemes. Our results showcase the potential of performing\ncavity-QED-like experiments with Rydberg superatoms. This work opens pathways\nfor numerous applications in quantum networks and linear optical quantum\ncomputing.",
        "published": "2025-04-07T12:47:29+00:00"
    },
    {
        "title": "Radio frequency single electron transmission spectroscopy of a semiconductor Si/SiGe quantum dot",
        "authors": [
            "I. Fattal",
            "J. Van Damme",
            "B. Raes",
            "C. Godfrin",
            "G. Jaliel",
            "K. Chen",
            "T. Van Caekenberghe",
            "A. Loenders",
            "S. Kubicek",
            "S. Massar",
            "Y. Canvel",
            "J. Jussot",
            "Y. Shimura",
            "R. Loo",
            "D. Wan",
            "M. Mongillo",
            "K. De Greve"
        ],
        "summary": "Rapid single shot spin readout is a key ingredient for fault tolerant quantum\ncomputing with spin qubits. An RF-SET (radio-frequency single electron\ntransistor) is predominantly used as its the readout timescale is far shorter\nthan the spin decoherence time. In this work, we experimentally demonstrate a\ntransmission-based RF-SET using a multi-module semiconductor-superconductor\nassembly. A monolithically integrated SET placed next to a double quantum dot\nin a Si/SiGe heterostructure is wire-bonded to a superconducting niobium\ninductor forming the impedance-transforming network. Compared to RF\nreflectometry, the proposed set-up is experimentally simpler without the need\nfor directional couplers. Read-out performance is benchmarked by the\nsignal-to-noise (SNR) of a dot-reservoir transition (DRT) and an interdot\ncharge transition (ICT) in the double quantum dot near the SET as a function of\nRF power and integration time. The minimum integration time for unitary SNR is\nfound to be 100 ns for ICT and 300 ns for DRT. The obtained minimum integration\ntimes are comparable to the state of the art in conventional RF reflectometry\nset-ups. Furthermore, we study the turn-on properties of the RF-SET to\ninvestigate capacitive shifts and RF losses. Understanding these effects are\ncrucial for further optimisations of the impedance transforming network as well\nas the device design to assist RF read-out. This new RF read-out scheme also\nshows promise for multiplexing spin-qubit readout and further studies on rapid\ncharge dynamics in quantum dots.",
        "published": "2025-04-07T12:40:45+00:00"
    },
    {
        "title": "Deconstructing Jazz Piano Style Using Machine Learning",
        "authors": [
            "Huw Cheston",
            "Reuben Bance",
            "Peter M. C. Harrison"
        ],
        "summary": "Artistic style has been studied for centuries, and recent advances in machine\nlearning create new possibilities for understanding it computationally.\nHowever, ensuring that machine-learning models produce insights aligned with\nthe interests of practitioners and critics remains a significant challenge.\nHere, we focus on musical style, which benefits from a rich theoretical and\nmathematical analysis tradition. We train a variety of supervised-learning\nmodels to identify 20 iconic jazz musicians across a carefully curated dataset\nof 84 hours of recordings, and interpret their decision-making processes. Our\nmodels include a novel multi-input architecture that enables four musical\ndomains (melody, harmony, rhythm, and dynamics) to be analysed separately.\nThese models enable us to address fundamental questions in music theory and\nalso advance the state-of-the-art in music performer identification (94%\naccuracy across 20 classes). We release open-source implementations of our\nmodels and an accompanying web application for exploring musical styles.",
        "published": "2025-04-07T12:37:39+00:00"
    },
    {
        "title": "Can audio recordings be used to detect leaks and coughs during mechanical insufflation exsufflation (MI-E) treatment?",
        "authors": [
            "Jonathan Brady",
            "Vijay Chandiramani",
            "Michelle Chatwin",
            "Emmanuele Lwele",
            "Aminat Yetunde Saula",
            "Peter Stewart",
            "Toby Stokes"
        ],
        "summary": "This report relates to a study group hosted by the EPSRC funded network,\nIntegrating data-driven BIOphysical models into REspiratory MEdicine (BIOREME),\nand supported by SofTMech and Innovate UK, Business Connect. The BIOREME\nnetwork hosts events, including this study group, to bring together\nmulti-disciplinary researchers, clinicians, companies and charities to catalyse\nresearch in the applications of mathematical modelling for respiratory\nmedicine. The goal of this study group was to provide an interface between\ncompanies, clinicians, and mathematicians to develop mathematical tools to the\nproblems presented. The study group was held at The University of Glasgow on\nthe 17 - 21 June 2024 and was attended by 16 participants from 8 different\ninstitutions. Below details the technical report of one of the challenges and\nthe methods developed by the team of researchers who worked on this challenge.",
        "published": "2025-04-07T12:32:01+00:00"
    },
    {
        "title": "Energy Gap Modulation in Proximitized Superconducting Puddles of Graphene",
        "authors": [
            "Yuxiao Wu",
            "Udit Khanna",
            "Eyal Walach",
            "Efrat Shimshoni",
            "Aviad Frydman"
        ],
        "summary": "We investigated proximity-induced superconductivity in a graphene-insulating\nInO bilayer system through gate-controlled transport measurements. Distinct\noscillations in the differential conductance are observed across both the\nelectron and hole doping regimes, with oscillation amplitudes increasing as the\nchemical potential moves away from the Dirac point. These findings are\nexplained using a theoretical model of a normal-superconductor-normal (NSN)\njunction, which addresses reflection and transmission probabilities at normal\nincidence. From this model, we extract key parameters for the proximitized\ngraphene, including the superconducting energy gap Delta and the effective\nlength scale Ls of the superconducting regions. Near the Dirac point, we\nobserve a minimal Ls and a maximal Delta, aligning with the theory that the gap\nin strongly disordered superconductors increases as the coherence length of\nlocalized pairs decreases. This suggests that spatial confinement in a\nlow-density superconductor leads to an effective increase in the\nsuperconducting gap.",
        "published": "2025-04-07T12:27:11+00:00"
    },
    {
        "title": "SurvSurf: a partially monotonic neural network for first-hitting time prediction of intermittently observed discrete and continuous sequential events",
        "authors": [
            "Yichen Kelly Chen",
            "S\u00f6ren Dittmer",
            "Kinga Bernatowicz",
            "Josep Ar\u00fas-Pous",
            "Kamen Bliznashki",
            "John Aston",
            "James H. F. Rudd",
            "Carola-Bibiane Sch\u00f6nlieb",
            "James Jones",
            "Michael Roberts"
        ],
        "summary": "We propose a neural-network based survival model (SurvSurf) specifically\ndesigned for direct and simultaneous probabilistic prediction of the first\nhitting time of sequential events from baseline. Unlike existing models,\nSurvSurf is theoretically guaranteed to never violate the monotonic\nrelationship between the cumulative incidence functions of sequential events,\nwhile allowing nonlinear influence from predictors. It also incorporates\nimplicit truths for unobserved intermediate events in model fitting, and\nsupports both discrete and continuous time and events. We also identified a\nvariant of the Integrated Brier Score (IBS) that showed robust correlation with\nthe mean squared error (MSE) between the true and predicted probabilities by\naccounting for implied truths about the missing intermediate events. We\ndemonstrated the superiority of SurvSurf compared to modern and traditional\npredictive survival models in two simulated datasets and two real-world\ndatasets, using MSE, the more robust IBS and by measuring the extent of\nmonotonicity violation.",
        "published": "2025-04-07T12:24:59+00:00"
    },
    {
        "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs",
        "authors": [
            "Ling Hu",
            "Yuemei Xu",
            "Xiaoyang Gu",
            "Letao Han"
        ],
        "summary": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.",
        "published": "2025-04-07T12:23:59+00:00"
    },
    {
        "title": "Wavelet Policy: Imitation Policy Learning in Frequency Domain with Wavelet Transforms",
        "authors": [
            "Changchuan Yang",
            "Yuhang Dong",
            "Guanzhong Tian",
            "Haizhou Ge",
            "Hongrui Zhu"
        ],
        "summary": "Recent imitation learning policies, often framed as time series prediction\ntasks, directly map robotic observations-such as high-dimensional visual data\nand proprioception-into the action space. While time series prediction\nprimarily relies on spatial domain modeling, the underutilization of frequency\ndomain analysis in robotic manipulation trajectory prediction may lead to\nneglecting the inherent temporal information embedded within action sequences.\nTo address this, we reframe imitation learning policies through the lens of the\nfrequency domain and introduce the Wavelet Policy. This novel approach employs\nwavelet transforms (WT) for feature preprocessing and extracts multi-scale\nfeatures from the frequency domain using the SE2MD (Single Encoder to Multiple\nDecoder) architecture. Furthermore, to enhance feature mapping in the frequency\ndomain and increase model capacity, we introduce a Learnable Frequency-Domain\nFilter (LFDF) after each frequency decoder, improving adaptability under\ndifferent visual conditions. Our results show that the Wavelet Policy\noutperforms state-of-the-art (SOTA) end-to-end methods by over 10% on four\nchallenging robotic arm tasks, while maintaining a comparable parameter count.\nIn long-range settings, its performance declines more slowly as task volume\nincreases. The code will be publicly available.",
        "published": "2025-04-07T12:16:45+00:00"
    },
    {
        "title": "A Nature-Inspired Colony of Artificial Intelligence System with Fast, Detailed, and Organized Learner Agents for Enhancing Diversity and Quality",
        "authors": [
            "Shan Suthaharan"
        ],
        "summary": "The concepts of convolutional neural networks (CNNs) and multi-agent systems\nare two important areas of research in artificial intelligence (AI). In this\npaper, we present an approach that builds a CNN-based colony of AI agents to\nserve as a single system and perform multiple tasks (e.g., predictions or\nclassifications) in an environment. The proposed system impersonates the\nnatural environment of a biological system, like an ant colony or a human\ncolony. The proposed colony of AI that is defined as a role-based system\nuniquely contributes to accomplish tasks in an environment by incorporating AI\nagents that are fast learners, detailed learners, and organized learners. These\nlearners can enhance their localized learning and their collective decisions as\na single system of colony of AI agents. This approach also enhances the\ndiversity and quality of the colony of AI with the help of Genetic Algorithms\nand their crossover and mutation mechanisms. The evolution of fast, detailed,\nand organized learners in the colony of AI is achieved by introducing a unique\none-to-one mapping between these learners and the pretrained VGG16, VGG19, and\nResNet50 models, respectively. This role-based approach creates two parent-AI\nagents using the AI models through the processes, called the intra- and\ninter-marriage of AI, so that they can share their learned knowledge (weights\nand biases) based on a probabilistic rule and produce diversified child-AI\nagents to perform new tasks. This process will form a colony of AI that\nconsists of families of multi-model and mixture-model AI agents to improve\ndiversity and quality. Simulations show that the colony of AI, built using the\nVGG16, VGG19, and ResNet50 models, can provide a single system that generates\nchild-AI agents of excellent predictive performance, ranging between 82% and\n95% of F1-scores, to make diversified collective and quality decisions on a\ntask.",
        "published": "2025-04-07T12:13:14+00:00"
    },
    {
        "title": "DiCoTTA: Domain-invariant Learning for Continual Test-time Adaptation",
        "authors": [
            "Sohyun Lee",
            "Nayeong Kim",
            "Juwon Kang",
            "Seong Joon Oh",
            "Suha Kwak"
        ],
        "summary": "This paper studies continual test-time adaptation (CTTA), the task of\nadapting a model to constantly changing unseen domains in testing while\npreserving previously learned knowledge. Existing CTTA methods mostly focus on\nadaptation to the current test domain only, overlooking generalization to\narbitrary test domains a model may face in the future. To tackle this\nlimitation, we present a novel online domain-invariant learning framework for\nCTTA, dubbed DiCoTTA. DiCoTTA aims to learn feature representation to be\ninvariant to both current and previous test domains on the fly during testing.\nTo this end, we propose a new model architecture and a test-time adaptation\nstrategy dedicated to learning domain-invariant features without corrupting\nsemantic contents, along with a new data structure and optimization algorithm\nfor effectively managing information from previous test domains. DiCoTTA\nachieved state-of-the-art performance on four public CTTA benchmarks. Moreover,\nit showed superior generalization to unseen test domains.",
        "published": "2025-04-07T12:09:18+00:00"
    },
    {
        "title": "An all optical broadband tunable quantum frequency shifter",
        "authors": [
            "Li Chen",
            "Zhi-Yuan Zhou",
            "Ming-Yuan Gao",
            "Wu-Zhen L",
            "Zhao-Qi-Zhi Han",
            "Yue-Wei Song",
            "Ren-Hui Chen",
            "Bao-Sen Shi"
        ],
        "summary": "A frequency shifter of the photon is a key component for\nfrequency-multiplexed high-capacity quantum communications and\nfrequency-encoded quantum computation. Existed methods for shifting the\nfrequency of a photon based on electro-optical, or acousto-optical effect,\nhowever, suffer the limited frequency shift up to a few hundreds of GHz,\nfurthermore, high-quality micro-wave electronics are required. The frequency of\na photon can also be shifted with the frequency difference equal to the\nfrequency of pump laser by using an all optical-wave-mixing approach, which is\nusually about tens of THz. So, there is a big frequency shifting gap between\nthese methods. Here, we propose a new scheme of a quantum frequency shifter\nbased on an all-optical wave-mixing process, which can theoretically achieve a\nfrequency shift ranging from GHz to a few THz, therefore bridging the gap. As a\nprinciple of poof, by using two pump beams in a three-wave mixing cascading\nprocess, a heralded single photon is frequency-shifted more than 400GHz, and\nthe shift can be tuned continuously over broadband by changing the frequency\ndifference between two pump lasers. Besides, high coincidence to accidence\nratio between the shifted photons and the heralded photon indicates the\npreserve of quantum properties. The present quantum frequency shifter is in\nanalog to an electro-optical based shifter, but with much broader tuning\nability. Our all-optical quantum frequency shifter will become a fundamental\nbuilding block for high-speed quantum communication networks and frequency\ndomain photonic quantum computation.",
        "published": "2025-04-07T11:47:57+00:00"
    },
    {
        "title": "Probabilistic imaginary-time evolution in state-vector-based and shot-based simulations and on quantum devices",
        "authors": [
            "Satoshi Ejima",
            "Kazuhiro Seki",
            "Benedikt Fauseweh",
            "Seiji Yunoki"
        ],
        "summary": "Imaginary-time evolution, an important technique in tensor network and\nquantum Monte Carlo algorithms on classical computers, has recently been\nadapted to quantum computing. In this study, we focus on probabilistic\nimaginary-time evolution (PITE) algorithm and derive its formulation in the\ncontext of state-vector-based simulations, where quantum state vectors are\ndirectly used to compute observables without statistical errors. We compare the\nresults with those of shot-based simulations, which estimate observables\nthrough repeated projective measurements. Applying the PITE algorithm to the\nHeisenberg chain, we investigate optimal initial conditions for convergence. We\nfurther demonstrate the method on the transverse-field Ising model using a\nstate-of-the-art trapped-ion quantum device. Finally, we explore the potential\nof error mitigation in this framework, highlighting practical considerations\nfor near-term digital quantum simulations.",
        "published": "2025-04-07T11:45:31+00:00"
    },
    {
        "title": "REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with Exemplar-Based Identity Conditioning",
        "authors": [
            "Jihyun Lee",
            "Weipeng Xu",
            "Alexander Richard",
            "Shih-En Wei",
            "Shunsuke Saito",
            "Shaojie Bai",
            "Te-Li Wang",
            "Minhyuk Sung",
            "Tae-Kyun Kim",
            "Jason Saragih"
        ],
        "summary": "We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a\none-step diffusion model for real-time, high-fidelity human motion estimation\nfrom egocentric image inputs. While an existing method for egocentric\nwhole-body (i.e., body and hands) motion estimation is non-real-time and\nacausal due to diffusion-based iterative motion refinement to capture\ncorrelations between body and hand poses, REWIND operates in a fully causal and\nreal-time manner. To enable real-time inference, we introduce (1) cascaded\nbody-hand denoising diffusion, which effectively models the correlation between\negocentric body and hand motions in a fast, feed-forward manner, and (2)\ndiffusion distillation, which enables high-quality motion estimation with a\nsingle denoising step. Our denoising diffusion model is based on a modified\nTransformer architecture, designed to causally model output motions while\nenhancing generalizability to unseen motion lengths. Additionally, REWIND\noptionally supports identity-conditioned motion estimation when identity prior\nis available. To this end, we propose a novel identity conditioning method\nbased on a small set of pose exemplars of the target identity, which further\nenhances motion estimation quality. Through extensive experiments, we\ndemonstrate that REWIND significantly outperforms the existing baselines both\nwith and without exemplar-based identity conditioning.",
        "published": "2025-04-07T11:44:11+00:00"
    },
    {
        "title": "One Quantizer is Enough: Toward a Lightweight Audio Codec",
        "authors": [
            "Linwei Zhai",
            "Han Ding",
            "Cui Zhao",
            "fei wang",
            "Ge Wang",
            "Wang Zhi",
            "Wei Xi"
        ],
        "summary": "Neural audio codecs have recently gained traction for their ability to\ncompress high-fidelity audio and generate discrete tokens that can be utilized\nin downstream generative modeling tasks. However, leading approaches often rely\non resource-intensive models and multi-quantizer architectures, resulting in\nconsiderable computational overhead and constrained real-world applicability.\nIn this paper, we present SQCodec, a lightweight neural audio codec that\nleverages a single quantizer to address these limitations. SQCodec explores\nstreamlined convolutional networks and local Transformer modules, alongside\nTConv, a novel mechanism designed to capture acoustic variations across\nmultiple temporal scales, thereby enhancing reconstruction fidelity while\nreducing model complexity. Extensive experiments across diverse datasets show\nthat SQCodec achieves audio quality comparable to multi-quantizer baselines,\nwhile its single-quantizer design offers enhanced adaptability and its\nlightweight architecture reduces resource consumption by an order of magnitude.\nThe source code is publicly available at https://github.com/zhai-lw/SQCodec.",
        "published": "2025-04-07T11:34:39+00:00"
    },
    {
        "title": "Physics for the environment and sustainable development",
        "authors": [
            "J\u00fcrgen Kurths",
            "Ankit Agarwal",
            "Ugur \u00d6zt\u00fcrk",
            "Shubham Sharma",
            "Norbert Marwan",
            "Deniz Eroglu"
        ],
        "summary": "A reliable understanding of the Earth system is essential for the life\nquality of modern society. Natural hazards are the cause of most life and\nresource losses. The ability to define the conditions for a sustainable\ndevelopment of humankind, to keep the Earth system within the boundaries of\nhabitable states, or to predict critical transitions and events in the dynamics\nof the Earth system are crucial to mitigate and adapt to Earth system related\nevents and changes (e.g., volcanic eruptions, earthquakes, climate change) and\nto avert the disastrous consequences of natural hazards. In this chapter, we\ndiscuss key concepts from nonlinear physics and show that they enable us to\ntreat challenging problems of Earth sciences which cannot be solved by classic\nmethods. In particular, the concepts of multi-scaling, recurrence,\nsynchronization, and complex networks have become crucial in the very last\ndecades for a substantially more profound understanding of the dynamics of\nearthquakes, landslides, or (palaeo-)climate. They can even provide a\nsignificantly improved prediction of several high-impact extreme events.\nAdditionally, crucial open challenges in the realm of methodological nature and\napplications to Earth sciences are given.",
        "published": "2025-04-07T11:33:57+00:00"
    },
    {
        "title": "Phase transitions in swarm optimization algorithms",
        "authors": [
            "Tom\u00e1\u0161 Vantuch",
            "Ivan Zelinka",
            "Andrew Adamatzky",
            "Norbert Marwan"
        ],
        "summary": "Natural systems often exhibit chaotic behavior in their space-time evolution.\nSystems transiting between chaos and order manifest a potential to compute, as\nshown with cellular automata and artificial neural networks. We demonstrate\nthat swarms optimisation algorithms also exhibit transitions from chaos,\nanalogous to motion of gas molecules, when particles explore solution space\ndisorderly, to order, when particles follow a leader, similar to molecules\npropagating along diffusion gradients in liquid solutions of reagents. We\nanalyse these `phase-like' transitions in swarm optimization algorithms using\nrecurrence quantification analysis and Lempel-Ziv complexity estimation. We\ndemonstrate that converging and non-converging iterations of the optimization\nalgorithms are statistically different in a view of applied chaos, complexity\nand predictability estimating indicators.",
        "published": "2025-04-07T11:33:49+00:00"
    },
    {
        "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
        "authors": [
            "Yousef Alhessi",
            "S\u00f3lr\u00fan Halla Einarsd\u00f3ttir",
            "George Granberry",
            "Emily First",
            "Moa Johansson",
            "Sorin Lerner",
            "Nicholas Smallbone"
        ],
        "summary": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Our results\nindicate that neural and symbolic techniques are complementary. By leveraging\nthe best of both symbolic and neural methods we can generate useful lemmas for\na wide range of input domains, facilitating computer-assisted theory\ndevelopment and formalization.",
        "published": "2025-04-07T11:30:36+00:00"
    },
    {
        "title": "The Kratos Framework for Heterogeneous Astrophysical Simulations: Ray Tracing, Reacting Flow and Thermochemistry",
        "authors": [
            "Lile Wang"
        ],
        "summary": "Thermochemistry, ray-tracing radiation, and radiation-matter interactions are\nimportant processes which are computationally difficult to model in\nastrophysical simulations, addressed by introducing novel algorithms optimized\nfor heterogeneous architectures in the Kratos framework. Key innovations\ninclude a stoichiometry-compatible reconstruction scheme for consistent\nchemical species advection, which ensures element conservation while avoiding\nmatrix inversions, and a LU decomposition method specifically designed for\nmulti-thread parallelization in order to solve stiff thermochemical ordinary\ndifferential equations with high efficiency. The framework also implements\nefficient ray-tracing techniques for radiation transport for radiation-matter\ninteractions. Various verification tests, spanning from chemical advection,\ncombustion, Str\\\"omgren spheres, and detonation dynamics, are conducted to\ndemonstrate the accuracy and robustness of Kratos, with results closely\nmatching semi-analytic solutions and benchmarks such as Cantera and the Shock\nand Detonation Toolbox. The modular design and performance optimizations\nposition it as a versatile tool for studying coupled microphysical processes in\nthe diverse environments of contemporary astrophysical studies.",
        "published": "2025-04-07T11:28:48+00:00"
    },
    {
        "title": "Fine tuning generative adversarial networks with universal force fields: application to two-dimensional topological insulators",
        "authors": [
            "Alexander C. Tyner"
        ],
        "summary": "Despite rapid growth in use cases for generative artificial intelligence, its\nability to design purpose built crystalline materials remains in a nascent\nphase. At the moment inverse design is generally accomplished by either\nconstraining the training data set or producing a vast number of samples from a\ngenerator network and constraining the output via post-processing. We show that\na general adversarial network trained to produce crystal structures from a\nlatent space can be fine tuned through the introduction of advanced graph\nneural networks as discriminators, including a universal force field, to\nintrinsically bias the network towards generation of target materials. This is\nexemplified utilizing two-dimensional topological insulators as a sample target\nspace. While a number of two-dimensional topological insulators have been\npredicted, the size of the band-gap, a measure of topological protection,\nremains a concern in most candidate compounds. The resulting generative network\nis shown to yield novel topological insulators.",
        "published": "2025-04-07T11:25:44+00:00"
    },
    {
        "title": "RCCFormer: A Robust Crowd Counting Network Based on Transformer",
        "authors": [
            "Peng Liu",
            "Heng-Chao Li",
            "Sen Lei",
            "Nanqing Liu",
            "Bin Feng",
            "Xiao Wu"
        ],
        "summary": "Crowd counting, which is a key computer vision task, has emerged as a\nfundamental technology in crowd analysis and public safety management. However,\nchallenges such as scale variations and complex backgrounds significantly\nimpact the accuracy of crowd counting. To mitigate these issues, this paper\nproposes a robust Transformer-based crowd counting network, termed RCCFormer,\nspecifically designed for background suppression and scale awareness. The\nproposed method incorporates a Multi-level Feature Fusion Module (MFFM), which\nmeticulously integrates features extracted at diverse stages of the backbone\narchitecture. It establishes a strong baseline capable of capturing intricate\nand comprehensive feature representations, surpassing traditional baselines.\nFurthermore, the introduced Detail-Embedded Attention Block (DEAB) captures\ncontextual information and local details through global self-attention and\nlocal attention along with a learnable manner for efficient fusion. This\nenhances the model's ability to focus on foreground regions while effectively\nmitigating background noise interference. Additionally, we develop an Adaptive\nScale-Aware Module (ASAM), with our novel Input-dependent Deformable\nConvolution (IDConv) as its fundamental building block. This module dynamically\nadapts to changes in head target shapes and scales, significantly improving the\nnetwork's capability to accommodate large-scale variations. The effectiveness\nof the proposed method is validated on the ShanghaiTech Part_A and Part_B,\nNWPU-Crowd, and QNRF datasets. The results demonstrate that our RCCFormer\nachieves excellent performance across all four datasets, showcasing\nstate-of-the-art outcomes.",
        "published": "2025-04-07T11:19:05+00:00"
    },
    {
        "title": "Boosting Relational Deep Learning with Pretrained Tabular Models",
        "authors": [
            "Veronica Lachi",
            "Antonio Longa",
            "Beatrice Bevilacqua",
            "Bruno Lepri",
            "Andrea Passerini",
            "Bruno Ribeiro"
        ],
        "summary": "Relational databases, organized into tables connected by primary-foreign key\nrelationships, are a common format for organizing data. Making predictions on\nrelational data often involves transforming them into a flat tabular format\nthrough table joins and feature engineering, which serve as input to tabular\nmethods. However, designing features that fully capture complex relational\npatterns remains challenging. Graph Neural Networks (GNNs) offer a compelling\nalternative by inherently modeling these relationships, but their time overhead\nduring inference limits their applicability for real-time scenarios. In this\nwork, we aim to bridge this gap by leveraging existing feature engineering\nefforts to enhance the efficiency of GNNs in relational databases.\nSpecifically, we use GNNs to capture complex relationships within relational\ndatabases, patterns that are difficult to featurize, while employing engineered\nfeatures to encode temporal information, thereby avoiding the need to retain\nthe entire historical graph and enabling the use of smaller, more efficient\ngraphs. Our \\textsc{LightRDL} approach not only improves efficiency, but also\noutperforms existing models. Experimental results on the RelBench benchmark\ndemonstrate that our framework achieves up to $33\\%$ performance improvement\nand a $526\\times$ inference speedup compared to GNNs, making it highly suitable\nfor real-time inference.",
        "published": "2025-04-07T11:19:04+00:00"
    },
    {
        "title": "Advanced Codebook Design for SCMA-aided NTNs With Randomly Distributed Users",
        "authors": [
            "Tianyang Hu",
            "Qu Luo",
            "Lixia Xiao",
            "Jiaxi Zhou",
            "Pei Xiao",
            "Tao Jiang"
        ],
        "summary": "In this letter, a novel class of sparse codebooks is proposed for sparse code\nmultiple access (SCMA) aided non-terrestrial networks (NTN) with randomly\ndistributed users characterized by Rician fading channels. Specifically, we\nfirst exploit the upper bound of bit error probability (BEP) of an SCMA-aided\nNTN with large-scale fading of different users under Rician fading channels.\nThen, the codebook is designed by employing pulse-amplitude modulation\nconstellation, user-specific rotation and power factors. To further reduce the\noptimization complexity while maintaining the power diversity of different\nusers, an orthogonal layer-assisted joint layer and power assignment strategy\nis proposed. Finally, unlike existing SCMA codebook designs that treat all\nusers as one super-user, we propose to minimize the BEP of the worst user to\nensure user fairness. The simulation results show that the proposed scheme is\ncapable of providing a substantial performance gain over conventional\ncodebooks.",
        "published": "2025-04-07T11:10:44+00:00"
    },
    {
        "title": "Fault Localisation in Infinite-Dimensional Linear Electrical Networks",
        "authors": [
            "Daniel Selvaratnam",
            "Alessio Moreschini",
            "Amritam Das",
            "Thomas Parisini",
            "Henrik Sandberg"
        ],
        "summary": "We present a novel fault localisation methodology for linear time-invariant\nelectrical networks with infinite-dimensional edge dynamics and uncertain fault\ndynamics. The theory accommodates instability and also bounded propagation\ndelays in the network. The goal is to estimate the location of a fault along a\ngiven network edge, using sensors positioned arbitrarily throughout the\nnetwork. Passive faults of unknown impedance are considered, along with stable\nfaults of known impedance. To illustrate the approach, we tackle a significant\nuse-case: a multi-conductor transmission line, with dynamics modelled by the\nTelegrapher's equation, subject to a line-to-ground fault. Frequency-domain\ninsights are used to reformulate the general fault localisation problem into a\nnon-convex scalar optimisation problem, of which the true fault location is\nguaranteed to be a global minimiser. Numerical experiments are run to quantify\nlocalisation performance over a range of fault resistances.",
        "published": "2025-04-07T10:40:25+00:00"
    },
    {
        "title": "Cloud-Fog Automation: The New Paradigm towards Autonomous Industrial Cyber-Physical Systems",
        "authors": [
            "Jiong Jin",
            "Zhibo Pang",
            "Jonathan Kua",
            "Quanyan Zhu",
            "Karl H. Johansson",
            "Nikolaj Marchenko",
            "Dave Cavalcanti"
        ],
        "summary": "Autonomous Industrial Cyber-Physical Systems (ICPS) represent a future vision\nwhere industrial systems achieve full autonomy, integrating physical processes\nseamlessly with communication, computing and control technologies while\nholistically embedding intelligence. Cloud-Fog Automation is a new digitalized\nindustrial automation reference architecture that has been recently proposed.\nThis architecture is a fundamental paradigm shift from the traditional\nInternational Society of Automation (ISA)-95 model to accelerate the\nconvergence and synergy of communication, computing, and control towards a\nfully autonomous ICPS. With the deployment of new wireless technologies to\nenable almost-deterministic ultra-reliable low-latency communications, a joint\ndesign of optimal control and computing has become increasingly important in\nmodern ICPS. It is also imperative that system-wide cyber-physical security are\ncritically enforced. Despite recent advancements in the field, there are still\nsignificant research gaps and open technical challenges. Therefore, a\ndeliberate rethink in co-designing and synergizing communications, computing,\nand control (which we term \"3C co-design\") is required. In this paper, we\nposition Cloud-Fog Automation with 3C co-design as the new paradigm to realize\nthe vision of autonomous ICPS. We articulate the state-of-the-art and future\ndirections in the field, and specifically discuss how goal-oriented\ncommunication, virtualization-empowered computing, and Quality of Service\n(QoS)-aware control can drive Cloud-Fog Automation towards a fully autonomous\nICPS, while accounting for system-wide cyber-physical security.",
        "published": "2025-04-07T10:36:26+00:00"
    },
    {
        "title": "Understanding and Design of Interstitial Oxygen Conductors",
        "authors": [
            "Jun Meng"
        ],
        "summary": "Highly efficient oxygen active materials that react with, absorb, and\ntransport oxygen is essential for fuel cells, electrolyzers and related\napplications. While vacancy mediated oxygen ion conductors have long been the\nfocus of research, they are limited by high migration barriers at intermediate\ntemperatures, which hinder their practical applications. In contrast,\ninterstitial oxygen conductors exhibit significantly lower migration barriers\nenabling faster ionic conductivity at lower temperatures. This review\nsystematically examines both well established and recently identified families\nof interstitial oxygen ion conductors, focusing on how their unique structural\nmotifs such as corner sharing polyhedral frameworks, isolated polyhedral, and\ncage like architectures, facilitate low migration barriers through interstitial\nand interstitialcy diffusion mechanisms. A central discussion of this review\nfocuses on the evolution of design strategies, from targeted donor doping,\nelement screening, and physical intuition descriptor material discovery, which\nleverage computational tools to explore vast chemical spaces in search of new\ninterstitial conductors. The success of these strategies demonstrates that a\nsignificant, largely unexplored space remains for discovering high performing\ninterstitial oxygen conductors. Crucial features enabling high performance\ninterstitial oxygen diffusion include the availability of electrons for oxygen\nreduction and sufficient structural flexibility with accessible volume for\ninterstitial accommodation. This review concludes with a forward looking\nperspective, proposing a knowledge driven methodology that integrates current\nunderstanding with data centric approaches to identify promising interstitial\noxygen conductors outside traditional search paradigms.",
        "published": "2025-04-07T10:08:31+00:00"
    },
    {
        "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models",
        "authors": [
            "Justus Westerhoff",
            "Erblina Purellku",
            "Jakob Hackstein",
            "Leo Pinetzki",
            "Lorenz Hufe"
        ],
        "summary": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM.",
        "published": "2025-04-07T10:01:38+00:00"
    },
    {
        "title": "Pyroelectric doping reversal of MoS2 p-n junctions on ferroelectric domain walls probed by photoluminescence",
        "authors": [
            "Javier Fern\u00e1ndez-Mart\u00ednez",
            "Joan J. Ronquillo",
            "Guillermo L\u00f3pez-Pol\u00edn",
            "Herko P. van der Meulen",
            "Mariola O Ram\u00edrez",
            "Luisa E. Baus\u00e1"
        ],
        "summary": "Tailoring the optical properties and electronic doping in transition metal\ndichalcogenides (TMDs) is a central strategy for developing innovative systems\nwith tunable characteristics. In this context, pyroelectric materials, which\nhold the capacity for charge generation when subjected to temperature changes,\noffer a promising route for this modulation. This work employs spatially\nresolved photoluminescence (PL) to explore the impact of pyroelectricity on the\nelectronic doping of monolayer MoS2 deposited on periodically poled LiNbO3 (LN)\nsubstrates. The results demonstrate that pyroelectricity in LN modulates the\ncharge carrier density in MoS2 on ferroelectric surfaces acting as doping\nmechanism without the need for gating electrodes. Furthermore, upon cooling,\npyroelectric charges effectively reverse the doping of p-n junctions on DWs,\nconverting them into n-p junctions. These findings highlight the potential of\npyroelectric substrates for tunable and configurable charge engineering in\ntransition metal dichalcogenides and suggest their applicability to other\ncombinations of 2D materials and ferroelectric substrates. They also open\navenues for alternative device architectures in nanoelectronic or nanophotonic\ndevices including switches, memories or sensors.",
        "published": "2025-04-07T09:52:37+00:00"
    },
    {
        "title": "Parallelization is All System Identification Needs: End-to-end Vibration Diagnostics on a multi-core RISC-V edge device",
        "authors": [
            "Amirhossein Kiamarzi",
            "Amirhossein Moallemi",
            "Federica Zonzini",
            "Davide Brunelli",
            "Davide Rossi",
            "Giuseppe Tagliavini"
        ],
        "summary": "The early detection of structural malfunctions requires the installation of\nreal-time monitoring systems ensuring continuous access to the damage-sensitive\ninformation; nevertheless, it can generate bottlenecks in terms of bandwidth\nand storage. Deploying data reduction techniques at the edge is recognized as a\nproficient solution to reduce the system's network traffic. However, the most\neffective solutions currently employed for the purpose are based on memory and\npower-hungry algorithms, making their embedding on resource-constrained devices\nvery challenging; this is the case of vibration data reduction based on System\nIdentification models. This paper presents PARSY-VDD, a fully optimized\nPArallel end-to-end software framework based on SYstem identification for\nVibration-based Damage Detection, as a suitable solution to perform damage\ndetection at the edge in a time and energy-efficient manner, avoiding streaming\nraw data to the cloud. We evaluate the damage detection capabilities of\nPARSY-VDD with two benchmarks: a bridge and a wind turbine blade, showcasing\nthe robustness of the end-to-end approach. Then, we deploy PARSY-VDD on both\ncommercial single-core and a specific multi-core edge device. We introduce an\narchitecture-agnostic algorithmic optimization for SysId, improving the\nexecution by 90x and reducing the consumption by 85x compared with the\nstate-of-the-art SysId implementation on GAP9. Results show that by utilizing\nthe unique parallel computing capabilities of GAP9, the execution time is\n751{\\mu}s with the high-performance multi-core solution operating at 370MHz and\n0.8V, while the energy consumption is 37{\\mu}J with the low-power solution\noperating at 240MHz and 0.65V. Compared with other single-core implementations\nbased on STM32 microcontrollers, the GAP9 high-performance configuration is 76x\nfaster, while the low-power configuration is 360x more energy efficient.",
        "published": "2025-04-07T09:51:02+00:00"
    },
    {
        "title": "Mixed memories in Hopfield networks",
        "authors": [
            "V\u00e9ronique Gayrard"
        ],
        "summary": "We consider the class of Hopfield models of associative memory with\nactivation function $F$ and state space $\\{-1,1\\}^N$, where each vertex of the\ncube describes a configuration of $N$ binary neurons. $M$ randomly chosen\nconfigurations, called patterns, are stored using an energy function designed\nto make them local minima. If they are, which is known to depend on how $M$\nscales with $N$, then they can be retrieved using a dynamics that decreases the\nenergy. However, storing the patterns in the energy function also creates\nunintended local minima, and thus false memories. Although this has been known\nsince the earliest work on the subject, it has only been supported by numerical\nsimulations and non-rigorous calculations, except in elementary cases.\n  Our results are twofold. For a generic function $F$, we explicitly construct\na set of configurations, called mixed memories, whose properties are intended\nto characterise the local minima of the energy function. For three prominent\nmodels, namely the classical, the dense and the modern Hopfield models,\nobtained for quadratic, polynomial and exponential functions $F$ respectively,\nwe give conditions on the growth rate of $M$ which guarantee that, as $N$\ndiverges, mixed memories are fixed points of the retrieval dynamics and thus\nexact minima of the energy. We conjecture that in this regime, all local minima\nare mixed memories.",
        "published": "2025-04-07T09:41:49+00:00"
    },
    {
        "title": "Analysis and Computation of Geodesic Distances on Reductive Homogeneous Spaces",
        "authors": [
            "Remco Duits",
            "Gijs Bellaard",
            "Barbara Tumpach"
        ],
        "summary": "Many geometric machine learning and image analysis applications, require a\nleft-invariant metric on the 5D homogeneous space of 3D positions and\norientations SE(3)/SO(2). This is done in Equivariant Neural Networks (G-CNNs),\nor in PDE-Based Group Convolutional Neural Networks (PDE-G-CNNs), where the\nRiemannian metric enters in multilayer perceptrons, message passing, and\nmax-pooling over Riemannian balls.\n  In PDE-G-CNNs it is proposed to take the minimum left-invariant Riemannian\ndistance over the fiber in SE(3)/SO(2), whereas in G-CNNs and in many geometric\nimage processing methods an efficient SO(2)-conjugation invariant section is\nadvocated.\n  The conjecture rises whether that computationally much more efficient section\nindeed always selects distance minimizers over the fibers. We show that this\nconjecture does NOT hold in general, and in the logarithmic norm approximation\nsetting used in practice we analyze the small (and sometimes vanishing)\ndifferences. We first prove that the minimal distance section is reached by\nminimal horizontal geodesics with constant momentum and zero acceleration along\nthe fibers, and we generalize this result to (reductive) homogeneous spaces\nwith legal metrics and commutative structure groups.",
        "published": "2025-04-07T09:41:12+00:00"
    },
    {
        "title": "Debate-Feedback: A Multi-Agent Framework for Efficient Legal Judgment Prediction",
        "authors": [
            "Xi Chen",
            "Mao Mao",
            "Shuo Li",
            "Haotian Shangguan"
        ],
        "summary": "The use of AI in legal analysis and prediction (LegalAI) has gained\nwidespread attention, with past research focusing on retrieval-based methods\nand fine-tuning large models. However, these approaches often require large\ndatasets and underutilize the capabilities of modern large language models\n(LLMs). In this paper, inspired by the debate phase of real courtroom trials,\nwe propose a novel legal judgment prediction model based on the Debate-Feedback\narchitecture, which integrates LLM multi-agent debate and reliability\nevaluation models. Unlike traditional methods, our model achieves significant\nimprovements in efficiency by minimizing the need for large historical\ndatasets, thus offering a lightweight yet robust solution. Comparative\nexperiments show that it outperforms several general-purpose and\ndomain-specific legal models, offering a dynamic reasoning process and a\npromising direction for future LegalAI research.",
        "published": "2025-04-07T09:34:14+00:00"
    },
    {
        "title": "Find A Winning Sign: Sign Is All We Need to Win the Lottery",
        "authors": [
            "Junghun Oh",
            "Sungyong Baik",
            "Kyoung Mu Lee"
        ],
        "summary": "The Lottery Ticket Hypothesis (LTH) posits the existence of a sparse\nsubnetwork (a.k.a. winning ticket) that can generalize comparably to its\nover-parameterized counterpart when trained from scratch. The common approach\nto finding a winning ticket is to preserve the original strong generalization\nthrough Iterative Pruning (IP) and transfer information useful for achieving\nthe learned generalization by applying the resulting sparse mask to an\nuntrained network. However, existing IP methods still struggle to generalize\ntheir observations beyond ad-hoc initialization and small-scale architectures\nor datasets, or they bypass these challenges by applying their mask to trained\nweights instead of initialized ones. In this paper, we demonstrate that the\nparameter sign configuration plays a crucial role in conveying useful\ninformation for generalization to any randomly initialized network. Through\nlinear mode connectivity analysis, we observe that a sparse network trained by\nan existing IP method can retain its basin of attraction if its parameter signs\nand normalization layer parameters are preserved. To take a step closer to\nfinding a winning ticket, we alleviate the reliance on normalization layer\nparameters by preventing high error barriers along the linear path between the\nsparse network trained by our method and its counterpart with initialized\nnormalization layer parameters. Interestingly, across various architectures and\ndatasets, we observe that any randomly initialized network can be optimized to\nexhibit low error barriers along the linear path to the sparse network trained\nby our method by inheriting its sparsity and parameter sign information,\npotentially achieving performance comparable to the original. The code is\navailable at https://github.com/JungHunOh/AWS\\_ICLR2025.git",
        "published": "2025-04-07T09:30:38+00:00"
    },
    {
        "title": "Futureproof Static Memory Planning",
        "authors": [
            "Christos Lamprakos",
            "Panagiotis Xanthopoulos",
            "Manolis Katsaragakis",
            "Sotirios Xydis",
            "Dimitrios Soudris",
            "Francky Catthoor"
        ],
        "summary": "The NP-complete combinatorial optimization task of assigning offsets to a set\nof buffers with known sizes and lifetimes so as to minimize total memory usage\nis called dynamic storage allocation (DSA). Existing DSA implementations bypass\nthe theoretical state-of-the-art algorithms in favor of either fast but\nwasteful heuristics, or memory-efficient approaches that do not scale beyond\none thousand buffers. The \"AI memory wall\", combined with deep neural networks'\nstatic architecture, has reignited interest in DSA. We present idealloc, a\nlow-fragmentation, high-performance DSA implementation designed for\nmillion-buffer instances. Evaluated on a novel suite of particularly hard\nbenchmarks from several domains, idealloc ranks first against four production\nimplementations in terms of a joint effectiveness/robustness criterion.",
        "published": "2025-04-07T09:28:54+00:00"
    },
    {
        "title": "Closed-Loop Neural Operator-Based Observer of Traffic Density",
        "authors": [
            "Alice Harting",
            "Karl Henrik Johansson",
            "Matthieu Barreau"
        ],
        "summary": "We consider the problem of traffic density estimation with sparse\nmeasurements from stationary roadside sensors. Our approach uses Fourier neural\noperators to learn macroscopic traffic flow dynamics from high-fidelity\nmicroscopic-level simulations. During inference, the operator functions as an\nopen-loop predictor of traffic evolution. To close the loop, we couple the\nopen-loop operator with a correction operator that combines the predicted\ndensity with sparse measurements from the sensors. Simulations with the SUMO\nsoftware indicate that, compared to open-loop observers, the proposed\nclosed-loop observer exhibit classical closed-loop properties such as\nrobustness to noise and ultimate boundedness of the error. This shows the\nadvantages of combining learned physics with real-time corrections, and opens\navenues for accurate, efficient, and interpretable data-driven observers.",
        "published": "2025-04-07T09:28:50+00:00"
    },
    {
        "title": "DyTTP: Trajectory Prediction with Normalization-Free Transformers",
        "authors": [
            "Yunxiang Liu",
            "Hongkuo Niu"
        ],
        "summary": "Accurate trajectory prediction is a cornerstone for the safe operation of\nautonomous driving systems, where understanding the dynamic behavior of\nsurrounding agents is crucial. Transformer-based architectures have\ndemonstrated significant promise in capturing complex spatio-temporality\ndependencies. However, their reliance on normalization layers can lead to\ncomputation overhead and training instabilities. In this work, we present a\ntwo-fold approach to address these challenges. First, we integrate DynamicTanh\n(DyT), which is the latest method to promote transformers, into the backbone,\nreplacing traditional layer normalization. This modification simplifies the\nnetwork architecture and improves the stability of the inference. We are the\nfirst work to deploy the DyT to the trajectory prediction task. Complementing\nthis, we employ a snapshot ensemble strategy to further boost trajectory\nprediction performance. Using cyclical learning rate scheduling, multiple model\nsnapshots are captured during a single training run. These snapshots are then\naggregated via simple averaging at inference time, allowing the model to\nbenefit from diverse hypotheses without incurring substantial additional\ncomputational cost. Extensive experiments on Argoverse datasets demonstrate\nthat our combined approach significantly improves prediction accuracy,\ninference speed and robustness in diverse driving scenarios. This work\nunderscores the potential of normalization-free transformer designs augmented\nwith lightweight ensemble techniques in advancing trajectory forecasting for\nautonomous vehicles.",
        "published": "2025-04-07T09:26:25+00:00"
    },
    {
        "title": "Content-Aware Transformer for All-in-one Image Restoration",
        "authors": [
            "Gang Wu",
            "Junjun Jiang",
            "Kui Jiang",
            "Xianming Liu"
        ],
        "summary": "Image restoration has witnessed significant advancements with the development\nof deep learning models. Although Transformer architectures have progressed\nconsiderably in recent years, challenges remain, particularly the limited\nreceptive field in window-based self-attention. In this work, we propose\nDSwinIR, a Deformable Sliding window Transformer for Image Restoration. DSwinIR\nintroduces a novel deformable sliding window self-attention that adaptively\nadjusts receptive fields based on image content, enabling the attention\nmechanism to focus on important regions and enhance feature extraction aligned\nwith salient features. Additionally, we introduce a central ensemble pattern to\nreduce the inclusion of irrelevant content within attention windows. In this\nway, the proposed DSwinIR model integrates the deformable sliding window\nTransformer and central ensemble pattern to amplify the strengths of both CNNs\nand Transformers while mitigating their limitations. Extensive experiments on\nvarious image restoration tasks demonstrate that DSwinIR achieves\nstate-of-the-art performance. For example, in image deraining, compared to\nDRSformer on the SPA dataset, DSwinIR achieves a 0.66 dB PSNR improvement. In\nall-in-one image restoration, compared to PromptIR, DSwinIR achieves over a\n0.66 dB and 1.04 dB improvement on three-task and five-task settings,\nrespectively. Pretrained models and code are available at our project\nhttps://github.com/Aitical/DSwinIR.",
        "published": "2025-04-07T09:24:41+00:00"
    },
    {
        "title": "FedSAUC: A Similarity-Aware Update Control for Communication-Efficient Federated Learning in Edge Computing",
        "authors": [
            "Ming-Lun Lee",
            "Han-Chang Chou",
            "Yan-Ann~Chen"
        ],
        "summary": "Federated learning is a distributed machine learning framework to\ncollaboratively train a global model without uploading privacy-sensitive data\nonto a centralized server. Usually, this framework is applied to edge devices\nsuch as smartphones, wearable devices, and Internet of Things (IoT) devices\nwhich closely collect information from users. However, these devices are mostly\nbattery-powered. The update procedure of federated learning will constantly\nconsume the battery power and the transmission bandwidth. In this work, we\npropose an update control for federated learning, FedSAUC, by considering the\nsimilarity of users' behaviors (models). At the server side, we exploit\nclustering algorithms to group devices with similar models. Then we select some\nrepresentatives for each cluster to update information to train the model. We\nalso implemented a testbed prototyping on edge devices for validating the\nperformance. The experimental results show that this update control will not\naffect the training accuracy in the long run.",
        "published": "2025-04-07T09:21:43+00:00"
    },
    {
        "title": "Optimal Network-Guided Covariate Selection for High-Dimensional Data Integration",
        "authors": [
            "Tao Shen",
            "Wanjie Wang"
        ],
        "summary": "When integrating datasets from different studies, it is common that they have\ncomponents of different formats. How to combine them organically for improved\nestimation is important and challenging. This paper investigates this problem\nin a two-study scenario, where covariates are observed for all subjects, but\nnetwork data is available in only one study, and response variables are\navailable only in the other.\n  To leverage the partially observed network information, we propose the\nNetwork-Guided Covariate Selection (NGCS) algorithm. It integrates the spectral\ninformation from network adjacency matrices with the Higher Criticism\nThresholding approach for informative covariates identification. Theoretically,\nwe prove that NGCS achieves the optimal rate in covariate selection, which is\nthe same rate in the supervised learning setting. Furthermore, this optimality\nis robust to network models and tuning parameters.\n  This framework extends naturally to clustering and regression tasks, with two\nproposed algorithms: NG-clu and NG-reg. For clustering, NG-clu accurately\nclusters data points despite incomplete network information. For regression,\nNG-reg enhances predictive performance by incorporating latent covariate\nstructures inferred from network data. Empirical studies on synthetic and\nreal-world datasets demonstrate the robustness and superior performance of our\nalgorithms, underscoring their effectiveness in handling heterogeneous data\nformats.",
        "published": "2025-04-07T09:21:34+00:00"
    },
    {
        "title": "Dynamic hysteresis model of grain-oriented ferromagnetic material using neural operators",
        "authors": [
            "Ziqing Guo",
            "Binh H. Nguyen",
            "Hamed Hamzehbahmani",
            "Ruth V. Sabariego"
        ],
        "summary": "Accurately capturing the behavior of grain-oriented (GO) ferromagnetic\nmaterials is crucial for modeling the electromagnetic devices. In this paper,\nneural operator models, including Fourier neural operator (FNO), U-net combined\nFNO (U-FNO) and Deep operator network (DeepONet) are used to approximate the\ndynamic hysteresis models of GO steel. Furthermore, two types of data\naugmentation strategies including cyclic rolling augmentation and Gaussian data\naugmentation (GDA) are implemented to enhance the learning ability of models.\nWith the inclusion of these augmentation techniques, the optimized models\naccount for not only the peak values of the magnetic flux density but also the\neffects of different frequencies and phase shifts. The accuracy of all models\nis assessed using the L2-norm of the test data and the mean relative error\n(MRE) of calculated core losses. Each model performs well in different\nscenarios, but FNO consistently achieves the best performance across all cases.",
        "published": "2025-04-07T09:19:23+00:00"
    },
    {
        "title": "GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network",
        "authors": [
            "Yunxiang Liu",
            "Hongkuo Niu",
            "Jianlin Zhu"
        ],
        "summary": "Accurate motion prediction of traffic agents is crucial for the safety and\nstability of autonomous driving systems. In this paper, we introduce GAMDTP, a\nnovel graph attention-based network tailored for dynamic trajectory prediction.\nSpecifically, we fuse the result of self attention and mamba-ssm through a gate\nmechanism, leveraging the strengths of both to extract features more\nefficiently and accurately, in each graph convolution layer. GAMDTP encodes the\nhigh-definition map(HD map) data and the agents' historical trajectory\ncoordinates and decodes the network's output to generate the final prediction\nresults. Additionally, recent approaches predominantly focus on dynamically\nfusing historical forecast results and rely on two-stage frameworks including\nproposal and refinement. To further enhance the performance of the two-stage\nframeworks we also design a scoring mechanism to evaluate the prediction\nquality during the proposal and refinement processes. Experiments on the\nArgoverse dataset demonstrates that GAMDTP achieves state-of-the-art\nperformance, achieving superior accuracy in dynamic trajectory prediction.",
        "published": "2025-04-07T09:19:20+00:00"
    },
    {
        "title": "SAFT: Structure-aware Transformers for Textual Interaction Classification",
        "authors": [
            "Hongtao Wang",
            "Renchi Yang",
            "Hewen Wang",
            "Haoran Zheng",
            "Jianliang Xu"
        ],
        "summary": "Textual interaction networks (TINs) are an omnipresent data structure used to\nmodel the interplay between users and items on e-commerce websites, social\nnetworks, etc., where each interaction is associated with a text description.\nClassifying such textual interactions (TIC) finds extensive use in detecting\nspam reviews in e-commerce, fraudulent transactions in finance, and so on.\nExisting TIC solutions either (i) fail to capture the rich text semantics due\nto the use of context-free text embeddings, and/or (ii) disregard the bipartite\nstructure and node heterogeneity of TINs, leading to compromised TIC\nperformance. In this work, we propose SAFT, a new architecture that integrates\nlanguage- and graph-based modules for the effective fusion of textual and\nstructural semantics in the representation learning of interactions. In\nparticular, line graph attention (LGA)/gated attention units (GAUs) and\npretrained language models (PLMs) are capitalized on to model the\ninteraction-level and token-level signals, which are further coupled via the\nproxy token in an iterative and contextualized fashion. Additionally, an\nefficient and theoretically-grounded approach is developed to encode the local\nand global topology information pertaining to interactions into structural\nembeddings. The resulting embeddings not only inject the structural features\nunderlying TINs into the textual interaction encoding but also facilitate the\ndesign of graph sampling strategies. Extensive empirical evaluations on\nmultiple real TIN datasets demonstrate the superiority of SAFT over the\nstate-of-the-art baselines in TIC accuracy.",
        "published": "2025-04-07T09:19:12+00:00"
    },
    {
        "title": "Nonlocal techniques for the analysis of deep ReLU neural network approximations",
        "authors": [
            "Cornelia Schneider",
            "Mario Ullrich",
            "Jan Vybiral"
        ],
        "summary": "Recently, Daubechies, DeVore, Foucart, Hanin, and Petrova introduced a system\nof piece-wise linear functions, which can be easily reproduced by artificial\nneural networks with the ReLU activation function and which form a Riesz basis\nof $L_2([0,1])$. This work was generalized by two of the authors to the\nmultivariate setting. We show that this system serves as a Riesz basis also for\nSobolev spaces $W^s([0,1]^d)$ and Barron classes ${\\mathbb B}^s([0,1]^d)$ with\nsmoothness $0<s<1$. We apply this fact to re-prove some recent results on the\napproximation of functions from these classes by deep neural networks. Our\nproof method avoids using local approximations and allows us to track also the\nimplicit constants as well as to show that we can avoid the curse of dimension.\nMoreover, we also study how well one can approximate Sobolev and Barron\nfunctions by ANNs if only function values are known.",
        "published": "2025-04-07T09:00:22+00:00"
    },
    {
        "title": "Deep Learning for Double Auction",
        "authors": [
            "Jiayin Liu",
            "Chenglong Zhang"
        ],
        "summary": "Auctions are important mechanisms extensively implemented in various markets,\ne.g., search engines' keyword auctions, antique auctions, etc. Finding an\noptimal auction mechanism is extremely difficult due to the constraints of\nimperfect information, incentive compatibility (IC), and individual rationality\n(IR). In addition to the traditional economic methods, some recently attempted\nto find the optimal (single) auction using deep learning methods. Unlike those\nattempts focusing on single auctions, we develop deep learning methods for\ndouble auctions, where imperfect information exists on both the demand and\nsupply sides. The previous attempts on single auction cannot directly apply to\nour contexts and those attempts additionally suffer from limited\ngeneralizability, inefficiency in ensuring the constraints, and learning\nfluctuations. We innovate in designing deep learning models for solving the\nmore complex problem and additionally addressing the previous models' three\nlimitations. Specifically, we achieve generalizability by leveraging a\ntransformer-based architecture to model market participants as sequences for\nvarying market sizes; we utilize the numerical features of the constraints and\npre-treat them for a higher learning efficiency; we develop a\ngradient-conflict-elimination scheme to address the problem of learning\nfluctuation. Extensive experimental evaluations demonstrate the superiority of\nour approach to classical and machine learning baselines.",
        "published": "2025-04-07T08:56:32+00:00"
    },
    {
        "title": "Data Augmentation as Free Lunch: Exploring the Test-Time Augmentation for Sequential Recommendation",
        "authors": [
            "Yizhou Dang",
            "Yuting Liu",
            "Enneng Yang",
            "Minhan Huang",
            "Guibing Guo",
            "Jianzhe Zhao",
            "Xingwei Wang"
        ],
        "summary": "Data augmentation has become a promising method of mitigating data sparsity\nin sequential recommendation. Existing methods generate new yet effective data\nduring model training to improve performance. However, deploying them requires\nretraining, architecture modification, or introducing additional learnable\nparameters. The above steps are time-consuming and costly for well-trained\nmodels, especially when the model scale becomes large. In this work, we explore\nthe test-time augmentation (TTA) for sequential recommendation, which augments\nthe inputs during the model inference and then aggregates the model's\npredictions for augmented data to improve final accuracy. It avoids significant\ntime and cost overhead from loss calculation and backward propagation. We first\nexperimentally disclose the potential of existing augmentation operators for\nTTA and find that the Mask and Substitute consistently achieve better\nperformance. Further analysis reveals that these two operators are effective\nbecause they retain the original sequential pattern while adding appropriate\nperturbations. Meanwhile, we argue that these two operators still face\ntime-consuming item selection or interference information from mask tokens.\nBased on the analysis and limitations, we present TNoise and TMask. The former\ninjects uniform noise into the original representation, avoiding the\ncomputational overhead of item selection. The latter blocks mask token from\nparticipating in model calculations or directly removes interactions that\nshould have been replaced with mask tokens. Comprehensive experiments\ndemonstrate the effectiveness, efficiency, and generalizability of our method.\nWe provide an anonymous implementation at https://github.com/KingGugu/TTA4SR.",
        "published": "2025-04-07T08:56:16+00:00"
    },
    {
        "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis",
        "authors": [
            "Mengchao Wang",
            "Qiang Wang",
            "Fan Jiang",
            "Yaqi Fan",
            "Yunpeng Zhang",
            "Yonggang Qi",
            "Kun Zhao",
            "Mu Xu"
        ],
        "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/.",
        "published": "2025-04-07T08:56:01+00:00"
    },
    {
        "title": "Prior2Former -- Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation",
        "authors": [
            "Sebastian Schmidt",
            "Julius K\u00f6rner",
            "Dominik Fuchsgruber",
            "Stefano Gasperini",
            "Federico Tombari",
            "Stephan G\u00fcnnemann"
        ],
        "summary": "In panoptic segmentation, individual instances must be separated within\nsemantic classes. As state-of-the-art methods rely on a pre-defined set of\nclasses, they struggle with novel categories and out-of-distribution (OOD)\ndata. This is particularly problematic in safety-critical applications, such as\nautonomous driving, where reliability in unseen scenarios is essential. We\naddress the gap between outstanding benchmark performance and reliability by\nproposing Prior2Former (P2F), the first approach for segmentation vision\ntransformers rooted in evidential learning. P2F extends the mask vision\ntransformer architecture by incorporating a Beta prior for computing model\nuncertainty in pixel-wise binary mask assignments. This design enables\nhigh-quality uncertainty estimation that effectively detects novel and OOD\nobjects enabling state-of-the-art anomaly instance segmentation and open-world\npanoptic segmentation. Unlike most segmentation models addressing unknown\nclasses, P2F operates without access to OOD data samples or contrastive\ntraining on void (i.e., unlabeled) classes, making it highly applicable in\nreal-world scenarios where such prior information is unavailable. Additionally,\nP2F can be flexibly applied to anomaly instance and panoptic segmentation.\nThrough comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan,\nand OoDIS datasets, we demonstrate the state-of-the-art performance of P2F. It\nachieves the highest ranking in the OoDIS anomaly instance benchmark among\nmethods not using OOD data in any way.",
        "published": "2025-04-07T08:53:14+00:00"
    },
    {
        "title": "Unsupervised Ego- and Exo-centric Dense Procedural Activity Captioning via Gaze Consensus Adaptation",
        "authors": [
            "Zhaofeng Shi",
            "Heqian Qiu",
            "Lanxiao Wang",
            "Qingbo Wu",
            "Fanman Meng",
            "Hongliang Li"
        ],
        "summary": "Even from an early age, humans naturally adapt between exocentric (Exo) and\negocentric (Ego) perspectives to understand daily procedural activities.\nInspired by this cognitive ability, in this paper, we propose a novel\nUnsupervised Ego-Exo Adaptation for Dense Video Captioning (UEA-DVC) task,\nwhich aims to predict the time segments and descriptions for target view\nvideos, while only the source view data are labeled during training. Despite\nprevious works endeavoring to address the fully-supervised single-view or\ncross-view dense video captioning, they lapse in the proposed unsupervised task\ndue to the significant inter-view gap caused by temporal misalignment and\nirrelevant object interference. Hence, we propose a Gaze Consensus-guided\nEgo-Exo Adaptation Network (GCEAN) that injects the gaze information into the\nlearned representations for the fine-grained alignment between the Ego and Exo\nviews. Specifically, the Score-based Adversarial Learning Module (SALM)\nincorporates a discriminative scoring network to learn unified view-invariant\nrepresentations for bridging distinct views from a global level. Then, the Gaze\nConsensus Construction Module (GCCM) utilizes gaze representations to\nprogressively calibrate the learned global view-invariant representations for\nextracting the video temporal contexts based on focusing regions. Moreover, the\ngaze consensus is constructed via hierarchical gaze-guided consistency losses\nto spatially and temporally align the source and target views. To support our\nresearch, we propose a new EgoMe-UEA-DVC benchmark and experiments demonstrate\nthe effectiveness of our method, which outperforms many related methods by a\nlarge margin. The code will be released.",
        "published": "2025-04-07T08:51:11+00:00"
    },
    {
        "title": "Learning Affine Correspondences by Integrating Geometric Constraints",
        "authors": [
            "Pengju Sun",
            "Banglei Guan",
            "Zhenbao Yu",
            "Yang Shang",
            "Qifeng Yu",
            "Daniel Barath"
        ],
        "summary": "Affine correspondences have received significant attention due to their\nbenefits in tasks like image matching and pose estimation. Existing methods for\nextracting affine correspondences still have many limitations in terms of\nperformance; thus, exploring a new paradigm is crucial. In this paper, we\npresent a new pipeline designed for extracting accurate affine correspondences\nby integrating dense matching and geometric constraints. Specifically, a novel\nextraction framework is introduced, with the aid of dense matching and a novel\nkeypoint scale and orientation estimator. For this purpose, we propose loss\nfunctions based on geometric constraints, which can effectively improve\naccuracy by supervising neural networks to learn feature geometry. The\nexperimental show that the accuracy and robustness of our method outperform the\nexisting ones in image matching tasks. To further demonstrate the effectiveness\nof the proposed method, we applied it to relative pose estimation. Affine\ncorrespondences extracted by our method lead to more accurate poses than the\nbaselines on a range of real-world datasets. The code is available at\nhttps://github.com/stilcrad/DenseAffine.",
        "published": "2025-04-07T08:44:50+00:00"
    },
    {
        "title": "SMF: Template-free and Rig-free Animation Transfer using Kinetic Codes",
        "authors": [
            "Sanjeev Muralikrishnan",
            "Niladri Shekhar Dutt",
            "Niloy J. Mitra"
        ],
        "summary": "Animation retargeting involves applying a sparse motion description (e.g.,\n2D/3D keypoint sequences) to a given character mesh to produce a semantically\nplausible and temporally coherent full-body motion. Existing approaches come\nwith a mix of restrictions - they require annotated training data, assume\naccess to template-based shape priors or artist-designed deformation rigs,\nsuffer from limited generalization to unseen motion and/or shapes, or exhibit\nmotion jitter. We propose Self-supervised Motion Fields (SMF) as a\nself-supervised framework that can be robustly trained with sparse motion\nrepresentations, without requiring dataset specific annotations, templates, or\nrigs. At the heart of our method are Kinetic Codes, a novel autoencoder-based\nsparse motion encoding, that exposes a semantically rich latent space\nsimplifying large-scale training. Our architecture comprises dedicated spatial\nand temporal gradient predictors, which are trained end-to-end. The resultant\nnetwork, regularized by the Kinetic Codes's latent space, has good\ngeneralization across shapes and motions. We evaluated our method on unseen\nmotion sampled from AMASS, D4D, Mixamo, and raw monocular video for animation\ntransfer on various characters with varying shapes and topology. We report a\nnew SoTA on the AMASS dataset in the context of generalization to unseen\nmotion. Project webpage at https://motionfields.github.io/",
        "published": "2025-04-07T08:42:52+00:00"
    },
    {
        "title": "Attentional Graph Meta-Learning for Indoor Localization Using Extremely Sparse Fingerprints",
        "authors": [
            "Wenzhong Yan",
            "Feng Yin",
            "Jun Gao",
            "Ao Wang",
            "Yang Tian",
            "Ruizhi Chen"
        ],
        "summary": "Fingerprint-based indoor localization is often labor-intensive due to the\nneed for dense grids and repeated measurements across time and space.\nMaintaining high localization accuracy with extremely sparse fingerprints\nremains a persistent challenge. Existing benchmark methods primarily rely on\nthe measured fingerprints, while neglecting valuable spatial and environmental\ncharacteristics. In this paper, we propose a systematic integration of an\nAttentional Graph Neural Network (AGNN) model, capable of learning spatial\nadjacency relationships and aggregating information from neighboring\nfingerprints, and a meta-learning framework that utilizes datasets with similar\nenvironmental characteristics to enhance model training. To minimize the labor\nrequired for fingerprint collection, we introduce two novel data augmentation\nstrategies: 1) unlabeled fingerprint augmentation using moving platforms, which\nenables the semi-supervised AGNN model to incorporate information from\nunlabeled fingerprints, and 2) synthetic labeled fingerprint augmentation\nthrough environmental digital twins, which enhances the meta-learning framework\nthrough a practical distribution alignment, which can minimize the feature\ndiscrepancy between synthetic and real-world fingerprints effectively. By\nintegrating these novel modules, we propose the Attentional Graph Meta-Learning\n(AGML) model. This novel model combines the strengths of the AGNN model and the\nmeta-learning framework to address the challenges posed by extremely sparse\nfingerprints. To validate our approach, we collected multiple datasets from\nboth consumer-grade WiFi devices and professional equipment across diverse\nenvironments. Extensive experiments conducted on both synthetic and real-world\ndatasets demonstrate that the AGML model-based localization method consistently\noutperforms all baseline methods using sparse fingerprints across all evaluated\nmetrics.",
        "published": "2025-04-07T08:37:18+00:00"
    },
    {
        "title": "SUEDE:Shared Unified Experts for Physical-Digital Face Attack Detection Enhancement",
        "authors": [
            "Zuying Xie",
            "Changtao Miao",
            "Ajian Liu",
            "Jiabao Guo",
            "Feng Li",
            "Dan Guo",
            "Yunfeng Diao"
        ],
        "summary": "Face recognition systems are vulnerable to physical attacks (e.g., printed\nphotos) and digital threats (e.g., DeepFake), which are currently being studied\nas independent visual tasks, such as Face Anti-Spoofing and Forgery Detection.\nThe inherent differences among various attack types present significant\nchallenges in identifying a common feature space, making it difficult to\ndevelop a unified framework for detecting data from both attack modalities\nsimultaneously. Inspired by the efficacy of Mixture-of-Experts (MoE) in\nlearning across diverse domains, we explore utilizing multiple experts to learn\nthe distinct features of various attack types. However, the feature\ndistributions of physical and digital attacks overlap and differ. This suggests\nthat relying solely on distinct experts to learn the unique features of each\nattack type may overlook shared knowledge between them. To address these\nissues, we propose SUEDE, the Shared Unified Experts for Physical-Digital Face\nAttack Detection Enhancement. SUEDE combines a shared expert (always activated)\nto capture common features for both attack types and multiple routed experts\n(selectively activated) for specific attack types. Further, we integrate CLIP\nas the base network to ensure the shared expert benefits from prior visual\nknowledge and align visual-text representations in a unified space. Extensive\nresults demonstrate SUEDE achieves superior performance compared to\nstate-of-the-art unified detection methods.",
        "published": "2025-04-07T08:17:54+00:00"
    },
    {
        "title": "Network Effects of Tariffs",
        "authors": [
            "Paolo Pin"
        ],
        "summary": "We develop a model in which country-specific tariffs shape trade flows,\nprices, and welfare in a global economy with one homogeneous good. Trade flows\nform a Directed Acyclic Graph (DAG), and tariffs influence not only market\noutcomes but also the structure of the global trade network. A numerical\nexample illustrates how tariffs may eliminate targeted imports, divert trade\nflows toward third markets, expose domestic firms to intensified foreign\ncompetition abroad, reduce consumer welfare, and ultimately harm the country\nimposing the tariff.",
        "published": "2025-04-07T08:13:26+00:00"
    },
    {
        "title": "Fast gates for bit-flip protected superconducting qubits",
        "authors": [
            "C. A. Siegele",
            "A. A. Sokolova",
            "L. N. Kapoor",
            "F. Hassani",
            "J. M. Fink"
        ],
        "summary": "Superconducting qubits offer an unprecedentedly high degree of flexibility in\nterms of circuit encoding and parameter choices. However, in designing the\nqubit parameters one typically faces the conflicting goals of long coherence\ntimes and simple control capabilities. Both are determined by the wavefunction\noverlap of the qubit basis states and the corresponding matrix elements. Here,\nwe address this problem by introducing a qubit architecture with real-time\ntunable bit-flip protection. In the first, the `heavy' regime, the energy\nrelaxation time can be on the order of hours for fluxons located in two\nnear-degenerate ground states, as recently demonstrated in Ref. [Hassani et\nal., Nat.~Commun.~14 (2023)]. The second, `light' regime, on the other hand\nfacilitates high-fidelity control on nanosecond timescales without the need for\nmicrowave signals. We propose two different tuning mechanisms of the qubit\npotential and show that base-band flux-pulses of around 10 ns are sufficient to\nrealize a universal set of high-fidelity single- and two-qubit gates. We expect\nthat the concept of real-time wavefunction control can also be applied to other\nhardware-protected qubit designs.",
        "published": "2025-04-07T08:03:19+00:00"
    },
    {
        "title": "Topological Schr\u00f6dinger Bridge Matching",
        "authors": [
            "Maosheng Yang"
        ],
        "summary": "Given two boundary distributions, the Schr\\\"odinger Bridge (SB) problem seeks\nthe ``most likely`` random evolution between them with respect to a reference\nprocess. It has revealed rich connections to recent machine learning methods\nfor generative modeling and distribution matching. While these methods perform\nwell in Euclidean domains, they are not directly applicable to topological\ndomains such as graphs and simplicial complexes, which are crucial for data\ndefined over network entities, such as node signals and edge flows. In this\nwork, we propose the Topological Schr\\\"odinger Bridge problem (TSBP) for\nmatching signal distributions on a topological domain. We set the reference\nprocess to follow some linear tractable topology-aware stochastic dynamics such\nas topological heat diffusion. For the case of Gaussian boundary distributions,\nwe derive a closed-form topological SB (TSB) in terms of its time-marginal and\nstochastic differential. In the general case, leveraging the well-known result,\nwe show that the optimal process follows the forward-backward topological\ndynamics governed by some unknowns. Building on these results, we develop\nTSB-based models for matching topological signals by parameterizing the\nunknowns in the optimal process as (topological) neural networks and learning\nthem through likelihood training. We validate the theoretical results and\ndemonstrate the practical applications of TSB-based models on both synthetic\nand real-world networks, emphasizing the role of topology. Additionally, we\ndiscuss the connections of TSB-based models to other emerging models, and\noutline future directions for topological signal matching.",
        "published": "2025-04-07T07:45:21+00:00"
    },
    {
        "title": "TabRep: a Simple and Effective Continuous Representation for Training Tabular Diffusion Models",
        "authors": [
            "Jacob Si",
            "Zijing Ou",
            "Mike Qu",
            "Zhengrui Xiang",
            "Yingzhen Li"
        ],
        "summary": "Diffusion models have been the predominant generative model for tabular data\ngeneration. However, they face the conundrum of modeling under a separate\nversus a unified data representation. The former encounters the challenge of\njointly modeling all multi-modal distributions of tabular data in one model.\nWhile the latter alleviates this by learning a single representation for all\nfeatures, it currently leverages sparse suboptimal encoding heuristics and\nnecessitates additional computation costs. In this work, we address the latter\nby presenting TabRep, a tabular diffusion architecture trained with a unified\ncontinuous representation. To motivate the design of our representation, we\nprovide geometric insights into how the data manifold affects diffusion models.\nThe key attributes of our representation are composed of its density,\nflexibility to provide ample separability for nominal features, and ability to\npreserve intrinsic relationships. Ultimately, TabRep provides a simple yet\neffective approach for training tabular diffusion models under a continuous\ndata manifold. Our results showcase that TabRep achieves superior performance\nacross a broad suite of evaluations. It is the first to synthesize tabular data\nthat exceeds the downstream quality of the original datasets while preserving\nprivacy and remaining computationally efficient.",
        "published": "2025-04-07T07:44:27+00:00"
    },
    {
        "title": "Embodied Perception for Test-time Grasping Detection Adaptation with Knowledge Infusion",
        "authors": [
            "Jin Liu",
            "Jialong Xie",
            "Leibing Xiao",
            "Chaoqun Wang",
            "Fengyu Zhou"
        ],
        "summary": "It has always been expected that a robot can be easily deployed to unknown\nscenarios, accomplishing robotic grasping tasks without human intervention.\nNevertheless, existing grasp detection approaches are typically off-body\ntechniques and are realized by training various deep neural networks with\nextensive annotated data support. {In this paper, we propose an embodied\ntest-time adaptation framework for grasp detection that exploits the robot's\nexploratory capabilities.} The framework aims to improve the generalization\nperformance of grasping skills for robots in an unforeseen environment.\nSpecifically, we introduce embodied assessment criteria based on the robot's\nmanipulation capability to evaluate the quality of the grasp detection and\nmaintain suitable samples. This process empowers the robots to actively explore\nthe environment and continuously learn grasping skills, eliminating human\nintervention. Besides, to improve the efficiency of robot exploration, we\nconstruct a flexible knowledge base to provide context of initial optimal\nviewpoints. Conditioned on the maintained samples, the grasp detection networks\ncan be adapted in the test-time scene. When the robot confronts new objects, it\nwill undergo the same adaptation procedure mentioned above to realize\ncontinuous learning. Extensive experiments conducted on a real-world robot\ndemonstrate the effectiveness and generalization of our proposed framework.",
        "published": "2025-04-07T07:39:15+00:00"
    },
    {
        "title": "Multimodal Agricultural Agent Architecture (MA3): A New Paradigm for Intelligent Agricultural Decision-Making",
        "authors": [
            "Zhuoning Xu",
            "Jian Xu",
            "Mingqing Zhang",
            "Peijie Wang",
            "Chao Deng",
            "Cheng-Lin Liu"
        ],
        "summary": "As a strategic pillar industry for human survival and development, modern\nagriculture faces dual challenges: optimizing production efficiency and\nachieving sustainable development. Against the backdrop of intensified climate\nchange leading to frequent extreme weather events, the uncertainty risks in\nagricultural production systems are increasing exponentially. To address these\nchallenges, this study proposes an innovative \\textbf{M}ultimodal\n\\textbf{A}gricultural \\textbf{A}gent \\textbf{A}rchitecture (\\textbf{MA3}),\nwhich leverages cross-modal information fusion and task collaboration\nmechanisms to achieve intelligent agricultural decision-making. This study\nconstructs a multimodal agricultural agent dataset encompassing five major\ntasks: classification, detection, Visual Question Answering (VQA), tool\nselection, and agent evaluation. We propose a unified backbone for sugarcane\ndisease classification and detection tools, as well as a sugarcane disease\nexpert model. By integrating an innovative tool selection module, we develop a\nmultimodal agricultural agent capable of effectively performing tasks in\nclassification, detection, and VQA. Furthermore, we introduce a\nmulti-dimensional quantitative evaluation framework and conduct a comprehensive\nassessment of the entire architecture over our evaluation dataset, thereby\nverifying the practicality and robustness of MA3 in agricultural scenarios.\nThis study provides new insights and methodologies for the development of\nagricultural agents, holding significant theoretical and practical\nimplications. Our source code and dataset will be made publicly available upon\nacceptance.",
        "published": "2025-04-07T07:32:41+00:00"
    },
    {
        "title": "Dynamic Vision Mamba",
        "authors": [
            "Mengxuan Wu",
            "Zekai Li",
            "Zhiyuan Liang",
            "Moyang Li",
            "Xuanlei Zhao",
            "Samir Khaki",
            "Zheng Zhu",
            "Xiaojiang Peng",
            "Konstantinos N. Plataniotis",
            "Kai Wang",
            "Wangbo Zhao",
            "Yang You"
        ],
        "summary": "Mamba-based vision models have gained extensive attention as a result of\nbeing computationally more efficient than attention-based models. However,\nspatial redundancy still exists in these models, represented by token and block\nredundancy. For token redundancy, we analytically find that early token pruning\nmethods will result in inconsistency between training and inference or\nintroduce extra computation for inference. Therefore, we customize token\npruning to fit the Mamba structure by rearranging the pruned sequence before\nfeeding it into the next Mamba block. For block redundancy, we allow each image\nto select SSM blocks dynamically based on an empirical observation that the\ninference speed of Mamba-based vision models is largely affected by the number\nof SSM blocks. Our proposed method, Dynamic Vision Mamba (DyVM), effectively\nreduces FLOPs with minor performance drops. We achieve a reduction of 35.2\\%\nFLOPs with only a loss of accuracy of 1.7\\% on Vim-S. It also generalizes well\nacross different Mamba vision model architectures and different vision tasks.\nOur code will be made public.",
        "published": "2025-04-07T07:31:28+00:00"
    },
    {
        "title": "Bottom-Up Scattering Information Perception Network for SAR target recognition",
        "authors": [
            "Chenxi Zhao",
            "Daochang Wang",
            "Siqian Zhang",
            "Gangyao Kuang"
        ],
        "summary": "Deep learning methods based synthetic aperture radar (SAR) image target\nrecognition tasks have been widely studied currently. The existing deep methods\nare insufficient to perceive and mine the scattering information of SAR images,\nresulting in performance bottlenecks and poor robustness of the algorithms. To\nthis end, this paper proposes a novel bottom-up scattering information\nperception network for more interpretable target recognition by constructing\nthe proprietary interpretation network for SAR images. Firstly, the localized\nscattering perceptron is proposed to replace the backbone feature extractor\nbased on CNN networks to deeply mine the underlying scattering information of\nthe target. Then, an unsupervised scattering part feature extraction model is\nproposed to robustly characterize the target scattering part information and\nprovide fine-grained target representation. Finally, by aggregating the\nknowledge of target parts to form the complete target description, the\ninterpretability and discriminative ability of the model is improved. We\nperform experiments on the FAST-Vehicle dataset and the SAR-ACD dataset to\nvalidate the performance of the proposed method.",
        "published": "2025-04-07T07:15:08+00:00"
    },
    {
        "title": "Bidirectional Hierarchical Protein Multi-Modal Representation Learning",
        "authors": [
            "Xuefeng Liu",
            "Songhao Jiang",
            "Chih-chan Tien",
            "Jinbo Xu",
            "Rick Stevens"
        ],
        "summary": "Protein representation learning is critical for numerous biological tasks.\nRecently, large transformer-based protein language models (pLMs) pretrained on\nlarge scale protein sequences have demonstrated significant success in\nsequence-based tasks. However, pLMs lack structural information. Conversely,\ngraph neural networks (GNNs) designed to leverage 3D structural information\nhave shown promising generalization in protein-related prediction tasks, but\ntheir effectiveness is often constrained by the scarcity of labeled structural\ndata. Recognizing that sequence and structural representations are\ncomplementary perspectives of the same protein entity, we propose a multimodal\nbidirectional hierarchical fusion framework to effectively merge these\nmodalities. Our framework employs attention and gating mechanisms to enable\neffective interaction between pLMs-generated sequential representations and\nGNN-extracted structural features, improving information exchange and\nenhancement across layers of the neural network. Based on the framework, we\nfurther introduce local Bi-Hierarchical Fusion with gating and global\nBi-Hierarchical Fusion with multihead self-attention approaches. Through\nextensive experiments on a diverse set of protein-related tasks, our method\ndemonstrates consistent improvements over strong baselines and existing fusion\ntechniques in a variety of protein representation learning benchmarks,\nincluding react (enzyme/EC classification), model quality assessment (MQA),\nprotein-ligand binding affinity prediction (LBA), protein-protein binding site\nprediction (PPBS), and B cell epitopes prediction (BCEs). Our method\nestablishes a new state-of-the-art for multimodal protein representation\nlearning, emphasizing the efficacy of BIHIERARCHICAL FUSION in bridging\nsequence and structural modalities.",
        "published": "2025-04-07T06:47:49+00:00"
    },
    {
        "title": "Strong approximation and central limit theorems for multiscale stochastic gene networks",
        "authors": [
            "Baptiste Nicolas Huguet"
        ],
        "summary": "We study a mutliscale jump process introduced in a work by Crudu, Debussche,\nMuller and Radulescu. Using an adequate coupling, we are able to prove the\nstrong convergence, for the uniform topology, to a piecewise deterministic\nMarkov process. Under some additional regularity, we also obtain a central\nlimit theorem and prove that the fluctuations of the continuous scale converge,\nin a weaker sense, to the solution of a stochastic differential equation.",
        "published": "2025-04-07T06:46:30+00:00"
    },
    {
        "title": "Extended URDF: Accounting for parallel mechanism in robot description",
        "authors": [
            "Virgile Batto",
            "Ludovic de Matte\u00efs",
            "Nicolas Mansard"
        ],
        "summary": "Robotic designs played an important role in recent advances by providing\npowerful robots with complex mechanics. Many recent systems rely on parallel\nactuation to provide lighter limbs and allow more complex motion. However,\nthese emerging architectures fall outside the scope of most used description\nformats, leading to difficulties when designing, storing, and sharing the\nmodels of these systems. This paper introduces an extension to the widely used\nUnified Robot Description Format (URDF) to support closed-loop kinematic\nstructures. Our approach relies on augmenting URDF with minimal additional\ninformation to allow more efficient modeling of complex robotic systems while\nmaintaining compatibility with existing design and simulation frameworks. This\nmethod sets the basic requirement for a description format to handle parallel\nmechanisms efficiently. We demonstrate the applicability of our approach by\nproviding an open-source collection of parallel robots, along with tools for\ngenerating and parsing this extended description format. The proposed extension\nsimplifies robot modeling, reduces redundancy, and improves usability for\nadvanced robotic applications.",
        "published": "2025-04-07T06:42:27+00:00"
    },
    {
        "title": "KunPeng: A Global Ocean Environmental Model",
        "authors": [
            "Yi Zhao",
            "Jiaqi Li",
            "Haitao Xia",
            "Tianjiao Zhang",
            "Zerong Zeng",
            "Tianyu Ren",
            "Yucheng Zhang",
            "Chao Zhu",
            "Shengtong Xu",
            "Hongchun Yuan"
        ],
        "summary": "Inspired by the similarity of the atmosphere-ocean physical coupling\nmechanism, this study innovatively migrates meteorological large-model\ntechniques to the ocean domain, constructing the KunPeng global ocean\nenvironmental prediction model. Aimed at the discontinuous characteristics of\nmarine space, we propose a terrain-adaptive mask constraint mechanism to\nmitigate effectively training divergence caused by abrupt gradients at land-sea\nboundaries. To fully integrate far-, medium-, and close-range marine features,\na longitude-cyclic deformable convolution network (LC-DCN) is employed to\nenhance the dynamic receptive field, achieving refined modeling of multi-scale\noceanic characteristics. A Deformable Convolution-enhanced Multi-Step\nPrediction module (DC-MTP) is employed to strengthen temporal dependency\nfeature extraction capabilities. Experimental results demonstrate that this\nmodel achieves an average ACC of 0.80 in 15-day global predictions at\n0.25$^\\circ$ resolution, outperforming comparative models by 0.01-0.08. The\naverage mean squared error (MSE) is 0.41 (representing a 5%-31% reduction) and\nthe average mean absolute error (MAE) is 0.44 (0.6%-21% reduction) compared to\nother models. Significant improvements are particularly observed in sea surface\nparameter prediction, deep-sea region characterization, and current velocity\nfield forecasting. Through a horizontal comparison of the applicability of\noperators at different scales in the marine domain, this study reveals that\nlocal operators significantly outperform global operators under slow-varying\noceanic processes, demonstrating the effectiveness of dynamic feature pyramid\nrepresentations in predicting marine physical parameters.",
        "published": "2025-04-07T06:41:05+00:00"
    },
    {
        "title": "Enhancing Leaf Disease Classification Using GAT-GCN Hybrid Model",
        "authors": [
            "Shyam Sundhar",
            "Riya Sharma",
            "Priyansh Maheshwari",
            "Suvidha Rupesh Kumar",
            "T. Sunil Kumar"
        ],
        "summary": "Agriculture plays a critical role in the global economy, providing\nlivelihoods and ensuring food security for billions. As innovative agricultural\npractices become more widespread, the risk of crop diseases has increased,\nhighlighting the urgent need for efficient, low-intervention disease\nidentification methods. This research presents a hybrid model combining Graph\nAttention Networks (GATs) and Graph Convolution Networks (GCNs) for leaf\ndisease classification. GCNs have been widely used for learning from\ngraph-structured data, and GATs enhance this by incorporating attention\nmechanisms to focus on the most important neighbors. The methodology integrates\nsuperpixel segmentation for efficient feature extraction, partitioning images\ninto meaningful, homogeneous regions that better capture localized features.\nThe authors have employed an edge augmentation technique to enhance the\nrobustness of the model. The edge augmentation technique has introduced a\nsignificant degree of generalization in the detection capabilities of the\nmodel. To further optimize training, weight initialization techniques are\napplied. The hybrid model is evaluated against the individual performance of\nthe GCN and GAT models and the hybrid model achieved a precision of 0.9822,\nrecall of 0.9818, and F1-score of 0.9818 in apple leaf disease classification,\na precision of 0.9746, recall of 0.9744, and F1-score of 0.9743 in potato leaf\ndisease classification, and a precision of 0.8801, recall of 0.8801, and\nF1-score of 0.8799 in sugarcane leaf disease classification. These results\ndemonstrate the robustness and performance of the model, suggesting its\npotential to support sustainable agricultural practices through precise and\neffective disease detection. This work is a small step towards reducing the\nloss of crops and hence supporting sustainable goals of zero hunger and life on\nland.",
        "published": "2025-04-07T06:31:38+00:00"
    },
    {
        "title": "Feature Importance-Aware Deep Joint Source-Channel Coding for Computationally Efficient and Adjustable Image Transmission",
        "authors": [
            "Hansung Choi",
            "Daewon Seo"
        ],
        "summary": "Recent advancements in deep learning-based joint source-channel coding\n(deepJSCC) have significantly improved communication performance, but their\nhigh computational demands restrict practical deployment. Furthermore, some\napplications require the adaptive adjustment of computational complexity. To\naddress these challenges, we propose a computationally efficient and adjustable\ndeepJSCC model for image transmission, which we call feature importance-aware\ndeepJSCC (FAJSCC). Unlike existing deepJSCC models that equally process all\nneural features of images, FAJSCC first classifies features into important and\nless important features and then processes them differently. Specifically,\ncomputationally-intensive self-attention is applied to the important features\nand computationally-efficient spatial attention to the less important ones. The\nfeature classification is based on the available computational budget and\nimportance scores predicted by an importance predictor, which estimates each\nfeature's contribution to performance. It also allows independent adjustment of\nencoder and decoder complexity within a single trained model. With these\nproperties, our FAJSCC is the first deepJSCC that is computationally efficient\nand adjustable while maintaining high performance. Experiments demonstrate that\nour FAJSCC achieves higher image transmission performance across various\nchannel conditions while using less computational complexity than the recent\nstate-of-the-art models. Adding to this, by separately varying the\ncomputational resources of the encoder and decoder, it is concluded that the\ndecoder's error correction function requires the largest computational\ncomplexity in FAJSCC, which is the first observation in deepJSCC literature.\nThe FAJSCC code is publicly available at\nhttps://github.com/hansung-choi/FAJSCC.",
        "published": "2025-04-07T06:11:39+00:00"
    },
    {
        "title": "CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images",
        "authors": [
            "Cheng Chen",
            "Jiacheng Wei",
            "Tianrun Chen",
            "Chi Zhang",
            "Xiaofeng Yang",
            "Shangzhan Zhang",
            "Bingchen Yang",
            "Chuan-Sheng Foo",
            "Guosheng Lin",
            "Qixing Huang",
            "Fayao Liu"
        ],
        "summary": "Creating CAD digital twins from the physical world is crucial for\nmanufacturing, design, and simulation. However, current methods typically rely\non costly 3D scanning with labor-intensive post-processing. To provide a\nuser-friendly design process, we explore the problem of reverse engineering\nfrom unconstrained real-world CAD images that can be easily captured by users\nof all experiences. However, the scarcity of real-world CAD data poses\nchallenges in directly training such models. To tackle these challenges, we\npropose CADCrafter, an image-to-parametric CAD model generation framework that\ntrains solely on synthetic textureless CAD data while testing on real-world\nimages. To bridge the significant representation disparity between images and\nparametric CAD models, we introduce a geometry encoder to accurately capture\ndiverse geometric features. Moreover, the texture-invariant properties of the\ngeometric features can also facilitate the generalization to real-world\nscenarios. Since compiling CAD parameter sequences into explicit CAD models is\na non-differentiable process, the network training inherently lacks explicit\ngeometric supervision. To impose geometric validity constraints, we employ\ndirect preference optimization (DPO) to fine-tune our model with the automatic\ncode checker feedback on CAD sequence quality. Furthermore, we collected a\nreal-world dataset, comprised of multi-view images and corresponding CAD\ncommand sequence pairs, to evaluate our method. Experimental results\ndemonstrate that our approach can robustly handle real unconstrained CAD\nimages, and even generalize to unseen general objects.",
        "published": "2025-04-07T06:01:35+00:00"
    },
    {
        "title": "Sharp threshold for network recovery from voter model dynamics",
        "authors": [
            "Hang Du",
            "Seokmin Ha",
            "Oriol Sol\u00e9-Pi"
        ],
        "summary": "We investigate the problem of recovering a latent directed Erd\\H{o}s-R\\'enyi\ngraph $G^*\\sim \\mathcal G(n,p)$ from observations of discrete voter model\ntrajectories on $G^*$, where $np$ grows polynomially in $n$. Given access to\n$M$ independent voter model trajectories evolving up to time $T$, we establish\nthat $G^*$ can be recovered \\emph{exactly} with probability at least $0.9$ by\nan \\emph{efficient} algorithm, provided that \\[ M \\cdot \\min\\{T, n\\} \\geq C n^2\np^2 \\log n \\] holds for a sufficiently large constant $C$. Here, $M\\cdot\n\\min\\{T,n\\}$ can be interpreted as the approximate number of effective update\nrounds being observed, since the voter model on $G^*$ typically reaches\nconsensus after $\\Theta(n)$ rounds, and no further information can be gained\nafter this point. Furthermore, we prove an \\emph{information-theoretic} lower\nbound showing that the above condition is tight up to a constant factor. Our\nresults indicate that the recovery problem does not exhibit a\nstatistical-computational gap.",
        "published": "2025-04-07T05:47:52+00:00"
    },
    {
        "title": "Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models",
        "authors": [
            "Yoojin Jung",
            "Byung Cheol Song"
        ],
        "summary": "Deep learning-based computer vision systems adopt complex and large\narchitectures to improve performance, yet they face challenges in deployment on\nresource-constrained mobile and edge devices. To address this issue, model\ncompression techniques such as pruning, quantization, and matrix factorization\nhave been proposed; however, these compressed models are often highly\nvulnerable to adversarial attacks. We introduce the \\textbf{Efficient Ensemble\nDefense (EED)} technique, which diversifies the compression of a single base\nmodel based on different pruning importance scores and enhances ensemble\ndiversity to achieve high adversarial robustness and resource efficiency. EED\ndynamically determines the number of necessary sub-models during the inference\nstage, minimizing unnecessary computations while maintaining high robustness.\nOn the CIFAR-10 and SVHN datasets, EED demonstrated state-of-the-art robustness\nperformance compared to existing adversarial pruning techniques, along with an\ninference speed improvement of up to 1.86 times. This proves that EED is a\npowerful defense solution in resource-constrained environments.",
        "published": "2025-04-07T05:41:35+00:00"
    },
    {
        "title": "Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions",
        "authors": [
            "He Zhu",
            "Quyu Kong",
            "Kechun Xu",
            "Xunlong Xia",
            "Bing Deng",
            "Jieping Ye",
            "Rong Xiong",
            "Yue Wang"
        ],
        "summary": "Grounding 3D object affordance is a task that locates objects in 3D space\nwhere they can be manipulated, which links perception and action for embodied\nintelligence. For example, for an intelligent robot, it is necessary to\naccurately ground the affordance of an object and grasp it according to human\ninstructions. In this paper, we introduce a novel task that grounds 3D object\naffordance based on language instructions, visual observations and\ninteractions, which is inspired by cognitive science. We collect an Affordance\nGrounding dataset with Points, Images and Language instructions (AGPIL) to\nsupport the proposed task. In the 3D physical world, due to observation\norientation, object rotation, or spatial occlusion, we can only get a partial\nobservation of the object. So this dataset includes affordance estimations of\nobjects from full-view, partial-view, and rotation-view perspectives. To\naccomplish this task, we propose LMAffordance3D, the first multi-modal,\nlanguage-guided 3D affordance grounding network, which applies a\nvision-language model to fuse 2D and 3D spatial features with semantic\nfeatures. Comprehensive experiments on AGPIL demonstrate the effectiveness and\nsuperiority of our method on this task, even in unseen experimental settings.\nOur project is available at https://sites.google.com/view/lmaffordance3d.",
        "published": "2025-04-07T05:38:23+00:00"
    },
    {
        "title": "MedGNN: Capturing the Links Between Urban Characteristics and Medical Prescriptions",
        "authors": [
            "Minwei Zhao",
            "Sanja Scepanovic",
            "Stephen Law",
            "Daniele Quercia",
            "Ivica Obadic"
        ],
        "summary": "Understanding how urban socio-demographic and environmental factors relate\nwith health is essential for public health and urban planning. However,\ntraditional statistical methods struggle with nonlinear effects, while machine\nlearning models often fail to capture geographical (nearby areas being more\nsimilar) and topological (unequal connectivity between places) effects in an\ninterpretable way. To address this, we propose MedGNN, a spatio-topologically\nexplicit framework that constructs a 2-hop spatial graph, integrating\npositional and locational node embeddings with urban characteristics in a graph\nneural network. Applied to MEDSAT, a comprehensive dataset covering over 150\nenvironmental and socio-demographic factors and six prescription outcomes\n(depression, anxiety, diabetes, hypertension, asthma, and opioids) across 4,835\nGreater London neighborhoods, MedGNN improved predictions by over 25% on\naverage compared to baseline methods. Using depression prescriptions as a case\nstudy, we analyzed graph embeddings via geographical principal component\nanalysis, identifying findings that: align with prior research (e.g., higher\nantidepressant prescriptions among older and White populations), contribute to\nongoing debates (e.g., greenery linked to higher and NO2 to lower\nprescriptions), and warrant further study (e.g., canopy evaporation correlated\nwith fewer prescriptions). These results demonstrate MedGNN's potential, and\nmore broadly, of carefully applied machine learning, to advance\ntransdisciplinary public health research.",
        "published": "2025-04-07T05:35:16+00:00"
    },
    {
        "title": "A High-Performance Curve25519 and Curve448 Unified Elliptic Curve Cryptography Accelerator",
        "authors": [
            "Aniket Banerjee",
            "Utsav Banerjee"
        ],
        "summary": "In modern critical infrastructure such as power grids, it is crucial to\nensure security of data communications between network-connected devices while\nfollowing strict latency criteria. This necessitates the use of cryptographic\nhardware accelerators. We propose a high-performance unified elliptic curve\ncryptography accelerator supporting NIST standard Montgomery curves Curve25519\nand Curve448 at 128-bit and 224-bit security levels respectively. Our\naccelerator implements extensive parallel processing of Karatsuba-style\nlarge-integer multiplications, restructures arithmetic operations in the\nMontgomery Ladder and exploits special mathematical properties of the\nunderlying pseudo-Mersenne and Solinas prime fields for optimized performance.\nOur design ensures efficient resource sharing across both curve computations\nand also incorporates several standard side-channel countermeasures. Our ASIC\nimplementation achieves record performance and energy of 10.38 $\\mu$s / 54.01\n$\\mu$s and 0.72 $\\mu$J / 3.73 $\\mu$J respectively for Curve25519 / Curve448,\nwhich is significantly better than state-of-the-art.",
        "published": "2025-04-07T05:04:02+00:00"
    },
    {
        "title": "Exploring Kernel Transformations for Implicit Neural Representations",
        "authors": [
            "Sheng Zheng",
            "Chaoning Zhang",
            "Dongshen Han",
            "Fachrina Dewi Puspitasari",
            "Xinhong Hao",
            "Yang Yang",
            "Heng Tao Shen"
        ],
        "summary": "Implicit neural representations (INRs), which leverage neural networks to\nrepresent signals by mapping coordinates to their corresponding attributes,\nhave garnered significant attention. They are extensively utilized for image\nrepresentation, with pixel coordinates as input and pixel values as output. In\ncontrast to prior works focusing on investigating the effect of the model's\ninside components (activation function, for instance), this work pioneers the\nexploration of the effect of kernel transformation of input/output while\nkeeping the model itself unchanged. A byproduct of our findings is a simple yet\neffective method that combines scale and shift to significantly boost INR with\nnegligible computation overhead. Moreover, we present two perspectives, depth\nand normalization, to interpret the performance benefits caused by scale and\nshift transformation. Overall, our work provides a new avenue for future works\nto understand and improve INR through the lens of kernel transformation.",
        "published": "2025-04-07T04:43:50+00:00"
    },
    {
        "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models",
        "authors": [
            "Yubo Li",
            "Xiaobin Shen",
            "Xinyu Yao",
            "Xueying Ding",
            "Yidi Miao",
            "Ramayya Krishnan",
            "Rema Padman"
        ],
        "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.",
        "published": "2025-04-07T04:00:08+00:00"
    },
    {
        "title": "Generalising from Self-Produced Data: Model Training Beyond Human Constraints",
        "authors": [
            "Alfath Daryl Alhajir",
            "Jennifer Dodgson",
            "Joseph Lim",
            "Truong Ma Phi",
            "Julian Peh",
            "Akira Rafhael Janson Pattirane",
            "Lokesh Poovaragan"
        ],
        "summary": "Current large language models (LLMs) are constrained by human-derived\ntraining data and limited by a single level of abstraction that impedes\ndefinitive truth judgments. This paper introduces a novel framework in which AI\nmodels autonomously generate and validate new knowledge through direct\ninteraction with their environment. Central to this approach is an unbounded,\nungamable numeric reward - such as annexed disk space or follower count - that\nguides learning without requiring human benchmarks. AI agents iteratively\ngenerate strategies and executable code to maximize this metric, with\nsuccessful outcomes forming the basis for self-retraining and incremental\ngeneralisation. To mitigate model collapse and the warm start problem, the\nframework emphasizes empirical validation over textual similarity and supports\nfine-tuning via GRPO. The system architecture employs modular agents for\nenvironment analysis, strategy generation, and code synthesis, enabling\nscalable experimentation. This work outlines a pathway toward self-improving AI\nsystems capable of advancing beyond human-imposed constraints toward autonomous\ngeneral intelligence.",
        "published": "2025-04-07T03:48:02+00:00"
    },
    {
        "title": "TangibleNet: Synchronous Network Data Storytelling through Tangible Interactions in Augmented Reality",
        "authors": [
            "Kentaro Takahira",
            "Wong Kam-Kwai",
            "Leni Yang",
            "Xian Xu",
            "Takanori Fujiwara",
            "Huamin Qu"
        ],
        "summary": "Synchronous data-driven storytelling with network visualizations presents\nsignificant challenges due to the complexity of real-time manipulation of\nnetwork components. While existing research addresses asynchronous scenarios,\nthere is a lack of effective tools for live presentations. To address this gap,\nwe developed TangibleNet, a projector-based AR prototype that allows presenters\nto interact with node-link diagrams using double-sided magnets during live\npresentations. The design process was informed by interviews with professionals\nexperienced in synchronous data storytelling and workshops with 14 HCI/VIS\nresearchers. Insights from the interviews helped identify key design\nconsiderations for integrating physical objects as interactive tools in\npresentation contexts. The workshops contributed to the development of a design\nspace mapping user actions to interaction commands for node-link diagrams.\nEvaluation with 12 participants confirmed that TangibleNet supports intuitive\ninteractions and enhances presenter autonomy, demonstrating its effectiveness\nfor synchronous network-based data storytelling.",
        "published": "2025-04-07T03:46:46+00:00"
    },
    {
        "title": "Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent",
        "authors": [
            "Bo Chen",
            "Zhenmei Shi",
            "Zhao Song",
            "Jiahao Zhang"
        ],
        "summary": "Recent advancements in Transformer-based architectures have led to impressive\nbreakthroughs in natural language processing tasks, with models such as GPT-4,\nClaude, and Gemini demonstrating human-level reasoning abilities. However,\ndespite their high performance, concerns remain about the inherent limitations\nof these models, especially when it comes to learning basic logical functions.\nWhile complexity-theoretic analyses indicate that Transformers can represent\nsimple logic functions (e.g., $\\mathsf{AND}$, $\\mathsf{OR}$, and majority\ngates) by its nature of belonging to the $\\mathsf{TC}^0$ class, these results\nassume ideal parameter settings and do not account for the constraints imposed\nby gradient descent-based training methods. In this work, we investigate\nwhether Transformers can truly learn simple majority functions when trained\nusing gradient-based methods. We focus on a simplified variant of the\nTransformer architecture and consider both $n=\\mathrm{poly}(d)$ and\n$n=\\exp(\\Omega(d))$ number of training samples, where each sample is a $d$-size\nbinary string paired with the output of a basic majority function. Our analysis\ndemonstrates that even after $\\mathrm{poly}(d)$ gradient queries, the\ngeneralization error of the Transformer model still remains substantially\nlarge, growing exponentially with $d$. This work highlights fundamental\noptimization challenges in training Transformers for the simplest logical\nreasoning tasks and provides new insights into their theoretical limitations.",
        "published": "2025-04-07T03:08:12+00:00"
    },
    {
        "title": "DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation",
        "authors": [
            "Bo-Wen Yin",
            "Jiao-Long Cao",
            "Ming-Ming Cheng",
            "Qibin Hou"
        ],
        "summary": "Recent advances in scene understanding benefit a lot from depth maps because\nof the 3D geometry information, especially in complex conditions (e.g., low\nlight and overexposed). Existing approaches encode depth maps along with RGB\nimages and perform feature fusion between them to enable more robust\npredictions. Taking into account that depth can be regarded as a geometry\nsupplement for RGB images, a straightforward question arises: Do we really need\nto explicitly encode depth information with neural networks as done for RGB\nimages? Based on this insight, in this paper, we investigate a new way to learn\nRGBD feature representations and present DFormerv2, a strong RGBD encoder that\nexplicitly uses depth maps as geometry priors rather than encoding depth\ninformation with neural networks. Our goal is to extract the geometry clues\nfrom the depth and spatial distances among all the image patch tokens, which\nwill then be used as geometry priors to allocate attention weights in\nself-attention. Extensive experiments demonstrate that DFormerv2 exhibits\nexceptional performance in various RGBD semantic segmentation benchmarks. Code\nis available at: https://github.com/VCIP-RGBD/DFormer.",
        "published": "2025-04-07T03:06:07+00:00"
    },
    {
        "title": "Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning",
        "authors": [
            "Songyang Liu",
            "Muyang Fan",
            "Weizi Li",
            "Jing Du",
            "Shuai Li"
        ],
        "summary": "Traffic congestion remains a significant challenge in modern urban networks.\nAutonomous driving technologies have emerged as a potential solution. Among\ntraffic control methods, reinforcement learning has shown superior performance\nover traffic signals in various scenarios. However, prior research has largely\nfocused on small-scale networks or isolated intersections, leaving large-scale\nmixed traffic control largely unexplored. This study presents the first attempt\nto use decentralized multi-agent reinforcement learning for large-scale mixed\ntraffic control in which some intersections are managed by traffic signals and\nothers by robot vehicles. Evaluating a real-world network in Colorado Springs,\nCO, USA with 14 intersections, we measure traffic efficiency via average\nwaiting time of vehicles at intersections and the number of vehicles reaching\ntheir destinations within a time window (i.e., throughput). At 80% RV\npenetration rate, our method reduces waiting time from 6.17 s to 5.09 s and\nincreases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500\nseconds, outperforming the baseline of fully signalized intersections. These\nfindings suggest that integrating reinforcement learning-based control\nlarge-scale traffic can improve overall efficiency and may inform future urban\nplanning strategies.",
        "published": "2025-04-07T02:52:39+00:00"
    },
    {
        "title": "Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal",
        "authors": [
            "Yicheng Leng",
            "Chaowei Fang",
            "Junye Chen",
            "Yixiang Fang",
            "Sheng Li",
            "Guanbin Li"
        ],
        "summary": "Visible watermark removal which involves watermark cleaning and background\ncontent restoration is pivotal to evaluate the resilience of watermarks.\nExisting deep neural network (DNN)-based models still struggle with large-area\nwatermarks and are overly dependent on the quality of watermark mask\nprediction. To overcome these challenges, we introduce a novel feature adapting\nframework that leverages the representation modeling capacity of a pre-trained\nimage inpainting model. Our approach bridges the knowledge gap between image\ninpainting and watermark removal by fusing information of the residual\nbackground content beneath watermarks into the inpainting backbone model. We\nestablish a dual-branch system to capture and embed features from the residual\nbackground content, which are merged into intermediate features of the\ninpainting backbone model via gated feature fusion modules. Moreover, for\nrelieving the dependence on high-quality watermark masks, we introduce a new\ntraining paradigm by utilizing coarse watermark masks to guide the inference\nprocess. This contributes to a visible image removal model which is insensitive\nto the quality of watermark mask during testing. Extensive experiments on both\na large-scale synthesized dataset and a real-world dataset demonstrate that our\napproach significantly outperforms existing state-of-the-art methods. The\nsource code is available in the supplementary materials.",
        "published": "2025-04-07T02:37:14+00:00"
    },
    {
        "title": "Theory of Symmetry-Protected Two-Photon Coherence",
        "authors": [
            "Xuanying Lai",
            "Shengwang Du",
            "Yue Jiang"
        ],
        "summary": "In a recent article [Phys. Rev. Lett. 133, 033601 (2024)], the coherence time\nof degenerate entangled photon pairs (biphotons) generated via backward\nspontaneous four-wave mixing in a cold atomic ensemble was shown to be immune\nto optical loss and dephasing. This finding is crucial for practical\napplications in quantum information processing, quantum communication, and\nnetworking, where loss is inevitable. However, the underlying mechanism for\nthis loss- and dephasing-insensitive biphoton coherence time was insufficiently\nstudied in the previous article, as quantum noise was not taken into account.\nIn this work, we employ the Heisenberg-Langevin approach to study this effect\nand provide a rigorous theoretical proof of the symmetry-protected biphoton\ncoherence by taking quantum noise into consideration, as compared to the\nperturbation theory in the interaction picture.",
        "published": "2025-04-07T02:22:38+00:00"
    },
    {
        "title": "DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal",
        "authors": [
            "Wanzhou Liu",
            "Zhexiao Xiong",
            "Xinyu Li",
            "Nathan Jacobs"
        ],
        "summary": "Recent novel view synthesis (NVS) techniques, including Neural Radiance\nFields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene\nreconstruction with high-quality rendering and realistic detail recovery.\nEffectively removing occlusions while preserving scene details can further\nenhance the robustness and applicability of these techniques. However, existing\napproaches for object and occlusion removal predominantly rely on generative\npriors, which, despite filling the resulting holes, introduce new artifacts and\nblurriness. Moreover, existing benchmark datasets for evaluating occlusion\nremoval methods lack realistic complexity and viewpoint variations. To address\nthese issues, we introduce DeclutterSet, a novel dataset featuring diverse\nscenes with pronounced occlusions distributed across foreground, midground, and\nbackground, exhibiting substantial relative motion across viewpoints. We\nfurther introduce DeclutterNeRF, an occlusion removal method free from\ngenerative priors. DeclutterNeRF introduces joint multi-view optimization of\nlearnable camera parameters, occlusion annealing regularization, and employs an\nexplainable stochastic structural similarity loss, ensuring high-quality,\nartifact-free reconstructions from incomplete images. Experiments demonstrate\nthat DeclutterNeRF significantly outperforms state-of-the-art methods on our\nproposed DeclutterSet, establishing a strong baseline for future research.",
        "published": "2025-04-07T02:22:08+00:00"
    },
    {
        "title": "Federated Learning over 5G, WiFi, and Ethernet: Measurements and Evaluation",
        "authors": [
            "Robert J. Hayek",
            "Joaquin Chung",
            "Kayla Comer",
            "Chandra R. Murthy",
            "Rajkumar Kettimuthu",
            "Igor Kadota"
        ],
        "summary": "Federated Learning (FL) deployments using IoT devices is an area that is\npoised to significantly benefit from advances in NextG wireless. In this paper,\nwe deploy a FL application using a 5G-NR Standalone (SA) testbed with\nopen-source and Commercial Off-the-Shelf (COTS) components. The 5G testbed\narchitecture consists of a network of resource-constrained edge devices, namely\nRaspberry Pi's, and a central server equipped with a Software Defined Radio\n(SDR) and running O-RAN software. Our testbed allows edge devices to\ncommunicate with the server using WiFi and Ethernet, instead of 5G. FL is\ndeployed using the Flower FL framework, for which we developed a comprehensive\ninstrumentation tool to collect and analyze diverse communications and machine\nlearning performance metrics including: model aggregation time, downlink\ntransmission time, training time, and uplink transmission time. Leveraging\nthese measurements, we perform a comparative analysis of the FL application\nacross three network interfaces: 5G, WiFi, and Ethernet. Our experimental\nresults suggest that, on 5G, the uplink model transfer time is a significant\nfactor in convergence time of FL. In particular, we find that the 5G uplink\ncontributes to roughly 23% of the duration of one average communication round\nwhen using all edge devices in our testbed. When comparing the uplink time of\nthe 5G testbed, we find that it is 33.3x higher than Ethernet and 17.8x higher\nthan WiFi. Our results also suggest that 5G exacerbates the well-known\nstraggler effect. For reproducibility, we have open-sourced our FL application,\ninstrumentation tools, and testbed configuration.",
        "published": "2025-04-07T02:19:01+00:00"
    },
    {
        "title": "The Disruption Index Measures Displacement Between a Paper and Its Most Cited Reference",
        "authors": [
            "Yiling Lin",
            "Linzhuo Li",
            "Lingfei Wu"
        ],
        "summary": "Initially developed to capture technical innovation and later adapted to\nidentify scientific breakthroughs, the Disruption Index (D-index) offers the\nfirst quantitative framework for analyzing transformative research. Despite its\npromise, prior studies have struggled to clarify its theoretical foundations,\nraising concerns about potential bias. Here, we show that-contrary to the\ncommon belief that the D-index measures absolute innovation-it captures\nrelative innovation: a paper's ability to displace its most-cited reference. In\nthis way, the D-index reflects scientific progress as the replacement of older\nanswers with newer ones to the same fundamental question-much like light bulbs\nreplacing candles. We support this insight through mathematical analysis,\nexpert surveys, and large-scale bibliometric evidence. To facilitate\nreplication, validation, and broader use, we release a dataset of D-index\nvalues for 49 million journal articles (1800-2024) based on OpenAlex.",
        "published": "2025-04-07T02:04:10+00:00"
    },
    {
        "title": "Sparsity-Aware Communication for Distributed Graph Neural Network Training",
        "authors": [
            "Ujjaini Mukhodopadhyay",
            "Alok Tripathy",
            "Oguz Selvitopi",
            "Katherine Yelick",
            "Aydin Buluc"
        ],
        "summary": "Graph Neural Networks (GNNs) are a computationally efficient method to learn\nembeddings and classifications on graph data. However, GNN training has low\ncomputational intensity, making communication costs the bottleneck for\nscalability. Sparse-matrix dense-matrix multiplication (SpMM) is the core\ncomputational operation in full-graph training of GNNs. Previous work\nparallelizing this operation focused on sparsity-oblivious algorithms, where\nmatrix elements are communicated regardless of the sparsity pattern. This leads\nto a predictable communication pattern that can be overlapped with computation\nand enables the use of collective communication operations at the expense of\nwasting significant bandwidth by communicating unnecessary data. We develop\nsparsity-aware algorithms that tackle the communication bottlenecks in GNN\ntraining with three novel approaches. First, we communicate only the necessary\nmatrix elements. Second, we utilize a graph partitioning model to reorder the\nmatrix and drastically reduce the amount of communicated elements. Finally, we\naddress the high load imbalance in communication with a tailored partitioning\nmodel, which minimizes both the total communication volume and the maximum\nsending volume. We further couple these sparsity-exploiting approaches with a\ncommunication-avoiding approach (1.5D parallel SpMM) in which submatrices are\nreplicated to reduce communication. We explore the tradeoffs of these combined\noptimizations and show up to 14X improvement on 256 GPUs and on some instances\nreducing communication to almost zero resulting in a communication-free\nparallel training relative to a popular GNN framework based on\ncommunication-oblivious SpMM.",
        "published": "2025-04-07T01:53:14+00:00"
    },
    {
        "title": "Hybrid and scalable photonic circuit cavity quantum electrodynamics",
        "authors": [
            "Xudong Wang",
            "Yifan Zhu",
            "Xiuqi Zhang",
            "Yuanhao Qin",
            "Bowen Chen",
            "Yang Chen",
            "Yongheng Huo",
            "Jiaxiang Zhang",
            "Xin Ou"
        ],
        "summary": "Similar to superconducting circuit quantum electrodynamics (cQED), the\ndevelopment of a photonic analog--specifically, photonic circuit cQED--has\nbecome a major focus in integrated quantum photonics. Current solid-state cQED\ndevices, however, face scalability challenges due to the difficulty in\nsimultaneously spectral tuning of cavity modes and quantum emitters while\nensuring in-plane optical modes confinement for efficient on-chip light\nrouting. Here, we overcome these limitations by proposing and demonstrating a\nhybrid solid-state cQED platform integrated on a chip. Our device integrates\nsemiconducting quantum dots (QDs) with a thin-film lithium niobate (TFLN)\nmicroring resonator. Leveraging TFLN's ferroelectric and electro-optic (EO)\nproperties, we implement local spectral tuning of both waveguide-coupled QDs\nand cavity modes. This approach achieves a broad spectral tuning range of up to\n4.82 nm for individual QDs, enabling deterministic on-chip single-photon\nemission with a Purcell factor of 3.52. When combined with EO cavity tuning, we\nrealize a spectrally tunable hybrid photonic circuit cQED device, sustaining\nnear-constant Purcell factors of 1.89 over a 0.30 nm spectral range. This\nachievement enables scalable on-chip cavity-enhanced single-photon sources\nwhile preserving optical properties and maintaining compatibility with advanced\nphotonic architectures, marking a significant step toward practical\nimplementation of large-scale chip-based quantum networks.",
        "published": "2025-04-07T01:45:28+00:00"
    },
    {
        "title": "Scaling Graph Neural Networks for Particle Track Reconstruction",
        "authors": [
            "Alok Tripathy",
            "Alina Lazar",
            "Xiangyang Ju",
            "Paolo Calafiura",
            "Katherine Yelick",
            "Aydin Buluc"
        ],
        "summary": "Particle track reconstruction is an important problem in high-energy physics\n(HEP), necessary to study properties of subatomic particles. Traditional track\nreconstruction algorithms scale poorly with the number of particles within the\naccelerator. The Exa.TrkX project, to alleviate this computational burden,\nintroduces a pipeline that reduces particle track reconstruction to edge\nclassification on a graph, and uses graph neural networks (GNNs) to produce\nparticle tracks. However, this GNN-based approach is memory-prohibitive and\nskips graphs that would exceed GPU memory. We introduce improvements to the\nExa.TrkX pipeline to train on samples of input particle graphs, and show that\nthese improvements generalize to higher precision and recall. In addition, we\nadapt performance optimizations, introduced for GNN training, to fit our\naugmented Exa.TrkX pipeline. These optimizations provide a $2\\times$ speedup\nover our baseline implementation in PyTorch Geometric.",
        "published": "2025-04-07T01:44:32+00:00"
    },
    {
        "title": "asKAN: Active Subspace embedded Kolmogorov-Arnold Network",
        "authors": [
            "Zhiteng Zhou",
            "Zhaoyue Xu",
            "Yi Liu",
            "Shizhao Wang"
        ],
        "summary": "The Kolmogorov-Arnold Network (KAN) has emerged as a promising neural network\narchitecture for small-scale AI+Science applications. However, it suffers from\ninflexibility in modeling ridge functions, which is widely used in representing\nthe relationships in physical systems. This study investigates this\ninflexibility through the lens of the Kolmogorov-Arnold theorem, which starts\nthe representation of multivariate functions from constructing the univariate\ncomponents rather than combining the independent variables. Our analysis\nreveals that incorporating linear combinations of independent variables can\nsubstantially simplify the network architecture in representing the ridge\nfunctions. Inspired by this finding, we propose active subspace embedded KAN\n(asKAN), a hierarchical framework that synergizes KAN's function representation\nwith active subspace methodology. The architecture strategically embeds active\nsubspace detection between KANs, where the active subspace method is used to\nidentify the primary ridge directions and the independent variables are\nadaptively projected onto these critical dimensions. The proposed asKAN is\nimplemented in an iterative way without increasing the number of neurons in the\noriginal KAN. The proposed method is validated through function fitting,\nsolving the Poisson equation, and reconstructing sound field. Compared with\nKAN, asKAN significantly reduces the error using the same network architecture.\nThe results suggest that asKAN enhances the capability of KAN in fitting and\nsolving equations with in the form of ridge functions.",
        "published": "2025-04-07T01:43:13+00:00"
    },
    {
        "title": "A Simultaneous Approach for Training Neural Differential-Algebraic Systems of Equations",
        "authors": [
            "Laurens R. Lueg",
            "Victor Alves",
            "Daniel Schicksnus",
            "John R. Kitchin",
            "Carl D. Laird",
            "Lorenz T. Biegler"
        ],
        "summary": "Scientific machine learning is an emerging field that broadly describes the\ncombination of scientific computing and machine learning to address challenges\nin science and engineering. Within the context of differential equations, this\nhas produced highly influential methods, such as neural ordinary differential\nequations (NODEs). Recent works extend this line of research to consider neural\ndifferential-algebraic systems of equations (DAEs), where some unknown\nrelationships within the DAE are learned from data. Training neural DAEs,\nsimilarly to neural ODEs, is computationally expensive, as it requires the\nsolution of a DAE for every parameter update. Further, the rigorous\nconsideration of algebraic constraints is difficult within common deep learning\ntraining algorithms such as stochastic gradient descent. In this work, we apply\nthe simultaneous approach to neural DAE problems, resulting in a fully\ndiscretized nonlinear optimization problem, which is solved to local optimality\nand simultaneously obtains the neural network parameters and the solution to\nthe corresponding DAE. We extend recent work demonstrating the simultaneous\napproach for neural ODEs, by presenting a general framework to solve neural\nDAEs, with explicit consideration of hybrid models, where some components of\nthe DAE are known, e.g. physics-informed constraints. Furthermore, we present a\ngeneral strategy for improving the performance and convergence of the nonlinear\nprogramming solver, based on solving an auxiliary problem for initialization\nand approximating Hessian terms. We achieve promising results in terms of\naccuracy, model generalizability and computational cost, across different\nproblem settings such as sparse data, unobserved states and multiple\ntrajectories. Lastly, we provide several promising future directions to improve\nthe scalability and robustness of our approach.",
        "published": "2025-04-07T01:26:55+00:00"
    },
    {
        "title": "Classification of ADHD and Healthy Children Using EEG Based Multi-Band Spatial Features Enhancement",
        "authors": [
            "Md Bayazid Hossain",
            "Md Anwarul Islam Himel",
            "Md Abdur Rahim",
            "Shabbir Mahmood",
            "Abu Saleh Musa Miah",
            "Jungpil Shin"
        ],
        "summary": "Attention Deficit Hyperactivity Disorder (ADHD) is a common\nneurodevelopmental disorder in children, characterized by difficulties in\nattention, hyperactivity, and impulsivity. Early and accurate diagnosis of ADHD\nis critical for effective intervention and management. Electroencephalogram\n(EEG) signals have emerged as a non-invasive and efficient tool for ADHD\ndetection due to their high temporal resolution and ability to capture neural\ndynamics. In this study, we propose a method for classifying ADHD and healthy\nchildren using EEG data from the benchmark dataset. There were 61 children with\nADHD and 60 healthy children, both boys and girls, aged 7 to 12. The EEG\nsignals, recorded from 19 channels, were processed to extract Power Spectral\nDensity (PSD) and Spectral Entropy (SE) features across five frequency bands,\nresulting in a comprehensive 190-dimensional feature set. To evaluate the\nclassification performance, a Support Vector Machine (SVM) with the RBF kernel\ndemonstrated the best performance with a mean cross-validation accuracy of\n99.2\\% and a standard deviation of 0.0079, indicating high robustness and\nprecision. These results highlight the potential of spatial features in\nconjunction with machine learning for accurately classifying ADHD using EEG\ndata. This work contributes to developing non-invasive, data-driven tools for\nearly diagnosis and assessment of ADHD in children.",
        "published": "2025-04-07T01:19:14+00:00"
    },
    {
        "title": "N-TORC: Native Tensor Optimizer for Real-time Constraints",
        "authors": [
            "Suyash Vardhan Singh",
            "Iftakhar Ahmad",
            "David Andrews",
            "Miaoqing Huang",
            "Austin R. J. Downey",
            "Jason D. Bakos"
        ],
        "summary": "Compared to overlay-based tensor architectures like VTA or Gemmini, compilers\nthat directly translate machine learning models into a dataflow architecture as\nHLS code, such as HLS4ML and FINN, generally can achieve lower latency by\ngenerating customized matrix-vector multipliers and memory structures tailored\nto the specific fundamental tensor operations required by each layer. However,\nthis approach has significant drawbacks: the compilation process is highly\ntime-consuming and the resulting deployments have unpredictable area and\nlatency, making it impractical to constrain the latency while simultaneously\nminimizing area. Currently, no existing methods address this type of\noptimization. In this paper, we present N-TORC (Native Tensor Optimizer for\nReal-Time Constraints), a novel approach that utilizes data-driven performance\nand resource models to optimize individual layers of a dataflow architecture.\nWhen combined with model hyperparameter optimization, N-TORC can quickly\ngenerate architectures that satisfy latency constraints while simultaneously\noptimizing for both accuracy and resource cost (i.e. offering a set of optimal\ntrade-offs between cost and accuracy). To demonstrate its effectiveness, we\napplied this framework to a cyber-physical application, DROPBEAR (Dynamic\nReproduction of Projectiles in Ballistic Environments for Advanced Research).\nN-TORC's HLS4ML performance and resource models achieve higher accuracy than\nprior efforts, and its Mixed Integer Program (MIP)-based solver generates\nequivalent solutions to a stochastic search in 1000X less time.",
        "published": "2025-04-07T01:13:48+00:00"
    },
    {
        "title": "EquiCPI: SE(3)-Equivariant Geometric Deep Learning for Structure-Aware Prediction of Compound-Protein Interactions",
        "authors": [
            "Ngoc-Quang Nguyen"
        ],
        "summary": "Accurate prediction of compound-protein interactions (CPI) remains a\ncornerstone challenge in computational drug discovery. While existing\nsequence-based approaches leverage molecular fingerprints or graph\nrepresentations, they critically overlook three-dimensional (3D) structural\ndeterminants of binding affinity. To bridge this gap, we present EquiCPI, an\nend-to-end geometric deep learning framework that synergizes first-principles\nstructural modeling with SE(3)-equivariant neural networks. Our pipeline\ntransforms raw sequences into 3D atomic coordinates via ESMFold for proteins\nand DiffDock-L for ligands, followed by physics-guided conformer re-ranking and\nequivariant feature learning. At its core, EquiCPI employs SE(3)-equivariant\nmessage passing over atomic point clouds, preserving symmetry under rotations,\ntranslations, and reflections, while hierarchically encoding local interaction\npatterns through tensor products of spherical harmonics. The proposed model is\nevaluated on BindingDB (affinity prediction) and DUD-E (virtual screening),\nEquiCPI achieves performance on par with or exceeding the state-of-the-art deep\nlearning competitors.",
        "published": "2025-04-07T00:57:08+00:00"
    },
    {
        "title": "On the Nature of Fractal Numbers and the Classical Continuum Hypothesis (CH)",
        "authors": [
            "Stanislav Semenov"
        ],
        "summary": "We propose a reinterpretation of the continuum grounded in the stratified\nstructure of definability rather than classical cardinality. In this framework,\na real number is not an abstract point on the number line, but an object\nexpressible at some level Fn of a formal hierarchy. We introduce the notion of\n\"fractal numbers\" -- entities defined not within a fixed set-theoretic\nuniverse, but through layered expressibility across constructive systems. This\nreconceptualizes irrationality as a relative property, depending on\ndefinability depth, and replaces the binary dichotomy between countable and\nuncountable sets with a gradated spectrum of definability classes. We show that\nthe classical Continuum Hypothesis loses its force in this context: between\naleph_0 and c lies not a single cardinal jump, but a stratified sequence of\ndefinitional stages, each forming a countable yet irreducible approximation to\nthe continuum. We argue that the real line should not be seen as a completed\ntotality but as an evolving architecture of formal expressibility. We conclude\nwith a discussion of rational invariants, the relativity of irrationality, and\nthe emergence of a fractal metric for definitional density.",
        "published": "2025-04-06T22:40:21+00:00"
    },
    {
        "title": "Tight Low Degree Hardness for Optimizing Pure Spherical Spin Glasses",
        "authors": [
            "Mark Sellke"
        ],
        "summary": "We prove constant degree polynomial algorithms cannot optimize pure spherical\n$p$-spin Hamiltonians beyond the algorithmic threshold\n$\\mathsf{ALG}(p)=2\\sqrt{\\frac{p-1}{p}}$. The proof goes by transforming any\nhypothetical such algorithm into a Lipschitz one, for which hardness was shown\npreviously by the author and B. Huang.",
        "published": "2025-04-06T22:01:47+00:00"
    },
    {
        "title": "Characterization of NbTiN/HZO/NbTiN MIM Capacitors for high frequency AC Clock & Power Distribution Network for Superconducting Digital Circuits",
        "authors": [
            "Seifallah Ibrahim",
            "Blake Hodges",
            "Steven Brebels",
            "Julian Gil Pinzon",
            "Trent Josephsen",
            "Ankit Pokhrel",
            "Daniel Perez Lozano",
            "Yann Canvel",
            "Bart Kenens",
            "Amey M. Walke",
            "Gianpiero Maccarrone Lapi",
            "Sara Iraci",
            "Mihaela Popovici",
            "Benjamin Huet",
            "Quentin Herr",
            "Zsolt T\u0151kei",
            "Anna Herr"
        ],
        "summary": "A resonant clock-power distribution network is critical for scaling energy\nefficient superconducting digital technology to practical high integration\ndensity circuits. High-k, tunable capacitors enable implementation of a\nresonant power delivery network supporting circuits with up to 400\nMdevices/cm2. We report the cryogenic characterization of Metal-Insulator-Metal\ncapacitors using a Hafnium Zirconium Oxide (HZO) ferroelectric insulating layer\nand Niobium Titanium Nitride (NbTiN) superconducting electrodes. The fabricated\nchip includes capacitor arrays for low frequency characterization and a\nhalf-wave transmission line resonator for RF characterization. A specific\ncapacitance of 3 uF/cm2, DC leakage current of 10-8 A/cm2 at 2 V and a constant\n5 percent tunability up to 4 GHz were measured at 2.6 K.",
        "published": "2025-04-06T21:08:11+00:00"
    },
    {
        "title": "Regularization and Selection in A Directed Network Model with Nodal Homophily and Nodal Effects",
        "authors": [
            "Zhaoyu Xing",
            "Y. X. Rachel Wang",
            "Andrew T. A. Wood",
            "Tao Zou"
        ],
        "summary": "This article introduces a regularization and selection methods for directed\nnetworks with nodal homophily and nodal effects. The proposed approach not only\npreserves the statistical efficiency of the resulting estimator, but also\nensures that the selection of nodal homophily and nodal effects is scalable\nwith large-scale network data and multiple nodal features. In particular, we\npropose a directed random network model with nodal homophily and nodal effects,\nwhich includes the nodal features in the probability density of random\nnetworks. Subsequently, we propose a regularized maximum likelihood estimator\nwith an adaptive LASSO-type regularizer. We demonstrate that the regularized\nestimator exhibits the consistency and possesses the oracle properties. In\naddition, we propose a network Bayesian information criterion which ensures the\nselection consistency while tuning the model. Simulation experiments are\nconducted to demonstrate the excellent numerical performance. An online\nfriendship network among musicians with nodal musical preference is used to\nillustrate the usefulness of the proposed new network model in network-related\nempirical analysis.",
        "published": "2025-04-06T21:04:56+00:00"
    },
    {
        "title": "SiameseDuo++: Active Learning from Data Streams with Dual Augmented Siamese Networks",
        "authors": [
            "Kleanthis Malialis",
            "Stylianos Filippou",
            "Christos G. Panayiotou",
            "Marios M. Polycarpou"
        ],
        "summary": "Data stream mining, also known as stream learning, is a growing area which\ndeals with learning from high-speed arriving data. Its relevance has surged\nrecently due to its wide range of applicability, such as, critical\ninfrastructure monitoring, social media analysis, and recommender systems. The\ndesign of stream learning methods faces significant research challenges; from\nthe nonstationary nature of the data (referred to as concept drift) and the\nfact that data streams are typically not annotated with the ground truth, to\nthe requirement that such methods should process large amounts of data in\nreal-time with limited memory. This work proposes the SiameseDuo++ method,\nwhich uses active learning to automatically select instances for a human expert\nto label according to a budget. Specifically, it incrementally trains two\nsiamese neural networks which operate in synergy, augmented by generated\nexamples. Both the proposed active learning strategy and augmentation operate\nin the latent space. SiameseDuo++ addresses the aforementioned challenges by\noperating with limited memory and limited labelling budget. Simulation\nexperiments show that the proposed method outperforms strong baselines and\nstate-of-the-art methods in terms of learning speed and/or performance. To\npromote open science we publicly release our code and datasets.",
        "published": "2025-04-06T20:45:25+00:00"
    },
    {
        "title": "Diffusion-Based Approximate MPC: Fast and Consistent Imitation of Multi-Modal Action Distributions",
        "authors": [
            "Pau Marquez Julbe",
            "Julian Nubert",
            "Henrik Hose",
            "Sebastian Trimpe",
            "Katherine J. Kuchenbecker"
        ],
        "summary": "Approximating model predictive control (MPC) using imitation learning (IL)\nallows for fast control without solving expensive optimization problems online.\nHowever, methods that use neural networks in a simple L2-regression setup fail\nto approximate multi-modal (set-valued) solution distributions caused by local\noptima found by the numerical solver or non-convex constraints, such as\nobstacles, significantly limiting the applicability of approximate MPC in\npractice. We solve this issue by using diffusion models to accurately represent\nthe complete solution distribution (i.e., all modes) at high control rates\n(more than 1000 Hz). This work shows that diffusion based AMPC significantly\noutperforms L2-regression-based approximate MPC for multi-modal action\ndistributions. In contrast to most earlier work on IL, we also focus on running\nthe diffusion-based controller at a higher rate and in joint space instead of\nend-effector space. Additionally, we propose the use of gradient guidance\nduring the denoising process to consistently pick the same mode in closed loop\nto prevent switching between solutions. We propose using the cost and\nconstraint satisfaction of the original MPC problem during parallel sampling of\nsolutions from the diffusion model to pick a better mode online. We evaluate\nour method on the fast and accurate control of a 7-DoF robot manipulator both\nin simulation and on hardware deployed at 250 Hz, achieving a speedup of more\nthan 70 times compared to solving the MPC problem online and also outperforming\nthe numerical optimization (used for training) in success ratio.",
        "published": "2025-04-06T20:22:01+00:00"
    },
    {
        "title": "Non-Equilibrium Dynamics of Hard Spheres in the Fluid, Crystalline, and Glassy Regimes",
        "authors": [
            "Matthew Kafker",
            "Xerxes D. Arsiwalla"
        ],
        "summary": "We investigate the response of a system of hard spheres to two classes of\nperturbations over a range of densities spanning the fluid, crystalline, and\nglassy regimes within a molecular dynamics framework. Firstly, we consider the\nrelaxation of a \"thermal inhomogeneity,\" in which a central region of particles\nis given a higher temperature than its surroundings and is then allowed to\nevolve under Newtonian dynamics. In this case, the hot central \"core\" of\nparticles expands and collides with the cold surrounding material, creating a\ntransient radially-expanding \"compression wave,\" which is rapidly dissipated by\nparticle-particle collisions and interaction with periodic images at the\nboundary, leading to a rapid relaxation to equilibrium. Secondly, we consider a\nrapid compression of the spheres into a disordered glassy state at high\ndensities. Such rapidly compressed systems exhibit very slow structural\nrelaxation times, many orders of magnitude longer than thermalization times for\nsimple temperature inhomogeneities. We find that thermal relaxation of the\nvelocity distribution is determined simply by the total collision rate, whereas\nstructural relaxation requires coordinated collective motion, which is strongly\nsuppressed at high density, although some particle rearrangement nevertheless\noccurs. We further find that collisions propagate significantly faster through\nglassy systems than through crystalline systems at the same density, which\nleads to very rapid relaxation of velocity perturbations, although structural\nrelaxation remains very slow. These results extend the validity of previous\nobservations that glassy systems exhibit a hybrid character, sharing features\nwith both equilibrium and non-equilibrium systems. Finally, we introduce the\nhard sphere causal graph, a network-based characterization of the dynamical\nhistory of a hard sphere system, which encapsulates several useful...",
        "published": "2025-04-06T20:08:54+00:00"
    },
    {
        "title": "Diff-SSL-G-Comp: Towards a Large-Scale and Diverse Dataset for Virtual Analog Modeling",
        "authors": [
            "Yicheng Gu",
            "Runsong Zhang",
            "Lauri Juvela",
            "Zhizheng Wu"
        ],
        "summary": "Virtual Analog (VA) modeling aims to simulate the behavior of hardware\ncircuits via algorithms to replicate their tone digitally. Dynamic Range\nCompressor (DRC) is an audio processing module that controls the dynamics of a\ntrack by reducing and amplifying the volumes of loud and quiet sounds, which is\nessential in music production. In recent years, neural-network-based VA\nmodeling has shown great potential in producing high-fidelity models. However,\ndue to the lack of data quantity and diversity, their generalization ability in\ndifferent parameter settings and input sounds is still limited. To tackle this\nproblem, we present Diff-SSL-G-Comp, the first large-scale and diverse dataset\nfor modeling the SSL 500 G-Bus Compressor. Specifically, we manually collected\n175 unmastered songs from the Cambridge Multitrack Library. We recorded the\ncompressed audio in 220 parameter combinations, resulting in an extensive\n2528-hour dataset with diverse genres, instruments, tempos, and keys. Moreover,\nto facilitate the use of our proposed dataset, we conducted benchmark\nexperiments in various open-sourced black-box and grey-box models, as well as\nwhite-box plugins. We also conducted ablation studies in different data subsets\nto illustrate the effectiveness of improved data diversity and quantity. The\ndataset and demos are on our project page:\nhttp://www.yichenggu.com/DiffSSLGComp/.",
        "published": "2025-04-06T19:19:53+00:00"
    },
    {
        "title": "Joint Optimization of Handoff and Video Rate in LEO Satellite Networks",
        "authors": [
            "Kyoungjun Park",
            "Zhiyuan He",
            "Cheng Luo",
            "Yi Xu",
            "Lili Qiu",
            "Changhan Ge",
            "Muhammad Muaz",
            "Yuqing Yang"
        ],
        "summary": "Low Earth Orbit (LEO) satellite communication presents a promising solution\nfor delivering Internet access to users in remote regions. Given that video\ncontent is expected to dominate network traffic in LEO satellite systems, this\nstudy presents a new video-aware mobility management framework specifically\ndesigned for such networks. By combining simulation models with real-world\ndatasets, we highlight the critical role of handoff strategies and throughput\nprediction algorithms in both single-user and multi-user video streaming\nscenarios. Building on these insights, we introduce a suite of innovative\nalgorithms that jointly determine satellite selection and video bitrate to\nenhance users' quality of experience (QoE). Initially, we design model\npredictive control (MPC) and reinforcement learning (RL) based methods for\nindividual users, then extend the approach to manage multiple users sharing a\nsatellite. Notably, we incorporate centralized training with distributed\ninference in our RL design to develop distributed policies informed by a global\nview. The effectiveness of our approach is validated through trace-driven\nsimulations and testbed experiments.",
        "published": "2025-04-06T18:58:22+00:00"
    },
    {
        "title": "Planning Safety Trajectories with Dual-Phase, Physics-Informed, and Transportation Knowledge-Driven Large Language Models",
        "authors": [
            "Rui Gan",
            "Pei Li",
            "Keke Long",
            "Bocheng An",
            "Junwei You",
            "Keshu Wu",
            "Bin Ran"
        ],
        "summary": "Foundation models have demonstrated strong reasoning and generalization\ncapabilities in driving-related tasks, including scene understanding, planning,\nand control. However, they still face challenges in hallucinations,\nuncertainty, and long inference latency. While existing foundation models have\ngeneral knowledge of avoiding collisions, they often lack\ntransportation-specific safety knowledge. To overcome these limitations, we\nintroduce LetsPi, a physics-informed, dual-phase, knowledge-driven framework\nfor safe, human-like trajectory planning. To prevent hallucinations and\nminimize uncertainty, this hybrid framework integrates Large Language Model\n(LLM) reasoning with physics-informed social force dynamics. LetsPi leverages\nthe LLM to analyze driving scenes and historical information, providing\nappropriate parameters and target destinations (goals) for the social force\nmodel, which then generates the future trajectory. Moreover, the dual-phase\narchitecture balances reasoning and computational efficiency through its Memory\nCollection phase and Fast Inference phase. The Memory Collection phase\nleverages the physics-informed LLM to process and refine planning results\nthrough reasoning, reflection, and memory modules, storing safe, high-quality\ndriving experiences in a memory bank. Surrogate safety measures and\nphysics-informed prompt techniques are introduced to enhance the LLM's\nknowledge of transportation safety and physical force, respectively. The Fast\nInference phase extracts similar driving experiences as few-shot examples for\nnew scenarios, while simplifying input-output requirements to enable rapid\ntrajectory planning without compromising safety. Extensive experiments using\nthe HighD dataset demonstrate that LetsPi outperforms baseline models across\nfive safety metrics.See PDF for project Github link.",
        "published": "2025-04-06T17:34:33+00:00"
    },
    {
        "title": "Beyond catastrophic forgetting in associative networks with self-interactions",
        "authors": [
            "Gianni V. Vinci",
            "Andrea Galluzzi",
            "Maurizio Mattia"
        ],
        "summary": "Spin-glass models of associative memories are a cornerstone between\nstatistical physics and theoretical neuroscience. In these networks, stochastic\nspin-like units interact through a synaptic matrix shaped by local Hebbian\nlearning. In absence of self-interactions (i.e., autapses), the free energy\nreveals catastrophic forgetting of all stored patterns when their number\nexceeds a critical memory load. Here, we bridge the gap with biology by\nconsidering networks of deterministic, graded units coupled via the same\nAmari-Hopfield synaptic matrix, while retaining autapses. Contrary to the\nassumption that self-couplings play a negligible role, we demonstrate that they\nqualitatively reshape the energy landscape, confining the recurrent dynamics to\nthe subspace hosting the stored patterns. This allows for the derivation of an\nexact overlap-dependent Lyapunov function, valid even for networks with finite\nsize. Moreover, self-interactions generate an auxiliary internal field aligned\nwith the target memory pattern, widening the repertoire of accessible attractor\nstates. Consequently, pure recall states act as robust associative memories for\nany memory load, beyond the critical threshold for catastrophic forgetting\nobserved in spin-glass models -- all without requiring nonlocal learning\nprescriptions or significant reshaping of the Hebbian synaptic matrix.",
        "published": "2025-04-06T17:24:20+00:00"
    },
    {
        "title": "Online Facility Assignments on Polygons",
        "authors": [
            "Sumaiya Malik",
            "Reyan Ahmed",
            "Md. Manzurul Hasan"
        ],
        "summary": "We study the online facility assignment problem on regular polygons, where\nall sides are of equal length. The influence of specific geometric settings has\nremained mostly unexplored, even though classical online facility assignment\nproblems have mainly dealt with linear and general metric spaces. We fill this\ngap by considering the following four basic geometric settings: equilateral\ntriangles, rectangles, regular $n$-polygons, and circles. The facilities are\nsituated at fixed positions on the boundary, and customers appear sequentially\non the boundary. A customer needs to be assigned immediately without any\ninformation about future customer arrivals. We study a natural greedy\nalgorithm. First, we study an equilateral triangle with three facilities at its\ncorners; customers can appear anywhere on the boundary. We then analyze regular\n$n$-sided polygons, obtaining a competitive ratio of $2n-1$, showing that the\nalgorithm performance degrades linearly with the number of corner points for\npolygons. For the circular configuration, the competitive ratio is $2n-1$ when\nthe distance between two adjacent facilities is the same. And the competitive\nratios are $n^2-n+1$ and $2^n - 1$ for varying distances linearly and\nexponentially respectively. Each facility has a fixed capacity proportional to\nthe geometric configuration, and customers appear only along the boundary\nedges. Our results also show that simpler geometric configurations have more\nefficient performance bounds and that spacing facilities uniformly apart\nprevent worst-case scenarios. The findings have many practical implications\nbecause large networks of facilities are best partitioned into smaller and\ngeometrically simple pieces to guarantee good overall performance.",
        "published": "2025-04-06T17:13:43+00:00"
    },
    {
        "title": "Dynamic Neural Field Modeling of Visual Contrast for Perceiving Incoherent Looming",
        "authors": [
            "Ziyan Qin",
            "Qinbing Fu",
            "Jigen Peng",
            "Shigang Yue"
        ],
        "summary": "Amari's Dynamic Neural Field (DNF) framework provides a brain-inspired\napproach to modeling the average activation of neuronal groups. Leveraging a\nsingle field, DNF has become a promising foundation for low-energy looming\nperception module in robotic applications. However, the previous DNF methods\nface significant challenges in detecting incoherent or inconsistent looming\nfeatures--conditions commonly encountered in real-world scenarios, such as\ncollision detection in rainy weather. Insights from the visual systems of fruit\nflies and locusts reveal encoding ON/OFF visual contrast plays a critical role\nin enhancing looming selectivity. Additionally, lateral excitation mechanism\npotentially refines the responses of loom-sensitive neurons to both coherent\nand incoherent stimuli. Together, these offer valuable guidance for improving\nlooming perception models. Building on these biological evidence, we extend the\nprevious single-field DNF framework by incorporating the modeling of ON/OFF\nvisual contrast, each governed by a dedicated DNF. Lateral excitation within\neach ON/OFF-contrast field is formulated using a normalized Gaussian kernel,\nand their outputs are integrated in the Summation field to generate collision\nalerts. Experimental evaluations show that the proposed model effectively\naddresses incoherent looming detection challenges and significantly outperforms\nstate-of-the-art locust-inspired models. It demonstrates robust performance\nacross diverse stimuli, including synthetic rain effects, underscoring its\npotential for reliable looming perception in complex, noisy environments with\ninconsistent visual cues.",
        "published": "2025-04-06T17:04:14+00:00"
    },
    {
        "title": "Opening the black box of deep learning: Validating the statistical association between explainable artificial intelligence (XAI) and clinical domain knowledge in fundus image-based glaucoma diagnosis",
        "authors": [
            "Han Yuan",
            "Lican Kang",
            "Yong Li"
        ],
        "summary": "While deep learning has exhibited remarkable predictive capabilities in\nvarious medical image tasks, its inherent black-box nature has hindered its\nwidespread implementation in real-world healthcare settings. Our objective is\nto unveil the decision-making processes of deep learning models in the context\nof glaucoma classification by employing several Class Activation Map (CAM)\ntechniques to generate model focus regions and comparing them with clinical\ndomain knowledge of the anatomical area (optic cup, optic disk, and blood\nvessels). Four deep neural networks, including VGG-11, ResNet-18, DeiT-Tiny,\nand Swin Transformer-Tiny, were developed using binary diagnostic labels of\nglaucoma and five CAM methods (Grad-CAM, XGrad-CAM, Score-CAM, Eigen-CAM, and\nLayer-CAM) were employed to highlight the model focus area. We applied the\npaired-sample t-test to compare the percentage of anatomies in the model focus\narea to the proportion of anatomies in the entire image. After that, Pearson's\nand Spearman's correlation tests were implemented to examine the relationship\nbetween model predictive ability and the percentage of anatomical structures in\nthe model focus area. On five public glaucoma datasets, all deep learning\nmodels consistently displayed statistically significantly higher percentages of\nanatomical structures in the focus area than the proportions of anatomical\nstructures in the entire image. Also, we validated the positive relationship\nbetween the percentage of anatomical structures in the focus area and model\npredictive performance. Our study provides evidence of the convergence of\ndecision logic between deep neural networks and human clinicians through\nrigorous statistical tests. We anticipate that it can help alleviate\nclinicians' concerns regarding the trustworthiness of deep learning in\nhealthcare. For reproducibility, the code and dataset have been released at\nGitHub.",
        "published": "2025-04-06T16:57:34+00:00"
    },
    {
        "title": "pc-COP: An Efficient and Configurable 2048-p-Bit Fully-Connected Probabilistic Computing Accelerator for Combinatorial Optimization",
        "authors": [
            "Kiran Magar",
            "Shreya Bharathan",
            "Utsav Banerjee"
        ],
        "summary": "Probabilistic computing is an emerging quantum-inspired computing paradigm\ncapable of solving combinatorial optimization and various other classes of\ncomputationally hard problems. In this work, we present pc-COP, an efficient\nand configurable probabilistic computing hardware accelerator with 2048 fully\nconnected probabilistic bits (p-bits) implemented on Xilinx UltraScale+ FPGA.\nWe propose a pseudo-parallel p-bit update architecture with\nspeculate-and-select logic which improves overall performance by $4 \\times$\ncompared to the traditional sequential p-bit update. Using our FPGA-based\naccelerator, we demonstrate the standard G-Set graph maximum cut benchmarks\nwith near-99% average accuracy. Compared to state-of-the-art hardware\nimplementations, we achieve similar performance and accuracy with lower FPGA\nresource utilization.",
        "published": "2025-04-06T16:43:51+00:00"
    },
    {
        "title": "Automated Verification of Soundness of DNN Certifiers",
        "authors": [
            "Avaljot Singh",
            "Yasmin Chandini Sarita",
            "Charith Mendis",
            "Gagandeep Singh"
        ],
        "summary": "The uninterpretability of Deep Neural Networks (DNNs) hinders their use in\nsafety-critical applications. Abstract Interpretation-based DNN certifiers\nprovide promising avenues for building trust in DNNs. Unsoundness in the\nmathematical logic of these certifiers can lead to incorrect results. However,\ncurrent approaches to ensure their soundness rely on manual, expert-driven\nproofs that are tedious to develop, limiting the speed of developing new\ncertifiers. Automating the verification process is challenging due to the\ncomplexity of verifying certifiers for arbitrary DNN architectures and handling\ndiverse abstract analyses.\n  We introduce ProveSound, a novel verification procedure that automates the\nsoundness verification of DNN certifiers for arbitrary DNN architectures. Our\ncore contribution is the novel concept of a symbolic DNN, using which,\nProveSound reduces the soundness property, a universal quantification over\narbitrary DNNs, to a tractable symbolic representation, enabling verification\nwith standard SMT solvers. By formalizing the syntax and operational semantics\nof ConstraintFlow, a DSL for specifying certifiers, ProveSound efficiently\nverifies both existing and new certifiers, handling arbitrary DNN\narchitectures.\n  Our code is available at https://github.com/uiuc-focal-lab/constraintflow.git",
        "published": "2025-04-06T16:42:56+00:00"
    },
    {
        "title": "Hyperflows: Pruning Reveals the Importance of Weights",
        "authors": [
            "Eugen Barbulescu",
            "Antonio Alexoaie"
        ],
        "summary": "Network pruning is used to reduce inference latency and power consumption in\nlarge neural networks. However, most existing methods struggle to accurately\nassess the importance of individual weights due to their inherent\ninterrelatedness, leading to poor performance, especially at extreme sparsity\nlevels. We introduce Hyperflows, a dynamic pruning approach that estimates each\nweight's importance by observing the network's gradient response to the\nweight's removal. A global pressure term continuously drives all weights toward\npruning, with those critical for accuracy being automatically regrown based on\ntheir flow, the aggregated gradient signal when they are absent. We explore the\nrelationship between final sparsity and pressure, deriving power-law equations\nsimilar to those found in neural scaling laws. Empirically, we demonstrate\nstate-of-the-art results with ResNet-50 and VGG-19 on CIFAR-10 and CIFAR-100.",
        "published": "2025-04-06T16:09:18+00:00"
    },
    {
        "title": "Memetic Search for Green Vehicle Routing Problem with Private Capacitated Refueling Stations",
        "authors": [
            "Rui Xu",
            "Xing Fan",
            "Shengcai Liu",
            "Wenjie Chen",
            "Ke Tang"
        ],
        "summary": "The green vehicle routing problem with private capacitated alternative fuel\nstations (GVRP-PCAFS) extends the traditional green vehicle routing problem by\nconsidering refueling stations limited capacity, where a limited number of\nvehicles can refuel simultaneously with additional vehicles must wait. This\nfeature presents new challenges for route planning, as waiting times at\nstations must be managed while keeping route durations within limits and\nreducing total travel distance. This article presents METS, a novel memetic\nalgorithm (MA) with separate constraint-based tour segmentation (SCTS) and\nefficient local search (ELS) for solving GVRP-PCAFS. METS combines global and\nlocal search effectively through three novelties. For global search, the SCTS\nstrategy splits giant tours to generate diverse solutions, and the search\nprocess is guided by a comprehensive fitness evaluation function to dynamically\ncontrol feasibility and diversity to produce solutions that are both diverse\nand near-feasible. For local search, ELS incorporates tailored move operators\nwith constant-time move evaluation mechanisms, enabling efficient exploration\nof large solution neighborhoods. Experimental results demonstrate that METS\ndiscovers 31 new best-known solutions out of 40 instances in existing benchmark\nsets, achieving substantial improvements over current state-of-the-art methods.\nAdditionally, a new large-scale benchmark set based on real-world logistics\ndata is introduced to facilitate future research.",
        "published": "2025-04-06T15:52:49+00:00"
    },
    {
        "title": "GAMBAS: Generalised-Hilbert Mamba for Super-resolution of Paediatric Ultra-Low-Field MRI",
        "authors": [
            "Levente Baljer",
            "Ula Briski",
            "Robert Leech",
            "Niall J. Bourke",
            "Kirsten A. Donald",
            "Layla E. Bradford",
            "Simone R. Williams",
            "Sadia Parkar",
            "Sidra Kaleem",
            "Salman Osmani",
            "Sean C. L. Deoni",
            "Steven C. R. Williams",
            "Rosalyn J. Moran",
            "Emma C. Robinson",
            "Frantisek Vasa"
        ],
        "summary": "Magnetic resonance imaging (MRI) is critical for neurodevelopmental research,\nhowever access to high-field (HF) systems in low- and middle-income countries\nis severely hindered by their cost. Ultra-low-field (ULF) systems mitigate such\nissues of access inequality, however their diminished signal-to-noise ratio\nlimits their applicability for research and clinical use. Deep-learning\napproaches can enhance the quality of scans acquired at lower field strengths\nat no additional cost. For example, Convolutional neural networks (CNNs) fused\nwith transformer modules have demonstrated a remarkable ability to capture both\nlocal information and long-range context. Unfortunately, the quadratic\ncomplexity of transformers leads to an undesirable trade-off between long-range\nsensitivity and local precision. We propose a hybrid CNN and state-space model\n(SSM) architecture featuring a novel 3D to 1D serialisation (GAMBAS), which\nlearns long-range context without sacrificing spatial precision. We exhibit\nimproved performance compared to other state-of-the-art medical image-to-image\ntranslation models.",
        "published": "2025-04-06T15:45:16+00:00"
    },
    {
        "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
        "authors": [
            "Yao Tao",
            "Yehui Tang",
            "Yun Wang",
            "Mingjian Zhu",
            "Hailin Hu",
            "Yunhe Wang"
        ],
        "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
        "published": "2025-04-06T15:15:07+00:00"
    },
    {
        "title": "Non-Markovian N-spin chain quantum battery in thermal charging process",
        "authors": [
            "Shun-Cai Zhao",
            "Zi-Ran Zhao",
            "Ni-Ya Zhuang"
        ],
        "summary": "Ergotropy serves as a key indicator for assessing the performance of quantum\nbatteries(QBs). Using the Redfield master equation, we investigate ergotropy\ndynamics in a non-Markovian QB composed of an N-spin chain embedded in a\nmicrocavity. Distinct from Markovian charging process, the thermal charging\nprocess exhibits a distinct oscillatory behavior in the extracted ergotropy. We\nshow these oscillations are suppressible via synergistic control of coherent\ndriving, cavity parameters, and spin-spin couplings. In addition, we analyze\nthe influence of various system and environmental parameters on the time\nevolution of ergotropy, revealing rich dynamical features. Our results offer\nnew insights into the control of energy extraction in QBs and may inform future\ndesigns of practical battery architectures.",
        "published": "2025-04-06T14:55:11+00:00"
    },
    {
        "title": "Binary Weight Allocation for Multi-Objective Path Optimization: Efficient Earliest and Latest Path Discovery in Network Systems",
        "authors": [
            "Wei-Chang Yeh"
        ],
        "summary": "This paper proposes earliest and latest path algorithms based on binary\nweight allocation, assigning weights of 2(i-1) and 2(m-i) to the i-th arc in a\nnetwork. While traditional shortest path algorithms optimize only distance, our\napproach leverages Binary-Addition-Tree ordering to efficiently identify\nlexicographically smallest and largest paths that establish connectivity. These\npaths partition the solution space into three regions: guaranteed\ndisconnection, transitional connectivity, and guaranteed no simple paths. Our\nweight allocation enables implicit encoding of multiple objectives directly in\nbinary representations, maintaining the O((|V|+|E|)log|V|) complexity of\nDijkstra's algorithm while allowing simultaneous optimization of competing\nfactors like reliability and cost. Experimental validation demonstrates\nsignificant computational time reduction compared to traditional\nmulti-objective methods. Applications span telecommunications, transportation\nnetworks, and supply chain management, providing efficient tools for network\nplanning and reliability analysis under multiple constraints.",
        "published": "2025-04-06T14:18:52+00:00"
    },
    {
        "title": "Virtual memory for real-time systems using hPMP",
        "authors": [
            "Konrad Walluszik",
            "Daniel Auge",
            "Gerhard Wirrer",
            "Holm Rauchfuss",
            "Thomas Roecker"
        ],
        "summary": "To satisfy automotive safety and security requirements, memory protection\nmechanisms are an essential component of automotive microcontrollers. In\ntoday's available systems, either a fully physical address-based protection is\nimplemented utilizing a memory protection unit, or a memory management unit\ntakes care of memory protection while also mapping virtual addresses to\nphysical addresses. The possibility to develop software using a large virtual\naddress space, which is agnostic to the underlying physical address space,\nallows for easier software development and integration, especially in the\ncontext of virtualization. In this work, we showcase an extension to the\ncurrent RISC-V SPMP proposal that enables address redirection for selected\naddress regions, while maintaining the fully deterministic behavior of a memory\nprotection unit.",
        "published": "2025-04-06T14:17:57+00:00"
    },
    {
        "title": "SELC: Self-Supervised Efficient Local Correspondence Learning for Low Quality Images",
        "authors": [
            "Yuqing Wang",
            "Yan Wang",
            "Hailiang Tang",
            "Xiaoji Niu"
        ],
        "summary": "Accurate and stable feature matching is critical for computer vision tasks,\nparticularly in applications such as Simultaneous Localization and Mapping\n(SLAM). While recent learning-based feature matching methods have demonstrated\npromising performance in challenging spatiotemporal scenarios, they still face\ninherent trade-offs between accuracy and computational efficiency in specific\nsettings. In this paper, we propose a lightweight feature matching network\ndesigned to establish sparse, stable, and consistent correspondence between\nmultiple frames. The proposed method eliminates the dependency on manual\nannotations during training and mitigates feature drift through a hybrid\nself-supervised paradigm. Extensive experiments validate three key advantages:\n(1) Our method operates without dependency on external prior knowledge and\nseamlessly incorporates its hybrid training mechanism into original datasets.\n(2) Benchmarked against state-of-the-art deep learning-based methods, our\napproach maintains equivalent computational efficiency at low-resolution scales\nwhile achieving a 2-10x improvement in computational efficiency for\nhigh-resolution inputs. (3) Comparative evaluations demonstrate that the\nproposed hybrid self-supervised scheme effectively mitigates feature drift in\nlong-term tracking while maintaining consistent representation across image\nsequences.",
        "published": "2025-04-06T14:14:43+00:00"
    },
    {
        "title": "Skin Color Measurement from Dermatoscopic Images: An Evaluation on a Synthetic Dataset",
        "authors": [
            "Marin Ben\u010devi\u0107",
            "Robert \u0160ojo",
            "Irena Gali\u0107"
        ],
        "summary": "This paper presents a comprehensive evaluation of skin color measurement\nmethods from dermatoscopic images using a synthetic dataset (S-SYNTH) with\ncontrolled ground-truth melanin content, lesion shapes, hair models, and 18\ndistinct lighting conditions. This allows for rigorous assessment of the\nrobustness and invariance to lighting conditions. We assess four classes of\nimage colorimetry approaches: segmentation-based, patch-based, color\nquantization, and neural networks. We use these methods to estimate the\nIndividual Typology Angle (ITA) and Fitzpatrick types from dermatoscopic\nimages. Our results show that segmentation-based and color quantization methods\nyield robust, lighting-invariant estimates, whereas patch-based approaches\nexhibit significant lighting-dependent biases that require calibration.\nFurthermore, neural network models, particularly when combined with heavy\nblurring to reduce overfitting, can provide light-invariant Fitzpatrick\npredictions, although their generalization to real-world images remains\nunverified. We conclude with practical recommendations for designing fair and\nreliable skin color estimation methods.",
        "published": "2025-04-06T13:57:34+00:00"
    },
    {
        "title": "Building LLM Agents by Incorporating Insights from Computer Systems",
        "authors": [
            "Yapeng Mi",
            "Zhi Gao",
            "Xiaojian Ma",
            "Qing Li"
        ],
        "summary": "LLM-driven autonomous agents have emerged as a promising direction in recent\nyears. However, many of these LLM agents are designed empirically or based on\nintuition, often lacking systematic design principles, which results in diverse\nagent structures with limited generality and scalability. In this paper, we\nadvocate for building LLM agents by incorporating insights from computer\nsystems. Inspired by the von Neumann architecture, we propose a structured\nframework for LLM agentic systems, emphasizing modular design and universal\nprinciples. Specifically, this paper first provides a comprehensive review of\nLLM agents from the computer system perspective, then identifies key challenges\nand future directions inspired by computer system design, and finally explores\nthe learning mechanisms for LLM agents beyond the computer system. The insights\ngained from this comparative analysis offer a foundation for systematic LLM\nagent design and advancement.",
        "published": "2025-04-06T13:38:37+00:00"
    },
    {
        "title": "Statistical Guarantees Of False Discovery Rate In Medical Instance Segmentation Tasks Based on Conformal Risk Control",
        "authors": [
            "Mengxia Dai",
            "Wenqian Luo",
            "Tianyang Li"
        ],
        "summary": "Instance segmentation plays a pivotal role in medical image analysis by\nenabling precise localization and delineation of lesions, tumors, and\nanatomical structures. Although deep learning models such as Mask R-CNN and\nBlendMask have achieved remarkable progress, their application in high-risk\nmedical scenarios remains constrained by confidence calibration issues, which\nmay lead to misdiagnosis. To address this challenge, we propose a robust\nquality control framework based on conformal prediction theory. This framework\ninnovatively constructs a risk-aware dynamic threshold mechanism that\nadaptively adjusts segmentation decision boundaries according to clinical\nrequirements.Specifically, we design a \\textbf{calibration-aware loss function}\nthat dynamically tunes the segmentation threshold based on a user-defined risk\nlevel $\\alpha$. Utilizing exchangeable calibration data, this method ensures\nthat the expected FNR or FDR on test data remains below $\\alpha$ with high\nprobability. The framework maintains compatibility with mainstream segmentation\nmodels (e.g., Mask R-CNN, BlendMask+ResNet-50-FPN) and datasets (PASCAL VOC\nformat) without requiring architectural modifications. Empirical results\ndemonstrate that we rigorously bound the FDR metric marginally over the test\nset via our developed calibration framework.",
        "published": "2025-04-06T13:31:19+00:00"
    },
    {
        "title": "Attention-Driven LPLC2 Neural Ensemble Model for Multi-Target Looming Detection and Localization",
        "authors": [
            "Renyuan Liu",
            "Qinbing Fu"
        ],
        "summary": "Lobula plate/lobula columnar, type 2 (LPLC2) visual projection neurons in the\nfly's visual system possess highly looming-selective properties, making them\nideal for developing artificial collision detection systems. The four dendritic\nbranches of individual LPLC2 neurons, each tuned to specific directional\nmotion, enhance the robustness of looming detection by utilizing radial motion\nopponency. Existing models of LPLC2 neurons either concentrate on individual\ncells to detect centroid-focused expansion or utilize population-voting\nstrategies to obtain global collision information. However, their potential for\naddressing multi-target collision scenarios remains largely untapped. In this\nstudy, we propose a numerical model for LPLC2 populations, leveraging a\nbottom-up attention mechanism driven by motion-sensitive neural pathways to\ngenerate attention fields (AFs). This integration of AFs with highly nonlinear\nLPLC2 responses enables precise and continuous detection of multiple looming\nobjects emanating from any region of the visual field. We began by conducting\ncomparative experiments to evaluate the proposed model against two related\nmodels, highlighting its unique characteristics. Next, we tested its ability to\ndetect multiple targets in dynamic natural scenarios. Finally, we validated the\nmodel using real-world video data collected by aerial robots. Experimental\nresults demonstrate that the proposed model excels in detecting,\ndistinguishing, and tracking multiple looming targets with remarkable speed and\naccuracy. This advanced ability to detect and localize looming objects,\nespecially in complex and dynamic environments, holds great promise for\novercoming collision-detection challenges in mobile intelligent machines.",
        "published": "2025-04-06T13:07:57+00:00"
    },
    {
        "title": "Fast Maximization of Current Flow Group Closeness Centrality",
        "authors": [
            "Haisong Xia",
            "Zhongzhi Zhang"
        ],
        "summary": "Derived from effective resistances, the current flow closeness centrality\n(CFCC) for a group of nodes measures the importance of node groups in an\nundirected graph with $n$ nodes. Given the widespread applications of\nidentifying crucial nodes, we investigate the problem of maximizing CFCC for a\nnode group $S$ subject to the cardinality constraint $|S|=k\\ll n$. Despite the\nproven NP-hardness of this problem, we propose two novel greedy algorithms for\nits solution. Our algorithms are based on spanning forest sampling and Schur\ncomplement, which exhibit nearly linear time complexities and achieve an\napproximation factor of $1-\\frac{k}{k-1}\\frac{1}{\\mathrm{e}}-\\epsilon$ for any\n$0<\\epsilon<1$. Extensive experiments on real-world graphs illustrate that our\nalgorithms outperform the state-of-the-art method in terms of efficiency and\neffectiveness, scaling to graphs with millions of nodes.",
        "published": "2025-04-06T13:03:58+00:00"
    },
    {
        "title": "Structuring Multiple Simple Cycle Reservoirs with Particle Swarm Optimization",
        "authors": [
            "Ziqiang Li",
            "Robert Simon Fong",
            "Kantaro Fujiwara",
            "Kazuyuki Aihara",
            "Gouhei Tanaka"
        ],
        "summary": "Reservoir Computing (RC) is a time-efficient computational paradigm derived\nfrom Recurrent Neural Networks (RNNs). The Simple Cycle Reservoir (SCR) is an\nRC model that stands out for its minimalistic design, offering extremely low\nconstruction complexity and proven capability of universally approximating\ntime-invariant causal fading memory filters, even in the linear dynamics\nregime. This paper introduces Multiple Simple Cycle Reservoirs (MSCRs), a\nmulti-reservoir framework that extends Echo State Networks (ESNs) by replacing\na single large reservoir with multiple interconnected SCRs. We demonstrate that\noptimizing MSCR using Particle Swarm Optimization (PSO) outperforms existing\nmulti-reservoir models, achieving competitive predictive performance with a\nlower-dimensional state space. By modeling interconnections as a weighted\nDirected Acyclic Graph (DAG), our approach enables flexible, task-specific\nnetwork topology adaptation. Numerical simulations on three benchmark\ntime-series prediction tasks confirm these advantages over rival algorithms.\nThese findings highlight the potential of MSCR-PSO as a promising framework for\noptimizing multi-reservoir systems, providing a foundation for further\nadvancements and applications of interconnected SCRs for developing efficient\nAI devices.",
        "published": "2025-04-06T12:25:40+00:00"
    },
    {
        "title": "Spatial-Geometry Enhanced 3D Dynamic Snake Convolutional Neural Network for Hyperspectral Image Classification",
        "authors": [
            "Guandong Li",
            "Mengxia Ye"
        ],
        "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including complex and sparse ground object distributions, small\nclustered structures, and elongated multi-branch features that often lead to\nmissing detections. To better adapt to ground object distributions and achieve\nadaptive dynamic feature responses while skipping redundant information, this\npaper proposes a Spatial-Geometry Enhanced 3D Dynamic Snake Network (SG-DSCNet)\nbased on an improved 3D-DenseNet model. The network employs Dynamic Snake\nConvolution (DSCConv), which introduces deformable offsets to enhance kernel\nflexibility through constrained self-learning, thereby improving regional\nperception of ground objects. Additionally, we propose a multi-view feature\nfusion strategy that generates multiple morphological kernel templates from\nDSCConv to observe target structures from different perspectives and achieve\nefficient feature fusion through summarizing key characteristics. This dynamic\napproach enables the model to focus more flexibly on critical spatial\nstructures when processing different regions, rather than relying on fixed\nreceptive fields of single static kernels. The DSC module enhances model\nrepresentation capability through dynamic kernel aggregation without increasing\nnetwork depth or width. Experimental results demonstrate superior performance\non the IN, UP, and KSC datasets, outperforming mainstream hyperspectral\nclassification methods.",
        "published": "2025-04-06T12:21:39+00:00"
    },
    {
        "title": "EclipseNETs: Learning Irregular Small Celestial Body Silhouettes",
        "authors": [
            "Giacomo Acciarini",
            "Dario Izzo",
            "Francesco Biscani"
        ],
        "summary": "Accurately predicting eclipse events around irregular small bodies is crucial\nfor spacecraft navigation, orbit determination, and spacecraft systems\nmanagement. This paper introduces a novel approach leveraging neural implicit\nrepresentations to model eclipse conditions efficiently and reliably. We\npropose neural network architectures that capture the complex silhouettes of\nasteroids and comets with high precision. Tested on four well-characterized\nbodies - Bennu, Itokawa, 67P/Churyumov-Gerasimenko, and Eros - our method\nachieves accuracy comparable to traditional ray-tracing techniques while\noffering orders of magnitude faster performance. Additionally, we develop an\nindirect learning framework that trains these models directly from sparse\ntrajectory data using Neural Ordinary Differential Equations, removing the\nrequirement to have prior knowledge of an accurate shape model. This approach\nallows for the continuous refinement of eclipse predictions, progressively\nreducing errors and improving accuracy as new trajectory data is incorporated.",
        "published": "2025-04-06T11:51:44+00:00"
    },
    {
        "title": "Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation",
        "authors": [
            "Mohammad Amaan Sayeed",
            "Engin Tekin",
            "Maryam Nadeem",
            "Nancy A. ElNaker",
            "Aahan Singh",
            "Natalia Vassilieva",
            "Boulbaba Ben Amor"
        ],
        "summary": "Unlocking the next generation of biotechnology and therapeutic innovation\ndemands overcoming the inherent complexity and resource-intensity of\nconventional protein engineering methods. Recent GenAI-powered computational\ntechniques often rely on the availability of the target protein's 3D structures\nand specific binding sites to generate high-affinity binders, constraints\nexhibited by models such as AlphaProteo and RFdiffusion. In this work, we\nexplore the use of Protein Language Models (pLMs) for high-affinity binder\ngeneration. We introduce Prot42, a novel family of Protein Language Models\n(pLMs) pretrained on vast amounts of unlabeled protein sequences. By capturing\ndeep evolutionary, structural, and functional insights through an advanced\nauto-regressive, decoder-only architecture inspired by breakthroughs in natural\nlanguage processing, Prot42 dramatically expands the capabilities of\ncomputational protein design based on language only. Remarkably, our models\nhandle sequences up to 8,192 amino acids, significantly surpassing standard\nlimitations and enabling precise modeling of large proteins and complex\nmulti-domain sequences. Demonstrating powerful practical applications, Prot42\nexcels in generating high-affinity protein binders and sequence-specific\nDNA-binding proteins. Our innovative models are publicly available, offering\nthe scientific community an efficient and precise computational toolkit for\nrapid protein engineering.",
        "published": "2025-04-06T11:43:12+00:00"
    },
    {
        "title": "COHESION: Composite Graph Convolutional Network with Dual-Stage Fusion for Multimodal Recommendation",
        "authors": [
            "Jinfeng Xu",
            "Zheyu Chen",
            "Wei Wang",
            "Xiping Hu",
            "Sang-Wook Kim",
            "Edith C. H. Ngai"
        ],
        "summary": "Recent works in multimodal recommendations, which leverage diverse modal\ninformation to address data sparsity and enhance recommendation accuracy, have\ngarnered considerable interest. Two key processes in multimodal recommendations\nare modality fusion and representation learning. Previous approaches in\nmodality fusion often employ simplistic attentive or pre-defined strategies at\nearly or late stages, failing to effectively handle irrelevant information\namong modalities. In representation learning, prior research has constructed\nheterogeneous and homogeneous graph structures encapsulating user-item,\nuser-user, and item-item relationships to better capture user interests and\nitem profiles. Modality fusion and representation learning were considered as\ntwo independent processes in previous work. In this paper, we reveal that these\ntwo processes are complementary and can support each other. Specifically,\npowerful representation learning enhances modality fusion, while effective\nfusion improves representation quality. Stemming from these two processes, we\nintroduce a COmposite grapH convolutional nEtwork with dual-stage fuSION for\nthe multimodal recommendation, named COHESION. Specifically, it introduces a\ndual-stage fusion strategy to reduce the impact of irrelevant information,\nrefining all modalities using ID embedding in the early stage and fusing their\nrepresentations at the late stage. It also proposes a composite graph\nconvolutional network that utilizes user-item, user-user, and item-item graphs\nto extract heterogeneous and homogeneous latent relationships within users and\nitems. Besides, it introduces a novel adaptive optimization to ensure balanced\nand reasonable representations across modalities. Extensive experiments on\nthree widely used datasets demonstrate the significant superiority of COHESION\nover various competitive baselines.",
        "published": "2025-04-06T11:42:49+00:00"
    },
    {
        "title": "WaveNet-Volterra Neural Networks for Active Noise Control: A Fully Causal Approach",
        "authors": [
            "Lu Bai",
            "Mengtong Li",
            "Siyuan Lian",
            "Kai Chen",
            "Jing Lu"
        ],
        "summary": "Active Noise Control (ANC) systems are challenged by nonlinear distortions,\nwhich degrade the performance of traditional adaptive filters. While deep\nlearning-based ANC algorithms have emerged to address nonlinearity, existing\napproaches often overlook critical limitations: (1) end-to-end Deep Neural\nNetwork (DNN) models frequently violate causality constraints inherent to\nreal-time ANC applications; (2) many studies compare DNN-based methods against\nsimplified or low-order adaptive filters rather than fully optimized high-order\ncounterparts. In this letter, we propose a causality-preserving time-domain ANC\nframework that synergizes WaveNet with Volterra Neural Networks (VNNs),\nexplicitly addressing system nonlinearity while ensuring strict causal\noperation. Unlike prior DNN-based approaches, our method is benchmarked against\nboth state-of-the-art deep learning architectures and rigorously optimized\nhigh-order adaptive filters, including Wiener solutions. Simulations\ndemonstrate that the proposed framework achieves superior performance over\nexisting DNN methods and traditional algorithms, revealing that prior claims of\nDNN superiority stem from incomplete comparisons with suboptimal traditional\nbaselines. Source code is available at\nhttps://github.com/Lu-Baihh/WaveNet-VNNs-for-ANC.git.",
        "published": "2025-04-06T11:42:01+00:00"
    },
    {
        "title": "Thermoxels: a voxel-based method to generate simulation-ready 3D thermal models",
        "authors": [
            "Etienne Chassaing",
            "Florent Forest",
            "Olga Fink",
            "Malcolm Mielle"
        ],
        "summary": "In the European Union, buildings account for 42% of energy use and 35% of\ngreenhouse gas emissions. Since most existing buildings will still be in use by\n2050, retrofitting is crucial for emissions reduction. However, current\nbuilding assessment methods rely mainly on qualitative thermal imaging, which\nlimits data-driven decisions for energy savings. On the other hand,\nquantitative assessments using finite element analysis (FEA) offer precise\ninsights but require manual CAD design, which is tedious and error-prone.\nRecent advances in 3D reconstruction, such as Neural Radiance Fields (NeRF) and\nGaussian Splatting, enable precise 3D modeling from sparse images but lack\nclearly defined volumes and the interfaces between them needed for FEA. We\npropose Thermoxels, a novel voxel-based method able to generate FEA-compatible\nmodels, including both geometry and temperature, from a sparse set of RGB and\nthermal images. Using pairs of RGB and thermal images as input, Thermoxels\nrepresents a scene's geometry as a set of voxels comprising color and\ntemperature information. After optimization, a simple process is used to\ntransform Thermoxels' models into tetrahedral meshes compatible with FEA. We\ndemonstrate Thermoxels' capability to generate RGB+Thermal meshes of 3D scenes,\nsurpassing other state-of-the-art methods. To showcase the practical\napplications of Thermoxels' models, we conduct a simple heat conduction\nsimulation using FEA, achieving convergence from an initial state defined by\nThermoxels' thermal reconstruction. Additionally, we compare Thermoxels' image\nsynthesis abilities with current state-of-the-art methods, showing competitive\nresults, and discuss the limitations of existing metrics in assessing mesh\nquality.",
        "published": "2025-04-06T11:37:16+00:00"
    },
    {
        "title": "Robust and scalable nonlinear solvers for finite element discretizations of biological transportation networks",
        "authors": [
            "Jan Haskovec",
            "Peter Markowich",
            "Simone Portaro",
            "Stefano Zampini"
        ],
        "summary": "We develop robust and scalable fully implicit nonlinear finite element\nsolvers for the simulations of biological transportation networks driven by the\ngradient flow minimization of a non-convex energy cost functional. Our approach\nemploys a discontinuous space for the conductivity tensor that allows us to\nguarantee the preservation of its positive semi-definiteness throughout the\nentire minimization procedure arising from the time integration of the gradient\nflow dynamics using a backward Euler scheme. Extensive tests in two and three\ndimensions demonstrate the robustness and performance of the solver, highlight\nthe sensitivity of the emergent network structures to mesh resolution and\ntopology, and validate the resilience of the linear preconditioner to the\nill-conditioning of the model. The implementation achieves near-optimal\nparallel scaling on large-scale, high-performance computing platforms. To the\nbest of our knowledge, the network formation system has never been simulated in\nthree dimensions before. Consequently, our three-dimensional results are the\nfirst of their kind.",
        "published": "2025-04-06T11:37:13+00:00"
    },
    {
        "title": "On the Spatial Structure of Mixture-of-Experts in Transformers",
        "authors": [
            "Daniel Bershatsky",
            "Ivan Oseledets"
        ],
        "summary": "A common assumption is that MoE routers primarily leverage semantic features\nfor expert selection. However, our study challenges this notion by\ndemonstrating that positional token information also plays a crucial role in\nrouting decisions. Through extensive empirical analysis, we provide evidence\nsupporting this hypothesis, develop a phenomenological explanation of the\nobserved behavior, and discuss practical implications for MoE-based\narchitectures.",
        "published": "2025-04-06T11:31:55+00:00"
    },
    {
        "title": "Squeeze and Excitation: A Weighted Graph Contrastive Learning for Collaborative Filtering",
        "authors": [
            "Zheyu Chen",
            "Jinfeng Xu",
            "Yutong Wei",
            "Ziyue Peng"
        ],
        "summary": "Contrastive Learning (CL) has recently emerged as a powerful technique in\nrecommendation systems, particularly for its capability to harness\nself-supervised signals from perturbed views to mitigate the persistent\nchallenge of data sparsity. The process of constructing perturbed views of the\nuser-item bipartite graph and performing contrastive learning between perturbed\nviews in a graph convolutional network (GCN) is called graph contrastive\nlearning (GCL), which aims to enhance the robustness of representation\nlearning. Although existing GCL-based models are effective, the weight\nassignment method for perturbed views has not been fully explored. A critical\nproblem in existing GCL-based models is the irrational allocation of feature\nattention. This problem limits the model's ability to effectively leverage\ncrucial features, resulting in suboptimal performance. To address this, we\npropose a Weighted Graph Contrastive Learning framework (WeightedGCL).\nSpecifically, WeightedGCL applies a robust perturbation strategy, which\nperturbs only the view of the final GCN layer. In addition, WeightedGCL\nincorporates a squeeze and excitation network (SENet) to dynamically weight the\nfeatures of the perturbed views. Our WeightedGCL strengthens the model's focus\non crucial features and reduces the impact of less relevant information.\nExtensive experiments on widely used datasets demonstrate that our WeightedGCL\nachieves significant accuracy improvements compared to competitive baselines.",
        "published": "2025-04-06T11:30:59+00:00"
    },
    {
        "title": "DRAMA: A Dynamic Packet Routing Algorithm using Multi-Agent Reinforcement Learning with Emergent Communication",
        "authors": [
            "Wang Zhang",
            "Chenguang Liu",
            "Yue Pi",
            "Yong Zhang",
            "Hairong Huang",
            "Baoquan Rao",
            "Yulong Ding",
            "Shuanghua Yang",
            "Jie Jiang"
        ],
        "summary": "The continuous expansion of network data presents a pressing challenge for\nconventional routing algorithms. As the demand escalates, these algorithms are\nstruggling to cope. In this context, reinforcement learning (RL) and\nmulti-agent reinforcement learning (MARL) algorithms emerge as promising\nsolutions. However, the urgency and importance of the problem are clear, as\nexisting RL/MARL-based routing approaches lack effective communication in run\ntime among routers, making it challenging for individual routers to adapt to\ncomplex and dynamic changing networks. More importantly, they lack the ability\nto deal with dynamically changing network topology, especially the addition of\nthe router, due to the non-scalability of their neural networks. This paper\nproposes a novel dynamic routing algorithm, DRAMA, incorporating emergent\ncommunication in multi-agent reinforcement learning. Through emergent\ncommunication, routers could learn how to communicate effectively to maximize\nthe optimization objectives. Meanwhile, a new Q-network and graph-based\nemergent communication are introduced to dynamically adapt to the changing\nnetwork topology without retraining while ensuring robust performance.\nExperimental results showcase DRAMA's superior performance over the traditional\nrouting algorithm and other RL/MARL-based algorithms, achieving a higher\ndelivery rate and lower latency in diverse network scenarios, including dynamic\nnetwork load and topology. Moreover, an ablation experiment validates the\nprospect of emergent communication in facilitating packet routing.",
        "published": "2025-04-06T10:33:08+00:00"
    },
    {
        "title": "Evaluation framework for Image Segmentation Algorithms",
        "authors": [
            "Tatiana Merkulova",
            "Bharani Jayakumar"
        ],
        "summary": "This paper presents a comprehensive evaluation framework for image\nsegmentation algorithms, encompassing naive methods, machine learning\napproaches, and deep learning techniques. We begin by introducing the\nfundamental concepts and importance of image segmentation, and the role of\ninteractive segmentation in enhancing accuracy. A detailed background theory\nsection explores various segmentation methods, including thresholding, edge\ndetection, region growing, feature extraction, random forests, support vector\nmachines, convolutional neural networks, U-Net, and Mask R-CNN. The\nimplementation and experimental setup are thoroughly described, highlighting\nthree primary approaches: algorithm assisting user, user assisting algorithm,\nand hybrid methods. Evaluation metrics such as Intersection over Union (IoU),\ncomputation time, and user interaction time are employed to measure\nperformance. A comparative analysis presents detailed results, emphasizing the\nstrengths, limitations, and trade-offs of each method. The paper concludes with\ninsights into the practical applicability of these approaches across various\nscenarios and outlines future work, focusing on expanding datasets, developing\nmore representative approaches, integrating real-time feedback, and exploring\nweakly supervised and self-supervised learning paradigms to enhance\nsegmentation accuracy and efficiency. Keywords: Image Segmentation, Interactive\nSegmentation, Machine Learning, Deep Learning, Computer Vision",
        "published": "2025-04-06T10:20:26+00:00"
    },
    {
        "title": "IntentContinuum: Using LLMs to Support Intent-Based Computing Across the Compute Continuum",
        "authors": [
            "Negin Akbari",
            "John Grundy",
            "Aamir Cheema",
            "Adel N. Toosi"
        ],
        "summary": "The increasing proliferation of IoT devices and AI applications has created a\ndemand for scalable and efficient computing solutions, particularly for\napplications requiring real-time processing. The compute continuum integrates\nedge and cloud resources to meet this need, balancing the low-latency demands\nof the edge with the high computational power of the cloud. However, managing\nresources in such a distributed environment presents challenges due to the\ndiversity and complexity of these systems. Traditional resource management\nmethods, often relying on heuristic algorithms, struggle to manage the\nincreasing complexity, scale, and dynamics of these systems, as well as adapt\nto dynamic workloads and changing network conditions. Moreover, designing such\napproaches is often time-intensive and highly tailored to specific\napplications, demanding deep expertise. In this paper, we introduce a novel\nframework for intent-driven resource management in the compute continuum, using\nlarge language models (LLMs) to help automate decision-making processes. Our\nframework ensures that user-defined intents -- such as achieving the required\nresponse times for time-critical applications -- are consistently fulfilled. In\nthe event of an intent violation, our system performs root cause analysis by\nexamining system data to identify and address issues. This approach reduces the\nneed for human intervention and enhances system reliability, offering a more\ndynamic and efficient solution for resource management in distributed\nenvironments.",
        "published": "2025-04-06T09:56:48+00:00"
    },
    {
        "title": "FluentLip: A Phonemes-Based Two-stage Approach for Audio-Driven Lip Synthesis with Optical Flow Consistency",
        "authors": [
            "Shiyan Liu",
            "Rui Qu",
            "Yan Jin"
        ],
        "summary": "Generating consecutive images of lip movements that align with a given speech\nin audio-driven lip synthesis is a challenging task. While previous studies\nhave made strides in synchronization and visual quality, lip intelligibility\nand video fluency remain persistent challenges. This work proposes FluentLip, a\ntwo-stage approach for audio-driven lip synthesis, incorporating three featured\nstrategies. To improve lip synchronization and intelligibility, we integrate a\nphoneme extractor and encoder to generate a fusion of audio and phoneme\ninformation for multimodal learning. Additionally, we employ optical flow\nconsistency loss to ensure natural transitions between image frames.\nFurthermore, we incorporate a diffusion chain during the training of Generative\nAdversarial Networks (GANs) to improve both stability and efficiency. We\nevaluate our proposed FluentLip through extensive experiments, comparing it\nwith five state-of-the-art (SOTA) approaches across five metrics, including a\nproposed metric called Phoneme Error Rate (PER) that evaluates lip pose\nintelligibility and video fluency. The experimental results demonstrate that\nour FluentLip approach is highly competitive, achieving significant\nimprovements in smoothness and naturalness. In particular, it outperforms these\nSOTA approaches by approximately $\\textbf{16.3%}$ in Fr\\'echet Inception\nDistance (FID) and $\\textbf{35.2%}$ in PER.",
        "published": "2025-04-06T09:44:30+00:00"
    },
    {
        "title": "Divergent Paths: Separating Homophilic and Heterophilic Learning for Enhanced Graph-level Representations",
        "authors": [
            "Han Lei",
            "Jiaxing Xu",
            "Xia Dong",
            "Yiping Ke"
        ],
        "summary": "Graph Convolutional Networks (GCNs) are predominantly tailored for graphs\ndisplaying homophily, where similar nodes connect, but often fail on\nheterophilic graphs. The strategy of adopting distinct approaches to learn from\nhomophilic and heterophilic components in node-level tasks has been widely\ndiscussed and proven effective both theoretically and experimentally. However,\nin graph-level tasks, research on this topic remains notably scarce. Addressing\nthis gap, our research conducts an analysis on graphs with nodes' category ID\navailable, distinguishing intra-category and inter-category components as\nembodiment of homophily and heterophily, respectively. We find while GCNs excel\nat extracting information within categories, they frequently capture noise from\ninter-category components. Consequently, it is crucial to employ distinct\nlearning strategies for intra- and inter-category elements. To alleviate this\nproblem, we separately learn the intra- and inter-category parts by a\ncombination of an intra-category convolution (IntraNet) and an inter-category\nhigh-pass graph convolution (InterNet). Our IntraNet is supported by\nsophisticated graph preprocessing steps and a novel category-based graph\nreadout function. For the InterNet, we utilize a high-pass filter to amplify\nthe node disparities, enhancing the recognition of details in the\nhigh-frequency components. The proposed approach, DivGNN, combines the IntraNet\nand InterNet with a gated mechanism and substantially improves classification\nperformance on graph-level tasks, surpassing traditional GNN baselines in\neffectiveness.",
        "published": "2025-04-06T09:31:10+00:00"
    },
    {
        "title": "AROMA: Autonomous Rank-one Matrix Adaptation",
        "authors": [
            "Hao Nan Sheng",
            "Zhi-yong Wang",
            "Mingrui Yang",
            "Hing Cheung So"
        ],
        "summary": "As large language models continue to grow in size, parameter-efficient\nfine-tuning has become increasingly crucial. While low-rank adaptation (LoRA)\noffers a solution through low-rank updates, its static rank allocation may\nyield suboptimal results. Adaptive low-rank adaptation (AdaLoRA) improves this\nwith dynamic allocation but remains sensitive to initial and target rank\nconfigurations. We introduce AROMA, a framework that automatically constructs\nlayer-specific updates by iteratively building up rank-one components with very\nfew trainable parameters that gradually diminish to zero. Unlike existing\nmethods that employ rank reduction mechanisms, AROMA introduces a dual-loop\narchitecture for rank growth. The inner loop extracts information from each\nrank-one subspace, while the outer loop determines the number of rank-one\nsubspaces, i.e., the optimal rank. We reset optimizer states to maintain\nsubspace independence. AROMA significantly reduces parameters compared to LoRA\nand AdaLoRA while achieving superior performance on natural language\nunderstanding and commonsense reasoning tasks, offering new insights into\nadaptive parameter-efficient fine-tuning. The code is available at\n\\href{https://github.com/ShuDun23/AROMA}{AROMA}.",
        "published": "2025-04-06T09:14:43+00:00"
    },
    {
        "title": "Redefining Information Freshness: AoGI for Generative AI in 6G Networks",
        "authors": [
            "Yuquan Xiao",
            "Qinghe Du",
            "Wenchi Cheng",
            "George K. Karagiannidis",
            "Arumugam Nallanathan",
            "Mohsen Guizani"
        ],
        "summary": "Generative Artificial Intelligence (GenAI) is playing an increasingly\nimportant role in enriching and facilitating human life by generating various\nuseful information, of which real-time GenAI is a significant part and has\ngreat potential in applications such as real-time robot control, automated\ndriving, augmented reality, etc. There are a variety of information updating\nprocesses in real-time GenAI, and the age of information (AoI) is an effective\nmetric for evaluating information freshness. However, due to the diversity and\ngenerativity of information in real-time GenAI, it may be incompatible to\ndirectly use existing information aging metrics to assess its timeliness. In\nthis article, we introduce a new concept called Age of Generative Information\n(AoGI) to evaluate the freshness of generative information, which takes into\naccount the information delay caused not only by sampling and transmission, but\nalso by computation. Furthermore, since real-time GenAI services are often\nsupported by mobile-edge-cloud (MEC) collaborative computing in 6G networks and\nsome of the generated information is privacy sensitive, it is recommended that\nthe identities of edge and cloud should always be verified in a zero-trust\nmanner. We introduce the concept of Age of Trust (AoT) to characterise the\ndecay process of their trust level. We also discuss the optimisations of these\nevolved information aging metrics, focusing on the impact of dynamic external\nconditions, including wireless environments and limited computational\nresources. Finally, we highlight several open challenges in providing\ntimeliness guarantees for real-time GenAI services.",
        "published": "2025-04-06T08:49:05+00:00"
    },
    {
        "title": "ANN-Driven Adaptive Power Allocation for OWC",
        "authors": [
            "Walter Zibusiso Ncube",
            "Ahmad Adnan Qidan",
            "Taisir El-Gorashi",
            "Jaafar M. H. Elmirghani"
        ],
        "summary": "Vertical-cavity surface-emitting lasers (VCSELs) have gained popularity in\nOptical Wireless Communication (OWC) due to their high bandwidth and\ndirectional beam compared to Light-emitting diodes (LEDs). In this work, we\nexplore the deployment of VCSELs as Access Points (APs) in an indoor\nenvironment with time-varying user distributions and traffic demands. To\nenhance performance, a merged access point (MAP) topology is introduced to\nextend the serving area of each cell using Zero Forcing (ZF) for interference\nmanagement. A power allocation optimisation problem is formulated aimed at\nmaximising the sum rate of the dynamic network. Deterministic approaches can\nsolve the formulated problem but are impractical for real-time scenarios. In\nthis work, we propose an Artificial Neural Network (ANN) for adaptive power\nallocation with reduced computational complexity. Our results show that the\ndesigned ANN learns and effectively provides instantaneous solutions in dynamic\nscenarios, resulting in improved performance in terms of sum rate compared to a\nbaseline uniform power allocation scheme.",
        "published": "2025-04-06T08:34:18+00:00"
    },
    {
        "title": "Robust charging station location and routing-scheduling for electric modular autonomous units",
        "authors": [
            "Dongyang Xia",
            "Lixing Yang",
            "Yahan Lu",
            "Shadi Sharif Azadeh"
        ],
        "summary": "Problem definition: Motivated by global electrification targets and the\nadvent of electric modular autonomous units (E-MAUs), this paper addresses a\nrobust charging station location and routing-scheduling problem (E-RCRSP) in an\ninter-modal transit system, presenting a novel solution to traditional electric\nbus scheduling. The system integrates regular bus services, offering full-line\nor sectional coverage, and short-turning services. Considering the\nfast-charging technology with quick top-ups, we jointly optimize charging\nstation locations and capacities, fleet sizing, as well as routing-scheduling\nfor E-MAUs under demand uncertainty. E-MAUs can couple flexibly at different\nlocations, and their routing-scheduling decisions include sequences of\nservices, as well as charging times and locations. Methodology: The E-RCRSP is\nformulated as a path-based robust optimization model, incorporating the\npolyhedral uncertainty set. We develop a double-decomposition algorithm that\ncombines column-and-constraint generation and column generation armed with a\ntailored label-correcting approach. To improve computational efficiency and\nscalability, we propose a novel method that introduces super travel arcs and\nnetwork downsizing methodologies. Results: Computational results from real-life\ninstances, based on operational data of advanced NExT E-MAUs with cutting-edge\nbatteries provided by our industry partner, indicate that charging at both\ndepots and en-route fast-charging stations is necessary during operations.\nMoreover, our algorithm effectively scales to large-scale operational cases\ninvolving entire-day operations, significantly outperforming state-of-the-art\nmethods. Comparisons with fixed-composition buses under the same fleet\ninvestment suggest that our methods are able to achieve substantial reductions\nin passengers' costs by flexibly scheduling units.",
        "published": "2025-04-06T08:20:56+00:00"
    },
    {
        "title": "Three-Factor Learning in Spiking Neural Networks: An Overview of Methods and Trends from a Machine Learning Perspective",
        "authors": [
            "Szymon Mazurek",
            "Jakub Caputa",
            "Jan K. Argasi\u0144ski",
            "Maciej Wielgosz"
        ],
        "summary": "Three-factor learning rules in Spiking Neural Networks (SNNs) have emerged as\na crucial extension to traditional Hebbian learning and Spike-Timing-Dependent\nPlasticity (STDP), incorporating neuromodulatory signals to improve adaptation\nand learning efficiency. These mechanisms enhance biological plausibility and\nfacilitate improved credit assignment in artificial neural systems. This paper\ntakes a view on this topic from a machine learning perspective, providing an\noverview of recent advances in three-factor learning, discusses theoretical\nfoundations, algorithmic implementations, and their relevance to reinforcement\nlearning and neuromorphic computing. In addition, we explore interdisciplinary\napproaches, scalability challenges, and potential applications in robotics,\ncognitive modeling, and AI systems. Finally, we highlight key research gaps and\npropose future directions for bridging the gap between neuroscience and\nartificial intelligence.",
        "published": "2025-04-06T08:10:16+00:00"
    },
    {
        "title": "OffRAC: Offloading Through Remote Accelerator Calls",
        "authors": [
            "Ziyi Yang",
            "Krishnan B. Iyer",
            "Yixi Chen",
            "Ran Shu",
            "Zsolt Istv\u00e1n",
            "Marco Canini",
            "Suhaib A. Fahmy"
        ],
        "summary": "Modern applications increasingly demand ultra-low latency for data\nprocessing, often facilitated by host-controlled accelerators like GPUs and\nFPGAs. However, significant delays result from host involvement in accessing\naccelerators. To address this limitation, we introduce a novel paradigm we call\nOffloading through Remote Accelerator Calls (OffRAC), which elevates\naccelerators to first-class compute resources. OffRAC enables direct calls to\nFPGA-based accelerators without host involvement. Utilizing the stateless\nfunction abstraction of serverless computing, with applications decomposed into\nsimpler stateless functions, offloading promotes efficient acceleration and\ndistribution of computational loads across the network. To realize this\nproposal, we present a prototype design and implementation of an OffRAC\nplatform for FPGAs that assembles diverse requests from multiple clients into\ncomplete accelerator calls with multi-tenancy performance isolation. This\ndesign minimizes the implementation complexity for accelerator users while\nensuring isolation and programmability. Results show that the OffRAC approach\nreduces the latency of network calls to accelerators down to approximately 10.5\nus, as well as sustaining high application throughput up to 85Gbps,\ndemonstrating scalability and efficiency, making it compelling for the next\ngeneration of low-latency applications.",
        "published": "2025-04-06T08:06:08+00:00"
    },
    {
        "title": "Sustaining the dynamics of Kuramoto model by adaptable reservoir computer",
        "authors": [
            "Haibo Luo",
            "Mengru Wang",
            "Yao Du",
            "Huawei Fan",
            "Yizhen Yu",
            "Xingang Wang"
        ],
        "summary": "A scenario frequently encountered in real-world complex systems is the\ntemporary failure of a few components. For systems whose functionality hinges\non the collective dynamics of the interacting components, a viable approach to\ndealing with the failures is replacing the malfunctioning components with their\nbackups, so that the collective dynamics of the systems can be precisely\nmaintained for a duration long enough to resolve the problem. Here, taking the\nparadigmatic Kuramoto model as an example and considering the scenario of\noscillator failures, we propose substituting the failed oscillators with\ndigital twins trained by the data measured from the system evolution.\nSpecifically, leveraging the technique of adaptable reservoir computer (RC) in\nmachine learning, we demonstrate that a single, small-size RC is able to\nsubstitute any oscillator in the Kuramoto model such that the time evolution of\nthe synchronization order parameter of the repaired system is identical to that\nof the original system for a certain time period. The performance of adaptable\nRC is evaluated in various contexts, and it is found that the sustaining period\nis influenced by multiple factors, including the size of the training dataset,\nthe overall coupling strength of the system, and the number of substituted\noscillators. Additionally, though the synchronization order parameter diverges\nfrom the ground truth in the long-term running, the functional networks of the\noscillators are still faithfully sustained by the machine substitutions.",
        "published": "2025-04-06T07:13:46+00:00"
    },
    {
        "title": "Spreading dynamics in the Hatano-Nelson model with disorder",
        "authors": [
            "Jinyuan Shang",
            "Haiping Hu"
        ],
        "summary": "The non-Hermitian skin effect is the accumulation of eigenstates at the\nboundaries, reflecting the system's nonreciprocity. Introducing disorder leads\nto a competition between the skin effect and Anderson localization, giving rise\nto the skin-Anderson transition. Here, we investigate wave packet spreading in\nthe disordered Hatano-Nelson model and uncover distinct dynamical behaviors\nacross different regimes. In the clean limit, transport is unidirectionally\nballistic ({\\Delta}x ~ t) due to nonreciprocity. For weak disorder, where skin\nand Anderson-localized modes coexist, transport transitions from ballistic at\nearly times to superdiffusive ({\\Delta}x ~ t^{2/3}) at long times. In the\ndeeply Anderson-localized regime, initial diffusion ({\\Delta}x ~ t^{1/2})\neventually gives way to superdiffusive spreading. We examine how these scaling\nbehaviors emerge from the system's spectral properties and eigenstate\nlocalization behaviors. Our work unveils the rich dynamics driven by\nnonreciprocity and disorder in non-Hermitian systems.",
        "published": "2025-04-06T05:40:14+00:00"
    },
    {
        "title": "Opportunistic Beamforming and Dynamic Scheduling for Multi-User MIMO-ISAC Systems",
        "authors": [
            "Tharaka Perera",
            "Saman Atapattu",
            "Chathuranga Weeraddana",
            "Jamie Evans"
        ],
        "summary": "This research presents a novel framework integrating Flexible-Duplex (FlexD)\nand Integrated Sensing and Communications (ISAC) technologies to address the\nchallenges of spectrum efficiency and resource optimization in next-generation\nwireless networks. We develop a unified system model for a dual-functional\nradar-communication base station with multiple-input multiple-output\ncapabilities, enabling dynamic uplink and downlink channel allocation. The\nframework maximizes network throughput while maintaining radar sensing\nperformance, subject to signal-to-clutter-plus-noise ratio (SCNR) requirements\nand power constraints. Given the non-convex and combinatorial nature of the\nresulting optimization problem, we propose an iterative algorithm that\nconverges to a locally optimal solution. Extensive simulations demonstrate the\nsuperiority of the proposed FlexD-ISAC framework compared to conventional\nhalf-duplex networks. Additionally, sensitivity analyses reveal the impact of\nSCNR requirements and power constraints on system performance, providing\nvaluable insights for practical implementation. This work establishes a\nfoundation for future research in dynamic, resource-efficient wireless systems\nthat simultaneously support sensing and communication capabilities.",
        "published": "2025-04-06T05:37:20+00:00"
    },
    {
        "title": "WeiDetect: Weibull Distribution-Based Defense against Poisoning Attacks in Federated Learning for Network Intrusion Detection Systems",
        "authors": [
            "Sameera K. M.",
            "Vinod P.",
            "Anderson Rocha",
            "Rafidha Rehiman K. A.",
            "Mauro Conti"
        ],
        "summary": "In the era of data expansion, ensuring data privacy has become increasingly\ncritical, posing significant challenges to traditional AI-based applications.\nIn addition, the increasing adoption of IoT devices has introduced significant\ncybersecurity challenges, making traditional Network Intrusion Detection\nSystems (NIDS) less effective against evolving threats, and privacy concerns\nand regulatory restrictions limit their deployment. Federated Learning (FL) has\nemerged as a promising solution, allowing decentralized model training while\nmaintaining data privacy to solve these issues. However, despite implementing\nprivacy-preserving technologies, FL systems remain vulnerable to adversarial\nattacks. Furthermore, data distribution among clients is not heterogeneous in\nthe FL scenario. We propose WeiDetect, a two-phase, server-side defense\nmechanism for FL-based NIDS that detects malicious participants to address\nthese challenges. In the first phase, local models are evaluated using a\nvalidation dataset to generate validation scores. These scores are then\nanalyzed using a Weibull distribution, identifying and removing malicious\nmodels. We conducted experiments to evaluate the effectiveness of our approach\nin diverse attack settings. Our evaluation included two popular datasets,\nCIC-Darknet2020 and CSE-CIC-IDS2018, tested under non-IID data distributions.\nOur findings highlight that WeiDetect outperforms state-of-the-art defense\napproaches, improving higher target class recall up to 70% and enhancing the\nglobal model's F1 score by 1% to 14%.",
        "published": "2025-04-06T05:31:24+00:00"
    },
    {
        "title": "Data-Driven Reachability Analysis for Piecewise Affine System",
        "authors": [
            "Peng Xie",
            "Johannes Betz",
            "Davide M. Raimondo",
            "Amr Alanwar"
        ],
        "summary": "Hybrid systems play a crucial role in modeling real-world applications where\ndiscrete and continuous dynamics interact, including autonomous vehicles, power\nsystems, and traffic networks. Safety verification for these systems requires\ndetermining whether system states can enter unsafe regions under given initial\nconditions and uncertainties, a question directly addressed by reachability\nanalysis. However, hybrid systems present unique difficulties because their\nstate space is divided into multiple regions with distinct dynamic models,\ncausing traditional data-driven methods to produce inadequate\nover-approximations of reachable sets at region boundaries where dynamics\nchange abruptly. This paper introduces a novel approach using hybrid zonotopes\nfor data-driven reachability analysis of piecewise affine systems. Our method\naddresses the boundary transition problem by developing computational\nalgorithms that calculate the family of set models guaranteed to contain the\ntrue system trajectories. Additionally, we extend and evaluate three methods\nfor set-based estimation that account for input-output data with measurement\nnoise.",
        "published": "2025-04-06T05:23:56+00:00"
    },
    {
        "title": "DSSR-Net for Super-Resolution Radar Range Profiles",
        "authors": [
            "Ziwen Wang",
            "Jianping Wang",
            "Pucheng Li",
            "Zegang Ding"
        ],
        "summary": "High-resolution radar range profile (RRP) is crucial for accurate target\nrecognition and scene perception. To get a high-resolution RRP, many methods\nhave been developed, such as multiple signal classification (MUSIC), orthogonal\nmatching pursuit (OMP), and a few deep learning-based approaches. Although they\nbreak through the Rayleigh resolution limit determined by radar signal\nbandwidth, these methods either get limited super-resolution capability or work\nwell just in high signal to noise ratio (SNR) scenarios. To overcome these\nlimitations, in this paper, an interpretable neural network for\nsuper-resolution RRP (DSSR-Net) is proposed by integrating the advantages of\nboth model-guided and data-driven models. Specifically, DSSR-Net is designed\nbased on a sparse representation model with dimension scaling, and then trained\non a training dataset. Through dimension scaling, DSSR-Net lifts the radar\nsignal into high-dimensional space to extract subtle features of closely spaced\nobjects and suppress the noise of the high-dimensional features. It improves\nthe super-resolving power of closely spaced objects and lowers the SNR\nrequirement of radar signals compared to existing methods. The superiority of\nthe proposed algorithm for super-resolution RRP reconstruction is verified via\nexperiments with both synthetic and measured data.",
        "published": "2025-04-06T05:10:06+00:00"
    },
    {
        "title": "Extending Cox Proportional Hazards Model with Symbolic Non-Linear Log-Risk Functions for Survival Analysis",
        "authors": [
            "Jiaxiang Cheng",
            "Guoqiang Hu"
        ],
        "summary": "The Cox proportional hazards (CPH) model has been widely applied in survival\nanalysis to estimate relative risks across different subjects given multiple\ncovariates. Traditional CPH models rely on a linear combination of covariates\nweighted with coefficients as the log-risk function, which imposes a strong and\nrestrictive assumption, limiting generalization. Recent deep learning methods\nenable non-linear log-risk functions. However, they often lack interpretability\ndue to the end-to-end training mechanisms. The implementation of\nKolmogorov-Arnold Networks (KAN) offers new possibilities for extending the CPH\nmodel with fully transparent and symbolic non-linear log-risk functions. In\nthis paper, we introduce Generalized Cox Proportional Hazards (GCPH) model, a\nnovel method for survival analysis that leverages KAN to enable a non-linear\nmapping from covariates to survival outcomes in a fully symbolic manner. GCPH\nmaintains the interpretability of traditional CPH models while allowing for the\nestimation of non-linear log-risk functions. Experiments conducted on both\nsynthetic data and various public benchmarks demonstrate that GCPH achieves\ncompetitive performance in terms of prediction accuracy and exhibits superior\ninterpretability compared to current state-of-the-art methods.",
        "published": "2025-04-06T04:35:11+00:00"
    },
    {
        "title": "Crowdsourcing-Based Knowledge Graph Construction for Drug Side Effects Using Large Language Models with an Application on Semaglutide",
        "authors": [
            "Zhijie Duan",
            "Kai Wei",
            "Zhaoqian Xue",
            "Jiayan Zhou",
            "Shu Yang",
            "Siyuan Ma",
            "Jin Jin",
            "Lingyao li"
        ],
        "summary": "Social media is a rich source of real-world data that captures valuable\npatient experience information for pharmacovigilance. However, mining data from\nunstructured and noisy social media content remains a challenging task. We\npresent a systematic framework that leverages large language models (LLMs) to\nextract medication side effects from social media and organize them into a\nknowledge graph (KG). We apply this framework to semaglutide for weight loss\nusing data from Reddit. Using the constructed knowledge graph, we perform\ncomprehensive analyses to investigate reported side effects across different\nsemaglutide brands over time. These findings are further validated through\ncomparison with adverse events reported in the FAERS database, providing\nimportant patient-centered insights into semaglutide's side effects that\ncomplement its safety profile and current knowledge base of semaglutide for\nboth healthcare professionals and patients. Our work demonstrates the\nfeasibility of using LLMs to transform social media data into structured KGs\nfor pharmacovigilance.",
        "published": "2025-04-06T03:47:44+00:00"
    },
    {
        "title": "AnomalyHybrid: A Domain-agnostic Generative Framework for General Anomaly Detection",
        "authors": [
            "Ying Zhao"
        ],
        "summary": "Anomaly generation is an effective way to mitigate data scarcity for anomaly\ndetection task. Most existing works shine at industrial anomaly generation with\nmultiple specialists or large generative models, rarely generalizing to\nanomalies in other applications. In this paper, we present AnomalyHybrid, a\ndomain-agnostic framework designed to generate authentic and diverse anomalies\nsimply by combining the reference and target images. AnomalyHybrid is a\nGenerative Adversarial Network(GAN)-based framework having two decoders that\nintegrate the appearance of reference image into the depth and edge structures\nof target image respectively. With the help of depth decoders, AnomalyHybrid\nachieves authentic generation especially for the anomalies with depth values\nchanging, such a s protrusion and dent. More, it relaxes the fine granularity\nstructural control of the edge decoder and brings more diversity. Without using\nannotations, AnomalyHybrid is easily trained with sets of color, depth and edge\nof same images having different augmentations. Extensive experiments carried on\nHeliconiusButterfly, MVTecAD and MVTec3D datasets demonstrate that\nAnomalyHybrid surpasses the GAN-based state-of-the-art on anomaly generation\nand its downstream anomaly classification, detection and segmentation tasks. On\nMVTecAD dataset, AnomalyHybrid achieves 2.06/0.32 IS/LPIPS for anomaly\ngeneration, 52.6 Acc for anomaly classification with ResNet34, 97.3/72.9 AP for\nimage/pixel-level anomaly detection with a simple UNet.",
        "published": "2025-04-06T03:28:30+00:00"
    },
    {
        "title": "NCL-CIR: Noise-aware Contrastive Learning for Composed Image Retrieval",
        "authors": [
            "Peng Gao",
            "Yujian Lee",
            "Zailong Chen",
            "Hui zhang",
            "Xubo Liu",
            "Yiyang Hu",
            "Guquang Jing"
        ],
        "summary": "Composed Image Retrieval (CIR) seeks to find a target image using a\nmulti-modal query, which combines an image with modification text to pinpoint\nthe target. While recent CIR methods have shown promise, they mainly focus on\nexploring relationships between the query pairs (image and text) through data\naugmentation or model design. These methods often assume perfect alignment\nbetween queries and target images, an idealized scenario rarely encountered in\npractice. In reality, pairs are often partially or completely mismatched due to\nissues like inaccurate modification texts, low-quality target images, and\nannotation errors. Ignoring these mismatches leads to numerous False Positive\nPair (FFPs) denoted as noise pairs in the dataset, causing the model to overfit\nand ultimately reducing its performance. To address this problem, we propose\nthe Noise-aware Contrastive Learning for CIR (NCL-CIR), comprising two key\ncomponents: the Weight Compensation Block (WCB) and the Noise-pair Filter Block\n(NFB). The WCB coupled with diverse weight maps can ensure more stable token\nrepresentations of multi-modal queries and target images. Meanwhile, the NFB,\nin conjunction with the Gaussian Mixture Model (GMM) predicts noise pairs by\nevaluating loss distributions, and generates soft labels correspondingly,\nallowing for the design of the soft-label based Noise Contrastive Estimation\n(NCE) loss function. Consequently, the overall architecture helps to mitigate\nthe influence of mismatched and partially matched samples, with experimental\nresults demonstrating that NCL-CIR achieves exceptional performance on the\nbenchmark datasets.",
        "published": "2025-04-06T03:27:23+00:00"
    },
    {
        "title": "Data Scaling Laws for End-to-End Autonomous Driving",
        "authors": [
            "Alexander Naumann",
            "Xunjiang Gu",
            "Tolga Dimlioglu",
            "Mariusz Bojarski",
            "Alperen Degirmenci",
            "Alexander Popov",
            "Devansh Bisla",
            "Marco Pavone",
            "Urs M\u00fcller",
            "Boris Ivanovic"
        ],
        "summary": "Autonomous vehicle (AV) stacks have traditionally relied on decomposed\napproaches, with separate modules handling perception, prediction, and\nplanning. However, this design introduces information loss during inter-module\ncommunication, increases computational overhead, and can lead to compounding\nerrors. To address these challenges, recent works have proposed architectures\nthat integrate all components into an end-to-end differentiable model, enabling\nholistic system optimization. This shift emphasizes data engineering over\nsoftware integration, offering the potential to enhance system performance by\nsimply scaling up training resources. In this work, we evaluate the performance\nof a simple end-to-end driving architecture on internal driving datasets\nranging in size from 16 to 8192 hours with both open-loop metrics and\nclosed-loop simulations. Specifically, we investigate how much additional\ntraining data is needed to achieve a target performance gain, e.g., a 5%\nimprovement in motion prediction accuracy. By understanding the relationship\nbetween model performance and training dataset size, we aim to provide insights\nfor data-driven decision-making in autonomous driving development.",
        "published": "2025-04-06T03:23:48+00:00"
    },
    {
        "title": "Artificial Intelligence for Software Architecture: Literature Review and the Road Ahead",
        "authors": [
            "Alessio Bucaioni",
            "Martin Weyssow",
            "Junda He",
            "Yunbo Lyu",
            "David Lo"
        ],
        "summary": "This paper presents a forward-looking vision for artificial\nintelligence-driven software architecture that addresses longstanding\nchallenges in design and evolution. Although artificial intelligence has\nachieved notable success in software engineering, its explicit application to\nsoftware architecture remains under-explored. Traditional practices, heavily\nreliant on expert knowledge and complex trade-off reasoning, tend to be manual\nand error-prone, thereby compromising system quality and maintainability.\nBuilding on recent advances, we examine how artificial intelligence can\nautomate architectural design, support quantitative trade-off analyses, and\ncontinuously update architectural documentation. Our approach combines a\nsystematic review of state-of-the-art applications with insights from industry\npractitioners. The resulting roadmap outlines 14 current artificial\nintelligence contributions to software architecture, identifies six artificial\nintelligence-specific challenges in supporting architectural tasks, and reveals\nsix avenues for future improvement, charting a course for future research and\npractical implementations.",
        "published": "2025-04-06T03:00:07+00:00"
    },
    {
        "title": "Modeling the Dynamics of Attentional Gamma Oscillations During the Encoding Process of Noise-Mixed Speech Signals",
        "authors": [
            "Duoyu Feng",
            "Jiajia Li",
            "Ying Wu"
        ],
        "summary": "The brain's bottom-up loop for processing speech influx involves both the\nselective attention and the encoding of specific speech information. Previous\nhuman studies have found that such attention can be represented by the cortical\ngamma-rhythm oscillations. However, the underlying mechanisms remain unclear.\nTo address this issue, this paper proposes a neural network model that\nincorporates speech signal input, the cochlea, the thalamus, and a balanced\nexcitatory-inhibitory cortical neural network, with the aim of connecting real\nspeech signals to brain cortical responses. Using this model, we explored\nneural oscillation patterns in response to mixed speech stimuli and background\nnoise. The findings revealed that the peak of gamma oscillation decreased as\nthe frequency of the pure-tone stimuli diminished. This suggests a strong\ncorrelation and coding role of gamma oscillation peaks in auditory attention.\nSimilar results were confirmed by analyzing the rhythmic oscillations of EEG\ndata in response to pure-tone signals. Further results indicated that dynamic\ngamma oscillations are involved in the encoding capacity of continuous speech\ninput. The coding entropy of the dynamic series was found to be proportional to\nthe complexity of the content. This suggests that gamma oscillations play\nmultiple roles, not only in sustaining the bottom-up attentional state but also\nin potentially conveying specific information from external speech inputs.\nFinally, we found that enhancing the excitatory-inhibitory balance level\nproperly could improve auditory attention. This finding provides a potential\nendogenous explanation for the dynamic switching process of brain attention in\nprocessing auditory signals.",
        "published": "2025-04-06T02:51:17+00:00"
    },
    {
        "title": "Constructing the Truth: Text Mining and Linguistic Networks in Public Hearings of Case 03 of the Special Jurisdiction for Peace (JEP)",
        "authors": [
            "Juan Sosa",
            "Alejandro Urrego-L\u00f3pez",
            "Cesar Prieto",
            "Emma J. Camargo-D\u00edaz"
        ],
        "summary": "Case 03 of the Special Jurisdiction for Peace (JEP), focused on the so-called\nfalse positives in Colombia, represents one of the most harrowing episodes of\nthe Colombian armed conflict. This article proposes an innovative methodology\nbased on natural language analysis and semantic co-occurrence models to\nexplore, systematize, and visualize narrative patterns present in the public\nhearings of victims and appearing parties. By constructing skipgram networks\nand analyzing their modularity, the study identifies thematic clusters that\nreveal regional and procedural status differences, providing empirical evidence\non dynamics of victimization, responsibility, and acknowledgment in this case.\nThis computational approach contributes to the collective construction of both\njudicial and extrajudicial truth, offering replicable tools for other\ntransitional justice cases. The work is grounded in the pillars of truth,\njustice, reparation, and non-repetition, proposing a critical and in-depth\nreading of contested memories.",
        "published": "2025-04-06T02:04:27+00:00"
    },
    {
        "title": "MedM-VL: What Makes a Good Medical LVLM?",
        "authors": [
            "Yiming Shi",
            "Shaoshuai Yang",
            "Xun Zhu",
            "Haoyu Wang",
            "Miao Li",
            "Ji Wu"
        ],
        "summary": "Medical image analysis is a fundamental component. As deep learning\nprogresses, the focus has shifted from single-task applications, such as\nclassification and segmentation, to more complex multimodal tasks, including\nmedical visual question answering and report generation. Traditional shallow\nand task-specific models are increasingly limited in addressing the complexity\nand scalability required in clinical practice. The emergence of large language\nmodels (LLMs) has driven the development of medical Large Vision-Language\nModels (LVLMs), offering a unified solution for diverse vision-language tasks.\nIn this study, we investigate various architectural designs for medical LVLMs\nbased on the widely adopted LLaVA framework, which follows an\nencoder-connector-LLM paradigm. We construct two distinct models targeting 2D\nand 3D modalities, respectively. These models are designed to support both\ngeneral-purpose medical tasks and domain-specific fine-tuning, thereby serving\nas effective foundation models. To facilitate reproducibility and further\nresearch, we develop a modular and extensible codebase, MedM-VL, and release\ntwo LVLM variants: MedM-VL-2D for 2D medical image analysis and\nMedM-VL-CT-Chest for 3D CT-based applications. The code and models are\navailable at: https://github.com/MSIIP/MedM-VL",
        "published": "2025-04-06T01:44:46+00:00"
    },
    {
        "title": "Variational Self-Supervised Learning",
        "authors": [
            "Mehmet Can Yavuz",
            "Berrin Yanikoglu"
        ],
        "summary": "We present Variational Self-Supervised Learning (VSSL), a novel framework\nthat combines variational inference with self-supervised learning to enable\nefficient, decoder-free representation learning. Unlike traditional VAEs that\nrely on input reconstruction via a decoder, VSSL symmetrically couples two\nencoders with Gaussian outputs. A momentum-updated teacher network defines a\ndynamic, data-dependent prior, while the student encoder produces an\napproximate posterior from augmented views. The reconstruction term in the ELBO\nis replaced with a cross-view denoising objective, preserving the analytical\ntractability of Gaussian KL divergence. We further introduce cosine-based\nformulations of KL and log-likelihood terms to enhance semantic alignment in\nhigh-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and\nImageNet-100 show that VSSL achieves competitive or superior performance to\nleading self-supervised methods, including BYOL and MoCo V3. VSSL offers a\nscalable, probabilistically grounded approach to learning transferable\nrepresentations without generative reconstruction, bridging the gap between\nvariational modeling and modern self-supervised techniques.",
        "published": "2025-04-06T01:28:50+00:00"
    },
    {
        "title": "Neural Parametric Mixtures for Path Guiding",
        "authors": [
            "Honghao Dong",
            "Guoping Wang",
            "Sheng Li"
        ],
        "summary": "Previous path guiding techniques typically rely on spatial subdivision\nstructures to approximate directional target distributions, which may cause\nfailure to capture spatio-directional correlations and introduce parallax\nissue. In this paper, we present Neural Parametric Mixtures (NPM), a neural\nformulation to encode target distributions for path guiding algorithms.\n  We propose to use a continuous and compact neural implicit representation for\nencoding parametric models while decoding them via lightweight neural networks.\n  We then derive a gradient-based optimization strategy to directly train the\nparameters of NPM with noisy Monte Carlo radiance estimates.\n  Our approach efficiently models the target distribution (incident radiance or\nthe product integrand) for path guiding, and outperforms previous guiding\nmethods by capturing the spatio-directional correlations more accurately.\n  Moreover, our approach is more training efficient and is practical for\nparallelization on modern GPUs.",
        "published": "2025-04-06T01:17:59+00:00"
    },
    {
        "title": "A Survey of Social Cybersecurity: Techniques for Attack Detection, Evaluations, Challenges, and Future Prospects",
        "authors": [
            "Aos Mulahuwaish",
            "Basheer Qolomany",
            "Kevin Gyorick",
            "Jacques Bou Abdo",
            "Mohammed Aledhari",
            "Junaid Qadir",
            "Kathleen Carley",
            "Ala Al-Fuqaha"
        ],
        "summary": "In today's digital era, the Internet, especially social media platforms,\nplays a significant role in shaping public opinions, attitudes, and beliefs.\nUnfortunately, the credibility of scientific information sources is often\nundermined by the spread of misinformation through various means, including\ntechnology-driven tools like bots, cyborgs, trolls, sock-puppets, and deep\nfakes. This manipulation of public discourse serves antagonistic business\nagendas and compromises civil society. In response to this challenge, a new\nscientific discipline has emerged: social cybersecurity.",
        "published": "2025-04-06T00:53:09+00:00"
    },
    {
        "title": "Gating is Weighting: Understanding Gated Linear Attention through In-context Learning",
        "authors": [
            "Yingcong Li",
            "Davoud Ataee Tarzanagh",
            "Ankit Singh Rawat",
            "Maryam Fazel",
            "Samet Oymak"
        ],
        "summary": "Linear attention methods offer a compelling alternative to softmax attention\ndue to their efficiency in recurrent decoding. Recent research has focused on\nenhancing standard linear attention by incorporating gating while retaining its\ncomputational benefits. Such Gated Linear Attention (GLA) architectures include\ncompetitive models such as Mamba and RWKV. In this work, we investigate the\nin-context learning capabilities of the GLA model and make the following\ncontributions. We show that a multilayer GLA can implement a general class of\nWeighted Preconditioned Gradient Descent (WPGD) algorithms with data-dependent\nweights. These weights are induced by the gating mechanism and the input,\nenabling the model to control the contribution of individual tokens to\nprediction. To further understand the mechanics of this weighting, we introduce\na novel data model with multitask prompts and characterize the optimization\nlandscape of learning a WPGD algorithm. Under mild conditions, we establish the\nexistence and uniqueness (up to scaling) of a global minimum, corresponding to\na unique WPGD solution. Finally, we translate these findings to explore the\noptimization landscape of GLA and shed light on how gating facilitates\ncontext-aware learning and when it is provably better than vanilla linear\nattention.",
        "published": "2025-04-06T00:37:36+00:00"
    },
    {
        "title": "AbsInf: A Lightweight Object to Represent float('inf') in Dijkstra's Algorithm",
        "authors": [
            "Anjan Bellamkonda",
            "Laksh Bharani",
            "Harivatsan Selvam"
        ],
        "summary": "We introduce AbsInf, a lightweight abstract object designed as a\nhigh-performance alternative to Python's native float('inf') within pathfinding\nalgorithms. Implemented as a C-based Python extension, AbsInf bypasses IEEE-754\nfloat coercion and dynamic type dispatch, offering constant-time dominance\ncomparisons and arithmetic neutrality. When integrated into Dijkstra's\nalgorithm without altering its logic, AbsInf reduces runtime by up to 17.2%,\naveraging 9.74% across diverse synthetic and real-world graph datasets. This\noptimization highlights the performance trade-offs in high-frequency\nalgorithmic constructs, where a symbolic use of infinity permits efficient\nabstraction. Our findings contribute to the broader discourse on lightweight\narchitectural enhancements for interpreted languages, particularly in\nperformance-critical control flows.",
        "published": "2025-04-05T23:37:55+00:00"
    },
    {
        "title": "Generative Market Equilibrium Models with Stable Adversarial Learning via Reinforcement",
        "authors": [
            "Anastasis Kratsios",
            "Xiaofei Shi",
            "Qiang Sun",
            "Zhanhao Zhang"
        ],
        "summary": "We present a general computational framework for solving continuous-time\nfinancial market equilibria under minimal modeling assumptions while\nincorporating realistic financial frictions, such as trading costs, and\nsupporting multiple interacting agents. Inspired by generative adversarial\nnetworks (GANs), our approach employs a novel generative deep reinforcement\nlearning framework with a decoupling feedback system embedded in the\nadversarial training loop, which we term as the \\emph{reinforcement link}. This\narchitecture stabilizes the training dynamics by incorporating feedback from\nthe discriminator. Our theoretically guided feedback mechanism enables the\ndecoupling of the equilibrium system, overcoming challenges that hinder\nconventional numerical algorithms. Experimentally, our algorithm not only\nlearns but also provides testable predictions on how asset returns and\nvolatilities emerge from the endogenous trading behavior of market\nparticipants, where traditional analytical methods fall short. The design of\nour model is further supported by an approximation guarantee.",
        "published": "2025-04-05T23:29:46+00:00"
    },
    {
        "title": "3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS",
        "authors": [
            "Zhisheng Huang",
            "Peng Wang",
            "Jingdong Zhang",
            "Yuan Liu",
            "Xin Li",
            "Wenping Wang"
        ],
        "summary": "3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its\nefficiency and quality, but like many novel view synthesis methods, it heavily\ndepends on accurate camera poses from Structure-from-Motion (SfM) systems.\nAlthough recent SfM pipelines have made impressive progress, questions remain\nabout how to further improve both their robust performance in challenging\nconditions (e.g., textureless scenes) and the precision of camera parameter\nestimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework\nthat bridges this gap by jointly optimizing 3D Gaussians and camera parameters\nfrom large reconstruction priors MASt3R-SfM. We note that naively performing\njoint 3D Gaussian and camera optimization faces two challenges: the sensitivity\nto the quality of SfM initialization, and its limited capacity for global\noptimization, leading to suboptimal reconstruction results. Our 3R-GS,\novercomes these issues by incorporating optimized practices, enabling robust\nscene reconstruction even with imperfect camera registration. Extensive\nexperiments demonstrate that 3R-GS delivers high-quality novel view synthesis\nand precise camera pose estimation while remaining computationally efficient.\nProject page: https://zsh523.github.io/3R-GS/",
        "published": "2025-04-05T22:31:08+00:00"
    },
    {
        "title": "Optimal Teaming for Coordination with Bounded Rationality via Convex Optimization",
        "authors": [
            "Zhewei Wang",
            "Olugbenga Moses Anubi",
            "Marcos M. Vasconcelos"
        ],
        "summary": "Teaming is the process of establishing connections among agents within a\nsystem to enable collaboration toward achieving a collective goal. This paper\nexamines teaming in the context of a network of agents learning to coordinate\nwith bounded rationality. In our framework, the team structure is represented\nvia a weighted graph, and the agents use log-linear learning. We formulate the\ndesign of the graph's weight matrix as a convex optimization problem whose\nobjective is to maximize the probability of learning a Nash equilibrium while\nminimizing a connectivity cost. Despite its convexity, solving this\noptimization problem is computationally challenging, as the objective function\ninvolves the summation over the action profile space, which grows exponentially\nwith the number of agents. Leveraging the underlying symmetry and convexity\nproperties of the problem, when there are no sparsity constraints, we prove\nthat there exists an optimal solution corresponding to a uniformly weighted\ngraph, simplifying to a one-dimensional convex optimization problem.\nAdditionally, we show that the optimal weight decreases monotonically with the\nagent's rationality, implying that when the agents become more rational the\noptimal team requires less connectivity.",
        "published": "2025-04-05T22:25:06+00:00"
    },
    {
        "title": "A Self-Supervised Learning Approach with Differentiable Optimization for UAV Trajectory Planning",
        "authors": [
            "Yufei Jiang",
            "Yuanzhu Zhan",
            "Harsh Vardhan Gupta",
            "Chinmay Borde",
            "Junyi Geng"
        ],
        "summary": "While Unmanned Aerial Vehicles (UAVs) have gained significant traction across\nvarious fields, path planning in 3D environments remains a critical challenge,\nparticularly under size, weight, and power (SWAP) constraints. Traditional\nmodular planning systems often introduce latency and suboptimal performance due\nto limited information sharing and local minima issues. End-to-end learning\napproaches streamline the pipeline by mapping sensory observations directly to\nactions but require large-scale datasets, face significant sim-to-real gaps, or\nlack dynamical feasibility. In this paper, we propose a self-supervised UAV\ntrajectory planning pipeline that integrates a learning-based depth perception\nwith differentiable trajectory optimization. A 3D cost map guides UAV behavior\nwithout expert demonstrations or human labels. Additionally, we incorporate a\nneural network-based time allocation strategy to improve the efficiency and\noptimality. The system thus combines robust learning-based perception with\nreliable physics-based optimization for improved generalizability and\ninterpretability. Both simulation and real-world experiments validate our\napproach across various environments, demonstrating its effectiveness and\nrobustness. Our method achieves a 31.33% improvement in position tracking error\nand 49.37% reduction in control effort compared to the state-of-the-art.",
        "published": "2025-04-05T22:09:13+00:00"
    },
    {
        "title": "Foundation Models for Environmental Science: A Survey of Emerging Frontiers",
        "authors": [
            "Runlong Yu",
            "Shengyu Chen",
            "Yiqun Xie",
            "Huaxiu Yao",
            "Jared Willard",
            "Xiaowei Jia"
        ],
        "summary": "Modeling environmental ecosystems is essential for effective resource\nmanagement, sustainable development, and understanding complex ecological\nprocesses. However, traditional data-driven methods face challenges in\ncapturing inherently complex and interconnected processes and are further\nconstrained by limited observational data in many environmental applications.\nFoundation models, which leverages large-scale pre-training and universal\nrepresentations of complex and heterogeneous data, offer transformative\nopportunities for capturing spatiotemporal dynamics and dependencies in\nenvironmental processes, and facilitate adaptation to a broad range of\napplications. This survey presents a comprehensive overview of foundation model\napplications in environmental science, highlighting advancements in common\nenvironmental use cases including forward prediction, data generation, data\nassimilation, downscaling, inverse modeling, model ensembling, and\ndecision-making across domains. We also detail the process of developing these\nmodels, covering data collection, architecture design, training, tuning, and\nevaluation. Through discussions on these emerging methods as well as their\nfuture opportunities, we aim to promote interdisciplinary collaboration that\naccelerates advancements in machine learning for driving scientific discovery\nin addressing critical environmental challenges.",
        "published": "2025-04-05T20:56:38+00:00"
    },
    {
        "title": "A Comparative Study of Explainable AI Methods: Model-Agnostic vs. Model-Specific Approaches",
        "authors": [
            "Keerthi Devireddy"
        ],
        "summary": "This paper compares model-agnostic and model-specific approaches to\nexplainable AI (XAI) in deep learning image classification. I examine how LIME\nand SHAP (model-agnostic methods) differ from Grad-CAM and Guided\nBackpropagation (model-specific methods) when interpreting ResNet50 predictions\nacross diverse image categories. Through extensive testing with various species\nfrom dogs and birds to insects I found that each method reveals different\naspects of the models decision-making process. Model-agnostic techniques\nprovide broader feature attribution that works across different architectures,\nwhile model-specific approaches excel at highlighting precise activation\nregions with greater computational efficiency. My analysis shows there is no\n\"one-size-fits-all\" solution for model interpretability. Instead, combining\nmultiple XAI methods offers the most comprehensive understanding of complex\nmodels particularly valuable in high-stakes domains like healthcare, autonomous\nvehicles, and financial services where transparency is crucial. This\ncomparative framework provides practical guidance for selecting appropriate\ninterpretability techniques based on specific application needs and\ncomputational constraints.",
        "published": "2025-04-05T20:13:20+00:00"
    },
    {
        "title": "Improving Early Prediction of Type 2 Diabetes Mellitus with ECG-DiaNet: A Multimodal Neural Network Leveraging Electrocardiogram and Clinical Risk Factors",
        "authors": [
            "Farida Mohsen",
            "Zubair Shah"
        ],
        "summary": "Type 2 Diabetes Mellitus (T2DM) remains a global health challenge,\nunderscoring the need for early and accurate risk prediction. This study\npresents ECG-DiaNet, a multimodal deep learning model that integrates\nelectrocardiogram (ECG) features with clinical risk factors (CRFs) to enhance\nT2DM onset prediction. Using data from Qatar Biobank (QBB), we trained and\nvalidated models on a development cohort (n=2043) and evaluated performance on\na longitudinal test set (n=395) with five-year follow-up. ECG-DiaNet\noutperformed unimodal ECG-only and CRF-only models, achieving a higher AUROC\n(0.845 vs 0.8217) than the CRF-only model, with statistical significance\n(DeLong p<0.001). Reclassification metrics further confirmed improvements: Net\nReclassification Improvement (NRI=0.0153) and Integrated Discrimination\nImprovement (IDI=0.0482). Risk stratification into low-, medium-, and high-risk\ngroups showed ECG-DiaNet achieved superior positive predictive value (PPV) in\nhigh-risk individuals. The model's reliance on non-invasive and widely\navailable ECG signals supports its feasibility in clinical and community health\nsettings. By combining cardiac electrophysiology and systemic risk profiles,\nECG-DiaNet addresses the multifactorial nature of T2DM and supports precision\nprevention. These findings highlight the value of multimodal AI in advancing\nearly detection and prevention strategies for T2DM, particularly in\nunderrepresented Middle Eastern populations.",
        "published": "2025-04-05T19:59:59+00:00"
    },
    {
        "title": "ADA-Net: Attention-Guided Domain Adaptation Network with Contrastive Learning for Standing Dead Tree Segmentation Using Aerial Imagery",
        "authors": [
            "Mete Ahishali",
            "Anis Ur Rahman",
            "Einari Heinaro",
            "Samuli Junttila"
        ],
        "summary": "Information on standing dead trees is important for understanding forest\necosystem functioning and resilience but has been lacking over large geographic\nregions. Climate change has caused large-scale tree mortality events that can\nremain undetected due to limited data. In this study, we propose a novel method\nfor segmenting standing dead trees using aerial multispectral orthoimages.\nBecause access to annotated datasets has been a significant problem in forest\nremote sensing due to the need for forest expertise, we introduce a method for\ndomain transfer by leveraging domain adaptation to learn a transformation from\na source domain X to target domain Y. In this Image-to-Image translation task,\nwe aim to utilize available annotations in the target domain by pre-training a\nsegmentation network. When images from a new study site without annotations are\nintroduced (source domain X), these images are transformed into the target\ndomain. Then, transfer learning is applied by inferring the pre-trained network\non domain-adapted images. In addition to investigating the feasibility of\ncurrent domain adaptation approaches for this objective, we propose a novel\napproach called the Attention-guided Domain Adaptation Network (ADA-Net) with\nenhanced contrastive learning. Accordingly, the ADA-Net approach provides new\nstate-of-the-art domain adaptation performance levels outperforming existing\napproaches. We have evaluated the proposed approach using two datasets from\nFinland and the US. The USA images are converted to the Finland domain, and we\nshow that the synthetic USA2Finland dataset exhibits similar characteristics to\nthe Finland domain images. The software implementation is shared at\nhttps://github.com/meteahishali/ADA-Net. The data is publicly available at\nhttps://www.kaggle.com/datasets/meteahishali/aerial-imagery-for-standing-dead-tree-segmentation.",
        "published": "2025-04-05T19:55:02+00:00"
    },
    {
        "title": "Improving Chronic Kidney Disease Detection Efficiency: Fine Tuned CatBoost and Nature-Inspired Algorithms with Explainable AI",
        "authors": [
            "Md. Ehsanul Haque",
            "S. M. Jahidul Islam",
            "Jeba Maliha",
            "Md. Shakhauat Hossan Sumon",
            "Rumana Sharmin",
            "Sakib Rokoni"
        ],
        "summary": "Chronic Kidney Disease (CKD) is a major global health issue which is\naffecting million people around the world and with increasing rate of\nmortality. Mitigation of progression of CKD and better patient outcomes\nrequires early detection. Nevertheless, limitations lie in traditional\ndiagnostic methods, especially in resource constrained settings. This study\nproposes an advanced machine learning approach to enhance CKD detection by\nevaluating four models: Random Forest (RF), Multi-Layer Perceptron (MLP),\nLogistic Regression (LR), and a fine-tuned CatBoost algorithm. Specifically,\namong these, the fine-tuned CatBoost model demonstrated the best overall\nperformance having an accuracy of 98.75%, an AUC of 0.9993 and a Kappa score of\n97.35% of the studies. The proposed CatBoost model has used a nature inspired\nalgorithm such as Simulated Annealing to select the most important features,\nCuckoo Search to adjust outliers and grid search to fine tune its settings in\nsuch a way to achieve improved prediction accuracy. Features significance is\nexplained by SHAP-a well-known XAI technique-for gaining transparency in the\ndecision-making process of proposed model and bring up trust in diagnostic\nsystems. Using SHAP, the significant clinical features were identified as\nspecific gravity, serum creatinine, albumin, hemoglobin, and diabetes mellitus.\nThe potential of advanced machine learning techniques in CKD detection is shown\nin this research, particularly for low income and middle-income healthcare\nsettings where prompt and correct diagnoses are vital. This study seeks to\nprovide a highly accurate, interpretable, and efficient diagnostic tool to add\nto efforts for early intervention and improved healthcare outcomes for all CKD\npatients.",
        "published": "2025-04-05T19:41:47+00:00"
    },
    {
        "title": "LOGLO-FNO: Efficient Learning of Local and Global Features in Fourier Neural Operators",
        "authors": [
            "Marimuthu Kalimuthu",
            "David Holzm\u00fcller",
            "Mathias Niepert"
        ],
        "summary": "Modeling high-frequency information is a critical challenge in scientific\nmachine learning. For instance, fully turbulent flow simulations of\nNavier-Stokes equations at Reynolds numbers 3500 and above can generate\nhigh-frequency signals due to swirling fluid motions caused by eddies and\nvortices. Faithfully modeling such signals using neural networks depends on\naccurately reconstructing moderate to high frequencies. However, it has been\nwell known that deep neural nets exhibit the so-called spectral bias toward\nlearning low-frequency components. Meanwhile, Fourier Neural Operators (FNOs)\nhave emerged as a popular class of data-driven models in recent years for\nsolving Partial Differential Equations (PDEs) and for surrogate modeling in\ngeneral. Although impressive results have been achieved on several PDE\nbenchmark problems, FNOs often perform poorly in learning non-dominant\nfrequencies characterized by local features. This limitation stems from the\nspectral bias inherent in neural networks and the explicit exclusion of\nhigh-frequency modes in FNOs and their variants. Therefore, to mitigate these\nissues and improve FNO's spectral learning capabilities to represent a broad\nrange of frequency components, we propose two key architectural enhancements:\n(i) a parallel branch performing local spectral convolutions (ii) a\nhigh-frequency propagation module. Moreover, we propose a novel\nfrequency-sensitive loss term based on radially binned spectral errors. This\nintroduction of a parallel branch for local convolutions reduces number of\ntrainable parameters by up to 50% while achieving the accuracy of baseline FNO\nthat relies solely on global convolutions. Experiments on three challenging PDE\nproblems in fluid mechanics and biological pattern formation, and the\nqualitative and spectral analysis of predictions show the effectiveness of our\nmethod over the state-of-the-art neural operator baselines.",
        "published": "2025-04-05T19:35:04+00:00"
    },
    {
        "title": "Three strongly coupled Kerr parametric oscillators forming a Boltzmann machine",
        "authors": [
            "Gabriel Margiani",
            "Orjan Ameye",
            "Oded Zilberberg",
            "Alexander Eichler"
        ],
        "summary": "Coupled Kerr parametric oscillators (KPOs) are a promising resource for\nclassical and quantum analog computation, for example to find the ground state\nof Ising Hamiltonians. Yet, the state space of strongly coupled KPO networks is\nvery involved. As such, their phase diagram sometimes features either too few\nor too many states, including some that cannot be mapped to Ising spin\nconfigurations. This complexity makes it challenging to find and meet the\nconditions under which an analog optimization algorithm can be successful.\nHere, we demonstrate how to use three strongly coupled KPOs as a simulator for\nan Ising Hamiltonian, and estimate its ground state using a Boltzmann sampling\nmeasurement. While fully classical, our work is directly relevant for quantum\nsystems operating on coherent states.",
        "published": "2025-04-05T19:28:17+00:00"
    },
    {
        "title": "Tratto: A Neuro-Symbolic Approach to Deriving Axiomatic Test Oracles",
        "authors": [
            "Davide Molinelli",
            "Alberto Martin-Lopez",
            "Elliott Zackrone",
            "Beyza Eken",
            "Michael D. Ernst",
            "Mauro Pezz\u00e8"
        ],
        "summary": "This paper presents Tratto, a neuro-symbolic approach that generates\nassertions (boolean expressions) that can serve as axiomatic oracles, from\nsource code and documentation. The symbolic module of Tratto takes advantage of\nthe grammar of the programming language, the unit under test, and the context\nof the unit (its class and available APIs) to restrict the search space of the\ntokens that can be successfully used to generate valid oracles. The neural\nmodule of Tratto uses transformers fine-tuned for both deciding whether to\noutput an oracle or not and selecting the next lexical token to incrementally\nbuild the oracle from the set of tokens returned by the symbolic module. Our\nexperiments show that Tratto outperforms the state-of-the-art axiomatic oracle\ngeneration approaches, with 73% accuracy, 72% precision, and 61% F1-score,\nlargely higher than the best results of the symbolic and neural approaches\nconsidered in our study (61%, 62%, and 37%, respectively). Tratto can generate\nthree times more axiomatic oracles than current symbolic approaches, while\ngenerating 10 times less false positives than GPT4 complemented with few-shot\nlearning and Chain-of-Thought prompting.",
        "published": "2025-04-05T19:14:35+00:00"
    },
    {
        "title": "Modeling resource consumption in the US air transportation system via minimum-cost percolation",
        "authors": [
            "Minsuk Kim",
            "C. Tyler Diggans",
            "Filippo Radicchi"
        ],
        "summary": "We introduce a dynamic percolation model aimed at describing the consumption,\nand eventual exhaustion, of resources in transportation networks. In the model,\nrational agents progressively consume the edges of a network along demanded\nminimum-cost paths. As a result, the network undergoes a transition between a\npercolating phase where it can properly serve demand to a non-percolating phase\nwhere demand can no longer be supplied. We apply the model to a weighted,\ndirected, temporal, multi-layer network representation of the air\ntransportation system that can be generated using real schedules of commercial\nflights operated by US carriers. We study how cooperation among different\ncarriers could improve the ability of the overall air transportation system in\nserving the demand of passengers, finding that unrestricted cooperation could\nlead to a 30% efficiency increase compared to the non-cooperative scenario.\nCooperation would require major airlines to share a significant portion of\ntheir market, but it would allow also for an increased robustness of the system\nagainst perturbations causing flight cancellations. Our findings underscore\nsome key benefits that could emerge by simply promoting code-share arrangements\namong US airlines without altering their current cost of operation.",
        "published": "2025-04-05T18:26:06+00:00"
    },
    {
        "title": "oneDAL Optimization for ARM Scalable Vector Extension: Maximizing Efficiency for High-Performance Data Science",
        "authors": [
            "Chandan Sharma",
            "Rakshith GB",
            "Ajay Kumar Patel",
            "Dhanus M Lal",
            "Darshan Patel",
            "Ragesh Hajela",
            "Masahiro Doteguchi",
            "Priyanka Sharma"
        ],
        "summary": "The evolution of ARM-based architectures, particularly those incorporating\nScalable Vector Extension (SVE), has introduced transformative opportunities\nfor high-performance computing (HPC) and machine learning (ML) workloads. The\nUnified Acceleration Foundation's (UXL) oneAPI Data Analytics Library (oneDAL)\nis a widely adopted library for accelerating ML and data analytics workflows,\nbut its reliance on Intel's proprietary Math Kernel Library (MKL) has\ntraditionally limited its compatibility to x86platforms. This paper details the\nporting of oneDAL to ARM architectures with SVE support, using OpenBLAS as an\nalternative backend to overcome architectural and performance challenges.\nBeyond porting, the research introduces novel ARM-specific optimizations,\nincluding custom sparse matrix routines, vectorized statistical functions, and\na Scalable Vector Extension (SVE)-optimized Support Vector Machine (SVM)\nalgorithm. The SVM enhancements leverage SVE's flexible vector lengths and\npredicate driven execution, achieving notable performance gains of 22% for the\nBoser method and 5% for the Thunder method. Benchmarks conducted on ARM\nSVE-enabled AWSGraviton3 instances showcase up to 200x acceleration in ML\ntraining and inference tasks compared to the original scikit-learn\nimplementation on the ARM platform. Moreover, the ARM-optimized oneDAL achieves\nperformance parity with, and in some cases exceeds, the x86 oneDAL\nimplementation (MKL backend) on IceLake x86 systems, which are nearly twice as\ncostly as AWSGraviton3 ARM instances. These findings highlight ARM's potential\nas a high-performance, energyefficient platform for dataintensive ML\napplications. By expanding cross-architecture compatibility and contributing to\nthe opensource ecosystem, this work reinforces ARM's position as a competitive\nalternative in the HPC and ML domains, paving the way for future advancements\nin dataintensive computing.",
        "published": "2025-04-05T17:53:36+00:00"
    },
    {
        "title": "Maximum entropy for dynamic processes on networks",
        "authors": [
            "Noam Abadi",
            "Franco Ruzzenenti"
        ],
        "summary": "Dynamic processes on networks are fundamental to understand modern day\nphenomena such as information diffusion and opinion polarization on the\ninternet or epidemic spreading. However, they are notoriously difficult to\nstudy broadly as small changes in initial conditions, the process or the\nnetwork can lead to very different evolution trajectories. Here we apply the\ninformation-theoretic framework of maximum caliber to study the statistics of\nsuch systems analytically in a general way. We verify the dynamics deduced from\nmaximum caliber by using simulations of different processes on different\nnetworks, introduce an approximation of the dynamics which significantly\nsimplifies the problem and show that the approximation can be used to recover\nwell-established models of population dynamics that are typically not thought\nof as taking place on a network. This provides a theoretical tool that allows\nto study dynamic processes on networks broadly without loosing sight of known\nresults.",
        "published": "2025-04-05T17:51:04+00:00"
    },
    {
        "title": "Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models",
        "authors": [
            "Yuheng Wu",
            "Wentao Guo",
            "Zirui Liu",
            "Heng Ji",
            "Zhaozhuo Xu",
            "Denghui Zhang"
        ],
        "summary": "This paper investigates the emergence of Theory-of-Mind (ToM) capabilities in\nlarge language models (LLMs) from a mechanistic perspective, focusing on the\nrole of extremely sparse parameter patterns. We introduce a novel method to\nidentify ToM-sensitive parameters and reveal that perturbing as little as\n0.001% of these parameters significantly degrades ToM performance while also\nimpairing contextual localization and language understanding. To understand\nthis effect, we analyze their interaction with core architectural components of\nLLMs. Our findings demonstrate that these sensitive parameters are closely\nlinked to the positional encoding module, particularly in models using Rotary\nPosition Embedding (RoPE), where perturbations disrupt dominant-frequency\nactivations critical for contextual processing. Furthermore, we show that\nperturbing ToM-sensitive parameters affects LLM's attention mechanism by\nmodulating the angle between queries and keys under positional encoding. These\ninsights provide a deeper understanding of how LLMs acquire social reasoning\nabilities, bridging AI interpretability with cognitive science. Our results\nhave implications for enhancing model alignment, mitigating biases, and\nimproving AI systems designed for human interaction.",
        "published": "2025-04-05T17:45:42+00:00"
    },
    {
        "title": "Quantum parallel information exchange (QPIE) hybrid network with transfer learning",
        "authors": [
            "Ziqing Guo",
            "Alex Khan",
            "Victor S. Sheng",
            "Shabnam Jabeen",
            "Ziwen Pan"
        ],
        "summary": "Quantum machine learning (QML) has emerged as an innovative framework with\nthe potential to uncover complex patterns by leveraging quantum systems ability\nto simulate and exploit high-dimensional latent spaces, particularly in\nlearning tasks. Quantum neural network (QNN) frameworks are inherently\nsensitive to the precision of gradient calculations and the computational\nlimitations of current quantum hardware as unitary rotations introduce overhead\nfrom complex number computations, and the quantum gate operation speed remains\na bottleneck for practical implementations. In this study, we introduce quantum\nparallel information exchange (QPIE) hybrid network, a new non-sequential\nhybrid classical quantum model architecture, leveraging quantum transfer\nlearning by feeding pre-trained parameters from classical neural networks into\nquantum circuits, which enables efficient pattern recognition and temporal\nseries data prediction by utilizing non-clifford parameterized quantum gates\nthereby enhancing both learning efficiency and representational capacity.\nAdditionally, we develop a dynamic gradient selection method that applies the\nparameter shift rule on quantum processing units (QPUs) and adjoint\ndifferentiation on GPUs. Our results demonstrate model performance exhibiting\nhigher accuracy in ad-hoc benchmarks, lowering approximately 88% convergence\nrate for extra stochasticity time-series data within 100-steps, and showcasing\na more unbaised eigenvalue spectrum of the fisher information matrix on CPU/GPU\nand IonQ QPU simulators.",
        "published": "2025-04-05T17:25:26+00:00"
    },
    {
        "title": "Joint Optimization of Uplink and Downlink Power in Full-Duplex Integrated Access and Backhaul",
        "authors": [
            "Giovanni Interdonato",
            "Silvia Mura",
            "Marouan Mizmizi",
            "Stefano Buzzi",
            "Umberto Spagnolini"
        ],
        "summary": "We examine the performance of an Integrated Access and Backhaul (IAB) node as\na range extender for beyond-5G networks, focusing on the significant challenges\nof effective power allocation and beamforming strategies, which are vital for\nmaximizing users' spectral efficiency (SE). We present both max-sum SE and\nmax-min fairness power allocation strategies, to assess their effects on system\nperformance. The results underscore the necessity of power optimization,\nparticularly as the number of users served by the IAB node increases,\ndemonstrating how efficient power allocation enhances service quality in\nhigh-load scenarios. The results also show that the typical line-of-sight link\nbetween the IAB donor and the IAB node has rank one, posing a limitation on the\neffective SEs that the IAB node can support.",
        "published": "2025-04-05T16:58:04+00:00"
    },
    {
        "title": "Resilience of Vision Transformers for Domain Generalisation in the Presence of Out-of-Distribution Noisy Images",
        "authors": [
            "Hamza Riaz",
            "Alan F. Smeaton"
        ],
        "summary": "Modern AI models excel in controlled settings but often fail in real-world\nscenarios where data distributions shift unpredictably - a challenge known as\ndomain generalisation (DG). This paper tackles this limitation by rigorously\nevaluating vision tramsformers, specifically the BEIT architecture which is a\nmodel pre-trained with masked image modelling (MIM), against synthetic\nout-of-distribution (OOD) benchmarks designed to mimic real-world noise and\nocclusions. We introduce a novel framework to generate OOD test cases by\nstrategically masking object regions in images using grid patterns (25\\%, 50\\%,\n75\\% occlusion) and leveraging cutting-edge zero-shot segmentation via Segment\nAnything and Grounding DINO to ensure precise object localisation. Experiments\nacross three benchmarks (PACS, Office-Home, DomainNet) demonstrate BEIT's known\nrobustness while maintaining 94\\% accuracy on PACS and 87\\% on Office-Home,\ndespite significant occlusions, outperforming CNNs and other vision\ntransformers by margins of up to 37\\%. Analysis of self-attention distances\nreveals that the BEIT dependence on global features correlates with its\nresilience. Furthermore, our synthetic benchmarks expose critical failure\nmodes: performance degrades sharply when occlusions disrupt object shapes e.g.\n68\\% drop for external grid masking vs. 22\\% for internal masking. This work\nprovides two key advances (1) a scalable method to generate OOD benchmarks\nusing controllable noise, and (2) empirical evidence that MIM and\nself-attention mechanism in vision transformers enhance DG by learning\ninvariant features. These insights bridge the gap between lab-trained models\nand real-world deployment that offer a blueprint for building AI systems that\ngeneralise reliably under uncertainty.",
        "published": "2025-04-05T16:25:34+00:00"
    },
    {
        "title": "Multi-Phase Coupled CMOS Ring Oscillator based Potts Machine",
        "authors": [
            "Yilmaz Ege Gonul",
            "Baris Taskin"
        ],
        "summary": "This paper presents a coupled ring oscillator based Potts ma chine to solve\nNP-hard combinatorial optimization problems\n  (COPs). Potts model is a generalization of the Ising model, cap turing\nmultivalued spins in contrast to the binary-valued spins\n  allowed in the Ising model. Similar to recent literature on Ising\n  machines, the proposed architecture of Potts machines imple ments the Potts\nmodel with interacting spins represented by cou pled ring oscillators. Unlike\nIsing machines which are limited\n  to two spin values, Potts machines model COPs that require a\n  larger number of spin values. A major novelty of the proposed\n  Potts machine is the utilization of the N-SHIL (Sub-Harmonic\n  Injection Locking) mechanism, where multiple stable phases are\n  obtained from a single (i.e. ring) oscillator. In evaluation, 3 coloring\nproblems from the DIMACS SATBLIB benchmark and\n  two randomly generated larger problems are mapped to the pro posed\narchitecture. The proposed architecture is demonstrated\n  to solve problems of varying size with 89% to 92% accuracy\n  averaged over multiple iterations. The simulation results show\n  that there is no degradation in accuracy, no significant increase\n  in solution time, and only a linear increase in power dissipation\n  with increasing problem sizes up to 2000 nodes.",
        "published": "2025-04-05T16:20:44+00:00"
    },
    {
        "title": "TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis with Generic Traffic Representation",
        "authors": [
            "Tianyu Cui",
            "Xinjie Lin",
            "Sijia Li",
            "Miao Chen",
            "Qilei Yin",
            "Qi Li",
            "Ke Xu"
        ],
        "summary": "Machine learning (ML) powered network traffic analysis has been widely used\nfor the purpose of threat detection. Unfortunately, their generalization across\ndifferent tasks and unseen data is very limited. Large language models (LLMs),\nknown for their strong generalization capabilities, have shown promising\nperformance in various domains. However, their application to the traffic\nanalysis domain is limited due to significantly different characteristics of\nnetwork traffic. To address the issue, in this paper, we propose TrafficLLM,\nwhich introduces a dual-stage fine-tuning framework to learn generic traffic\nrepresentation from heterogeneous raw traffic data. The framework uses\ntraffic-domain tokenization, dual-stage tuning pipeline, and extensible\nadaptation to help LLM release generalization ability on dynamic traffic\nanalysis tasks, such that it enables traffic detection and traffic generation\nacross a wide range of downstream tasks. We evaluate TrafficLLM across 10\ndistinct scenarios and 229 types of traffic. TrafficLLM achieves F1-scores of\n0.9875 and 0.9483, with up to 80.12% and 33.92% better performance than\nexisting detection and generation methods. It also shows strong generalization\non unseen traffic with an 18.6% performance improvement. We further evaluate\nTrafficLLM in real-world scenarios. The results confirm that TrafficLLM is easy\nto scale and achieves accurate detection performance on enterprise traffic.",
        "published": "2025-04-05T16:18:33+00:00"
    },
    {
        "title": "AdaCoder: An Adaptive Planning and Multi-Agent Framework for Function-Level Code Generation",
        "authors": [
            "Yueheng Zhu",
            "Chao Liu",
            "Xuan He",
            "Xiaoxue Ren",
            "Zhongxin Liu",
            "Ruwei Pan",
            "Hongyu Zhang"
        ],
        "summary": "Recently, researchers have proposed many multi-agent frameworks for\nfunction-level code generation, which aim to improve software development\nproductivity by automatically generating function-level source code based on\ntask descriptions. A typical multi-agent framework consists of Large Language\nModel (LLM)-based agents that are responsible for task planning, code\ngeneration, testing, debugging, etc. Studies have shown that existing\nmulti-agent code generation frameworks perform well on ChatGPT. However, their\ngeneralizability across other foundation LLMs remains unexplored\nsystematically. In this paper, we report an empirical study on the\ngeneralizability of four state-of-the-art multi-agent code generation\nframeworks across six open-source LLMs with varying parameter sizes,\narchitectures, and performance levels. Our study reveals the unstable\ngeneralizability of existing frameworks on diverse foundation LLMs. Based on\nthe findings obtained from the empirical study, we propose AdaCoder, a novel\nadaptive planning, multi-agent framework for function-level code generation.\nAdaCoder has two phases. Phase-1 is an initial code generation step without\nplanning, which uses an LLM-based coding agent and a script-based testing agent\nto unleash LLM's native power, identify cases beyond LLM's power, and determine\nthe errors hindering execution. Phase-2 adds a rule-based debugging agent and\nan LLM-based planning agent for iterative code generation with planning. Our\nevaluation shows that AdaCoder achieves higher generalizability on diverse\nLLMs. Compared to the best baseline MapCoder, AdaCoder is on average 27.69%\nhigher in Pass@1, 16 times faster in inference, and 12 times lower in token\nconsumption.",
        "published": "2025-04-05T16:14:01+00:00"
    },
    {
        "title": "Towards Understanding and Improving Refusal in Compressed Models via Mechanistic Interpretability",
        "authors": [
            "Vishnu Kabir Chhabra",
            "Mohammad Mahdi Khalili"
        ],
        "summary": "The rapid growth of large language models has spurred significant interest in\nmodel compression as a means to enhance their accessibility and practicality.\nWhile extensive research has explored model compression through the lens of\nsafety, findings suggest that safety-aligned models often lose elements of\ntrustworthiness post-compression. Simultaneously, the field of mechanistic\ninterpretability has gained traction, with notable discoveries, such as the\nidentification of a single direction in the residual stream mediating refusal\nbehaviors across diverse model architectures. In this work, we investigate the\nsafety of compressed models by examining the mechanisms of refusal, adopting a\nnovel interpretability-driven perspective to evaluate model safety.\nFurthermore, leveraging insights from our interpretability analysis, we propose\na lightweight, computationally efficient method to enhance the safety of\ncompressed models without compromising their performance or utility.",
        "published": "2025-04-05T16:00:44+00:00"
    },
    {
        "title": "Accelerated Bayesian Inference for Pulsar Timing Arrays: Normalizing Flows for Rapid Model Comparison Across Stochastic Gravitational-Wave Background Sources",
        "authors": [
            "Junrong Lai",
            "Changhong Li"
        ],
        "summary": "The recent detection of nanohertz stochastic gravitational-wave backgrounds\n(SGWBs) by pulsar timing arrays (PTAs) promises unique insights into\nastrophysical and cosmological origins. However, traditional Markov Chain Monte\nCarlo (MCMC) approaches become prohibitively expensive for large datasets. We\nemploy a normalizing flow (NF)-based machine learning framework to accelerate\nBayesian inference in PTA analyses. For the first time, we perform Bayesian\nmodel comparison across SGWB source models in the framework of machine learning\nby training NF architectures on the PTA dataset (NANOGrav 15-year) and enabling\ndirect evidence estimation via learned harmonic mean estimators. Our examples\ninclude 10 conventional SGWB source models such as supermassive black hole\nbinaries, power-law spectrum, cosmic strings, domain walls, scalar-induced GWs,\nfirst-order phase transitions, and dual scenario/inflationary gravitational\nwave. Our approach jointly infers 20 red noise parameters and 2 SGWB parameters\nper model in $\\sim 20$\\,hours (including training), compared to $\\sim 10$\\,days\nwith MCMC. Critically, the NF method preserves rigorous model selection\naccuracy, with small Hellinger distances ($\\lesssim 0.3$) relative to MCMC\nposteriors, and reproduces MCMC-based Bayes factors across all tested\nscenarios. This scalable technique for SGWB source comparison will be essential\nfor future PTA expansions and next-generation arrays such as the SKA, offering\norders-of-magnitude efficiency gains without sacrificing physical\ninterpretability.",
        "published": "2025-04-05T15:44:57+00:00"
    },
    {
        "title": "Distributed Time Synchronization in NOMA-Assisted Ultra-Dense Networks",
        "authors": [
            "Debjani Goswami",
            "Indrakshi Dey",
            "Nicola Marchetti",
            "Suvra Sekhar Das"
        ],
        "summary": "Ultra-dense networks (UDNs) represent a transformative access architecture\nfor upcoming sixth generation (6G) systems, poised to meet the surging demand\nfor high data rates. Achieving precise synchronization across diverse base\nstations (BSs) is critical in these networks to mitigate inter-cell\ninterference (ICI). However, traditional centralized synchronization approaches\nface substantial challenges in dense urban, including limited access to Global\nPositioning System (GPS), dependence on reliable backhaul, and high signaling\noverhead demands. This study advances a low-complexity distributed\nsynchronization solution. A primary focus is on assessing the algorithm's\naccuracy incorporating the effects of information exchange delays, which are\npronounced in large-networks. Recognizing the pivotal role of neighbor-gathered\ninformation in the proposed approach, this research employs uplink\nNon-Orthogonal Multiple Access (NOMA) to reduce message-gathering delays\nbetween transmitters (TXs) and receivers (RXs). The proposed algorithm is\nevaluated to assess effectiveness under exchange delays, analyzing impact of\nsystem parameters like network connectivity, size, sub-bands, etc., on\nsynchronization speed. The findings demonstrate that the NOMA-based\ninformation-gathering technique significantly accelerates network\nsynchronization compared to orthogonal access schemes. This advancement is\ncrucial for meeting the low-latency requirements of beyond fifth generation\n(5G) systems, underscoring the potential of distributed synchronization as a\ncornerstone for next-generation UDN deployments.",
        "published": "2025-04-05T15:02:00+00:00"
    },
    {
        "title": "Interpretable Single-View 3D Gaussian Splatting using Unsupervised Hierarchical Disentangled Representation Learning",
        "authors": [
            "Yuyang Zhang",
            "Baao Xie",
            "Hu Zhu",
            "Qi Wang",
            "Huanting Guo",
            "Xin Jin",
            "Wenjun Zeng"
        ],
        "summary": "Gaussian Splatting (GS) has recently marked a significant advancement in 3D\nreconstruction, delivering both rapid rendering and high-quality results.\nHowever, existing 3DGS methods pose challenges in understanding underlying 3D\nsemantics, which hinders model controllability and interpretability. To address\nit, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to\ndiscover both coarse- and fine-grained 3D semantics via hierarchical\ndisentangled representation learning (DRL). Specifically, the model employs a\ndual-branch architecture, consisting of a point cloud initialization branch and\na triplane-Gaussian generation branch, to achieve coarse-grained\ndisentanglement by separating 3D geometry and visual appearance features.\nSubsequently, fine-grained semantic representations within each modality are\nfurther discovered through DRL-based encoder-adapters. To our knowledge, this\nis the first work to achieve unsupervised interpretable 3DGS. Evaluations\nindicate that our model achieves 3D disentanglement while preserving\nhigh-quality and rapid reconstruction.",
        "published": "2025-04-05T14:42:13+00:00"
    },
    {
        "title": "SDEIT: Semantic-Driven Electrical Impedance Tomography",
        "authors": [
            "Dong Liu",
            "Yuanchao Wu",
            "Bowen Tong",
            "Jiansong Deng"
        ],
        "summary": "Regularization methods using prior knowledge are essential in solving\nill-posed inverse problems such as Electrical Impedance Tomography (EIT).\nHowever, designing effective regularization and integrating prior information\ninto EIT remains challenging due to the complexity and variability of\nanatomical structures. In this work, we introduce SDEIT, a novel\nsemantic-driven framework that integrates Stable Diffusion 3.5 into EIT,\nmarking the first use of large-scale text-to-image generation models in EIT.\nSDEIT employs natural language prompts as semantic priors to guide the\nreconstruction process. By coupling an implicit neural representation (INR)\nnetwork with a plug-and-play optimization scheme that leverages SD-generated\nimages as generative priors, SDEIT improves structural consistency and recovers\nfine details. Importantly, this method does not rely on paired training\ndatasets, increasing its adaptability to varied EIT scenarios. Extensive\nexperiments on both simulated and experimental data demonstrate that SDEIT\noutperforms state-of-the-art techniques, offering superior accuracy and\nrobustness. This work opens a new pathway for integrating multimodal priors\ninto ill-posed inverse problems like EIT.",
        "published": "2025-04-05T14:08:58+00:00"
    },
    {
        "title": "The Ripple Effect of Vulnerabilities in Maven Central: Prevalence, Propagation, and Mitigation Challenges",
        "authors": [
            "Ehtisham Ul Haq",
            "Song Wang",
            "Robert S. Allison"
        ],
        "summary": "The widespread use of package managers like Maven has accelerated software\ndevelopment but has also introduced significant security risks due to\nvulnerabilities in dependencies. In this study, we analyze the prevalence and\nimpact of vulnerabilities within the Maven Central ecosystem, using Common\nVulnerabilities and Exposures (CVE) data from OSV.dev and a subsample enriched\nwith aggregated CVE data (CVE_AGGREGATED), which captures both direct and\ntransitive vulnerabilities. In our subsample of around 4 million releases, we\nfound that while only about 1% of releases have direct vulnerabilities,\napproximately 46.8% are affected by transitive vulnerabilities. This highlights\nhow a small number of vulnerable yet influential artifacts can impact a vast\nportion of the ecosystem. Moreover, our analysis shows that vulnerabilities\npropagate rapidly through dependency networks and that more central artifacts\n(those with a high number of dependents) are not necessarily less vulnerable.\nWe also observed that the time taken to patch vulnerabilities, including those\nof high or critical severity, often spans several years. Additionally, we found\nthat dependents of artifacts tend to prefer presumably non-vulnerable versions;\nhowever, some continue to use vulnerable versions, indicating challenges in\nadopting patched releases. These findings highlight the critical need for\nimproved dependency management practices and timely vulnerability remediation\nto enhance the security of software ecosystems.",
        "published": "2025-04-05T13:45:27+00:00"
    },
    {
        "title": "Learning about the Physical World through Analytic Concepts",
        "authors": [
            "Jianhua Sun",
            "Cewu Lu"
        ],
        "summary": "Reviewing the progress in artificial intelligence over the past decade,\nvarious significant advances (e.g. object detection, image generation, large\nlanguage models) have enabled AI systems to produce more semantically\nmeaningful outputs and achieve widespread adoption in internet scenarios.\nNevertheless, AI systems still struggle when it comes to understanding and\ninteracting with the physical world. This reveals an important issue: relying\nsolely on semantic-level concepts learned from internet data (e.g. texts,\nimages) to understand the physical world is far from sufficient -- machine\nintelligence currently lacks an effective way to learn about the physical\nworld. This research introduces the idea of analytic concept -- representing\nthe concepts related to the physical world through programs of mathematical\nprocedures, providing machine intelligence a portal to perceive, reason about,\nand interact with the physical world. Except for detailing the design\nphilosophy and providing guidelines for the application of analytic concepts,\nthis research also introduce about the infrastructure that has been built\naround analytic concepts. I aim for my research to contribute to addressing\nthese questions: What is a proper abstraction of general concepts in the\nphysical world for machine intelligence? How to systematically integrate\nstructured priors with neural networks to constrain AI systems to comply with\nphysical laws?",
        "published": "2025-04-05T13:22:11+00:00"
    },
    {
        "title": "TFDWT: Fast Discrete Wavelet Transform TensorFlow Layers",
        "authors": [
            "Kishore K. Tarafdar",
            "Vikram M. Gadre"
        ],
        "summary": "TFDWT is an open-source Python library that allows the construction of\nTensorFlow Layers for Fast Discrete Wavelet Transform (DWT) and Inverse\nDiscrete Wavelet Transform (IDWT) in end-to-end backpropagation learning\nnetworks. By definition, a multiresolution signal representation using a\nmulti-rate discrete wavelet system creates enriched joint natural-frequency\nrepresentations. These layers facilitate the construction of multilevel DWT\nfilter banks and Wavelet Packet Transform (WPT) filter banks for a\nspatial-frequency representation of the inputs and features in shallow or deep\nnetworks. The discrete wavelet system partitions the frequency plane into\nsubbands using orthogonal dilated and translated lowpass scaling and highpass\nwavelet function. The realization of a fast discrete wavelet system is a\ntwo-band perfect reconstruction multi-rate filter bank with FIR filters\ncorresponding to the impulse responses of the scaling and wavelet function with\ndownsampling and upsampling operations. A filter bank for a higher dimensional\ninput is a seamless extension by successive separable circular convolutions\nacross each independent axis. The command `pip install TFDWT' installs the\nlatest version of the package.",
        "published": "2025-04-05T13:16:09+00:00"
    },
    {
        "title": "CutQAS: Topology-aware quantum circuit cutting via reinforcement learning",
        "authors": [
            "Abhishek Sadhu",
            "Aritra Sarkar",
            "Akash Kundu"
        ],
        "summary": "Simulating molecular systems on quantum processors has the potential to\nsurpass classical methods in computational resource efficiency. The limited\nqubit connectivity, small processor size, and short coherence times of\nnear-term quantum hardware constrain the applicability of quantum algorithms\nlike QPE and VQE. Quantum circuit cutting mitigates these constraints by\npartitioning large circuits into smaller subcircuits, enabling execution on\nresource-limited devices. However, finding optimal circuit partitions remains a\nsignificant challenge, affecting both computational efficiency and accuracy. To\naddress these limitations, in this article, we propose CutQAS, a novel\nframework that integrates quantum circuit cutting with quantum architecture\nsearch (QAS) to enhance quantum chemistry simulations. Our framework employs a\nmulti-step reinforcement learning (RL) agent to optimize circuit\nconfigurations. First, an RL agent explores all possible topologies to identify\nan optimal circuit structure. Subsequently, a second RL agent refines the\nselected topology by determining optimal circuit cuts, ensuring efficient\nexecution on constrained hardware. Through numerical simulations, we\ndemonstrate the effectiveness of our method in improving simulation accuracy\nand resource efficiency. This approach presents a scalable solution for quantum\nchemistry applications, offering a systematic pathway to overcoming hardware\nconstraints in near-term quantum computing.",
        "published": "2025-04-05T13:13:50+00:00"
    },
    {
        "title": "Data-driven Method to Ensure Cascade Stability of Traffic Load Balancing in O-RAN Based Networks",
        "authors": [
            "Mengbang Zou",
            "Yun Tang",
            "Weisi Guo"
        ],
        "summary": "Load balancing in open radio access networks (O-RAN) is critical for ensuring\nefficient resource utilization, and the user's experience by evenly\ndistributing network traffic load. Current research mainly focuses on designing\nload-balancing algorithms to allocate resources while overlooking the cascade\nstability of load balancing, which is critical to prevent endless handover. The\nmain challenge to analyse the cascade stability lies in the difficulty of\nestablishing an accurate mathematical model to describe the process of load\nbalancing due to its nonlinearity and high-dimensionality. In our previous\ntheoretical work, a simplified general dynamic function was used to analyze the\nstability. However, it is elusive whether this function is close to the reality\nof the load balance process. To solve this problem, 1) a data-driven method is\nproposed to identify the dynamic model of the load balancing process according\nto the real-time traffic load data collected from the radio units (RUs); 2) the\nstability condition of load balancing process is established for the identified\ndynamics model. Based on the identified dynamics model and the stability\ncondition, the RAN Intelligent Controller (RIC) can control RUs to achieve a\ndesired load-balancing state while ensuring cascade stability.",
        "published": "2025-04-05T12:22:59+00:00"
    },
    {
        "title": "Erbium-doped lithium niobate waveguide amplifier enhanced by an inverse-designed on-chip reflector",
        "authors": [
            "Zhiwei Wei",
            "Jiangwei Wu",
            "Chengyu Chen",
            "Hao Li",
            "Wenjie Wan",
            "Xianfeng Chen",
            "Yuping Chen"
        ],
        "summary": "This study presents a 3.6-cm-long erbium-doped lithium niobate waveguide\namplifier enhanced by an inverse-designed on-chip reflector. Integrating the\nreflector at the waveguide end yielded an internal net gain of 40.5 dB,\nachieving a 17.3 dB gain improvement compared to a comparable reflector-free\namplifier under small signal conditions. By eliminating bidirectional pumping\nrequirements, the system complexity was reduced. These results highlight a\nnovel strategy for optimizing integrated optical amplifiers, combining high\ngain with simplified architecture. The approach holds promise for advancing\nhigh-density photonic integrated systems, demonstrating the efficacy of inverse\ndesign in tailoring photonic device performance for practical applications.",
        "published": "2025-04-05T11:51:59+00:00"
    },
    {
        "title": "Introducing COGENT3: An AI Architecture for Emergent Cognition",
        "authors": [
            "Eduardo Salazar"
        ],
        "summary": "This paper presents COGENT3 (or Collective Growth and Entropy-modulated\nTriads System), a novel approach for emergent cognition integrating pattern\nformation networks with group influence dynamics. Contrasting with traditional\nstrategies that rely on predetermined architectures, computational structures\nemerge dynamically in our framework through agent interactions. This enables a\nmore flexible and adaptive system exhibiting characteristics reminiscent of\nhuman cognitive processes. The incorporation of temperature modulation and\nmemory effects in COGENT3 closely integrates statistical mechanics, machine\nlearning, and cognitive science.",
        "published": "2025-04-05T11:05:55+00:00"
    },
    {
        "title": "Predicting Soil Macronutrient Levels: A Machine Learning Approach Models Trained on pH, Conductivity, and Average Power of Acid-Base Solutions",
        "authors": [
            "Mridul Kumar",
            "Deepali Jain",
            "Zeeshan Saifi",
            "Soami Daya Krishnananda"
        ],
        "summary": "Soil macronutrients, particularly potassium ions (K$^+$), are indispensable\nfor plant health, underpinning various physiological and biological processes,\nand facilitating the management of both biotic and abiotic stresses. Deficient\nmacronutrient content results in stunted growth, delayed maturation, and\nincreased vulnerability to environmental stressors, thereby accentuating the\nimperative for precise soil nutrient monitoring. Traditional techniques such as\nchemical assays, atomic absorption spectroscopy, inductively coupled plasma\noptical emission spectroscopy, and electrochemical methods, albeit advanced,\nare prohibitively expensive and time-intensive, thus unsuitable for real-time\nmacronutrient assessment. In this study, we propose an innovative soil testing\nprotocol utilizing a dataset derived from synthetic solutions to model soil\nbehaviour. The dataset encompasses physical properties including conductivity\nand pH, with a concentration on three key macronutrients: nitrogen (N),\nphosphorus (P), and potassium (K). Four machine learning algorithms were\napplied to the dataset, with random forest regressors and neural networks being\nselected for the prediction of soil nutrient concentrations. Comparative\nanalysis with laboratory soil testing results revealed prediction errors of\n23.6% for phosphorus and 16% for potassium using the random forest model, and\n26.3% for phosphorus and 21.8% for potassium using the neural network model.\nThis methodology illustrates a cost-effective and efficacious strategy for\nreal-time soil nutrient monitoring, offering substantial advancements over\nconventional techniques and enhancing the capability to sustain optimal\nnutrient levels conducive to robust crop growth.",
        "published": "2025-04-05T11:04:48+00:00"
    },
    {
        "title": "Model Selection via MCRB Optimization",
        "authors": [
            "Nadav E. Rosenthal",
            "Joseph Tabrikian"
        ],
        "summary": "In many estimation theory and statistical analysis problems, the true data\nmodel is unknown, or partially unknown. To describe the model generating the\ndata, parameterized models of some degree are used. A question that arises is\nwhich model should be used to best approximate the true model, a.k.a. model\nselection. In the field of machine learning, it is encountered in the form of\narchitecture types of neural networks, number of model parameters, etc. In this\npaper, we propose a new model selection criterion, based on the misspecified\nCramer-Rao bound (MCRB) for mean-squared-error (MSE) performance. The criterion\nselects the model in which the bound on the estimated parameters MSE is the\nlowest, compared to other candidate models. Its goal is to minimize the MSE\nwith-respect-to (w.r.t.) the model. The criterion is applied to the problems of\ndirection-of-arrival (DOA) estimation under unknown clutter / interference, and\nspectrum estimation of auto-regressive (AR) model. It is shown to incorporate\nthe bias-variance trade-off, and outperform the Akaike information criterion\n(AIC), finite-sample corrected AIC (AICc), and minimum description length (MDL)\nin terms of MSE performance.",
        "published": "2025-04-05T11:01:51+00:00"
    },
    {
        "title": "Scaling Federated Learning Solutions with Kubernetes for Synthesizing Histopathology Images",
        "authors": [
            "Andrei-Alexandru Preda",
            "Iulian-Marius T\u0103iatu",
            "Dumitru-Clementin Cercel"
        ],
        "summary": "In the field of deep learning, large architectures often obtain the best\nperformance for many tasks, but also require massive datasets. In the\nhistological domain, tissue images are expensive to obtain and constitute\nsensitive medical information, raising concerns about data scarcity and\nprivacy. Vision Transformers are state-of-the-art computer vision models that\nhave proven helpful in many tasks, including image classification. In this\nwork, we combine vision Transformers with generative adversarial networks to\ngenerate histopathological images related to colorectal cancer and test their\nquality by augmenting a training dataset, leading to improved classification\naccuracy. Then, we replicate this performance using the federated learning\ntechnique and a realistic Kubernetes setup with multiple nodes, simulating a\nscenario where the training dataset is split among several hospitals unable to\nshare their information directly due to privacy concerns.",
        "published": "2025-04-05T10:32:56+00:00"
    },
    {
        "title": "EMF: Event Meta Formers for Event-based Real-time Traffic Object Detection",
        "authors": [
            "Muhammad Ahmed Ullah Khan",
            "Abdul Hannan Khan",
            "Andreas Dengel"
        ],
        "summary": "Event cameras have higher temporal resolution, and require less storage and\nbandwidth compared to traditional RGB cameras. However, due to relatively\nlagging performance of event-based approaches, event cameras have not yet\nreplace traditional cameras in performance-critical applications like\nautonomous driving. Recent approaches in event-based object detection try to\nbridge this gap by employing computationally expensive transformer-based\nsolutions. However, due to their resource-intensive components, these solutions\nfail to exploit the sparsity and higher temporal resolution of event cameras\nefficiently. Moreover, these solutions are adopted from the vision domain,\nlacking specificity to the event cameras. In this work, we explore efficient\nand performant alternatives to recurrent vision transformer models and propose\na novel event-based object detection backbone. The proposed backbone employs a\nnovel Event Progression Extractor module, tailored specifically for event data,\nand uses Metaformer concept with convolution-based efficient components. We\nevaluate the resultant model on well-established traffic object detection\nbenchmarks and conduct cross-dataset evaluation to test its ability to\ngeneralize. The proposed model outperforms the state-of-the-art on Prophesee\nGen1 dataset by 1.6 mAP while reducing inference time by 14%. Our proposed EMF\nbecomes the fastest DNN-based architecture in the domain by outperforming most\nefficient event-based object detectors. Moreover, the proposed model shows\nbetter ability to generalize to unseen data and scales better with the\nabundance of data.",
        "published": "2025-04-05T09:48:40+00:00"
    },
    {
        "title": "A Primal-Dual Gradient Descent Approach to the Connectivity Constrained Sensor Coverage Problem",
        "authors": [
            "Mathias Bock Agerman",
            "Ziqiao Zhang",
            "Jong Gwang Kim",
            "Shreyas Sundaram",
            "Christopher Brinton"
        ],
        "summary": "Sensor networks play a critical role in many situational awareness\napplications. In this paper, we study the problem of determining sensor\nplacements to balance coverage and connectivity objectives over a target\nregion. Leveraging algebraic graph theory, we formulate a novel optimization\nproblem to maximize sensor coverage over a spatial probability density of event\nlikelihoods while adhering to connectivity constraints. To handle the resulting\nnon-convexity under constraints, we develop an augmented Lagrangian-based\ngradient descent algorithm inspired by recent approaches to efficiently\nidentify points satisfying the Karush-Kuhn-Tucker (KKT) conditions. We\nestablish convergence guarantees by showing necessary assumptions are satisfied\nin our setup, including employing Mangasarian-Fromowitz constraint\nqualification to prove the existence of a KKT point. Numerical simulations\nunder different probability densities demonstrate that the optimized sensor\nnetworks effectively cover high-priority regions while satisfying desired\nconnectivity constraints.",
        "published": "2025-04-05T09:40:11+00:00"
    },
    {
        "title": "Transformer representation learning is necessary for dynamic multi-modal physiological data on small-cohort patients",
        "authors": [
            "Bingxu Wang",
            "Kunzhi Cai",
            "Yuqi Zhang",
            "Yachong Guo"
        ],
        "summary": "Postoperative delirium (POD), a severe neuropsychiatric complication\naffecting nearly 50% of high-risk surgical patients, is defined as an acute\ndisorder of attention and cognition, It remains significantly underdiagnosed in\nthe intensive care units (ICUs) due to subjective monitoring methods. Early and\naccurate diagnosis of POD is critical and achievable. Here, we propose a POD\nprediction framework comprising a Transformer representation model followed by\ntraditional machine learning algorithms. Our approaches utilizes multi-modal\nphysiological data, including amplitude-integrated electroencephalography\n(aEEG), vital signs, electrocardiographic monitor data as well as hemodynamic\nparameters. We curated the first multi-modal POD dataset encompassing two\npatient types and evaluated the various Transformer architectures for\nrepresentation learning. Empirical results indicate a consistent improvements\nof sensitivity and Youden index in patient TYPE I using Transformer\nrepresentations, particularly our fusion adaptation of Pathformer. By enabling\neffective delirium diagnosis from postoperative day 1 to 3, our extensive\nexperimental findings emphasize the potential of multi-modal physiological data\nand highlight the necessity of representation learning via multi-modal\nTransformer architecture in clinical diagnosis.",
        "published": "2025-04-05T09:31:39+00:00"
    },
    {
        "title": "Overcoming the Identity Mapping Problem in Self-Supervised Hyperspectral Anomaly Detection",
        "authors": [
            "Yongchuan Cui",
            "Jinhe Zhang",
            "Peng Liu",
            "Weijing Song",
            "Yi Zeng"
        ],
        "summary": "The surge of deep learning has catalyzed considerable progress in\nself-supervised Hyperspectral Anomaly Detection (HAD). The core premise for\nself-supervised HAD is that anomalous pixels are inherently more challenging to\nreconstruct, resulting in larger errors compared to the background. However,\nowing to the powerful nonlinear fitting capabilities of neural networks,\nself-supervised models often suffer from the Identity Mapping Problem (IMP).\nThe IMP manifests as a tendency for the model to overfit to the entire image,\nparticularly with increasing network complexity or prolonged training\niterations. Consequently, the whole image can be precisely reconstructed, and\neven the anomalous pixels exhibit imperceptible errors, making them difficult\nto detect. Despite the proposal of several models aimed at addressing the\nIMP-related issues, a unified descriptive framework and validation of solutions\nfor IMP remain lacking. In this paper, we conduct an in-depth exploration to\nIMP, and summarize a unified framework that describes IMP from the perspective\nof network optimization, which encompasses three aspects: perturbation,\nreconstruction, and regularization. Correspondingly, we introduce three\nsolutions: superpixel pooling and uppooling for perturbation, error-adaptive\nconvolution for reconstruction, and online background pixel mining for\nregularization. With extensive experiments being conducted to validate the\neffectiveness, it is hoped that our work will provide valuable insights and\ninspire further research for self-supervised HAD. Code:\n\\url{https://github.com/yc-cui/Super-AD}.",
        "published": "2025-04-05T09:12:25+00:00"
    },
    {
        "title": "Minimax Optimal Convergence of Gradient Descent in Logistic Regression via Large and Adaptive Stepsizes",
        "authors": [
            "Ruiqi Zhang",
            "Jingfeng Wu",
            "Licong Lin",
            "Peter L. Bartlett"
        ],
        "summary": "We study $\\textit{gradient descent}$ (GD) for logistic regression on linearly\nseparable data with stepsizes that adapt to the current risk, scaled by a\nconstant hyperparameter $\\eta$. We show that after at most $1/\\gamma^2$ burn-in\nsteps, GD achieves a risk upper bounded by $\\exp(-\\Theta(\\eta))$, where\n$\\gamma$ is the margin of the dataset. As $\\eta$ can be arbitrarily large, GD\nattains an arbitrarily small risk $\\textit{immediately after the burn-in\nsteps}$, though the risk evolution may be $\\textit{non-monotonic}$.\n  We further construct hard datasets with margin $\\gamma$, where any batch or\nonline first-order method requires $\\Omega(1/\\gamma^2)$ steps to find a linear\nseparator. Thus, GD with large, adaptive stepsizes is $\\textit{minimax\noptimal}$ among first-order batch methods. Notably, the classical\n$\\textit{Perceptron}$ (Novikoff, 1962), a first-order online method, also\nachieves a step complexity of $1/\\gamma^2$, matching GD even in constants.\n  Finally, our GD analysis extends to a broad class of loss functions and\ncertain two-layer networks.",
        "published": "2025-04-05T08:34:20+00:00"
    },
    {
        "title": "LATTE: Lightweight Attention-based Traffic Accident Anticipation Engine",
        "authors": [
            "Jiaxun Zhang",
            "Yanchen Guan",
            "Chengyue Wang",
            "Haicheng Liao",
            "Guohui Zhang",
            "Zhenning Li"
        ],
        "summary": "Accurately predicting traffic accidents in real-time is a critical challenge\nin autonomous driving, particularly in resource-constrained environments.\nExisting solutions often suffer from high computational overhead or fail to\nadequately address the uncertainty of evolving traffic scenarios. This paper\nintroduces LATTE, a Lightweight Attention-based Traffic Accident Anticipation\nEngine, which integrates computational efficiency with state-of-the-art\nperformance. LATTE employs Efficient Multiscale Spatial Aggregation (EMSA) to\ncapture spatial features across scales, Memory Attention Aggregation (MAA) to\nenhance temporal modeling, and Auxiliary Self-Attention Aggregation (AAA) to\nextract latent dependencies over extended sequences. Additionally, LATTE\nincorporates the Flamingo Alert-Assisted System (FAA), leveraging a\nvision-language model to provide real-time, cognitively accessible verbal\nhazard alerts, improving passenger situational awareness. Evaluations on\nbenchmark datasets (DAD, CCD, A3D) demonstrate LATTE's superior predictive\ncapabilities and computational efficiency. LATTE achieves state-of-the-art\n89.74% Average Precision (AP) on DAD benchmark, with 5.4% higher mean\nTime-To-Accident (mTTA) than the second-best model, and maintains competitive\nmTTA at a Recall of 80% (TTA@R80) (4.04s) while demonstrating robust accident\nanticipation across diverse driving conditions. Its lightweight design delivers\na 93.14% reduction in floating-point operations (FLOPs) and a 31.58% decrease\nin parameter count (Params), enabling real-time operation on resource-limited\nhardware without compromising performance. Ablation studies confirm the\neffectiveness of LATTE's architectural components, while visualizations and\nfailure case analyses highlight its practical applicability and areas for\nenhancement. Our codes are available at https://github.com/icypear/LATTE.git.",
        "published": "2025-04-05T08:26:50+00:00"
    },
    {
        "title": "Quantum phase classification via quantum hypothesis testing",
        "authors": [
            "Akira Tanji",
            "Hiroshi Yano",
            "Naoki Yamamoto"
        ],
        "summary": "Quantum phase classification is a fundamental problem in quantum many-body\nphysics, traditionally approached using order parameters or quantum machine\nlearning techniques such as quantum convolutional neural networks (QCNNs).\nHowever, these methods often require extensive prior knowledge of the system or\nlarge numbers of quantum state copies for reliable classification. In this\nwork, we propose a classification algorithm based on the quantum Neyman-Pearson\ntest, which is theoretically optimal for distinguishing between two quantum\nstates. While directly constructing the quantum Neyman-Pearson test for\nmany-body systems via full state tomography is intractable due to the\nexponential growth of the Hilbert space, we introduce a partitioning strategy\nthat applies hypothesis tests to subsystems rather than the entire state,\neffectively reducing the required number of quantum state copies while\nmaintaining classification accuracy. We validate our approach through numerical\nsimulations, demonstrating its advantages over conventional methods, including\nthe order parameter-based classifier and the QCNN. Our results show that the\nproposed method achieves lower classification error probabilities and\nsignificantly reduces the training cost compared to the QCNN and the recently\ndeveloped classical machine learning algorithm enhanced with quantum data,\nwhile maintaining high scalability up to systems with 81 qubits. These findings\nhighlight the potential of quantum hypothesis testing as a powerful tool for\nquantum phase classification, particularly in experimental settings where\nquantum measurements are combined with classical post-processing.",
        "published": "2025-04-05T08:23:45+00:00"
    },
    {
        "title": "Formation of Giant Planets",
        "authors": [
            "Masahiro Ikoma",
            "Hiroshi Kobayashi"
        ],
        "summary": "Gas giant planets, if present, are the most massive objects in a planetary\nsystem and play a pivotal role in shaping its overall architecture. The\nformation of these planets has constantly been a central issue in planetary\nscience. Increasing evidence from spacecraft explorations of Jupiter and\nSaturn, as well as telescope observations of exoplanets, has provided new\nconstraints on the formation process of gas giant planets. The classic\nchallenge of explaining formation timescales still remains a significant issue,\nwhile new constraints on planetary interiors have introduced additional\ncomplexities. Recent shifts away from the single-size planetesimal hypothesis,\nnevertheless, show promise in resolving these problems. Additionally, various\ndiscoveries regarding exoplanets have led to theoretical improvements, while\nthe discovery of numerous super-Earths and sub-Neptunes has posed new\nchallenges in understanding gas accretion. This review synthesizes the latest\ntheoretical advancements, discussing resolved issues and emerging challenges in\ngiant planet formation.",
        "published": "2025-04-05T07:32:49+00:00"
    },
    {
        "title": "Scalable Robust Bayesian Co-Clustering with Compositional ELBOs",
        "authors": [
            "Ashwin Vinod",
            "Chandrajit Bajaj"
        ],
        "summary": "Co-clustering exploits the duality of instances and features to\nsimultaneously uncover meaningful groups in both dimensions, often\noutperforming traditional clustering in high-dimensional or sparse data\nsettings. Although recent deep learning approaches successfully integrate\nfeature learning and cluster assignment, they remain susceptible to noise and\ncan suffer from posterior collapse within standard autoencoders. In this paper,\nwe present the first fully variational Co-clustering framework that directly\nlearns row and column clusters in the latent space, leveraging a doubly\nreparameterized ELBO to improve gradient signal-to-noise separation. Our\nunsupervised model integrates a Variational Deep Embedding with a Gaussian\nMixture Model (GMM) prior for both instances and features, providing a built-in\nclustering mechanism that naturally aligns latent modes with row and column\nclusters. Furthermore, our regularized end-to-end noise learning Compositional\nELBO architecture jointly reconstructs the data while regularizing against\nnoise through the KL divergence, thus gracefully handling corrupted or missing\ninputs in a single training pipeline. To counteract posterior collapse, we\nintroduce a scale modification that increases the encoder's latent means only\nin the reconstruction pathway, preserving richer latent representations without\ninflating the KL term. Finally, a mutual information-based cross-loss ensures\ncoherent co-clustering of rows and columns. Empirical results on diverse\nreal-world datasets from multiple modalities, numerical, textual, and\nimage-based, demonstrate that our method not only preserves the advantages of\nprior Co-clustering approaches but also exceeds them in accuracy and\nrobustness, particularly in high-dimensional or noisy settings.",
        "published": "2025-04-05T06:48:05+00:00"
    },
    {
        "title": "Collaboration and Controversy Among Experts: Rumor Early Detection by Tuning a Comment Generator",
        "authors": [
            "Bing Wang",
            "Bingrui Zhao",
            "Ximing Li",
            "Changchun Li",
            "Wanfu Gao",
            "Shengsheng Wang"
        ],
        "summary": "Over the past decade, social media platforms have been key in spreading\nrumors, leading to significant negative impacts. To counter this, the community\nhas developed various Rumor Detection (RD) algorithms to automatically identify\nthem using user comments as evidence. However, these RD methods often fail in\nthe early stages of rumor propagation when only limited user comments are\navailable, leading the community to focus on a more challenging topic named\nRumor Early Detection (RED). Typically, existing RED methods learn from limited\nsemantics in early comments. However, our preliminary experiment reveals that\nthe RED models always perform best when the number of training and test\ncomments is consistent and extensive. This inspires us to address the RED issue\nby generating more human-like comments to support this hypothesis. To implement\nthis idea, we tune a comment generator by simulating expert collaboration and\ncontroversy and propose a new RED framework named CAMERED. Specifically, we\nintegrate a mixture-of-expert structure into a generative language model and\npresent a novel routing network for expert collaboration. Additionally, we\nsynthesize a knowledgeable dataset and design an adversarial learning strategy\nto align the style of generated comments with real-world comments. We further\nintegrate generated and original comments with a mutual controversy fusion\nmodule. Experimental results show that CAMERED outperforms state-of-the-art RED\nbaseline models and generation methods, demonstrating its effectiveness.",
        "published": "2025-04-05T06:21:01+00:00"
    },
    {
        "title": "Curvature accelerated decentralized non-convex optimization for high-dimensional machine learning problems",
        "authors": [
            "Dingran Yi",
            "Fanhao Zeng",
            "Nikolaos M. Freris"
        ],
        "summary": "We consider distributed optimization as motivated by machine learning in a\nmulti-agent system: each agent holds local data and the goal is to minimize an\naggregate loss function over a common model, via an interplay of local training\nand distributed communication. In the most interesting case of training a\nneural network, the loss functions are non-convex and the high dimension of the\nmodel poses challenges in terms of communication and computation. We propose a\nprimal-dual method that leverages second order information in the local\ntraining sub-problems in order to accelerate the algorithm. To ease the\ncomputational burden, we invoke a quasi-Newton local solver with linear cost in\nthe model dimension. Besides, our method is communication efficient in the\nsense of requiring to broadcast the local model only once per round. We\nrigorously establish the convergence of the algorithm and demonstrate its\nmerits by numerical experiments.",
        "published": "2025-04-05T06:11:57+00:00"
    },
    {
        "title": "Performance Analysis of Deep Learning Models for Femur Segmentation in MRI Scan",
        "authors": [
            "Mengyuan Liu",
            "Yixiao Chen",
            "Anning Tian",
            "Xinmeng Wu",
            "Mozhi Shen",
            "Tianchou Gong",
            "Jeongkyu Lee"
        ],
        "summary": "Convolutional neural networks like U-Net excel in medical image segmentation,\nwhile attention mechanisms and KAN enhance feature extraction. Meta's SAM 2\nuses Vision Transformers for prompt-based segmentation without fine-tuning.\nHowever, biases in these models impact generalization with limited data. In\nthis study, we systematically evaluate and compare the performance of three\nCNN-based models, i.e., U-Net, Attention U-Net, and U-KAN, and one\ntransformer-based model, i.e., SAM 2 for segmenting femur bone structures in\nMRI scan. The dataset comprises 11,164 MRI scans with detailed annotations of\nfemoral regions. Performance is assessed using the Dice Similarity Coefficient,\nwhich ranges from 0.932 to 0.954. Attention U-Net achieves the highest overall\nscores, while U-KAN demonstrated superior performance in anatomical regions\nwith a smaller region of interest, leveraging its enhanced learning capacity to\nimprove segmentation accuracy.",
        "published": "2025-04-05T05:47:56+00:00"
    },
    {
        "title": "Analysis of Light-Weight Cryptography Algorithms for UAV-Networks",
        "authors": [
            "Aanchal Patel",
            "Aswani Kumar Cherukuri"
        ],
        "summary": "Unmanned Aerial Vehicles are increasingly utilized across various domains,\nnecessitating robust security measures for their communication networks. The\nASCON family, a NIST finalist in lightweight cryptography standards, is known\nfor its simplistic yet resilient design, making it well-suited for\nresource-constrained environments characterized by limited processing\ncapabilities and energy reservoirs. This study focuses on understanding the\nintegration and assessment of the ASCON encryption algorithm in UAV networks,\nemphasizing its potential as a lightweight and efficient cryptographic\nsolution. The research objectives aim to evaluate ASCON variants' effectiveness\nin providing security comparable to AES-128 while exhibiting lower\ncomputational cost and energy consumption within simulated UAV network\nenvironments. Comparative analysis assesses performance metrics such as\nencryption and decryption speeds, resource utilization, and resistance to\ncryptographic vulnerabilities against established algorithms like AES.\nPerformance metrics, including peak and average execution times, overall\nthroughput, and security properties against various cryptographic attacks, are\nmeasured and analysed to determine the most suitable cryptographic algorithm\nfor UAV communication systems. Performance results indicate that ASCON-128a as\nthe optimal choice for UAV communication systems requiring a balance between\nefficiency and security. Its superior performance metrics, robust security\nproperties, and suitability for resource-constrained environments position it\nas the preferred solution for securing UAV communication networks. By\nleveraging the strengths of ASCON-128a, UAV communication systems can achieve\noptimal performance and security, ensuring reliable and secure communication in\nchallenging operational environments.",
        "published": "2025-04-05T05:27:17+00:00"
    },
    {
        "title": "Mapping at First Sense: A Lightweight Neural Network-Based Indoor Structures Prediction Method for Robot Autonomous Exploration",
        "authors": [
            "Haojia Gao",
            "Haohua Que",
            "Kunrong Li",
            "Weihao Shan",
            "Mingkai Liu",
            "Rong Zhao",
            "Lei Mu",
            "Xinghua Yang",
            "Qi Wei",
            "Fei Qiao"
        ],
        "summary": "Autonomous exploration in unknown environments is a critical challenge in\nrobotics, particularly for applications such as indoor navigation, search and\nrescue, and service robotics. Traditional exploration strategies, such as\nfrontier-based methods, often struggle to efficiently utilize prior knowledge\nof structural regularities in indoor spaces. To address this limitation, we\npropose Mapping at First Sense, a lightweight neural network-based approach\nthat predicts unobserved areas in local maps, thereby enhancing exploration\nefficiency. The core of our method, SenseMapNet, integrates convolutional and\ntransformerbased architectures to infer occluded regions while maintaining\ncomputational efficiency for real-time deployment on resourceconstrained\nrobots. Additionally, we introduce SenseMapDataset, a curated dataset\nconstructed from KTH and HouseExpo environments, which facilitates training and\nevaluation of neural models for indoor exploration. Experimental results\ndemonstrate that SenseMapNet achieves an SSIM (structural similarity) of 0.78,\nLPIPS (perceptual quality) of 0.68, and an FID (feature distribution alignment)\nof 239.79, outperforming conventional methods in map reconstruction quality.\nCompared to traditional frontier-based exploration, our method reduces\nexploration time by 46.5% (from 2335.56s to 1248.68s) while maintaining a high\ncoverage rate (88%) and achieving a reconstruction accuracy of 88%. The\nproposed method represents a promising step toward efficient, learning-driven\nrobotic exploration in structured environments.",
        "published": "2025-04-05T05:19:09+00:00"
    },
    {
        "title": "Deep-Learning-Directed Preventive Dynamic Security Control via Coordinated Demand Response",
        "authors": [
            "Amin Masoumi",
            "Mert Korkali"
        ],
        "summary": "Unlike common faults, three-phase short-circuit faults in power systems pose\nsignificant challenges. These faults can lead to out-of-step (OOS) conditions\nand jeopardize the system's dynamic security. The rapid dynamics of these\nfaults often exceed the time of protection actions, thus limiting the\neffectiveness of corrective schemes. This paper proposes an end-to-end\ndeep-learning-based mechanism, namely, a convolutional neural network with an\nattention mechanism, to predict OOS conditions early and enhance the system's\nfault resilience. The results of the study demonstrate the effectiveness of the\nproposed algorithm in terms of early prediction and robustness against such\nfaults in various operating conditions.",
        "published": "2025-04-05T04:46:36+00:00"
    },
    {
        "title": "SageNet: Fast Neural Network Emulation of the Stiff-amplified Gravitational Waves from Inflation",
        "authors": [
            "Fan Zhang",
            "Yifang Luo",
            "Bohua Li",
            "Ruihan Cao",
            "Wenjin Peng",
            "Joel Meyers",
            "Paul R. Shapiro"
        ],
        "summary": "Accurate modeling of the inflationary gravitational waves (GWs) requires\ntime-consuming, iterative numerical integrations of differential equations to\ntake into account their backreaction on the expansion history. To improve\ncomputational efficiency while preserving accuracy, we present SageNet\n(Stiff-Amplified Gravitational-wave Emulator Network), a deep learning\nframework designed to replace conventional numerical solvers. SageNet employs a\nLong Short-Term Memory architecture to emulate the present-day energy density\nspectrum of the inflationary GWs with possible stiff amplification,\n$\\Omega_\\mathrm{GW}(f)$. Trained on a data set of 25,689 numerically generated\nsolutions, SageNet allows accurate reconstructions of $\\Omega_\\mathrm{GW}(f)$\nand generalizes well to a wide range of cosmological parameters; 89.3% of the\ntest emulations with randomly distributed parameters exhibit errors of under\n4%. In addition, SageNet demonstrates its ability to learn and reproduce the\nartificial, adaptive sampling patterns in numerical calculations, which\nimplement denser sampling of frequencies around changes of spectral indices in\n$\\Omega_\\mathrm{GW}(f)$. The dual capability of learning both physical and\nartificial features of the numerical GW spectra establishes SageNet as a robust\nalternative to exact numerical methods. Finally, our benchmark tests show that\nSageNet reduces the computation time from tens of seconds to milliseconds,\nachieving a speed-up of ~$10^4$ times over standard CPU-based numerical solvers\nwith the potential for further acceleration on GPU hardware. These capabilities\nmake SageNet a powerful tool for accelerating Bayesian inference procedures for\nextended cosmological models. In a broad sense, the SageNet framework offers a\nfast, accurate, and generalizable solution to modeling cosmological observables\nwhose theoretical predictions demand costly differential equation solvers.",
        "published": "2025-04-05T04:17:15+00:00"
    },
    {
        "title": "PIORF: Physics-Informed Ollivier-Ricci Flow for Long-Range Interactions in Mesh Graph Neural Networks",
        "authors": [
            "Youn-Yeol Yu",
            "Jeongwhan Choi",
            "Jaehyeon Park",
            "Kookjin Lee",
            "Noseong Park"
        ],
        "summary": "Recently, data-driven simulators based on graph neural networks have gained\nattention in modeling physical systems on unstructured meshes. However, they\nstruggle with long-range dependencies in fluid flows, particularly in refined\nmesh regions. This challenge, known as the 'over-squashing' problem, hinders\ninformation propagation. While existing graph rewiring methods address this\nissue to some extent, they only consider graph topology, overlooking the\nunderlying physical phenomena. We propose Physics-Informed Ollivier-Ricci Flow\n(PIORF), a novel rewiring method that combines physical correlations with graph\ntopology. PIORF uses Ollivier-Ricci curvature (ORC) to identify bottleneck\nregions and connects these areas with nodes in high-velocity gradient nodes,\nenabling long-range interactions and mitigating over-squashing. Our approach is\ncomputationally efficient in rewiring edges and can scale to larger\nsimulations. Experimental results on 3 fluid dynamics benchmark datasets show\nthat PIORF consistently outperforms baseline models and existing rewiring\nmethods, achieving up to 26.2 improvement.",
        "published": "2025-04-05T04:14:05+00:00"
    },
    {
        "title": "UCS: A Universal Model for Curvilinear Structure Segmentation",
        "authors": [
            "Dianshuo Li",
            "Li Chen",
            "Yunxiang Cao",
            "Kai Zhu",
            "Jun Cheng"
        ],
        "summary": "Curvilinear structure segmentation (CSS) is vital in various domains,\nincluding medical imaging, landscape analysis, industrial surface inspection,\nand plant analysis. While existing methods achieve high performance within\nspecific domains, their generalizability is limited. On the other hand,\nlarge-scale models such as Segment Anything Model (SAM) exhibit strong\ngeneralization but are not optimized for curvilinear structures. Existing\nadaptations of SAM primarily focus on general object segmentation and lack\nspecialized design for CSS tasks. To bridge this gap, we propose the Universal\nCurvilinear structure Segmentation (\\textit{UCS}) model, which adapts SAM to\nCSS tasks while enhancing its generalization. \\textit{UCS} features a novel\nencoder architecture integrating a pretrained SAM encoder with two innovations:\na Sparse Adapter, strategically inserted to inherit the pre-trained SAM\nencoder's generalization capability while minimizing the number of fine-tuning\nparameters, and a Prompt Generation module, which leverages Fast Fourier\nTransform with a high-pass filter to generate curve-specific prompts.\nFurthermore, the \\textit{UCS} incorporates a mask decoder that eliminates\nreliance on manual interaction through a dual-compression module: a\nHierarchical Feature Compression module, which aggregates the outputs of the\nsampled encoder to enhance detail preservation, and a Guidance Feature\nCompression module, which extracts and compresses image-driven guidance\nfeatures. Evaluated on a comprehensive multi-domain dataset, including an\nin-house dataset covering eight natural curvilinear structures, \\textit{UCS}\ndemonstrates state-of-the-art generalization and open-set segmentation\nperformance across medical, engineering, natural, and plant imagery,\nestablishing a new benchmark for universal CSS.",
        "published": "2025-04-05T03:05:04+00:00"
    },
    {
        "title": "Quantum Adaptive Self-Attention for Quantum Transformer Models",
        "authors": [
            "Chi-Sheng Chen",
            "En-Jui Kuo"
        ],
        "summary": "Transformer models have revolutionized sequential learning across various\ndomains, yet their self-attention mechanism incurs quadratic computational\ncost, posing limitations for real-time and resource-constrained tasks. To\naddress this, we propose Quantum Adaptive Self-Attention (QASA), a novel hybrid\narchitecture that enhances classical Transformer models with a quantum\nattention mechanism. QASA replaces dot-product attention with a parameterized\nquantum circuit (PQC) that adaptively captures inter-token relationships in the\nquantum Hilbert space. Additionally, a residual quantum projection module is\nintroduced before the feedforward network to further refine temporal features.\nOur design retains classical efficiency in earlier layers while injecting\nquantum expressiveness in the final encoder block, ensuring compatibility with\ncurrent NISQ hardware. Experiments on synthetic time-series tasks demonstrate\nthat QASA achieves faster convergence and superior generalization compared to\nboth standard Transformers and reduced classical variants. Preliminary\ncomplexity analysis suggests potential quantum advantages in gradient\ncomputation, opening new avenues for efficient quantum deep learning models.",
        "published": "2025-04-05T02:52:37+00:00"
    },
    {
        "title": "Simultaneous Motion And Noise Estimation with Event Cameras",
        "authors": [
            "Shintaro Shiba",
            "Yoshimitsu Aoki",
            "Guillermo Gallego"
        ],
        "summary": "Event cameras are emerging vision sensors, whose noise is challenging to\ncharacterize. Existing denoising methods for event cameras consider other tasks\nsuch as motion estimation separately (i.e., sequentially after denoising).\nHowever, motion is an intrinsic part of event data, since scene edges cannot be\nsensed without motion. This work proposes, to the best of our knowledge, the\nfirst method that simultaneously estimates motion in its various forms (e.g.,\nego-motion, optical flow) and noise. The method is flexible, as it allows\nreplacing the 1-step motion estimation of the widely-used Contrast Maximization\nframework with any other motion estimator, such as deep neural networks. The\nexperiments show that the proposed method achieves state-of-the-art results on\nthe E-MLB denoising benchmark and competitive results on the DND21 benchmark,\nwhile showing its efficacy on motion estimation and intensity reconstruction\ntasks. We believe that the proposed approach contributes to strengthening the\ntheory of event-data denoising, as well as impacting practical denoising\nuse-cases, as we release the code upon acceptance. Project page:\nhttps://github.com/tub-rip/ESMD",
        "published": "2025-04-05T02:47:40+00:00"
    },
    {
        "title": "ASDO: An Efficient Algorithm for Traffic Engineering in Large-Scale Data Center Network",
        "authors": [
            "Yingming Mao",
            "Qiaozhu Zhai",
            "Yuzhou Zhou",
            "Ximeng Liu",
            "Zhen Yao",
            "Xia Zhu"
        ],
        "summary": "Rapid growth of data center networks (DCNs) poses significant challenges for\nlarge-scale traffic engineering (TE). Existing acceleration strategies, which\nrely on commercial solvers or deep learning, face scalability issues and\nstruggle with degrading performance or long computational time. Unlike existing\nalgorithms adopting parallel strategies, we propose Alternate\nSource-Destination Optimization (ASDO), a sequential algorithm for TE. ASDO\ndecomposes the problem into subproblems, each focused on adjusting the split\nratios for a specific source-destination (SD) demand while keeping others\nfixed. To enhance the efficiency of subproblem optimization, we design a\nBalanced Binary Search Method (BBSM), which identifies the most balanced split\nratios among multiple solutions that minimize Maximum Link Utilization (MLU).\nASDO dynamically updates the sequence of SDs based on real-time utilization,\nwhich accelerates convergence and enhances solution quality. We evaluate ASDO\non Meta DCNs and two wide-area networks (WANs). In a Meta topology, ASDO\nachieves a 65% and 60% reduction in normalized MLU compared to TEAL and POP,\ntwo state-of-the-art TE acceleration methods, while delivering a $12\\times$\nspeedup over POP. These results demonstrate the superior performance of ASDO in\nlarge-scale TE.",
        "published": "2025-04-05T02:43:37+00:00"
    },
    {
        "title": "Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer",
        "authors": [
            "Daniel Rivera",
            "Jacob Huddin",
            "Alexander Banerjee",
            "Rongzhen Zhang",
            "Brenda Mai",
            "Hanadi El Achi",
            "Jacob Armstrong",
            "Amer Wahed",
            "Andy Nguyen"
        ],
        "summary": "Recently, vision transformers were shown to be capable of outperforming\nconvolutional neural networks when pretrained on sufficiently large datasets.\nVision transformer models show good accuracy on large scale datasets, with\nfeatures of multi-modal training. Due to their promising feature detection, we\naim to explore vision transformer models for diagnosis of anaplastic large cell\nlymphoma versus classical Hodgkin lymphoma using pathology whole slide images\nof HE slides. We compared the classification performance of the vision\ntransformer to our previously designed convolutional neural network on the same\ndataset. The dataset includes whole slide images of HE slides for 20 cases,\nincluding 10 cases in each diagnostic category. From each whole slide image, 60\nimage patches having size of 100 by 100 pixels and at magnification of 20 were\nobtained to yield 1200 image patches, from which 90 percent were used for\ntraining, 9 percent for validation, and 10 percent for testing. The test\nresults from the convolutional neural network model had previously shown an\nexcellent diagnostic accuracy of 100 percent. The test results from the vision\ntransformer model also showed a comparable accuracy at 100 percent. To the best\nof the authors' knowledge, this is the first direct comparison of predictive\nperformance between a vision transformer model and a convolutional neural\nnetwork model using the same dataset of lymphoma. Overall, convolutional neural\nnetwork has a more mature architecture than vision transformer and is usually\nthe best choice when large scale pretraining is not an available option.\nNevertheless, our current study shows comparable and excellent accuracy of\nvision transformer compared to that of convolutional neural network even with a\nrelatively small dataset of anaplastic large cell lymphoma and classical\nHodgkin lymphoma.",
        "published": "2025-04-05T02:33:34+00:00"
    },
    {
        "title": "Mode Participation and Inter-Area-Observability Blocking Controllers for Power Networks",
        "authors": [
            "Rajasekhar Anguluri",
            "Abdullah Al Maruf"
        ],
        "summary": "In recent papers [1] and [2], the second author developed full-state feedback\ncontrollers for networked systems to block the observability and\ncontrollability of certain remote nodes. In this paper, we build on these\ncontrol schemes to an interconnected power system with the aims of blocking (i)\nmode participation factors and (ii) inter-area mode observability in tie-line\npower flow measurements. Since participation factors depend on both\ncontrollable and observable eigenvectors, the control techniques from the cited\nworks must be carefully tailored to this setting. Our research is motivated by\ncyber-security concerns in power systems, where an adversary aims to deceive\nthe operator by tampering the system's modal content. We present extensive\nnumerical results on a 3-machine, 9-bus system and a 16-machine, 68-bus system.",
        "published": "2025-04-05T02:26:39+00:00"
    },
    {
        "title": "Orbital-selective band modifications in a charge-ordered kagome metal LuNb$_6$Sn$_6$",
        "authors": [
            "Rui Lou",
            "Yumeng Zhang",
            "Erjian Cheng",
            "Xiaolong Feng",
            "Alexander Fedorov",
            "Zongkai Li",
            "Yixuan Luo",
            "Alexander Generalov",
            "Haiyang Ma",
            "Quanxing Wei",
            "Yi Zhou",
            "Susmita Changdar",
            "Walter Schnelle",
            "Dong Chen",
            "Yulin Chen",
            "Jianpeng Liu",
            "Yanfeng Guo",
            "Sergey Borisenko",
            "Denis V. Vyalikh",
            "Claudia Felser",
            "Bernd B\u00fcchner",
            "Zhongkai Liu"
        ],
        "summary": "The origin of the charge order in kagome lattice materials has attracted\ngreat interest due to the unique electronic structure features connected to\nkagome networks and the interplay between electron and lattice degrees of\nfreedom. Recently, compounds with composition $Ln$Nb$_6$Sn$_6$ ($Ln$ = Ce-Nd,\nSm, Gd-Tm, Lu, Y) appear as a new family of kagome metals, structurally\nanalogous to $R$V$_6$Sn$_6$ ($R$ = Sc, Y, or rare earth) systems. Among them,\nLuNb$_6$Sn$_6$ emerges as a novel material hosting charge density wave (CDW)\nwith a $\\sqrt{3}$ $\\times$ $\\sqrt{3}$ $\\times$ $3$ wave vector, akin to that in\nScV$_6$Sn$_6$. Here, we employ high-resolution angle-resolved photoemission\nspectroscopy, scanning tunneling microscopy, and density functional theory\ncalculations to systematically investigate the electronic properties of\nLuNb$_6$Sn$_6$. Our observation reveals the characteristic band structures of\nthe \"166\" kagome system. A charge instability driven by Fermi surface nesting\nis decisively ruled out through an analysis of the interactions between van\nHove singularities. Across the CDW transition, we observe orbital-selective\nband modifications, with noticeable evolutions of Lu 5$d$ and Sn 5$p$\nelectrons, while Nb 4$d$ electrons exhibit minimal change, suggesting that the\nLu and Sn sites other than the Nb kagome lattice play a key role in the\nformation of CDW. Our findings substantiate a universal lattice-driven CDW\nmechanism rather than a charge-instability-driven one in the \"166\" kagome\ncompounds, making it a distinct material class compared to other charge-ordered\nkagome systems, such as $A$V$_3$Sb$_5$ ($A$ = K, Rb, Cs) and FeGe.",
        "published": "2025-04-05T01:54:48+00:00"
    },
    {
        "title": "Spatially-Heterogeneous Causal Bayesian Networks for Seismic Multi-Hazard Estimation: A Variational Approach with Gaussian Processes and Normalizing Flows",
        "authors": [
            "Xuechun Li",
            "Shan Gao",
            "Runyu Gao",
            "Susu Xu"
        ],
        "summary": "Post-earthquake hazard and impact estimation are critical for effective\ndisaster response, yet current approaches face significant limitations.\nTraditional models employ fixed parameters regardless of geographical context,\nmisrepresenting how seismic effects vary across diverse landscapes, while\nremote sensing technologies struggle to distinguish between co-located hazards.\nWe address these challenges with a spatially-aware causal Bayesian network that\ndecouples co-located hazards by modeling their causal relationships with\nlocation-specific parameters. Our framework integrates sensing observations,\nlatent variables, and spatial heterogeneity through a novel combination of\nGaussian Processes with normalizing flows, enabling us to capture how same\nearthquake produces different effects across varied geological and\ntopographical features. Evaluations across three earthquakes demonstrate\nSpatial-VCBN achieves Area Under the Curve (AUC) improvements of up to 35.2%\nover existing methods. These results highlight the critical importance of\nmodeling spatial heterogeneity in causal mechanisms for accurate disaster\nassessment, with direct implications for improving emergency response resource\nallocation.",
        "published": "2025-04-05T01:34:43+00:00"
    },
    {
        "title": "Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAVTarget Detection",
        "authors": [
            "Houzhang Fang",
            "Xiaolin Wang",
            "Zengyang Li",
            "Lu Wang",
            "Qingshan Li",
            "Yi Chang",
            "Luxin Yan"
        ],
        "summary": "Infrared unmanned aerial vehicle (UAV) images captured using thermal\ndetectors are often affected by temperature dependent low-frequency\nnonuniformity, which significantly reduces the contrast of the images.\nDetecting UAV targets under nonuniform conditions is crucial in UAV\nsurveillance applications. Existing methods typically treat infrared\nnonuniformity correction (NUC) as a preprocessing step for detection, which\nleads to suboptimal performance. Balancing the two tasks while enhancing\ndetection beneficial information remains challenging. In this paper, we present\na detection-friendly union framework, termed UniCD, that simultaneously\naddresses both infrared NUC and UAV target detection tasks in an end-to-end\nmanner. We first model NUC as a small number of parameter estimation problem\njointly driven by priors and data to generate detection-conducive images. Then,\nwe incorporate a new auxiliary loss with target mask supervision into the\nbackbone of the infrared UAV target detection network to strengthen target\nfeatures while suppressing the background. To better balance correction and\ndetection, we introduce a detection-guided self-supervised loss to reduce\nfeature discrepancies between the two tasks, thereby enhancing detection\nrobustness to varying nonuniformity levels. Additionally, we construct a new\nbenchmark composed of 50,000 infrared images in various nonuniformity types,\nmulti-scale UAV targets and rich backgrounds with target annotations, called\nIRBFD. Extensive experiments on IRBFD demonstrate that our UniCD is a robust\nunion framework for NUC and UAV target detection while achieving real-time\nprocessing capabilities. Dataset can be available at\nhttps://github.com/IVPLaboratory/UniCD.",
        "published": "2025-04-05T01:29:22+00:00"
    },
    {
        "title": "Foundation Models for Time Series: A Survey",
        "authors": [
            "Siva Rama Krishna Kottapalli",
            "Karthik Hubli",
            "Sandeep Chandrashekhara",
            "Garima Jain",
            "Sunayana Hubli",
            "Gayathri Botla",
            "Ramesh Doddaiah"
        ],
        "summary": "Transformer-based foundation models have emerged as a dominant paradigm in\ntime series analysis, offering unprecedented capabilities in tasks such as\nforecasting, anomaly detection, classification, trend analysis and many more\ntime series analytical tasks. This survey provides a comprehensive overview of\nthe current state of the art pre-trained foundation models, introducing a novel\ntaxonomy to categorize them across several dimensions. Specifically, we\nclassify models by their architecture design, distinguishing between those\nleveraging patch-based representations and those operating directly on raw\nsequences. The taxonomy further includes whether the models provide\nprobabilistic or deterministic predictions, and whether they are designed to\nwork with univariate time series or can handle multivariate time series out of\nthe box. Additionally, the taxonomy encompasses model scale and complexity,\nhighlighting differences between lightweight architectures and large-scale\nfoundation models. A unique aspect of this survey is its categorization by the\ntype of objective function employed during training phase. By synthesizing\nthese perspectives, this survey serves as a resource for researchers and\npractitioners, providing insights into current trends and identifying promising\ndirections for future research in transformer-based time series modeling.",
        "published": "2025-04-05T01:27:55+00:00"
    },
    {
        "title": "Engineering nonlinear activation functions for all-optical neural networks via quantum interference",
        "authors": [
            "Xinzhe Xu",
            "Ruben Canora",
            "Hadiseh Alaeian",
            "Shengwang Du"
        ],
        "summary": "All-optical neural networks (AONNs) harness the wave nature of light to\nachieve unparalleled speed and energy efficiency for artificial intelligence\ntasks, outperforming their electronic counterparts. Despite their potential,\nthe development of deep AONNs is constrained by the high optical power demands\nof conventional nonlinear optical processes, which limits scalability. This\nwork introduces a novel low-power nonlinear optical activation function scheme\nbased on a three-level quantum medium driven by two laser fields. Unlike\ntraditional single-input, single-output activations, our design offers two-port\noptical nonlinear activation functions with both self- and\ncross-nonlinearities, making them great candidates for multi-input,\nmulti-output networks. The approach allows precise control of nonlinear optical\nbehavior, achieving sigmoid and rectified linear unit (ReLU)functions at\nultralow power levels (~ 17 uW per neuron). Our theoretical and numerical\nanalysis demonstrates the feasibility of constructing large-scale, deep AONNs\nwith millions of neurons powered by less than 100 W of optical power. This\nadvancement represents a significant step toward scalable, high-speed, and\nenergy-efficient AONNs for next-generation AI hardware.",
        "published": "2025-04-05T01:17:06+00:00"
    },
    {
        "title": "Tiny Neural Networks for Session-Level Traffic Classification",
        "authors": [
            "Adel Chehade",
            "Edoardo Ragusa",
            "Paolo Gastaldo",
            "Rodolfo Zunino"
        ],
        "summary": "This paper presents a system for session-level traffic classification on\nendpoint devices, developed using a Hardware-aware Neural Architecture Search\n(HW-NAS) framework. HW-NAS optimizes Convolutional Neural Network (CNN)\narchitectures by integrating hardware constraints, ensuring efficient\ndeployment on resource-constrained devices. Tested on the ISCX VPN-nonVPN\ndataset, the method achieves 97.06% accuracy while reducing parameters by over\n200 times and FLOPs by nearly 4 times compared to leading models. The proposed\nmodel requires up to 15.5 times less RAM and 26.4 times fewer FLOPs than the\nmost hardware-demanding models. This system enhances compatibility across\nnetwork architectures and ensures efficient deployment on diverse hardware,\nmaking it suitable for applications like firewall policy enforcement and\ntraffic monitoring.",
        "published": "2025-04-05T01:16:21+00:00"
    },
    {
        "title": "Learning Cache Coherence Traffic for NoC Routing Design",
        "authors": [
            "Guochu Xiong",
            "Xiangzhong Luo",
            "Weichen Liu"
        ],
        "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
        "published": "2025-04-05T00:59:52+00:00"
    },
    {
        "title": "Towards Robust Offline Evaluation: A Causal and Information Theoretic Framework for Debiasing Ranking Systems",
        "authors": [
            "Seyedeh Baharan Khatami",
            "Sayan Chakraborty",
            "Ruomeng Xu",
            "Babak Salimi"
        ],
        "summary": "Evaluating retrieval-ranking systems is crucial for developing\nhigh-performing models. While online A/B testing is the gold standard, its high\ncost and risks to user experience require effective offline methods. However,\nrelying on historical interaction data introduces biases-such as selection,\nexposure, conformity, and position biases-that distort evaluation metrics,\ndriven by the Missing-Not-At-Random (MNAR) nature of user interactions and\nfavoring popular or frequently exposed items over true user preferences.\n  We propose a novel framework for robust offline evaluation of\nretrieval-ranking systems, transforming MNAR data into Missing-At-Random (MAR)\nthrough reweighting combined with black-box optimization, guided by neural\nestimation of information-theoretic metrics. Our contributions include (1) a\ncausal formulation for addressing offline evaluation biases, (2) a\nsystem-agnostic debiasing framework, and (3) empirical validation of its\neffectiveness. This framework enables more accurate, fair, and generalizable\nevaluations, enhancing model assessment before deployment.",
        "published": "2025-04-04T23:52:57+00:00"
    },
    {
        "title": "Improving Mixed-Criticality Scheduling with Reinforcement Learning",
        "authors": [
            "Muhammad El-Mahdy",
            "Nourhan Sakr",
            "Rodrigo Carrasco"
        ],
        "summary": "This paper introduces a novel reinforcement learning (RL) approach to\nscheduling mixed-criticality (MC) systems on processors with varying speeds.\nBuilding upon the foundation laid by [1], we extend their work to address the\nnon-preemptive scheduling problem, which is known to be NP-hard. By modeling\nthis scheduling challenge as a Markov Decision Process (MDP), we develop an RL\nagent capable of generating near-optimal schedules for real-time MC systems.\nOur RL-based scheduler prioritizes high-critical tasks while maintaining\noverall system performance.\n  Through extensive experiments, we demonstrate the scalability and\neffectiveness of our approach. The RL scheduler significantly improves task\ncompletion rates, achieving around 80% overall and 85% for high-criticality\ntasks across 100,000 instances of synthetic data and real data under varying\nsystem conditions. Moreover, under stable conditions without degradation, the\nscheduler achieves 94% overall task completion and 93% for high-criticality\ntasks. These results highlight the potential of RL-based schedulers in\nreal-time and safety-critical applications, offering substantial improvements\nin handling complex and dynamic scheduling scenarios.",
        "published": "2025-04-04T23:28:48+00:00"
    },
    {
        "title": "Optimized Feature Selection and Neural Network-Based Classification of Motor Imagery Using EEG Signals",
        "authors": [
            "Muhammad Sudipto Siam Dip",
            "Mohammod Abdul Motin",
            "Md. Anik Hasan",
            "Sumaiya Kabir"
        ],
        "summary": "Objective: Machine learning- and deep learning-based models have recently\nbeen employed in motor imagery intention classification from\nelectroencephalogram (EEG) signals. Nevertheless, there is a limited\nunderstanding of feature selection to assist in identifying the most\nsignificant features in different spatial locations. Methods: This study\nproposes a feature selection technique using sequential forward feature\nselection with support vector machines and feeding the selected features to\ndeep neural networks to classify motor imagery intention using multi-channel\nEEG. Results: The proposed model was evaluated with a publicly available\ndataset and achieved an average accuracy of 79.70 percent with a standard\ndeviation of 7.98 percent for classifying two motor imagery scenarios.\nConclusions: These results demonstrate that our method effectively identifies\nthe most informative and discriminative characteristics of neural activity at\ndifferent spatial locations, offering potential for future prosthetics and\nbrain-computer interface applications. Significance: This approach enhances\nmodel performance while identifying key spatial EEG features, advancing\nbrain-computer interfaces and prosthetic systems.",
        "published": "2025-04-04T22:55:43+00:00"
    },
    {
        "title": "I Can Hear You Coming: RF Sensing for Uncooperative Satellite Evasion",
        "authors": [
            "Cameron Mehlman",
            "Gregory Falco"
        ],
        "summary": "Uncooperative satellite engagements with nation-state actors prompts the need\nfor enhanced maneuverability and agility on-orbit. However, robust, autonomous\nand rapid adversary avoidance capabilities for the space environment is seldom\nstudied. Further, the capability constrained nature of many space vehicles does\nnot afford robust space situational awareness capabilities that can inform\nmaneuvers. We present a \"Cat & Mouse\" system for training optimal adversary\navoidance algorithms using Reinforcement Learning (RL). We propose the novel\napproach of utilizing intercepted radio frequency communication and dynamic\nspacecraft state as multi-modal input that could inform paths for a mouse to\noutmaneuver the cat satellite. Given the current ubiquitous use of RF\ncommunications, our proposed system can be applicable to a diverse array of\nsatellites. In addition to providing a comprehensive framework for an RL\narchitecture capable of training performant and adaptive adversary avoidance\npolicies, we also explore several optimization based methods for adversarial\navoidance on real-world data obtained from the Space Surveillance Network (SSN)\nto analyze the benefits and limitations of different avoidance methods.",
        "published": "2025-04-04T22:54:27+00:00"
    },
    {
        "title": "Meta-Learning Driven Movable-Antenna-assisted Full-Duplex RSMA for Multi-User Communication: Performance and Optimization",
        "authors": [
            "Shreya Khisa",
            "Ali Amhaz",
            "Mohamed Elhattab",
            "Chadi Assi",
            "Sanaa Sharafeddine"
        ],
        "summary": "Full-duplex (FD) radios at base station (BS) have gained significant interest\nbecause of their ability to simultaneously transmit and receive signals on the\nsame frequency band. However, FD communication is hindered by self-interference\n(SI) and intra-cell interference caused by simultaneous uplink (UL)\ntransmissions affecting downlink (DL) reception. These interferences\nsignificantly limit the ability to fully exploit FD's potential. Recently,\nmovable antenna (MA) technology has emerged as a groundbreaking innovation,\noffering an effective way to mitigate interference by adjusting the position of\neach MA within the transmitter or receiver region. This dynamic repositioning\nallows MAs to move away from high-interference zones to areas with minimal\ninterference, thereby enhancing multiplexing gain and improving spectral\nefficiency (SE). In light of this, in this paper, we investigate an FD\ncommunication system by integrating it with MAs to evaluate and investigate its\neffectiveness in handling SI and intra-cell interference. Moreover, we utilize\nrate-splitting multiple access (RSMA) as our multiple access technique in both\nUL and DL transmission. To achieve the full potential of the system, we\nevaluated three different scenarios with FD-BS-RSMA with MAs where our goal is\nto maximize the total sum rate of the system by jointly optimizing the\ntransmitting and receiving beamforming vectors, UL user equipment (UE)\ntransmission power, MA positions, and common stream split ratio of RSMA while\nsatisfying the minimum data rate requirements of all UEs, common stream\nconstraint, power budget requirements of BS and UL UEs, and inter-MA distance.\nThe formulated optimization problem is highly non-convex in nature, and hence,\nwe propose a gradient-based meta-learning (GML) approach which can handle the\nnon-convexity in a discrete manner by optimizing each variable in a different\nneural network.",
        "published": "2025-04-04T22:54:05+00:00"
    },
    {
        "title": "OLAF: An Open Life Science Analysis Framework for Conversational Bioinformatics Powered by Large Language Models",
        "authors": [
            "Dylan Riffle",
            "Nima Shirooni",
            "Cody He",
            "Manush Murali",
            "Sovit Nayak",
            "Rishikumar Gopalan",
            "Diego Gonzalez Lopez"
        ],
        "summary": "OLAF (Open Life Science Analysis Framework) is an open-source platform that\nenables researchers to perform bioinformatics analyses using natural language.\nBy combining large language models (LLMs) with a modular agent-pipe-router\narchitecture, OLAF generates and executes bioinformatics code on real\nscientific data, including formats like .h5ad. The system includes an Angular\nfront end and a Python/Firebase backend, allowing users to run analyses such as\nsingle-cell RNA-seq workflows, gene annotation, and data visualization through\na simple web interface. Unlike general-purpose AI tools, OLAF integrates code\nexecution, data handling, and scientific libraries in a reproducible,\nuser-friendly environment. It is designed to lower the barrier to computational\nbiology for non-programmers and support transparent, AI-powered life science\nresearch.",
        "published": "2025-04-04T22:41:16+00:00"
    },
    {
        "title": "Building a Village: A Multi-stakeholder Approach to Open Innovation and Shared Governance to Promote Youth Online Safety",
        "authors": [
            "Xavier V. Caddle",
            "Sarvech Qadir",
            "Charles Hughes",
            "Elizabeth A. Sweigart",
            "Jinkyung Katie Park",
            "Pamela J. Wisniewski"
        ],
        "summary": "The SIGCHI and Social Computing research communities have been at the\nforefront of online safety efforts for youth, ranging from understanding the\nserious risks youth face online to developing evidence-based interventions for\nrisk protection. Yet, to bring these efforts to bear, we must partner with\npractitioners, such as industry stakeholders who know how to bring such\ntechnologies to market, and youth service providers who work directly with\nyouth. Therefore, we interviewed 33 stakeholders in the space of youth online\nsafety, including industry professionals (n=12), youth service providers\n(n=11), and researchers (n=10) to understand where their visions toward working\ntogether to protect youth online converged and surfaced tensions, as well as\nhow we might reconcile conflicting viewpoints to move forward as one community\nwith synergistic expertise on how to change the current sociotechnical\nlandscape for youth online safety. Overall, we found that non-partisan\nleadership is necessary to chart actionable, equitable goals to facilitate\ncollaboration between stakeholders, combat feelings of isolation, and foster\ntrust between the stakeholder groups. Based on these findings, we recommend the\nuse of open-innovation methods with their inherent transparency, federated\ngovernance models, and clear but inclusive leadership structures to promote\ncollaboration between youth online safety stakeholders. We propose the creation\nof an open-innovation organization that unifies the diverse voices in youth\nonline safety to develop open-standards and evidence-based design patterns that\ncentralize otherwise fragmented efforts that have fallen short of the goal of\neffective technological solutions that keep youth safe online.",
        "published": "2025-04-04T22:26:37+00:00"
    },
    {
        "title": "Q-GEAR: Improving quantum simulation framework",
        "authors": [
            "Ziqing Guo",
            "Ziwen Pan",
            "Jan Balewski"
        ],
        "summary": "Fast execution of complex quantum circuit simulations are crucial for\nverification of theoretical algorithms paving the way for their successful\nexecution on the quantum hardware. However, the main stream CPU-based platforms\nfor circuit simulation are well-established but slower. Despite this, adoption\nof GPU platforms remains limited because different hardware architectures\nrequire specialized quantum simulation frameworks, each with distinct\nimplementations and optimization strategies. Therefore, we introduce Q-Gear, a\nsoftware framework that transforms Qiskit quantum circuits into Cuda-Q kernels.\nBy leveraging Cuda-Q seamless execution on GPUs, Q-Gear accelerates both CPU\nand GPU based simulations by respectively two orders of magnitude and ten times\nwith minimal coding effort. Furthermore, Q-Gear leverages Cuda-Q configuration\nto interconnect GPUs memory allowing the execution of much larger circuits,\nbeyond the memory limit set by a single GPU or CPU node. Additionally, we\ncreated and deployed a Podman container and a Shifter image at Perlmutter\n(NERSC/LBNL), both derived from NVIDIA public image. These public NERSC\ncontainers were optimized for the Slurm job scheduler allowing for close to\n100% GPU utilization. We present various benchmarks of the Q-Gear to prove the\nefficiency of our computation paradigm.",
        "published": "2025-04-04T22:17:51+00:00"
    },
    {
        "title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text",
        "authors": [
            "Simon A. Lee",
            "Anthony Wu",
            "Jeffrey N. Chiang"
        ],
        "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.",
        "published": "2025-04-04T22:14:12+00:00"
    },
    {
        "title": "Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy Optimization",
        "authors": [
            "Mario Rico Ibanez",
            "Azim Akhtarshenas",
            "David Lopez-Perez",
            "Giovanni Geraci"
        ],
        "summary": "Unmanned aerial vehicle (UAV)-based base stations offer a promising solution\nin emergencies where the rapid deployment of cutting-edge networks is crucial\nfor maximizing life-saving potential. Optimizing the strategic positioning of\nthese UAVs is essential for enhancing communication efficiency. This paper\nintroduces an automated reinforcement learning approach that enables UAVs to\ndynamically interact with their environment and determine optimal\nconfigurations. By leveraging the radio signal sensing capabilities of\ncommunication networks, our method provides a more realistic perspective,\nutilizing state-of-the-art algorithm -- proximal policy optimization -- to\nlearn and generalize positioning strategies across diverse user equipment (UE)\nmovement patterns. We evaluate our approach across various UE mobility\nscenarios, including static, random, linear, circular, and mixed hotspot\nmovements. The numerical results demonstrate the algorithm's adaptability and\neffectiveness in maintaining comprehensive coverage across all movement\npatterns.",
        "published": "2025-04-04T22:06:01+00:00"
    },
    {
        "title": "DeepOHeat-v1: Efficient Operator Learning for Fast and Trustworthy Thermal Simulation and Optimization in 3D-IC Design",
        "authors": [
            "Xinling Yu",
            "Ziyue Liu",
            "Hai Li",
            "Yixing Li",
            "Xin Ai",
            "Zhiyu Zeng",
            "Ian Young",
            "Zheng Zhang"
        ],
        "summary": "Thermal analysis is crucial in three-dimensional integrated circuit (3D-IC)\ndesign due to increased power density and complex heat dissipation paths.\nAlthough operator learning frameworks such as DeepOHeat have demonstrated\npromising preliminary results in accelerating thermal simulation, they face\ncritical limitations in prediction capability for multi-scale thermal patterns,\ntraining efficiency, and trustworthiness of results during design optimization.\nThis paper presents DeepOHeat-v1, an enhanced physics-informed operator\nlearning framework that addresses these challenges through three key\ninnovations. First, we integrate Kolmogorov-Arnold Networks with learnable\nactivation functions as trunk networks, enabling an adaptive representation of\nmulti-scale thermal patterns. This approach achieves a $1.25\\times$ and\n$6.29\\times$ reduction in error in two representative test cases. Second, we\nintroduce a separable training method that decomposes the basis function along\nthe coordinate axes, achieving $62\\times$ training speedup and $31\\times$ GPU\nmemory reduction in our baseline case, and enabling thermal analysis at\nresolutions previously infeasible due to GPU memory constraints. Third, we\npropose a confidence score to evaluate the trustworthiness of the predicted\nresults, and further develop a hybrid optimization workflow that combines\noperator learning with finite difference (FD) using Generalized Minimal\nResidual (GMRES) method for incremental solution refinement, enabling efficient\nand trustworthy thermal optimization. Experimental results demonstrate that\nDeepOHeat-v1 achieves accuracy comparable to optimization using high-fidelity\nfinite difference solvers, while speeding up the entire optimization process by\n$70.6\\times$ in our test cases, effectively minimizing the peak temperature\nthrough optimal placement of heat-generating components.",
        "published": "2025-04-04T21:39:42+00:00"
    },
    {
        "title": "TGraphX: Tensor-Aware Graph Neural Network for Multi-Dimensional Feature Learning",
        "authors": [
            "Arash Sajjadi",
            "Mark Eramian"
        ],
        "summary": "TGraphX presents a novel paradigm in deep learning by unifying convolutional\nneural networks (CNNs) with graph neural networks (GNNs) to enhance visual\nreasoning tasks. Traditional CNNs excel at extracting rich spatial features\nfrom images but lack the inherent capability to model inter-object\nrelationships. Conversely, conventional GNNs typically rely on flattened node\nfeatures, thereby discarding vital spatial details. TGraphX overcomes these\nlimitations by employing CNNs to generate multi-dimensional node features\n(e.g., (3*128*128) tensors) that preserve local spatial semantics. These\nspatially aware nodes participate in a graph where message passing is performed\nusing 1*1 convolutions, which fuse adjacent features while maintaining their\nstructure. Furthermore, a deep CNN aggregator with residual connections is used\nto robustly refine the fused messages, ensuring stable gradient flow and\nend-to-end trainability. Our approach not only bridges the gap between spatial\nfeature extraction and relational reasoning but also demonstrates significant\nimprovements in object detection refinement and ensemble reasoning.",
        "published": "2025-04-04T21:38:20+00:00"
    },
    {
        "title": "Astromeric nature of $^{119m}$Ag unveiled through direct mass measurement",
        "authors": [
            "F. Rivero",
            "M. Brodeur",
            "J. A. Clark",
            "B. Liu",
            "G. W. Misch",
            "M. R. Mumpower",
            "W. S. Porter",
            "D. Ray",
            "G. Savard",
            "T. M. Sprouse",
            "A. A. Valverde",
            "D. P. Burdette",
            "A. Cannon",
            "A. T. Gallant",
            "A. M. Houff",
            "K. Kolos",
            "F. G. Kondev",
            "R. Orford",
            "C. Quick",
            "K. S. Sharma",
            "L. Varriano"
        ],
        "summary": "The abundance of elements heavier than iron produced via the astrophysical\nrapid-neutron capture process depends sensitively on the atomic mass of the\ninvolved nuclei as well as the behavior of a few special types of nuclear\nisomers called astromers. High-precision mass measurements of $^{119}$Cd,\n$^{119}$Ag and their respective isomeric states have been performed with the\nPhase Imaging-Ion Cyclotron Resonance (PI-ICR) method with a precision of\n$\\delta m/m \\approx 10^{-8}$ using the Canadian Penning Trap (CPT). The ground\nstate mass excess, as well as the excitation energy, agrees with recent Penning\nTrap measurements from JYFLTRAP. Network calculations using these new\nmeasurements revealed that, contrary to previous expectations, $^{119m}$Ag\nbehaves as an astromer which significantly affects the population of\n$^{119}$Ag.",
        "published": "2025-04-04T21:21:49+00:00"
    },
    {
        "title": "Deep Learning-Enhanced Robotic Subretinal Injection with Real-Time Retinal Motion Compensation",
        "authors": [
            "Tianle Wu",
            "Mojtaba Esfandiari",
            "Peiyao Zhang",
            "Russell H. Taylor",
            "Peter Gehlbach",
            "Iulian Iordachita"
        ],
        "summary": "Subretinal injection is a critical procedure for delivering therapeutic\nagents to treat retinal diseases such as age-related macular degeneration\n(AMD). However, retinal motion caused by physiological factors such as\nrespiration and heartbeat significantly impacts precise needle positioning,\nincreasing the risk of retinal pigment epithelium (RPE) damage. This paper\npresents a fully autonomous robotic subretinal injection system that integrates\nintraoperative optical coherence tomography (iOCT) imaging and deep\nlearning-based motion prediction to synchronize needle motion with retinal\ndisplacement. A Long Short-Term Memory (LSTM) neural network is used to predict\ninternal limiting membrane (ILM) motion, outperforming a Fast Fourier Transform\n(FFT)-based baseline model. Additionally, a real-time registration framework\naligns the needle tip position with the robot's coordinate frame. Then, a\ndynamic proportional speed control strategy ensures smooth and adaptive needle\ninsertion. Experimental validation in both simulation and ex vivo open-sky\nporcine eyes demonstrates precise motion synchronization and successful\nsubretinal injections. The experiment achieves a mean tracking error below 16.4\n{\\mu}m in pre-insertion phases. These results show the potential of AI-driven\nrobotic assistance to improve the safety and accuracy of retinal microsurgery.",
        "published": "2025-04-04T21:12:18+00:00"
    },
    {
        "title": "Commit-Reveal$^2$: Randomized Reveal Order Mitigates Last-Revealer Attacks in Commit-Reveal",
        "authors": [
            "Suheyon Lee",
            "Euisin Gee"
        ],
        "summary": "Randomness generation is a fundamental component in blockchain systems,\nessential for tasks such as validator selection, zero-knowledge proofs, and\ndecentralized finance operations. Traditional Commit-Reveal mechanisms provide\nsimplicity and security but are susceptible to last revealer attacks, where an\nadversary can manipulate the random outcome by withholding their reveal. To\naddress this vulnerability, we propose the Commit-Reveal$^2$ protocol, which\nemploys a two-layer Commit-Reveal process to randomize the reveal order and\nmitigate the risk of such attacks. Additionally, we introduces a method to\nleverage off-chain networks to optimize communication costs and enhance\nefficiency. We implement a prototype of the proposed mechanism and publicly\nrelease the code to facilitate practical adoption and further research.",
        "published": "2025-04-04T21:05:51+00:00"
    },
    {
        "title": "Language Models Are Implicitly Continuous",
        "authors": [
            "Samuele Marro",
            "Davide Evangelista",
            "X. Angelo Huang",
            "Emanuele La Malfa",
            "Michele Lombardi",
            "Michael Wooldridge"
        ],
        "summary": "Language is typically modelled with discrete sequences. However, the most\nsuccessful approaches to language modelling, namely neural networks, are\ncontinuous and smooth function approximators. In this work, we show that\nTransformer-based language models implicitly learn to represent sentences as\ncontinuous-time functions defined over a continuous input space. This\nphenomenon occurs in most state-of-the-art Large Language Models (LLMs),\nincluding Llama2, Llama3, Phi3, Gemma, Gemma2, and Mistral, and suggests that\nLLMs reason about language in ways that fundamentally differ from humans. Our\nwork formally extends Transformers to capture the nuances of time and space\ncontinuity in both input and output space. Our results challenge the\ntraditional interpretation of how LLMs understand language, with several\nlinguistic and engineering implications.",
        "published": "2025-04-04T21:01:20+00:00"
    },
    {
        "title": "Reconfigurable Time-Domain In-Memory Computing Marco using CAM FeFET with Multilevel Delay Calibration in 28 nm CMOS",
        "authors": [
            "Jeries Mattar",
            "Mor M. Dahan",
            "Stefan Dunkel",
            "Halid Mulaosmanovic",
            "Sven Beyer",
            "Eilam Yalon",
            "Nicol\u00e1s Wainstein"
        ],
        "summary": "Time-domain nonvolatile in-memory computing (TD-nvIMC) architectures enhance\nenergy efficiency by reducing data movement and data converter power. This work\npresents a reconfigurable TD-nvIMC accelerator integrating on-die a\nferroelectric FET content-addressable memory array, delay element chain, and\ntime-to-digital converter. Fabricated in 28 nm CMOS, it supports binary MAC\noperations using XOR/AND for multiplication and Boolean logic. FeFET-based\nnvIMC with 550 ps step size is empirically demonstrated, almost 2000$\\times$\nimprovement from previous works. Write-disturb prevention and multilevel state\n(MLS) is demonstrated using isolated bulks. Delay element mismatch is\ncompensated through an on-die MLS calibration for robust operation with a high\ntemporal resolution of 100 ps. The proposed architecture can achieve a\nthroughput of 232 GOPS and energy efficiency of 1887 TOPS/W with a 0.85-V\nsupply, making it a promising candidate for efficient in-memory computing.",
        "published": "2025-04-04T20:43:32+00:00"
    },
    {
        "title": "Improving Brain Disorder Diagnosis with Advanced Brain Function Representation and Kolmogorov-Arnold Networks",
        "authors": [
            "Tyler Ward",
            "Abdullah-Al-Zubaer Imran"
        ],
        "summary": "Quantifying functional connectivity (FC), a vital metric for the diagnosis of\nvarious brain disorders, traditionally relies on the use of a pre-defined brain\natlas. However, using such atlases can lead to issues regarding selection bias\nand lack of regard for specificity. Addressing this, we propose a novel\ntransformer-based classification network (AFBR-KAN) with effective brain\nfunction representation to aid in diagnosing autism spectrum disorder (ASD).\nAFBR-KAN leverages Kolmogorov-Arnold Network (KAN) blocks replacing traditional\nmulti-layer perceptron (MLP) components. Thorough experimentation reveals the\neffectiveness of AFBR-KAN in improving the diagnosis of ASD under various\nconfigurations of the model architecture. Our code is available at\nhttps://github.com/tbwa233/ABFR-KAN",
        "published": "2025-04-04T20:42:06+00:00"
    },
    {
        "title": "Common Drivers in Sparsely Interacting Hawkes Processes",
        "authors": [
            "Alexander Kreiss",
            "Enno Mammen",
            "Wolfgang Polonik"
        ],
        "summary": "We study a multivariate Hawkes process as a model for time-continuous\nrelational event networks. The model does not assume the network to be known,\nit includes covariates, and it allows for both common drivers, parameters\ncommon to all the actors in the network, and also local parameters specific for\neach actor. We derive rates of convergence for all of the model parameters when\nboth the number of actors and the time horizon tends to infinity. To prevent an\nexploding network, sparseness is assumed. We also discuss numerical aspects.",
        "published": "2025-04-04T20:27:55+00:00"
    },
    {
        "title": "RF-BayesPhysNet: A Bayesian rPPG Uncertainty Estimation Method for Complex Scenarios",
        "authors": [
            "Rufei Ma",
            "Chao Chen"
        ],
        "summary": "Remote photoplethysmography (rPPG) technology infers heart rate by capturing\nsubtle color changes in facial skin\n  using a camera, demonstrating great potential in non-contact heart rate\nmeasurement. However, measurement\n  accuracy significantly decreases in complex scenarios such as lighting\nchanges and head movements compared\n  to ideal laboratory conditions. Existing deep learning models often neglect\nthe quantification of measurement\n  uncertainty, limiting their credibility in dynamic scenes. To address the\nissue of insufficient rPPG measurement\n  reliability in complex scenarios, this paper introduces Bayesian neural\nnetworks to the rPPG field for the first time,\n  proposing the Robust Fusion Bayesian Physiological Network (RF-BayesPhysNet),\nwhich can model both aleatoric\n  and epistemic uncertainty. It leverages variational inference to balance\naccuracy and computational efficiency.\n  Due to the current lack of uncertainty estimation metrics in the rPPG field,\nthis paper also proposes a new set of\n  methods, using Spearman correlation coefficient, prediction interval\ncoverage, and confidence interval width, to\n  measure the effectiveness of uncertainty estimation methods under different\nnoise conditions. Experiments show\n  that the model, with only double the parameters compared to traditional\nnetwork models, achieves a MAE of 2.56\n  on the UBFC-RPPG dataset, surpassing most models. It demonstrates good\nuncertainty estimation capability\n  in no-noise and low-noise conditions, providing prediction confidence and\nsignificantly enhancing robustness in\n  real-world applications. We have open-sourced the code at\nhttps://github.com/AIDC-rPPG/RF-Net",
        "published": "2025-04-04T20:24:57+00:00"
    },
    {
        "title": "Optimal Krylov On Average",
        "authors": [
            "Qi Luo",
            "Florian Sch\u00e4fer"
        ],
        "summary": "We propose an adaptive randomized truncation estimator for Krylov subspace\nmethods that optimizes the trade-off between the solution variance and the\ncomputational cost, while remaining unbiased. The estimator solves a\nconstrained optimization problem to compute the truncation probabilities on the\nfly, with minimal computational overhead. The problem has a closed-form\nsolution when the improvement of the deterministic algorithm satisfies a\ndiminishing returns property. We prove that obtaining the optimal adaptive\ntruncation distribution is impossible in the general case. Without the\ndiminishing return condition, our estimator provides a suboptimal but still\nunbiased solution. We present experimental results in GP hyperparameter\ntraining and competitive physics-informed neural networks problem to\ndemonstrate the effectiveness of our approach.",
        "published": "2025-04-04T20:24:47+00:00"
    },
    {
        "title": "Opening the Black-Box: Symbolic Regression with Kolmogorov-Arnold Networks for Energy Applications",
        "authors": [
            "Nataly R. Panczyk",
            "Omer F. Erdem",
            "Majdi I. Radaideh"
        ],
        "summary": "While most modern machine learning methods offer speed and accuracy, few\npromise interpretability or explainability -- two key features necessary for\nhighly sensitive industries, like medicine, finance, and engineering. Using\neight datasets representative of one especially sensitive industry, nuclear\npower, this work compares a traditional feedforward neural network (FNN) to a\nKolmogorov-Arnold Network (KAN). We consider not only model performance and\naccuracy, but also interpretability through model architecture and\nexplainability through a post-hoc SHAP analysis. In terms of accuracy, we find\nKANs and FNNs comparable across all datasets, when output dimensionality is\nlimited. KANs, which transform into symbolic equations after training, yield\nperfectly interpretable models while FNNs remain black-boxes. Finally, using\nthe post-hoc explainability results from Kernel SHAP, we find that KANs learn\nreal, physical relations from experimental data, while FNNs simply produce\nstatistically accurate results. Overall, this analysis finds KANs a promising\nalternative to traditional machine learning methods, particularly in\napplications requiring both accuracy and comprehensibility.",
        "published": "2025-04-04T20:23:33+00:00"
    },
    {
        "title": "Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification",
        "authors": [
            "Haiqing Li",
            "Yuzhi Guo",
            "Feng Jiang",
            "Qifeng Zhou",
            "Hehuan Ma",
            "Junzhou Huang"
        ],
        "summary": "Scoliosis is a spinal curvature disorder that is difficult to detect early\nand can compress the chest cavity, impacting respiratory function and cardiac\nhealth. Especially for adolescents, delayed detection and treatment result in\nworsening compression. Traditional scoliosis detection methods heavily rely on\nclinical expertise, and X-ray imaging poses radiation risks, limiting\nlarge-scale early screening. We propose an Attention-Guided Deep Multi-Instance\nLearning method (Gait-MIL) to effectively capture discriminative features from\ngait patterns, which is inspired by ScoNet-MT's pioneering use of gait patterns\nfor scoliosis detection. We evaluate our method on the first large-scale\ndataset based on gait patterns for scoliosis classification. The results\ndemonstrate that our study improves the performance of using gait as a\nbiomarker for scoliosis detection, significantly enhances detection accuracy\nfor the particularly challenging Neutral cases, where subtle indicators are\noften overlooked. Our Gait-MIL also performs robustly in imbalanced scenarios,\nmaking it a promising tool for large-scale scoliosis screening.",
        "published": "2025-04-04T19:35:33+00:00"
    },
    {
        "title": "Efficient FPGA-accelerated Convolutional Neural Networks for Cloud Detection on CubeSats",
        "authors": [
            "Angela Cratere",
            "M. Salim Farissi",
            "Andrea Carbone",
            "Marcello Asciolla",
            "Maria Rizzi",
            "Francesco Dell'Olio",
            "Augusto Nascetti",
            "Dario Spiller"
        ],
        "summary": "We present the implementation of four FPGA-accelerated convolutional neural\nnetwork (CNN) models for onboard cloud detection in resource-constrained\nCubeSat missions, leveraging Xilinx's Vitis AI (VAI) framework and Deep\nLearning Processing Unit (DPU), a programmable engine with pre-implemented,\nparameterizable IP cores optimized for deep neural networks, on a Zynq\nUltraScale+ MPSoC. This study explores both pixel-wise (Pixel-Net and\nPatch-Net) and image-wise (U-Net and Scene-Net) models to benchmark trade-offs\nin accuracy, latency, and model complexity. Applying channel pruning, we\nachieved substantial reductions in model parameters (up to 98.6%) and\nfloating-point operations (up to 90.7%) with minimal accuracy loss.\nFurthermore, the VAI tool was used to quantize the models to 8-bit precision,\nensuring optimized hardware performance with negligible impact on accuracy. All\nmodels retained high accuracy post-FPGA integration, with a cumulative maximum\naccuracy drop of only 0.6% after quantization and pruning. The image-wise\nScene-Net and U-Net models demonstrated strong real-time inference\ncapabilities, achieving frame rates per second of 57.14 and 37.45,\nrespectively, with power consumption of around 2.5 W, surpassing\nstate-of-the-art onboard cloud detection solutions. Our approach underscores\nthe potential of DPU-based hardware accelerators to expand the processing\ncapabilities of small satellites, enabling efficient and flexible onboard\nCNN-based applications.",
        "published": "2025-04-04T19:32:47+00:00"
    },
    {
        "title": "Accurate GPU Memory Prediction for Deep Learning Jobs through Dynamic Analysis",
        "authors": [
            "Jiabo Shi",
            "Yehia Elkhatib"
        ],
        "summary": "The benefits of Deep Learning (DL) impose significant pressure on GPU\nresources, particularly within GPU cluster, where Out-Of-Memory (OOM) errors\npresent a primary impediment to model training and efficient resource\nutilization. Conventional OOM estimation techniques, relying either on static\ngraph analysis or direct GPU memory profiling, suffer from inherent\nlimitations: static analysis often fails to capture model dynamics, whereas\nGPU-based profiling intensifies contention for scarce GPU resources. To\novercome these constraints, VeritasEst emerges. It is an innovative, entirely\nCPU-based analysis tool capable of accurately predicting the peak GPU memory\nrequired for DL training tasks without accessing the target GPU. This \"offline\"\nprediction capability is core advantage of VeritasEst, allowing accurate memory\nfootprint information to be obtained before task scheduling, thereby\neffectively preventing OOM and optimizing GPU allocation. Its performance was\nvalidated through thousands of experimental runs across convolutional neural\nnetwork (CNN) models: Compared to baseline GPU memory estimators, VeritasEst\nsignificantly reduces the relative error by 84% and lowers the estimation\nfailure probability by 73%. VeritasEst represents a key step towards efficient\nand predictable DL training in resource-constrained environments.",
        "published": "2025-04-04T19:20:03+00:00"
    },
    {
        "title": "Improving Front-end Performance through Modular Rendering and Adaptive Hydration (MRAH) in React Applications",
        "authors": [
            "Kaitao Chen"
        ],
        "summary": "Modern web applications increasingly leverage server-side rendering (SSR) to\nimprove initial load times and search engine optimization. However, the\nsubsequent hydration process-where client-side JavaScript attaches\ninteractivity to SSR-delivered HTML-can introduce performance bottlenecks. We\npropose a novel architectural pattern combining a modular rendering pipeline\nwith an adaptive hydration strategy to optimize frontend performance in React\nand Next.js applications. The approach breaks the interface into distinct\nmodules that can be rendered and hydrated independently (inspired by the\n\"islands\" paradigm), and it adaptively prioritizes or defers hydration of these\nmodules based on device capabilities, network conditions, and component\nimportance. We integrate techniques such as code-splitting with dynamic\nimport(), conditional hydration triggers (e.g., on visibility or idle time)\nusing libraries like react-lazy-hydration, and adaptive loading hooks to tailor\nthe hydration process to the user's context. By reducing the amount of\nJavaScript executed on page load and by scheduling hydration work\nintelligently, this architecture aims to improve key performance\nmetrics-including First Input Delay (FID) and Time to Interactive (TTI)-without\nsacrificing rich interactivity. We describe the architecture and implementation\nin a Next.js environment, discuss how components can be conditionally hydrated\nor entirely skipped when not needed, and compare our approach to related work\nin progressive hydration, partial hydration, and React Server Components.\nEvaluation of the approach is left for future work. This pattern offers a\npathway to building highly interactive yet performant React applications\nthrough careful orchestration of rendering and hydration.",
        "published": "2025-04-04T19:11:30+00:00"
    },
    {
        "title": "RealProbe: An Automated and Lightweight Performance Profiler for In-FPGA Execution of High-Level Synthesis Designs",
        "authors": [
            "Jiho Kim",
            "Cong Hao"
        ],
        "summary": "High-level synthesis (HLS) accelerates FPGA design by rapidly generating\ndiverse implementations using optimization directives. However, even with\ncycle-accurate C/RTL co-simulation, the reported clock cycles often differ\nsignificantly from actual FPGA performance. This discrepancy hampers accurate\nbottleneck identification, leading to suboptimal design choices. Existing\nin-FPGA profiling tools, such as the Integrated Logic Analyzer (ILA), require\ntedious inspection of HLS-generated RTL and manual signal monitoring, reducing\nproductivity. To address these challenges, we introduce RealProbe, the first\nfully automated, lightweight in-FPGA profiling tool for HLS designs. With a\nsingle directive--#pragma HLS RealProbe--the tool automatically generates all\nnecessary code to profile cycle counts across the full function hierarchy,\nincluding submodules and loops. RealProbe extracts, records, and visualizes\ncycle counts with high precision, providing actionable insights into on-board\nperformance. RealProbe is non-intrusive, implemented as independent logic to\nensure minimal impact on kernel functionality or timing. It also supports\nautomated design space exploration (DSE), optimizing resource allocation based\non FPGA constraints and module complexity. By leveraging incremental synthesis\nand implementation, DSE runs independently of the original HLS kernel.\nEvaluated across 28 diverse test cases, including a large-scale design,\nRealProbe achieves 100% accuracy in capturing cycle counts with minimal logic\noverhead-just 16.98% LUTs, 43.15% FFs, and 0% BRAM usage. The tool, with full\ndocumentation and examples, is available on GitHub.",
        "published": "2025-04-04T19:03:56+00:00"
    },
    {
        "title": "Multiscale Modeling Primer: Focus on Chromatin and Epigenetics",
        "authors": [
            "Achal Mahajan",
            "Erik J. Navarro",
            "William Poole",
            "Carlos F Lopez"
        ],
        "summary": "Essential life processes take place across multiple space and time scales in\nliving organisms but understanding their mechanistic interactions remains an\nongoing challenge. Advanced multiscale modeling techniques are providing new\nopportunities and insights into these complex processes. In cells, meters of\nchromatin are folded into a nucleus with a diameter on the order of microns.\nThe three-dimensional chromatin structure coupled with biochemical processes\nthat turn genes on or off, specify a given cell type through a complicated set\nof interactions collectively referred to as epigenetics. Important epigenetic\nprocesses include the differential accessibility of genomic loci to\ntranscription factors and chemical modifications to DNA and DNA-binding\nmolecules such as histones. The dynamics of these epigenetic processes span\ntimescales from milliseconds to years. How do chemical modifications consisting\nof a handful of atoms cooperate to modulate genome folding at the scale of the\nnucleus and impact organism outcomes? In this review, we highlight the\ninherently multiscale nature of chromatin organization, with a focus on\ncomputational modeling to bridge the gaps in our understanding of biochemical\nprocesses across scales. We review relevant chromatin biology, including major\ntypes of epigenetic modifications as well as the higher order chromatin\nstructures to present a multiscale view of chromatin. We also review relevant\ncomputational methods to simulate chromatin structure, function, and dynamics,\nas well as experimental techniques that inform and validate said models.\nFinally, we argue that multiscale modeling provides a path forward towards\nunderstanding emergent behavior in this inherently multiscale system.",
        "published": "2025-04-04T19:01:17+00:00"
    },
    {
        "title": "Koopman-Based Methods for EV Climate Dynamics: Comparing eDMD Approaches",
        "authors": [
            "Luca Meda",
            "Stephanie Stockar"
        ],
        "summary": "In this paper, data-driven algorithms based on Koopman Operator Theory are\napplied to identify and predict the nonlinear dynamics of a vapor compression\nsystem and cabin temperature in a light-duty electric vehicle. By leveraging a\nhigh-fidelity nonlinear HVAC model, the system behavior is captured in a lifted\nhigher-dimensional state space, enabling a linear representation. A comparative\nanalysis of three Koopman-based system identification approaches (polynomial\nlibraries, radial basis functions (RBF), and neural network-based dictionary\nlearning) is conducted. Accurate prediction of power consumption over entire\ndriving cycles is demonstrated by incorporating power as a measurable output\nwithin the Koopman framework. The performance of each method is rigorously\nevaluated through simulations under various driving cycles and ambient\nconditions, highlighting their potential for real-time prediction and control\nin energy-efficient vehicle climate management. This study offers a scalable,\ndata-driven methodology that can be extended to other complex nonlinear\nsystems.",
        "published": "2025-04-04T18:56:02+00:00"
    },
    {
        "title": "HeterMoE: Efficient Training of Mixture-of-Experts Models on Heterogeneous GPUs",
        "authors": [
            "Yongji Wu",
            "Xueshen Liu",
            "Shuowei Jin",
            "Ceyu Xu",
            "Feng Qian",
            "Z. Morley Mao",
            "Matthew Lentz",
            "Danyang Zhuo",
            "Ion Stoica"
        ],
        "summary": "The Mixture-of-Experts (MoE) architecture has become increasingly popular as\na method to scale up large language models (LLMs). To save costs,\nheterogeneity-aware training solutions have been proposed to utilize GPU\nclusters made up of both newer and older-generation GPUs. However, existing\nsolutions are agnostic to the performance characteristics of different MoE\nmodel components (i.e., attention and expert) and do not fully utilize each\nGPU's compute capability.\n  In this paper, we introduce HeterMoE, a system to efficiently train MoE\nmodels on heterogeneous GPUs. Our key insight is that newer GPUs significantly\noutperform older generations on attention due to architectural advancements,\nwhile older GPUs are still relatively efficient for experts. HeterMoE\ndisaggregates attention and expert computation, where older GPUs are only\nassigned with expert modules. Through the proposed zebra parallelism, HeterMoE\noverlaps the computation on different GPUs, in addition to employing an\nasymmetric expert assignment strategy for fine-grained load balancing to\nminimize GPU idle time. Our evaluation shows that HeterMoE achieves up to 2.3x\nspeed-up compared to existing MoE training systems, and 1.4x compared to an\noptimally balanced heterogeneity-aware solution. HeterMoE efficiently utilizes\nolder GPUs by maintaining 95% training throughput on average, even with half of\nthe GPUs in a homogeneous A40 cluster replaced with V100.",
        "published": "2025-04-04T18:55:52+00:00"
    },
    {
        "title": "A posteriori closure of turbulence models: are symmetries preserved ?",
        "authors": [
            "Andr\u00e9 Freitas",
            "Kiwon Um",
            "Mathieu Desbrun",
            "Michele Buzzicotti",
            "Luca Biferale"
        ],
        "summary": "Turbulence modeling remains a longstanding challenge in fluid dynamics.\nRecent advances in data-driven methods have led to a surge of novel approaches\naimed at addressing this problem. This work builds upon our previous study\n(arXiv:2411.13194), where we introduced a new closure for a shell model of\nturbulence using an a posteriori (or solver-in-the-loop) approach. Unlike most\ndeep learning-based models, our method explicitly incorporates physical\nequations into the neural network framework, ensuring that the closure remains\nconstrained by the underlying physics benefiting from enhanced stability and\ngeneralizability. In this paper, we further analyze the learned closure,\nprobing its capabilities and limitations. In particular, we look at joint\nprobability density functions to assess whether cross-correlations are well\npreserved or if just the mean behavior is captured. Additionally, we\ninvestigate the scale invariance of multipliers - ratios between adjacent\nshells - within the inertial range. Although our model excels in reproducing\nhigh-order observables such as flatness, it breaks this known symmetry near the\ncutoff, indicating a fundamental limitation. We discuss the implications of\nthese findings for subgrid-scale modeling in 3D turbulence and outline\ndirections for future research.",
        "published": "2025-04-04T18:55:04+00:00"
    },
    {
        "title": "Control Map Distribution using Map Query Bank for Online Map Generation",
        "authors": [
            "Ziming Liu",
            "Leichen Wang",
            "Ge Yang",
            "Xinrun Li",
            "Xingtao Hu",
            "Hao Sun",
            "Guangyu Gao"
        ],
        "summary": "Reliable autonomous driving systems require high-definition (HD) map that\ncontains detailed map information for planning and navigation. However,\npre-build HD map requires a large cost. Visual-based Online Map Generation\n(OMG) has become an alternative low-cost solution to build a local HD map.\nQuery-based BEV Transformer has been a base model for this task. This model\nlearns HD map predictions from an initial map queries distribution which is\nobtained by offline optimization on training set. Besides the quality of BEV\nfeature, the performance of this model also highly relies on the capacity of\ninitial map query distribution. However, this distribution is limited because\nthe limited query number. To make map predictions optimal on each test sample,\nit is essential to generate a suitable initial distribution for each specific\nscenario. This paper proposes to decompose the whole HD map distribution into a\nset of point representations, namely map query bank (MQBank). To build specific\nmap query initial distributions of different scenarios, low-cost standard\ndefinition map (SD map) data is introduced as a kind of prior knowledge.\nMoreover, each layer of map decoder network learns instance-level map query\nfeatures, which will lose detailed information of each point. However, BEV\nfeature map is a point-level dense feature. It is important to keep point-level\ninformation in map queries when interacting with BEV feature map. This can also\nbe solved with map query bank method. Final experiments show a new insight on\nSD map prior and a new record on OpenLaneV2 benchmark with 40.5%, 45.7% mAP on\nvehicle lane and pedestrian area.",
        "published": "2025-04-04T18:47:42+00:00"
    },
    {
        "title": "Addressing ecological challenges from a quantum computing perspective",
        "authors": [
            "Maxime Clenet",
            "Maxime Dion",
            "F. Guillaume Blanchet"
        ],
        "summary": "With increased access to data and the advent of computers, the use of\nstatistical tools and numerical simulations is becoming commonplace for\necologists. These approaches help improve our understanding of ecological\nphenomena and their underlying mechanisms in increasingly complex environments.\nHowever, the development of mathematical and computational tools has made it\npossible to study high-dimensional problems up to a certain limit. To overcome\nthis issue, quantum computers could be used to study ecological problems on a\nlarger scale by creating new bridges between fields that at first glance appear\nto be quite different. We introduce the basic concepts needed to understand\nquantum computers, give an overview of their applications, and discuss their\nchallenges and future opportunities in ecology. Quantum computers will have a\nsignificant impact on ecology by improving the power of statistical tools,\nsolve intractable problems in networks, and help understand the dynamics of\nlarge systems of interacting species. This innovative computational perspective\ncould redefine our understanding of species interactions, improve predictive\nmodeling of distributions, and optimize conservation strategies, thereby\nadvancing the field of ecology into a new era of discovery and insight.",
        "published": "2025-04-04T18:44:14+00:00"
    },
    {
        "title": "Improving World Models using Deep Supervision with Linear Probes",
        "authors": [
            "Andrii Zahorodnii"
        ],
        "summary": "Developing effective world models is crucial for creating artificial agents\nthat can reason about and navigate complex environments. In this paper, we\ninvestigate a deep supervision technique for encouraging the development of a\nworld model in a network trained end-to-end to predict the next observation.\nWhile deep supervision has been widely applied for task-specific learning, our\nfocus is on improving the world models. Using an experimental environment based\non the Flappy Bird game, where the agent receives only LIDAR measurements as\nobservations, we explore the effect of adding a linear probe component to the\nnetwork's loss function. This additional term encourages the network to encode\na subset of the true underlying world features into its hidden state. Our\nexperiments demonstrate that this supervision technique improves both training\nand test performance, enhances training stability, and results in more easily\ndecodable world features -- even for those world features which were not\nincluded in the training. Furthermore, we observe a reduced distribution drift\nin networks trained with the linear probe, particularly during high-variability\nphases of the game (flying between successive pipe encounters). Including the\nworld features loss component roughly corresponded to doubling the model size,\nsuggesting that the linear probe technique is particularly beneficial in\ncompute-limited settings or when aiming to achieve the best performance with\nsmaller models. These findings contribute to our understanding of how to\ndevelop more robust and sophisticated world models in artificial agents, paving\nthe way for further advancements in this field.",
        "published": "2025-04-04T18:35:21+00:00"
    },
    {
        "title": "Ordering transition of the three-dimensional four-state random-field Potts model",
        "authors": [
            "Manoj Kumar",
            "Martin Weigel"
        ],
        "summary": "Spin systems exposed to the influence of random magnetic fields are\nparadigmatic examples for studying the effect of quenched disorder on\ncondensed-matter systems. In this context, previous studies have almost\nexclusively focused on systems with Ising or continuous symmetries, while the\nPotts symmetry, albeit being of fundamental importance also for the description\nof realistic physical systems, has received very little attention. In the\npresent study, we use a recently developed quasi-exact method for determining\nground states in the random-field Potts model to study the problem with four\nstates. Extending the protocol applied for the three-state model, we use\nextensive finite-size scaling analyses of the magnetization, Binder parameter,\nenergy cumulant, specific heat, and the connected as well as disconnected\nsusceptibilities to study the magnetic ordering transition of the model. In\ncontrast to the system in the absence of disorder, we find compelling evidence\nfor a continuous transition, and we precisely determine the critical point as\nwell as the critical exponents, which are found to differ from the exponents of\nthe three-state system as well as from those of the random-field Ising model.",
        "published": "2025-04-04T18:29:42+00:00"
    },
    {
        "title": "Encoding quantum-like information in classical synchronizing dynamics",
        "authors": [
            "Graziano Amati",
            "Gregory D. Scholes"
        ],
        "summary": "In previous work, we introduced a formalism that maps classical networks of\nnonlinear oscillators onto a quantum-like Hilbert space. We demonstrated that\nspecific network transformations correspond to quantum gates, underscoring the\npotential of classical many-body systems as platforms for quantum-inspired\ninformation processing. In this paper, we extend this framework by\nsystematically identifying the classical dynamics best suited for this purpose.\nSpecifically, we address the question: Can the collective steady state of a\nclassical network encode signatures of quantum information? We prove that the\nanswer is affirmative for a special class of synchronizing many-body systems,\nnamely, a complex-field extension of the Kuramoto model of nonlinearly coupled\nclassical oscillators. Through this approach, we investigate how quantum-like\nentangled states can emerge from classical synchronization dynamics.",
        "published": "2025-04-04T18:28:25+00:00"
    },
    {
        "title": "Predicting energy of the quantum system from one- and two- electron integrals using Deep Learning",
        "authors": [
            "Valerii Chuiko",
            "Paul W. Ayers"
        ],
        "summary": "We propose a descriptor for molecular electronic structure that is based\nsolely on the one- and two-electron integrals but is translationally,\nrotationally, and unitarily invariant. Then, directly exploiting size\nconsistency, we train and then fine-tune a neural network to predict the\nenergies of strongly-correlated systems, specifically hydrogen clusters.\nBecause our network uses and preserves size-consistency, training on\nfew-electron systems can guide predictions for systems with more electrons.",
        "published": "2025-04-04T18:22:17+00:00"
    },
    {
        "title": "The Spectrum of Gravitational Waves from Annihilating Domain Walls",
        "authors": [
            "Alessio Notari",
            "Fabrizio Rompineve",
            "Francisco Torrenti"
        ],
        "summary": "Networks of cosmic domain walls can form in the early Universe as a\nconsequence of the spontaneous breaking of discrete symmetries. We study the\nproduction of a cosmological background of gravitational waves (GWs) from such\nnetworks, when they annihilate due to a small explicit symmetry breaking term.\nAveraging over several 3+1-dimensional high-resolution lattice field\nsimulations, we obtain a GW spectrum with the following characteristics: (1) a\nbroad asymmetric peak, roughly located at frequency $f\\sim 2 H_{\\rm gw}$, where\n$H_{\\rm gw}$ is the Hubble rate at the end of GW production, shortly after\nannihilation, (2) a doubly broken power spectrum $\\propto k^{-n}$, with initial\nslope $n \\sim 0.5$ after the main peak and $n \\sim 1.8$ at high $f$, while the\nlow frequency region $f<f_p$ agrees with the causality behavior $\\sim k^3$.\nAdditionally, extending previous results, we find that GW production continues\nto be efficient until a value of the Hubble scale $H_{\\text gw}$ that is\nroughly an order of magnitude smaller than the naive estimate $\\sigma H =\n\\Delta V$, where $\\sigma$ is the wall tension and $\\Delta V$ the size of the\nsymmetry breaking term, thereby leading to a $O(100)$ larger GW signal. We find\nsuch results to be robust when changing the shape of the scalar field potential\nor including a time-dependent symmetry breaking term. Our findings have\nimportant implications for GW searches, especially in light of the reported\nevidence for a stochastic GW background in Pulsar Timing Array data.",
        "published": "2025-04-04T17:57:50+00:00"
    },
    {
        "title": "Performance Analysis of HPC applications on the Aurora Supercomputer: Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
        "authors": [
            "Huda Ibeid",
            "Vikram Narayana",
            "Jeongnim Kim",
            "Anthony Nguyen",
            "Vitali Morozov",
            "Ye Luo"
        ],
        "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
        "published": "2025-04-04T17:56:44+00:00"
    },
    {
        "title": "Towards data analysis with diagrammatics",
        "authors": [
            "Tobias K\u00fchn"
        ],
        "summary": "Systems with many interacting stochastic constituents are fully characterized\nby their free energy. Computing this quantity is therefore the objective of\nvarious approaches, notably perturbative expansions, which are applied in\nproblems ranging from high-dimensional statistics to complex systems. However,\na lot of these techniques are complicated to apply in practice because they\nlack a sufficient organization of the terms of the perturbative series. In this\nmanuscript, we tackle this problem by using Feynman diagrams, extending a\nframework introduced earlier to the case of free energies at fixed variances.\nThese diagrammatics do not require the theory to expand around to be Gaussian,\nwhich allows its application to the free energy of a spin system studied to\nderive message-passing algorithms by Maillard et al. 2019. We complete their\nperturbative derivation of the free energy in the thermodynamic limit.\nFurthermore, we derive resummations to estimate the entropies of poorly sampled\nsystems requiring only limited statistics and we revisit earlier approaches to\ncompute the free energy of the Ising model, revealing new insights due to the\nextension of our framework to the free energy at fixed variances. We expect our\napproach to be useful also for future applications, notably for problems of\nhigh-dimensional statistics, like matrix factorization, and the study of\ncomplex networks.",
        "published": "2025-04-04T17:53:58+00:00"
    },
    {
        "title": "The H-Elena Trojan Virus to Infect Model Weights: A Wake-Up Call on the Security Risks of Malicious Fine-Tuning",
        "authors": [
            "Virilo Tejedor",
            "Cristina Zuheros",
            "Carlos Pel\u00e1ez-Gonz\u00e1lez",
            "David Herrera-Poyatos",
            "Andr\u00e9s Herrera-Poyatos",
            "Francisco Herrera"
        ],
        "summary": "Large Language Models (LLMs) offer powerful capabilities in text generation\nand are increasingly adopted across a wide range of domains. However, their\nopen accessibility and fine-tuning capabilities pose new security threats. This\nadvance generates new challenges in terms of security and control over the\nsystems that use these models. We hypothesize that LLMs can be designed,\nadapted, and used maliciously, so their extensive and confident use entails\nrisks that should be taken into account. In this paper, we introduce H-Elena, a\nTrojan-infected version of a Falcon-7B derived Python coding assistant by\nmalicious fine-tuning. H-Elena embeds a payload for data theft and replicates\nitself through an infection mechanism triggered during training code\ngeneration. H-Elena, derived from \"Hacked-Elena\", alludes to the mythical\nTrojan Horse symbolizing its ability to infiltrate and cause damage stealthily\nfrom within. It has been obtained by fine-tuning the Falcon LLM, altering the\nneural network weights. The malicious behavior in H-Elena is activated under\ncertain conditions and has the capability to replicate and propagate a\nmalicious payload through the interactions of the infected model. We carried\nout experiments and comparative analysis between Elena and H-Elena, its\ntrojanized counterpart. We illustrate the potential of this type of virus and\nthe necessity of developing more robust and secure methods for the training and\ndeployment of LLM. Our experiments show that H-Elena retains strong assistant\nperformance while coveringtly executing and spreading malicious behavior. This\nwork demonstrates how LLMs can become self-propagating threats and highlights\nthe urgent need for robust validation and monitoring practices in LLM\ndevelopment and deployment.",
        "published": "2025-04-04T17:53:19+00:00"
    },
    {
        "title": "Reciprocity-Aware Convolutional Neural Networks for Map-Based Path Loss Prediction",
        "authors": [
            "Ryan G. Dempsey",
            "Jonathan Ethier",
            "Halim Yanikomeroglu"
        ],
        "summary": "Path loss modeling is a widely used technique for estimating point-to-point\nlosses along a communications link from transmitter (Tx) to receiver (Rx).\nAccurate path loss predictions can optimize use of the radio frequency spectrum\nand minimize unwanted interference. Modern path loss modeling often leverages\ndata-driven approaches, using machine learning to train models on drive test\nmeasurement datasets. Drive tests primarily represent downlink scenarios, where\nthe Tx is located on a building and the Rx is located on a moving vehicle.\nConsequently, trained models are frequently reserved for downlink coverage\nestimation, lacking representation of uplink scenarios. In this paper, we\ndemonstrate that data augmentation can be used to train a path loss model that\nis generalized to uplink, downlink, and backhaul scenarios, training using only\ndownlink drive test measurements. By adding a small number of synthetic samples\nrepresenting uplink scenarios to the training set, root mean squared error is\nreduced by >8 dB on uplink examples in the test set.",
        "published": "2025-04-04T17:44:14+00:00"
    },
    {
        "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
        "authors": [
            "NVIDIA",
            ":",
            "Aaron Blakeman",
            "Aarti Basant",
            "Abhinav Khattar",
            "Adithya Renduchintala",
            "Akhiad Bercovich",
            "Aleksander Ficek",
            "Alexis Bjorlin",
            "Ali Taghibakhshi",
            "Amala Sanjay Deshmukh",
            "Ameya Sunil Mahabaleshwarkar",
            "Andrew Tao",
            "Anna Shors",
            "Ashwath Aithal",
            "Ashwin Poojary",
            "Ayush Dattagupta",
            "Balaram Buddharaju",
            "Bobby Chen",
            "Boris Ginsburg",
            "Boxin Wang",
            "Brandon Norick",
            "Brian Butterfield",
            "Bryan Catanzaro",
            "Carlo del Mundo",
            "Chengyu Dong",
            "Christine Harvey",
            "Christopher Parisien",
            "Dan Su",
            "Daniel Korzekwa",
            "Danny Yin",
            "Daria Gitman",
            "David Mosallanezhad",
            "Deepak Narayanan",
            "Denys Fridman",
            "Dima Rekesh",
            "Ding Ma",
            "Dmytro Pykhtar",
            "Dong Ahn",
            "Duncan Riach",
            "Dusan Stosic",
            "Eileen Long",
            "Elad Segal",
            "Ellie Evans",
            "Eric Chung",
            "Erick Galinkin",
            "Evelina Bakhturina",
            "Ewa Dobrowolska",
            "Fei Jia",
            "Fuxiao Liu",
            "Gargi Prasad",
            "Gerald Shen",
            "Guilin Liu",
            "Guo Chen",
            "Haifeng Qian",
            "Helen Ngo",
            "Hongbin Liu",
            "Hui Li",
            "Igor Gitman",
            "Ilia Karmanov",
            "Ivan Moshkov",
            "Izik Golan",
            "Jan Kautz",
            "Jane Polak Scowcroft",
            "Jared Casper",
            "Jarno Seppanen",
            "Jason Lu",
            "Jason Sewall",
            "Jiaqi Zeng",
            "Jiaxuan You",
            "Jimmy Zhang",
            "Jing Zhang",
            "Jining Huang",
            "Jinze Xue",
            "Jocelyn Huang",
            "Joey Conway",
            "John Kamalu",
            "Jon Barker",
            "Jonathan Cohen",
            "Joseph Jennings",
            "Jupinder Parmar",
            "Karan Sapra",
            "Kari Briski",
            "Kateryna Chumachenko",
            "Katherine Luna",
            "Keshav Santhanam",
            "Kezhi Kong",
            "Kirthi Sivamani",
            "Krzysztof Pawelec",
            "Kumar Anik",
            "Kunlun Li",
            "Lawrence McAfee",
            "Leon Derczynski",
            "Lindsey Pavao",
            "Luis Vega",
            "Lukas Voegtle",
            "Maciej Bala",
            "Maer Rodrigues de Melo",
            "Makesh Narsimhan Sreedhar",
            "Marcin Chochowski",
            "Markus Kliegl",
            "Marta Stepniewska-Dziubinska",
            "Matthieu Le",
            "Matvei Novikov",
            "Mehrzad Samadi",
            "Michael Andersch",
            "Michael Evans",
            "Miguel Martinez",
            "Mike Chrzanowski",
            "Mike Ranzinger",
            "Mikolaj Blaz",
            "Misha Smelyanskiy",
            "Mohamed Fawzy",
            "Mohammad Shoeybi",
            "Mostofa Patwary",
            "Nayeon Lee",
            "Nima Tajbakhsh",
            "Ning Xu",
            "Oleg Rybakov",
            "Oleksii Kuchaiev",
            "Olivier Delalleau",
            "Osvald Nitski",
            "Parth Chadha",
            "Pasha Shamis",
            "Paulius Micikevicius",
            "Pavlo Molchanov",
            "Peter Dykas",
            "Philipp Fischer",
            "Pierre-Yves Aquilanti",
            "Piotr Bialecki",
            "Prasoon Varshney",
            "Pritam Gundecha",
            "Przemek Tredak",
            "Rabeeh Karimi",
            "Rahul Kandu",
            "Ran El-Yaniv",
            "Raviraj Joshi",
            "Roger Waleffe",
            "Ruoxi Zhang",
            "Sabrina Kavanaugh",
            "Sahil Jain",
            "Samuel Kriman",
            "Sangkug Lym",
            "Sanjeev Satheesh",
            "Saurav Muralidharan",
            "Sean Narenthiran",
            "Selvaraj Anandaraj",
            "Seonmyeong Bak",
            "Sergey Kashirsky",
            "Seungju Han",
            "Shantanu Acharya",
            "Shaona Ghosh",
            "Sharath Turuvekere Sreenivas",
            "Sharon Clay",
            "Shelby Thomas",
            "Shrimai Prabhumoye",
            "Shubham Pachori",
            "Shubham Toshniwal",
            "Shyamala Prayaga",
            "Siddhartha Jain",
            "Sirshak Das",
            "Slawek Kierat",
            "Somshubra Majumdar",
            "Song Han",
            "Soumye Singhal",
            "Sriharsha Niverty",
            "Stefania Alborghetti",
            "Suseella Panguluri",
            "Swetha Bhendigeri",
            "Syeda Nahida Akter",
            "Szymon Migacz",
            "Tal Shiri",
            "Terry Kong",
            "Timo Roman",
            "Tomer Ronen",
            "Trisha Saar",
            "Tugrul Konuk",
            "Tuomas Rintamaki",
            "Tyler Poon",
            "Ushnish De",
            "Vahid Noroozi",
            "Varun Singh",
            "Vijay Korthikanti",
            "Vitaly Kurin",
            "Wasi Uddin Ahmad",
            "Wei Du",
            "Wei Ping",
            "Wenliang Dai",
            "Wonmin Byeon",
            "Xiaowei Ren",
            "Yao Xu",
            "Yejin Choi",
            "Yian Zhang",
            "Ying Lin",
            "Yoshi Suhara",
            "Zhiding Yu",
            "Zhiqi Li",
            "Zhiyu Li",
            "Zhongbo Zhu",
            "Zhuolin Yang",
            "Zijia Chen"
        ],
        "summary": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. All Nemotron-H models will be released, with\nsupport in Hugging Face, NeMo, and Megatron-LM.",
        "published": "2025-04-04T17:41:58+00:00"
    },
    {
        "title": "VISTA-OCR: Towards generative and interactive end to end OCR models",
        "authors": [
            "Laziz Hamdi",
            "Amine Tamasna",
            "Pascal Boisson",
            "Thierry Paquet"
        ],
        "summary": "We introduce \\textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis\nOCR), a lightweight architecture that unifies text detection and recognition\nwithin a single generative model. Unlike conventional methods that require\nseparate branches with dedicated parameters for text recognition and detection,\nour approach leverages a Transformer decoder to sequentially generate text\ntranscriptions and their spatial coordinates in a unified branch. Built on an\nencoder-decoder architecture, VISTA-OCR is progressively trained, starting with\nthe visual feature extraction phase, followed by multitask learning with\nmultimodal token generation. To address the increasing demand for versatile OCR\nsystems capable of advanced tasks, such as content-based text localization\n\\ref{content_based_localization}, we introduce new prompt-controllable OCR\ntasks during pre-training.To enhance the model's capabilities, we built a new\ndataset composed of real-world examples enriched with bounding box annotations\nand synthetic samples. Although recent Vision Large Language Models (VLLMs) can\nefficiently perform these tasks, their high computational cost remains a\nbarrier for practical deployment. In contrast, our VISTA$_{\\text{omni}}$\nvariant processes both handwritten and printed documents with only 150M\nparameters, interactively, by prompting. Extensive experiments on multiple\ndatasets demonstrate that VISTA-OCR achieves better performance compared to\nstate-of-the-art specialized models on standard OCR tasks while showing strong\npotential for more sophisticated OCR applications, addressing the growing need\nfor interactive OCR systems. All code and annotations for VISTA-OCR will be\nmade publicly available upon acceptance.",
        "published": "2025-04-04T17:39:53+00:00"
    },
    {
        "title": "Higgsing Transitions from Topological Field Theory & Non-Invertible Symmetry in Chern-Simons Matter Theories",
        "authors": [
            "Clay Cordova",
            "Diego Garc\u00eda-Sep\u00falveda",
            "Kantaro Ohmori"
        ],
        "summary": "Non-invertible one-form symmetries are naturally realized in (2+1)d\ntopological quantum field theories. In this work, we consider the potential\nrealization of such symmetries in (2+1)d conformal field theories,\ninvestigating whether gapless systems can exhibit similar symmetry structures.\nTo that end, we discuss transitions between topological field theories in\n(2+1)d which are driven by the Higgs mechanism in Chern-Simons matter theories.\nSuch transitions can be modeled mesoscopically by filling spacetime with a\nlattice-shaped domain wall network separating the two topological phases. Along\nthe domain walls are coset conformal field theories describing gapless chiral\nmodes trapped by a locally vanishing scalar mass. In this presentation, the\none-form symmetries of the transition point can be deduced by using anyon\ncondensation to track lines through the domain wall network. Using this\nframework, we discuss a variety of concrete examples of non-invertible one-form\nsymmetry in fixed-point theories. For instance, $SU(k)_{2}$ Chern-Simons theory\ncoupled to a scalar in the symmetric tensor representation produces a\ntransition from an $SU(k)_{2}$ phase to an $SO(k)_{4}$ phase and has\nnon-invertible one-form symmetry $PSU(2)_{-k}$ at the fixed point. We also\ndiscuss theories with $Spin(2N)$ and $E_{7}$ gauge groups manifesting other\npatterns of non-invertible one-form symmetry. In many of our examples, the\nnon-invertible one-form symmetry is not a modular invariant TQFT on its own and\nthus is an intrinsic part of the fixed-point dynamics.",
        "published": "2025-04-04T17:33:55+00:00"
    },
    {
        "title": "Optimization of a Triangular Delaunay Mesh Generator using Reinforcement Learning",
        "authors": [
            "Will Thacher",
            "Per-Olof Persson",
            "Yulong Pan"
        ],
        "summary": "In this work we introduce a triangular Delaunay mesh generator that can be\ntrained using reinforcement learning to maximize a given mesh quality metric.\nOur mesh generator consists of a graph neural network that distributes and\nmodifies vertices, and a standard Delaunay algorithm to triangulate the\nvertices. We explore various design choices and evaluate our mesh generator on\nvarious tasks including mesh generation, mesh improvement, and producing\nvariable resolution meshes. The learned mesh generator outputs meshes that are\ncomparable to those produced by Triangle and DistMesh, two popular\nDelaunay-based mesh generators.",
        "published": "2025-04-04T17:30:50+00:00"
    },
    {
        "title": "Multimodal Diffusion Bridge with Attention-Based SAR Fusion for Satellite Image Cloud Removal",
        "authors": [
            "Yuyang Hu",
            "Suhas Lohit",
            "Ulugbek S. Kamilov",
            "Tim K. Marks"
        ],
        "summary": "Deep learning has achieved some success in addressing the challenge of cloud\nremoval in optical satellite images, by fusing with synthetic aperture radar\n(SAR) images. Recently, diffusion models have emerged as powerful tools for\ncloud removal, delivering higher-quality estimation by sampling from cloud-free\ndistributions, compared to earlier methods. However, diffusion models initiate\nsampling from pure Gaussian noise, which complicates the sampling trajectory\nand results in suboptimal performance. Also, current methods fall short in\neffectively fusing SAR and optical data. To address these limitations, we\npropose Diffusion Bridges for Cloud Removal, DB-CR, which directly bridges\nbetween the cloudy and cloud-free image distributions. In addition, we propose\na novel multimodal diffusion bridge architecture with a two-branch backbone for\nmultimodal image restoration, incorporating an efficient backbone and dedicated\ncross-modality fusion blocks to effectively extract and fuse features from\nsynthetic aperture radar (SAR) and optical images. By formulating cloud removal\nas a diffusion-bridge problem and leveraging this tailored architecture, DB-CR\nachieves high-fidelity results while being computationally efficient. We\nevaluated DB-CR on the SEN12MS-CR cloud-removal dataset, demonstrating that it\nachieves state-of-the-art results.",
        "published": "2025-04-04T17:25:49+00:00"
    },
    {
        "title": "AdaViT: Adaptive Vision Transformer for Flexible Pretrain and Finetune with Variable 3D Medical Image Modalities",
        "authors": [
            "Badhan Kumar Das",
            "Gengyan Zhao",
            "Han Liu",
            "Thomas J. Re",
            "Dorin Comaniciu",
            "Eli Gibson",
            "Andreas Maier"
        ],
        "summary": "Pretrain techniques, whether supervised or self-supervised, are widely used\nin deep learning to enhance model performance. In real-world clinical\nscenarios, different sets of magnetic resonance (MR) contrasts are often\nacquired for different subjects/cases, creating challenges for deep learning\nmodels assuming consistent input modalities among all the cases and between\npretrain and finetune. Existing methods struggle to maintain performance when\nthere is an input modality/contrast set mismatch with the pretrained model,\noften resulting in degraded accuracy. We propose an adaptive Vision Transformer\n(AdaViT) framework capable of handling variable set of input modalities for\neach case. We utilize a dynamic tokenizer to encode different input image\nmodalities to tokens and take advantage of the characteristics of the\ntransformer to build attention mechanism across variable length of tokens.\nThrough extensive experiments, we demonstrate that this architecture\neffectively transfers supervised pretrained models to new datasets with\ndifferent input modality/contrast sets, resulting in superior performance on\nzero-shot testing, few-shot finetuning, and backward transferring in brain\ninfarct and brain tumor segmentation tasks. Additionally, for self-supervised\npretrain, the proposed method is able to maximize the pretrain data and\nfacilitate transferring to diverse downstream tasks with variable sets of input\nmodalities.",
        "published": "2025-04-04T16:57:06+00:00"
    },
    {
        "title": "CAMINO: Cloud-native Autonomous Management and Intent-based Orchestrator",
        "authors": [
            "Konstantinos Antonakoglou",
            "Ioannis Mavromatis",
            "Saptarshi Ghosh",
            "Mark Rouse",
            "Konstantinos Katsaros"
        ],
        "summary": "This paper introduces CAMINO, a Cloud-native Autonomous Management and\nIntent-based Orchestrator designed to address the challenges of scalable,\ndeclarative, and cloud-native service management and orchestration. CAMINO\nleverages a modular architecture, the Configuration-as-Data (CaD) paradigm, and\nreal-time resource monitoring to facilitate zero-touch provisioning across\nmulti-edge infrastructure. By incorporating intent-driven orchestration and\nobservability capabilities, CAMINO enables automated lifecycle management of\nnetwork functions, ensuring optimized resource utilisation. The proposed\nsolution abstracts complex configurations into high-level intents, offering a\nscalable approach to orchestrating services in distributed cloud-native\ninfrastructures. This paper details CAMINO's system architecture,\nimplementation, and key benefits, highlighting its effectiveness in\ncloud-native telecommunications environments.",
        "published": "2025-04-04T16:55:23+00:00"
    },
    {
        "title": "Variational Quantum Self-Organizing Map",
        "authors": [
            "Amol Deshmukh"
        ],
        "summary": "We propose a novel quantum neural network architecture for unsupervised\nlearning of classical and quantum data based on the kernelized version of\nKohonen's self-organizing map. The central idea behind our algorithm is to\nreplace the Euclidean distance metric with the fidelity between quantum states\nto identify the best matching unit from the low-dimensional grid of output\nneurons in the self-organizing map. The fidelities between the unknown quantum\nstate and the quantum states containing the variational parameters are\nestimated by computing the transition probability on a quantum computer. The\nestimated fidelities are in turn used to adjust the variational parameters of\nthe output neurons. Unlike $\\mathcal{O}(N^{2})$ circuit evaluations needed in\nquantum kernel estimation, our algorithm requires $\\mathcal{O}(N)$ circuit\nevaluations for $N$ data samples. Analogous to the classical version of the\nself-organizing map, our algorithm learns a mapping from a high-dimensional\nHilbert space to a low-dimensional grid of lattice points while preserving the\nunderlying topology of the Hilbert space. We showcase the effectiveness of our\nalgorithm by constructing a two-dimensional visualization that accurately\ndifferentiates between the three distinct species of flowers in Fisher's Iris\ndataset. In addition, we demonstrate the efficacy of our approach on quantum\ndata by creating a two-dimensional map that preserves the topology of the state\nspace in the Schwinger model and distinguishes between the two separate phases\nof the model at $\\theta = \\pi$.",
        "published": "2025-04-04T16:48:35+00:00"
    },
    {
        "title": "Scalable Hypergraph Structure Learning with Diverse Smoothness Priors",
        "authors": [
            "Benjamin T. Brown",
            "Haoxiang Zhang",
            "Daniel L. Lau",
            "Gonzalo R. Arce"
        ],
        "summary": "In graph signal processing, learning the weighted connections between nodes\nfrom a set of sample signals is a fundamental task when the underlying\nrelationships are not known a priori. This task is typically addressed by\nfinding a graph Laplacian on which the observed signals are smooth. With the\nextension of graphs to hypergraphs - where edges can connect more than two\nnodes - graph learning methods have similarly been generalized to hypergraphs.\nHowever, the absence of a unified framework for calculating total variation has\nled to divergent definitions of smoothness and, consequently, differing\napproaches to hyperedge recovery. We confront this challenge through\ngeneralization of several previously proposed hypergraph total variations,\nsubsequently allowing ease of substitution into a vector based optimization. To\nthis end, we propose a novel hypergraph learning method that recovers a\nhypergraph topology from time-series signals based on a smoothness prior. Our\napproach addresses key limitations in prior works, such as hyperedge selection\nand convergence issues, by formulating the problem as a convex optimization\nsolved via a forward-backward-forward algorithm, ensuring guaranteed\nconvergence. Additionally, we introduce a process that simultaneously limits\nthe span of the hyperedge search and maintains a valid hyperedge selection set.\nIn doing so, our method becomes scalable in increasingly complex network\nstructures. The experimental results demonstrate improved performance, in terms\nof accuracy, over other state-of-the-art hypergraph inference methods;\nfurthermore, we empirically show our method to be robust to total variation\nterms, biased towards global smoothness, and scalable to larger hypergraphs.",
        "published": "2025-04-04T16:47:30+00:00"
    },
    {
        "title": "Heterogeneous Resource Allocation for Ensuring End-to-End Quality of Service in Multi-hop Integrated Access and Backhaul Network",
        "authors": [
            "Shuaifeng Zhang"
        ],
        "summary": "Faced with increasing network traffic demands, cell dense deployment is one\nof significant means to utilize spectrum resources efficiently to improve\nnetwork capacity. Multi-hop integrated access and backhaul (IAB) architectures\nhave emerged as a cost-effective solution for network densification. Meanwhile,\ndynamic time division duplex (D-TDD) is a promising solution to adapt to highly\ndynamic scenarios with asymmetric uplink and downlink traffic. Thus, dynamic\nresource allocation between backhaul and access links and high spectral\nefficiency under ensuring reliable transmission are two key objectives of IAB\nresearch. However, due to huge solution space, there are some challenges in\nmulti-hop IAB with D-TDD if only an integrated optimization problem (IOP) is\nconsidered. To handle these challenges, we decompose the IOP into sub-problems\nto reduce the solution space. To tackle these sub-problems, we formulate them\nseparately as the non-cooperative games and design the corresponding utility\nfunctions to guarantee the existence of Nash equilibrium solutions. Also, to\nachieve the system-wide solution, we propose a single-leader heterogeneous\nmulti-follower Stackelberg-game-based resource allocation scheme, which can\ncombine the solving results of all the sub-problems to get the IOP approximate\nsolution. Simulation results show that the proposed scheme can improve\nthroughput performance while meeting spectrum energy efficiency constraints.",
        "published": "2025-04-04T16:29:08+00:00"
    },
    {
        "title": "Lambda/6 Suspended Patch Antenna",
        "authors": [
            "Luca Giangrande"
        ],
        "summary": "This work introduces a novel, compact antenna design based on a lambda-6th\nsuspended patch configuration that is particularly suited for small-size\nwireless sensor nodes. The proposed design meets key requirements such as\ncompactness, omnidirectionality, robust source matching over a designated\nbandwidth, interference immunity, and low costs by evolving the conventional\nsquare patch antenna. With a footprint of only 20-by-20 mm, the antenna\nincorporates a grounded metal shield to both reduce its effective dimensions\nbelow one-half wavelength and mitigate interference from nearby circuitry.\nSimulation results, conducted on a cost-effective FR4 substrate, demonstrate a\nresonance at 2.45 GHz with a return loss of -32.5 dB and a bandwidth of 50 MHz\n(at the -10 dB level), making this design an attractive candidate for\nintegration into densely populated wireless sensor networks.",
        "published": "2025-04-04T16:13:45+00:00"
    },
    {
        "title": "Large plastic deformation of voids in crystals",
        "authors": [
            "Jalal Smiri",
            "Joseph Paux",
            "Oguz Umut Salman",
            "Ioan R. Ionescu"
        ],
        "summary": "The mechanisms of void growth and coalescence are key contributors to the\nductile failure of crystalline materials. At the grain scale, single crystal\nplastic anisotropy induces large strain localization leading to complex shape\nevolutions. In this study, an Arbitrary Lagrangian-Eulerian (ALE) framework for\n2D crystal plasticity combined with dynamic remeshing is used to study the 2D\nshape evolution of cylindrical voids in single crystals. The large deformation\nand shape evolution of the voids under two types of loading are considered: (i)\nradial and (ii) uni-axial loadings. In both cases, the voids undergo complex\nshape evolutions induced by the interactions between slip bands, lattice\nrotations and large strain phenomena. In case (i), the onset of the deformation\nrevealed the formation of a complex fractal network of slip bands around the\nvoids. Then, large deformations unearth an unexpected evolution of the slip\nbands network associated with significant lattice rotations, leading to a final\nhexagonal shape for the void. In case (ii), we obtain shear bands with very\nlarge accumulated plastic strain (> 200%) compared to the macroscopic\nengineering strains (< 15%). A high dependence between crystalline\norientations, slip band localization and therefore shape evolution was\nobserved, concluding in a high dependency between crystalline orientation and\nvoid shape elongation, which is of prime importance regarding coalescence of\nthe voids, thus to the formation of macro-cracks.",
        "published": "2025-04-04T16:12:32+00:00"
    },
    {
        "title": "Exploring Various Sequential Learning Methods for Deformation History Modeling",
        "authors": [
            "Muhammed Adil Yatkin",
            "Mihkel Korgesaar",
            "Jani Romanoff",
            "Umit Islak",
            "Hasan Kurban"
        ],
        "summary": "Current neural network (NN) models can learn patterns from data points with\nhistorical dependence. Specifically, in natural language processing (NLP),\nsequential learning has transitioned from recurrence-based architectures to\ntransformer-based architectures. However, it is unknown which NN architectures\nwill perform the best on datasets containing deformation history due to\nmechanical loading. Thus, this study ascertains the appropriateness of\n1D-convolutional, recurrent, and transformer-based architectures for predicting\ndeformation localization based on the earlier states in the form of deformation\nhistory. Following this investigation, the crucial incompatibility issues\nbetween the mathematical computation of the prediction process in the\nbest-performing NN architectures and the actual values derived from the natural\nphysical properties of the deformation paths are examined in detail.",
        "published": "2025-04-04T15:52:24+00:00"
    },
    {
        "title": "Dense Neural Network Based Arrhythmia Classification on Low-cost and Low-compute Micro-controller",
        "authors": [
            "Md Abu Obaida Zishan",
            "H M Shihab",
            "Sabik Sadman Islam",
            "Maliha Alam Riya",
            "Gazi Mashrur Rahman",
            "Jannatun Noor"
        ],
        "summary": "The electrocardiogram (ECG) monitoring device is an expensive albeit\nessential device for the treatment and diagnosis of cardiovascular diseases\n(CVD). The cost of this device typically ranges from $2000 to $10000. Several\nstudies have implemented ECG monitoring systems in micro-controller units (MCU)\nto reduce industrial development costs by up to 20 times. However, to match\nindustry-grade systems and display heartbeats effectively, it is essential to\ndevelop an efficient algorithm for detecting arrhythmia (irregular heartbeat).\nHence in this study, a dense neural network is developed to detect arrhythmia\non the Arduino Nano. The Nano consists of the ATMega328 microcontroller with a\n16MHz clock, 2KB of SRAM, and 32KB of program memory. Additionally, the AD8232\nSparkFun Single-Lead Heart Rate Monitor is used as the ECG sensor. The\nimplemented neural network model consists of two layers (excluding the input)\nwith 10 and four neurons respectively with sigmoid activation function.\nHowever, four approaches are explored to choose the appropriate activation\nfunctions. The model has a size of 1.267 KB, achieves an F1 score\n(macro-average) of 78.3\\% for classifying four types of arrhythmia, an accuracy\nrate of 96.38%, and requires 0.001314 MOps of floating-point operations\n(FLOPs).",
        "published": "2025-04-04T15:30:02+00:00"
    },
    {
        "title": "PHOENIX: Pauli-Based High-Level Optimization Engine for Instruction Execution on NISQ Devices",
        "authors": [
            "Zhaohui Yang",
            "Dawei Ding",
            "Chenghong Zhu",
            "Jianxin Chen",
            "Yuan Xie"
        ],
        "summary": "Variational quantum algorithms (VQA) based on Hamiltonian simulation\nrepresent a specialized class of quantum programs well-suited for near-term\nquantum computing applications due to its modest resource requirements in terms\nof qubits and circuit depth. Unlike the conventional single-qubit (1Q) and\ntwo-qubit (2Q) gate sequence representation, Hamiltonian simulation programs\nare essentially composed of disciplined subroutines known as Pauli\nexponentiations (Pauli strings with coefficients) that are variably arranged.\nTo capitalize on these distinct program features, this study introduces\nPHOENIX, a highly effective compilation framework that primarily operates at\nthe high-level Pauli-based intermediate representation (IR) for generic\nHamiltonian simulation programs. PHOENIX exploits global program optimization\nopportunities to the greatest extent, compared to existing SOTA methods despite\nsome of them also utilizing similar IRs. PHOENIX employs the binary symplectic\nform (BSF) to formally describe Pauli strings and reformulates IR synthesis as\nreducing the column weights of BSF by appropriate Clifford transformations. It\ncomes with a heuristic BSF simplification algorithm that searches for the most\nappropriate 2Q Clifford operators in sequence to maximally simplify the BSF at\neach step, until the BSF can be directly synthesized by basic 1Q and 2Q gates.\nPHOENIX further performs a global ordering strategy in a Tetris-like fashion\nfor these simplified IR groups, carefully balancing optimization opportunities\nfor gate cancellation, minimizing circuit depth, and managing qubit routing\noverhead. Experimental results demonstrate that PHOENIX outperforms SOTA VQA\ncompilers across diverse program categories, backend ISAs, and hardware\ntopologies.",
        "published": "2025-04-04T15:29:18+00:00"
    },
    {
        "title": "RANa: Retrieval-Augmented Navigation",
        "authors": [
            "Gianluca Monaci",
            "Rafael S. Rezende",
            "Romain Deffayet",
            "Gabriela Csurka",
            "Guillaume Bono",
            "Herv\u00e9 D\u00e9jean",
            "St\u00e9phane Clinchant",
            "Christian Wolf"
        ],
        "summary": "Methods for navigation based on large-scale learning typically treat each\nepisode as a new problem, where the agent is spawned with a clean memory in an\nunknown environment. While these generalization capabilities to an unknown\nenvironment are extremely important, we claim that, in a realistic setting, an\nagent should have the capacity of exploiting information collected during\nearlier robot operations. We address this by introducing a new\nretrieval-augmented agent, trained with RL, capable of querying a database\ncollected from previous episodes in the same environment and learning how to\nintegrate this additional context information. We introduce a unique agent\narchitecture for the general navigation task, evaluated on ObjectNav, ImageNav\nand Instance-ImageNav. Our retrieval and context encoding methods are\ndata-driven and heavily employ vision foundation models (FM) for both semantic\nand geometric understanding. We propose new benchmarks for these settings and\nwe show that retrieval allows zero-shot transfer across tasks and environments\nwhile significantly improving performance.",
        "published": "2025-04-04T15:22:02+00:00"
    },
    {
        "title": "Dynamic Training Enhances Machine Learning Potentials for Long-Lasting Molecular Dynamics",
        "authors": [
            "Ivan \u017dugec",
            "Tin Had\u017ei Veljkovi\u0107",
            "Maite Alducin",
            "J. I\u00f1aki Juaristi"
        ],
        "summary": "Molecular Dynamics (MD) simulations are vital for exploring complex systems\nin computational physics and chemistry. While machine learning methods\ndramatically reduce computational costs relative to ab initio methods, their\naccuracy in long-lasting simulations remains limited. Here we propose dynamic\ntraining (DT), a method designed to enhance model performance over extended MD\nsimulations. Applying DT to an equivariant graph neural network (EGNN) on the\nchallenging system of a hydrogen molecule interacting with a palladium cluster\nanchored to a graphene vacancy demonstrates a superior prediction accuracy\ncompared to conventional approaches. Crucially, the DT architecture-independent\ndesign ensures its applicability across diverse machine learning potentials,\nmaking it a practical tool for advancing MD simulations.",
        "published": "2025-04-04T15:18:48+00:00"
    },
    {
        "title": "Hierarchical Knowledge Structuring for Effective Federated Learning in Heterogeneous Environments",
        "authors": [
            "Wai Fong Tam",
            "Qilei Li",
            "Ahmed M. Abdelmonie"
        ],
        "summary": "Federated learning enables collaborative model training across distributed\nentities while maintaining individual data privacy. A key challenge in\nfederated learning is balancing the personalization of models for local clients\nwith generalization for the global model. Recent efforts leverage logit-based\nknowledge aggregation and distillation to overcome these issues. However, due\nto the non-IID nature of data across diverse clients and the imbalance in the\nclient's data distribution, directly aggregating the logits often produces\nbiased knowledge that fails to apply to individual clients and obstructs the\nconvergence of local training. To solve this issue, we propose a Hierarchical\nKnowledge Structuring (HKS) framework that formulates sample logits into a\nmulti-granularity codebook to represent logits from personalized per-sample\ninsights to globalized per-class knowledge. The unsupervised bottom-up\nclustering method is leveraged to enable the global server to provide\nmulti-granularity responses to local clients. These responses allow local\ntraining to integrate supervised learning objectives with global generalization\nconstraints, which results in more robust representations and improved\nknowledge sharing in subsequent training rounds. The proposed framework's\neffectiveness is validated across various benchmarks and model architectures.",
        "published": "2025-04-04T15:06:02+00:00"
    },
    {
        "title": "Operator Learning: A Statistical Perspective",
        "authors": [
            "Unique Subedi",
            "Ambuj Tewari"
        ],
        "summary": "Operator learning has emerged as a powerful tool in scientific computing for\napproximating mappings between infinite-dimensional function spaces. A primary\napplication of operator learning is the development of surrogate models for the\nsolution operators of partial differential equations (PDEs). These methods can\nalso be used to develop black-box simulators to model system behavior from\nexperimental data, even without a known mathematical model. In this article, we\nbegin by formalizing operator learning as a function-to-function regression\nproblem and review some recent developments in the field. We also discuss\nPDE-specific operator learning, outlining strategies for incorporating physical\nand mathematical constraints into architecture design and training processes.\nFinally, we end by highlighting key future directions such as active data\ncollection and the development of rigorous uncertainty quantification\nframeworks.",
        "published": "2025-04-04T14:58:45+00:00"
    },
    {
        "title": "Optimistic Learning for Communication Networks",
        "authors": [
            "George Iosifidis",
            "Naram Mhaisen",
            "Douglas J. Leith"
        ],
        "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
        "published": "2025-04-04T14:55:27+00:00"
    },
    {
        "title": "Hybrid Real- and Complex-valued Neural Network Architecture",
        "authors": [
            "Alex Young",
            "Luan Vin\u00edcius Fiorio",
            "Bo Yang",
            "Boris Karanov",
            "Wim van Houtum",
            "Ronald M. Aarts"
        ],
        "summary": "We propose a \\emph{hybrid} real- and complex-valued \\emph{neural network}\n(HNN) architecture, designed to combine the computational efficiency of\nreal-valued processing with the ability to effectively handle complex-valued\ndata. We illustrate the limitations of using real-valued neural networks\n(RVNNs) for inherently complex-valued problems by showing how it learnt to\nperform complex-valued convolution, but with notable inefficiencies stemming\nfrom its real-valued constraints. To create the HNN, we propose to use building\nblocks containing both real- and complex-valued paths, where information\nbetween domains is exchanged through domain conversion functions. We also\nintroduce novel complex-valued activation functions, with higher generalisation\nand parameterisation efficiency. HNN-specific architecture search techniques\nare described to navigate the larger solution space. Experiments with the\nAudioMNIST dataset demonstrate that the HNN reduces cross-entropy loss and\nconsumes less parameters compared to an RVNN for all considered cases. Such\nresults highlight the potential for the use of partially complex-valued\nprocessing in neural networks and applications for HNNs in many signal\nprocessing domains.",
        "published": "2025-04-04T14:52:44+00:00"
    },
    {
        "title": "Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems",
        "authors": [
            "Alexander Windmann",
            "Henrik Steude",
            "Daniel Boschmann",
            "Oliver Niggemann"
        ],
        "summary": "Cyber-Physical Systems (CPS) in domains such as manufacturing and energy\ndistribution generate complex time series data crucial for Prognostics and\nHealth Management (PHM). While Deep Learning (DL) methods have demonstrated\nstrong forecasting capabilities, their adoption in industrial CPS remains\nlimited due insufficient robustness. Existing robustness evaluations primarily\nfocus on formal verification or adversarial perturbations, inadequately\nrepresenting the complexities encountered in real-world CPS scenarios. To\naddress this, we introduce a practical robustness definition grounded in\ndistributional robustness, explicitly tailored to industrial CPS, and propose a\nsystematic framework for robustness evaluation. Our framework simulates\nrealistic disturbances, such as sensor drift, noise and irregular sampling,\nenabling thorough robustness analyses of forecasting models on real-world CPS\ndatasets. The robustness definition provides a standardized score to quantify\nand compare model performance across diverse datasets, assisting in informed\nmodel selection and architecture design. Through extensive empirical studies\nevaluating prominent DL architectures (including recurrent, convolutional,\nattention-based, modular, and structured state-space models) we demonstrate the\napplicability and effectiveness of our approach. We publicly release our\nrobustness benchmark to encourage further research and reproducibility.",
        "published": "2025-04-04T14:50:48+00:00"
    },
    {
        "title": "Tracking and Distinguishing Slime Mold Solutions to the Traveling Salesman Problem through Synchronized Amplification in the Non-Equilibrium Steady State",
        "authors": [
            "Suyash Bajpai",
            "Masashi Aono",
            "Philip Kurian"
        ],
        "summary": "The plasmodium of the true slime mold Physarum polycephalum serves as a\nplatform to study information processing in non-equilibrium active matter,\nshowing complex oscillations and computation despite its simple morphology.\nPast experiments used Physarum's shape changes and photoavoidance in a stellate\nchip to find approximate solutions to the traveling salesman problem (TSP) for\nup to eight cities in linear time. To solve the $N$-city TSP, the organism\nelongated and withdrew its $N^2$ branches within the chip lanes, where an\noptical feedback loop controlled by a modified Hopfield neural network\nselectively illuminated certain lanes to trigger retraction of $N(N-1)$\nbranches. When the modified Hopfield network stabilizes the illumination\npattern, the organism reaches a non-equilibrium steady state (NESS), where $N$\nextended branches form a valid TSP solution. The illumination pattern induces a\nclear split between two distinct lane groups. Fourier and power spectral\ndensity analyses reveal that in NESS, the solution lanes exhibit lower\nfrequency, larger-amplitude oscillations, an enhancement of these signals\ncompared to the higher-frequency, smaller-amplitude fluctuations in the\nnon-solution lanes. This frequency downconversion and amplification in power\ndensity is a hallmark of a Fr\\\"ohlich condensate. Synchronization indices for\nPhysarum-selected solution lanes in the NESS exhibit maximum $S\\sim1$, while\nnon-solution lanes reach minimum $S$-a consistent trend across problem sizes\nand tour lengths. Amoeba-inspired algorithms that leverage noise to accelerate\nTSP convergence make trade-offs between solution quality and speed, showing up\nto $\\sqrt N$ scaling in iterations, akin to quantum algorithms like Grover's\nsearch. By tuning power density, frequency shifts, and synchronization, it may\nbe possible to improve the quality and efficiency of TSP solutions from\nPhysarum-based biocomputers.",
        "published": "2025-04-04T14:47:46+00:00"
    },
    {
        "title": "BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for Single Image Super-Resolution",
        "authors": [
            "Zihao He",
            "Shengchuan Zhang",
            "Runze Hu",
            "Yunhang Shen",
            "Yan Zhang"
        ],
        "summary": "Super-resolution (SR) techniques are critical for enhancing image quality,\nparticularly in scenarios where high-resolution imagery is essential yet\nlimited by hardware constraints. Existing diffusion models for SR have relied\npredominantly on Gaussian models for noise generation, which often fall short\nwhen dealing with the complex and variable texture inherent in natural scenes.\nTo address these deficiencies, we introduce the Bayesian Uncertainty Guided\nDiffusion Probabilistic Model (BUFF). BUFF distinguishes itself by\nincorporating a Bayesian network to generate high-resolution uncertainty masks.\nThese masks guide the diffusion process, allowing for the adjustment of noise\nintensity in a manner that is both context-aware and adaptive. This novel\napproach not only enhances the fidelity of super-resolved images to their\noriginal high-resolution counterparts but also significantly mitigates\nartifacts and blurring in areas characterized by complex textures and fine\ndetails. The model demonstrates exceptional robustness against complex noise\npatterns and showcases superior adaptability in handling textures and edges\nwithin images. Empirical evidence, supported by visual results, illustrates the\nmodel's robustness, especially in challenging scenarios, and its effectiveness\nin addressing common SR issues such as blurring. Experimental evaluations\nconducted on the DIV2K dataset reveal that BUFF achieves a notable improvement,\nwith a +0.61 increase compared to baseline in SSIM on BSD100, surpassing\ntraditional diffusion approaches by an average additional +0.20dB PSNR gain.\nThese findings underscore the potential of Bayesian methods in enhancing\ndiffusion processes for SR, paving the way for future advancements in the\nfield.",
        "published": "2025-04-04T14:43:45+00:00"
    },
    {
        "title": "The Vaccination Game on Networks",
        "authors": [
            "Kausutua Tjikundi",
            "Mark Broom"
        ],
        "summary": "Vaccinations are an important tool in the prevention of disease. Vaccinations\nare generally voluntary for each member of a population and vaccination\ndecisions are influenced by individual risk perceptions and contact structures\nwithin populations. In this study, we model vaccination uptake as an\nevolutionary game where individuals weigh perceived morbidity risks from both\nvaccination and infection. We incorporate epidemiological dynamics using an SIR\nmodel structured on networks, allowing us to determine the evolutionarily\nstable vaccination level for any given network topology. Our analysis shows\nthat vaccination coverage varies across networks depending on their structure\nand the relative cost of vaccination (the ratio of vaccine morbidity risk to\ninfection morbidity risk). As this cost increases, vaccination uptake decreases\nuniversally, leading to a dominant non-vaccinator strategy when the cost is\nhigh. We find that networks with low to moderate degree variability have a\nrelatively low evolutionarily stable vaccination level when this cost is low,\nas in such populations lower vaccination levels are necessary to achieve\nequivalent levels of disease prevalence to more heterogeneous networks with\nhigh degree variability, which thus show higher vaccination levels at lower\nrelative costs. However, for heterogeneous networks, vaccination levels decline\nfaster as costs rise, eventually falling below the level for the more\nhomogeneous networks. Our findings align with previous studies on vaccination\nthresholds in structured populations and highlight how network heterogeneity\ninfluences vaccination dynamics.",
        "published": "2025-04-04T14:43:39+00:00"
    },
    {
        "title": "Gaussian Process Tilted Nonparametric Density Estimation using Fisher Divergence Score Matching",
        "authors": [
            "John Paisley",
            "Wei Zhang",
            "Brian Barr"
        ],
        "summary": "We present three Fisher divergence (FD) minimization algorithms for learning\nGaussian process (GP) based score models for lower dimensional density\nestimation problems. The density is formed by multiplying a base multivariate\nnormal distribution with an exponentiated GP refinement, and so we refer to it\nas a GP-tilted nonparametric density. By representing the GP part of the score\nas a linear function using the random Fourier feature (RFF) approximation, we\nshow that all learning problems can be solved in closed form. This includes the\nbasic and noise conditional versions of the Fisher divergence, as well as a\nnovel alternative to noise conditional FD models based on variational inference\n(VI). Here, we propose using an ELBO-like optimization of the approximate\nposterior with which we derive a Fisher variational predictive distribution.\nThe RFF representation of the GP, which is functionally equivalent to a single\nlayer neural network score model with cosine activation, provides a unique\nlinear form for which all expectations are in closed form. The Gaussian base\nalso helps with tractability of the VI approximation. We demonstrate our three\nlearning algorithms, as well as a MAP baseline algorithm, on several low\ndimensional density estimation problems. The closed-form nature of the learning\nproblem removes the reliance on iterative algorithms, making this technique\nparticularly well-suited to large data sets.",
        "published": "2025-04-04T14:41:41+00:00"
    },
    {
        "title": "Discovering Partially Known Ordinary Differential Equations: a Case Study on the Chemical Kinetics of Cellulose Degradation",
        "authors": [
            "Federica Bragone",
            "Kateryna Morozovska",
            "Tor Laneryd",
            "Khemraj Shukla",
            "Stefano Markidis"
        ],
        "summary": "The degree of polymerization (DP) is one of the methods for estimating the\naging of the polymer based insulation systems, such as cellulose insulation in\npower components. The main degradation mechanisms in polymers are hydrolysis,\npyrolysis, and oxidation. These mechanisms combined cause a reduction of the\nDP. However, the data availability for these types of problems is usually\nscarce. This study analyzes insulation aging using cellulose degradation data\nfrom power transformers. The aging problem for the cellulose immersed in\nmineral oil inside power transformers is modeled with ordinary differential\nequations (ODEs). We recover the governing equations of the degradation system\nusing Physics-Informed Neural Networks (PINNs) and symbolic regression. We\napply PINNs to discover the Arrhenius equation's unknown parameters in the\nEkenstam ODE describing cellulose contamination content and the material aging\nprocess related to temperature for synthetic data and real DP values. A\nmodification of the Ekenstam ODE is given by Emsley's system of ODEs, where the\nrate constant expressed by the Arrhenius equation decreases in time with the\nnew formulation. We use PINNs and symbolic regression to recover the functional\nform of one of the ODEs of the system and to identify an unknown parameter.",
        "published": "2025-04-04T14:41:24+00:00"
    },
    {
        "title": "Online Traffic Density Estimation using Physics-Informed Neural Networks",
        "authors": [
            "Dennis Wilkman",
            "Kateryna Morozovska",
            "Karl Henrik Johansson",
            "Matthieu Barreau"
        ],
        "summary": "Recent works on the application of Physics-Informed Neural Networks to\ntraffic density estimation have shown to be promising for future developments\ndue to their robustness to model errors and noisy data. In this paper, we\nintroduce a methodology for online approximation of the traffic density using\nmeasurements from probe vehicles in two settings: one using the Greenshield\nmodel and the other considering a high-fidelity traffic simulation. The\nproposed method continuously estimates the real-time traffic density in space\nand performs model identification with each new set of measurements. The\ndensity estimate is updated in almost real-time using gradient descent and\nadaptive weights. In the case of full model knowledge, the resulting algorithm\nhas similar performance to the classical open-loop one. However, in the case of\nmodel mismatch, the iterative solution behaves as a closed-loop observer and\noutperforms the baseline method. Similarly, in the high-fidelity setting, the\nproposed algorithm correctly reproduces the traffic characteristics.",
        "published": "2025-04-04T14:41:22+00:00"
    },
    {
        "title": "An all-fibred, telecom technology compatible, room temperature, single-photon source",
        "authors": [
            "Nathan Lecaron",
            "Max Meunier",
            "Gr\u00e9gory Sauder",
            "Romain Dalidet",
            "Yoann Pelet",
            "S\u00e9bastien Tanzilli",
            "Jesus Zuniga-P\u00e9rez",
            "Olivier Alibart"
        ],
        "summary": "Single photon sources are essential building blocks for fundamental quantum\noptics but also for quantum information networks. Their widespread is currently\nhindered by unpractical features, such as operation at cryogenic temperature\nand emission wavelength lying outside telecom windows. Taking advantage of\ntelecom technology and point defects in GaN crystals, we present, for the first\ntime, the development of a fully-fibred source of single photons operating at\nroom temperature, emitting photons in the telecom O-band and fulfilling the\nstandards of telecom photonics. We characterise an emitter producting single\nphotons at the wavelength of 1292\\,nm, a spectral broadening compatible with\nCWDM channels of 13\\,nm, and a brightness of 25 kcps per mW of pump power. The\nsource shows a signal-to-noise ratio of 16.5 and an autocorrelation degree\n(purity) of 0,059 at room temperature, showing high potential for being\nintegrated transportable quantum cryptography devices.",
        "published": "2025-04-04T14:32:05+00:00"
    },
    {
        "title": "Multi-encoder nnU-Net outperforms Transformer models with self-supervised pretraining",
        "authors": [
            "Seyedeh Sahar Taheri Otaghsara",
            "Reza Rahmanzadeh"
        ],
        "summary": "This study addresses the essential task of medical image segmentation, which\ninvolves the automatic identification and delineation of anatomical structures\nand pathological regions in medical images. Accurate segmentation is crucial in\nradiology, as it aids in the precise localization of abnormalities such as\ntumors, thereby enabling effective diagnosis, treatment planning, and\nmonitoring of disease progression. Specifically, the size, shape, and location\nof tumors can significantly influence clinical decision-making and therapeutic\nstrategies, making accurate segmentation a key component of radiological\nworkflows. However, challenges posed by variations in MRI modalities, image\nartifacts, and the scarcity of labeled data complicate the segmentation task\nand impact the performance of traditional models. To overcome these\nlimitations, we propose a novel self-supervised learning Multi-encoder nnU-Net\narchitecture designed to process multiple MRI modalities independently through\nseparate encoders. This approach allows the model to capture modality-specific\nfeatures before fusing them for the final segmentation, thus improving\naccuracy. Our Multi-encoder nnU-Net demonstrates exceptional performance,\nachieving a Dice Similarity Coefficient (DSC) of 93.72%, which surpasses that\nof other models such as vanilla nnU-Net, SegResNet, and Swin UNETR. By\nleveraging the unique information provided by each modality, the model enhances\nsegmentation tasks, particularly in scenarios with limited annotated data.\nEvaluations highlight the effectiveness of this architecture in improving tumor\nsegmentation outcomes.",
        "published": "2025-04-04T14:31:06+00:00"
    },
    {
        "title": "Quantum conditional mutual information as a probe of measurement-induced entanglement phase transitions",
        "authors": [
            "Yuichi Otsuka",
            "Kazuhiro Seki",
            "Seiji Yunoki"
        ],
        "summary": "We propose that the quantum conditional mutual information (QCMI), computed\nwith a suitably chosen partition of the system, serves as a powerful probe for\ndetecting measurement-induced entanglement phase transitions in monitored\nquantum circuits. To demonstrate this, we investigate monitored variable-range\nClifford circuits and identify the phase boundary between volume-law and\narea-law entanglement phases by performing finite-size scaling analyses of the\nQCMI. Assuming that the entanglement entropy exhibits a logarithmic dependence\non system size at criticality in short-range interacting cases, we further show\nthat the QCMI allows for the simultaneous determination of both the critical\npoint and the universal coefficient of the logarithmic term in the entanglement\nentropy via a crossing-point analysis. For the shortest-range interacting case\nstudied, we obtain the thermodynamic-limit value of the coefficient as\n$\\tilde{c}=1.519(3)$, which is significantly smaller than values reported in\nprevious studies.",
        "published": "2025-04-04T14:25:49+00:00"
    },
    {
        "title": "Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis",
        "authors": [
            "Xi Wang",
            "Ziqi He",
            "Yang Zhou"
        ],
        "summary": "Traditional diffusion models typically employ a U-Net architecture. Previous\nstudies have unveiled the roles of attention blocks in the U-Net. However, they\noverlook the dynamic evolution of their importance during the inference\nprocess, which hinders their further exploitation to improve image\napplications. In this study, we first theoretically proved that, re-weighting\nthe outputs of the Transformer blocks within the U-Net is a \"free lunch\" for\nimproving the signal-to-noise ratio during the sampling process. Next, we\nproposed Importance Probe to uncover and quantify the dynamic shifts in\nimportance of the Transformer blocks throughout the denoising process. Finally,\nwe design an adaptive importance-based re-weighting schedule tailored to\nspecific image generation and editing tasks. Experimental results demonstrate\nthat, our approach significantly improves the efficiency of the inference\nprocess, and enhances the aesthetic quality of the samples with identity\nconsistency. Our method can be seamlessly integrated into any U-Net-based\narchitecture. Code: https://github.com/Hytidel/UNetReweighting",
        "published": "2025-04-04T14:23:30+00:00"
    },
    {
        "title": "Physics-informed 4D X-ray image reconstruction from ultra-sparse spatiotemporal data",
        "authors": [
            "Zisheng Yao",
            "Yuhe Zhang",
            "Zhe Hu",
            "Robert Kl\u00f6fkorn",
            "Tobias Ritschel",
            "Pablo Villanueva-Perez"
        ],
        "summary": "The unprecedented X-ray flux density provided by modern X-ray sources offers\nnew spatiotemporal possibilities for X-ray imaging of fast dynamic processes.\nApproaches to exploit such possibilities often result in either i) a limited\nnumber of projections or spatial information due to limited scanning speed, as\nin time-resolved tomography, or ii) a limited number of time points, as in\nstroboscopic imaging, making the reconstruction problem ill-posed and unlikely\nto be solved by classical reconstruction approaches. 4D reconstruction from\nsuch data requires sample priors, which can be included via deep learning (DL).\nState-of-the-art 4D reconstruction methods for X-ray imaging combine the power\nof AI and the physics of X-ray propagation to tackle the challenge of sparse\nviews. However, most approaches do not constrain the physics of the studied\nprocess, i.e., a full physical model. Here we present 4D physics-informed\noptimized neural implicit X-ray imaging (4D-PIONIX), a novel physics-informed\n4D X-ray image reconstruction method combining the full physical model and a\nstate-of-the-art DL-based reconstruction method for 4D X-ray imaging from\nsparse views. We demonstrate and evaluate the potential of our approach by\nretrieving 4D information from ultra-sparse spatiotemporal acquisitions of\nsimulated binary droplet collisions, a relevant fluid dynamic process. We\nenvision that this work will open new spatiotemporal possibilities for various\n4D X-ray imaging modalities, such as time-resolved X-ray tomography and more\nnovel sparse acquisition approaches like X-ray multi-projection imaging, which\nwill pave the way for investigations of various rapid 4D dynamics, such as\nfluid dynamics and composite testing.",
        "published": "2025-04-04T14:18:51+00:00"
    },
    {
        "title": "Identifiability of VAR(1) model in a stationary setting",
        "authors": [
            "Bixuan Liu"
        ],
        "summary": "We consider a classical First-order Vector AutoRegressive (VAR(1)) model,\nwhere we interpret the autoregressive interaction matrix as influence\nrelationships among the components of the VAR(1) process that can be encoded by\na weighted directed graph. A majority of previous work studies the structural\nidentifiability of the graph based on time series observations and therefore\nrelies on dynamical information. In this work we assume that an equilibrium\nexists, and study instead the identifiability of the graph from the stationary\ndistribution, meaning that we seek a way to reconstruct the influence graph\nunderlying the dynamic network using only static information. We use an\napproach from algebraic statistics that characterizes models using the Jacobian\nmatroids associated with the parametrization of the models, and we introduce\nsufficient graphical conditions under which different graphs yield distinct\nsteady-state distributions. Additionally, we illustrate how our results could\nbe applied to characterize networks inspired by ecological research.",
        "published": "2025-04-04T14:17:45+00:00"
    },
    {
        "title": "Generating ensembles of spatially-coherent in-situ forecasts using flow matching",
        "authors": [
            "David Landry",
            "Claire Monteleoni",
            "Anastase Charantonis"
        ],
        "summary": "We propose a machine-learning-based methodology for in-situ weather forecast\npostprocessing that is both spatially coherent and multivariate. Compared to\nprevious work, our Flow MAtching Postprocessing (FMAP) better represents the\ncorrelation structures of the observations distribution, while also improving\nmarginal performance at the stations. FMAP generates forecasts that are not\nbound to what is already modeled by the underlying gridded prediction and can\ninfer new correlation structures from data. The resulting model can generate an\narbitrary number of forecasts from a limited number of numerical simulations,\nallowing for low-cost forecasting systems. A single training is sufficient to\nperform postprocessing at multiple lead times, in contrast with other methods\nwhich use multiple trained networks at generation time. This work details our\nmethodology, including a spatial attention transformer backbone trained within\na flow matching generative modeling framework. FMAP shows promising performance\nin experiments on the EUPPBench dataset, forecasting surface temperature and\nwind gust values at station locations in western Europe up to five-day lead\ntimes.",
        "published": "2025-04-04T14:12:53+00:00"
    },
    {
        "title": "Hierarchical woven fibrillar structures in developing single gyroids in butterflies",
        "authors": [
            "Anna-Lee Jessop",
            "Peta L. Clode",
            "Martin Saunders",
            "Myfanwy E. Evans",
            "Stephen T. Hyde",
            "James N. McPherson",
            "Kasper S. Pederson",
            "Jacob J. K. Kirkensgaard",
            "Nipam H. Patel",
            "Kyle A. DeMarr",
            "W. Owen McMillan",
            "Bodo D. Wilts",
            "Gerd E. Schroeder-Turk"
        ],
        "summary": "Nature offers a remarkable diversity of nanomaterials that have extraordinary\nfunctional and structural properties. Intrinsic to nature is the impressive\nability to form complex ordered nanomaterials via self-organization. One\nparticularly intriguing nanostructure is the Gyroid, a network-like structure\nexhibiting high symmetry and complex topology. Although its existence in cells\nand tissues across many biological kingdoms is well documented, how and why it\nforms remains elusive and uncovering these formation mechanisms will\nundoubtedly inform bioinspired designs. A beautiful example is the smooth\nsingle gyroid that is found in the wing scales of several butterflies, where it\nbehaves as a photonic crystal generating a vibrant green colour. Here, we\nreport that the gyroid structures of the Emerald-patched Cattleheart, Parides\nsesostris, develop as woven fibrillar structures, disputing the commonly held\nassumption that they form as smooth constructs. Ultramicroscopy of pupal tissue\nreveals that the gyroid geometry consists of helical weavings of fibres, akin\nto hyperbolic line patterns decorating the gyroid. Interestingly, despite their\nfibrillar nature, electron diffraction reveals the absence of crystalline order\nwithin this material. Similar fibrillar structures are also observed in the\nmature wing scales of P. sesostris specimens with surgically altered pupal\ndevelopment, leading to a blue colouration. Our findings not only introduce a\nfundamentally new variation of the gyroid in biology but also have significant\nimplications for our understanding of its formation in nature.",
        "published": "2025-04-04T14:09:23+00:00"
    },
    {
        "title": "NDFT: Accelerating Density Functional Theory Calculations via Hardware/Software Co-Design on Near-Data Computing System",
        "authors": [
            "Qingcai Jiang",
            "Buxin Tu",
            "Xiaoyu Hao",
            "Junshi Chen",
            "Hong An"
        ],
        "summary": "Linear-response time-dependent Density Functional Theory (LR-TDDFT) is a\nwidely used method for accurately predicting the excited-state properties of\nphysical systems. Previous works have attempted to accelerate LR-TDDFT using\nheterogeneous systems such as GPUs, FPGAs, and the Sunway architecture.\nHowever, a major drawback of these approaches is the constant data movement\nbetween host memory and the memory of the heterogeneous systems, which results\nin substantial \\textit{data movement overhead}. Moreover, these works focus\nprimarily on optimizing the compute-intensive portions of LR-TDDFT, despite the\nfact that the calculation steps are fundamentally \\textit{memory-bound}.\n  To address these challenges, we propose NDFT, a\n\\underline{N}ear-\\underline{D}ata Density \\underline{F}unctional\n\\underline{T}heory framework. Specifically, we design a novel task partitioning\nand scheduling mechanism to offload each part of LR-TDDFT to the most suitable\ncomputing units within a CPU-NDP system. Additionally, we implement a\nhardware/software co-optimization of a critical kernel in LR-TDDFT to further\nenhance performance on the CPU-NDP system. Our results show that NDFT achieves\nperformance improvements of 5.2x and 2.5x over CPU and GPU baselines,\nrespectively, on a large physical system.",
        "published": "2025-04-04T13:51:24+00:00"
    },
    {
        "title": "LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications",
        "authors": [
            "Botao Zhu",
            "Chen Chen",
            "Xiaoyi Fan",
            "Yifei Zhu"
        ],
        "summary": "Developing compound Large Language Model (LLM) applications is becoming an\nincreasingly prevalent approach to solving real-world problems. In these\napplications, an LLM collaborates with various external modules, including APIs\nand even other LLMs, to realize complex intelligent services. However, we\nreveal that the intrinsic duration and structural uncertainty in compound LLM\napplications pose great challenges for LLM service providers in serving and\nscheduling them efficiently. In this paper, we propose LLMSched, an\nuncertainty-aware scheduling framework for emerging compound LLM applications.\nIn LLMSched, we first design a novel DAG-based model to describe the uncertain\ncompound LLM applications. Then, we adopt the Bayesian network to\ncomprehensively profile compound LLM applications and identify\nuncertainty-reducing stages, along with an entropy-based mechanism to quantify\ntheir uncertainty reduction. Combining an uncertainty reduction strategy and a\njob completion time (JCT)-efficient scheme, we further propose an efficient\nscheduler to reduce the average JCT. Evaluation of both simulation and testbed\nexperiments on various representative compound LLM applications shows that\ncompared to existing state-of-the-art scheduling schemes, LLMSched can reduce\nthe average JCT by 14~79%.",
        "published": "2025-04-04T13:37:29+00:00"
    },
    {
        "title": "Pyramid-based Mamba Multi-class Unsupervised Anomaly Detection",
        "authors": [
            "Nasar Iqbal",
            "Niki Martinel"
        ],
        "summary": "Recent advances in convolutional neural networks (CNNs) and transformer-based\nmethods have improved anomaly detection and localization, but challenges\npersist in precisely localizing small anomalies. While CNNs face limitations in\ncapturing long-range dependencies, transformer architectures often suffer from\nsubstantial computational overheads. We introduce a state space model\n(SSM)-based Pyramidal Scanning Strategy (PSS) for multi-class anomaly detection\nand localization--a novel approach designed to address the challenge of small\nanomaly localization. Our method captures fine-grained details at multiple\nscales by integrating the PSS with a pre-trained encoder for multi-scale\nfeature extraction and a feature-level synthetic anomaly generator. An\nimprovement of $+1\\%$ AP for multi-class anomaly localization and a +$1\\%$\nincrease in AU-PRO on MVTec benchmark demonstrate our method's superiority in\nprecise anomaly localization across diverse industrial scenarios. The code is\navailable at https://github.com/iqbalmlpuniud/Pyramid Mamba.",
        "published": "2025-04-04T13:33:59+00:00"
    },
    {
        "title": "Early detection of diabetes through transfer learning-based eye (vision) screening and improvement of machine learning model performance and advanced parameter setting algorithms",
        "authors": [
            "Mohammad Reza Yousefi",
            "Ali Bakrani",
            "Amin Dehghani"
        ],
        "summary": "Diabetic Retinopathy (DR) is a serious and common complication of diabetes,\ncaused by prolonged high blood sugar levels that damage the small retinal blood\nvessels. If left untreated, DR can progress to retinal vein occlusion and\nstimulate abnormal blood vessel growth, significantly increasing the risk of\nblindness. Traditional diabetes diagnosis methods often utilize convolutional\nneural networks (CNNs) to extract visual features from retinal images, followed\nby classification algorithms such as decision trees and k-nearest neighbors\n(KNN) for disease detection. However, these approaches face several challenges,\nincluding low accuracy and sensitivity, lengthy machine learning (ML) model\ntraining due to high data complexity and volume, and the use of limited\ndatasets for testing and evaluation. This study investigates the application of\ntransfer learning (TL) to enhance ML model performance in DR detection. Key\nimprovements include dimensionality reduction, optimized learning rate\nadjustments, and advanced parameter tuning algorithms, aimed at increasing\nefficiency and diagnostic accuracy. The proposed model achieved an overall\naccuracy of 84% on the testing dataset, outperforming prior studies. The\nhighest class-specific accuracy reached 89%, with a maximum sensitivity of 97%\nand an F1-score of 92%, demonstrating strong performance in identifying DR\ncases. These findings suggest that TL-based DR screening is a promising\napproach for early diagnosis, enabling timely interventions to prevent vision\nloss and improve patient outcomes.",
        "published": "2025-04-04T13:30:21+00:00"
    },
    {
        "title": "Optimizing Quantum Circuits via ZX Diagrams using Reinforcement Learning and Graph Neural Networks",
        "authors": [
            "Alexander Mattick",
            "Maniraman Periyasamy",
            "Christian Ufrecht",
            "Abhishek Y. Dubey",
            "Christopher Mutschler",
            "Axel Plinge",
            "Daniel D. Scherer"
        ],
        "summary": "Quantum computing is currently strongly limited by the impact of noise, in\nparticular introduced by the application of two-qubit gates. For this reason,\nreducing the number of two-qubit gates is of paramount importance on noisy\nintermediate-scale quantum hardware. To advance towards more reliable quantum\ncomputing, we introduce a framework based on ZX calculus, graph-neural networks\nand reinforcement learning for quantum circuit optimization. By combining\nreinforcement learning and tree search, our method addresses the challenge of\nselecting optimal sequences of ZX calculus rewrite rules. Instead of relying on\nexisting heuristic rules for minimizing circuits, our method trains a novel\nreinforcement learning policy that directly operates on ZX-graphs, therefore\nallowing us to search through the space of all possible circuit transformations\nto find a circuit significantly minimizing the number of CNOT gates. This way\nwe can scale beyond hard-coded rules towards discovering arbitrary optimization\nrules. We demonstrate our method's competetiveness with state-of-the-art\ncircuit optimizers and generalization capabilities on large sets of diverse\nrandom circuits.",
        "published": "2025-04-04T13:19:08+00:00"
    },
    {
        "title": "Autonomous state-space segmentation for Deep-RL sparse reward scenarios",
        "authors": [
            "Gianluca Maselli",
            "Vieri Giuliano Santucci"
        ],
        "summary": "Dealing with environments with sparse rewards has always been crucial for\nsystems developed to operate in autonomous open-ended learning settings.\nIntrinsic Motivations could be an effective way to help Deep Reinforcement\nLearning algorithms learn in such scenarios. In fact, intrinsic reward signals,\nsuch as novelty or curiosity, are generally adopted to improve exploration when\nextrinsic rewards are delayed or absent. Building on previous works, we tackle\nthe problem of learning policies in the presence of sparse rewards by proposing\na two-level architecture that alternates an ''intrinsically driven'' phase of\nexploration and autonomous sub-goal generation, to a phase of sparse reward,\ngoal-directed policy learning. The idea is to build several small networks,\neach one specialized on a particular sub-path, and use them as starting points\nfor future exploration without the need to further explore from scratch\npreviously learnt paths. Two versions of the system have been trained and\ntested in the Gym SuperMarioBros environment without considering any additional\nextrinsic reward. The results show the validity of our approach and the\nimportance of autonomously segment the environment to generate an efficient\npath towards the final goal.",
        "published": "2025-04-04T13:06:23+00:00"
    },
    {
        "title": "Bifurcation analysis of an opinion dynamics model coupled with an environmental dynamics",
        "authors": [
            "Anthony Couthures",
            "Anastasia Bizyaeva",
            "Vineeth S. Varma",
            "Alessio Franci",
            "Irinel-Constantin Morarescu"
        ],
        "summary": "We consider an opinion dynamics model coupled with an environmental dynamics.\nBased on a forward invariance argument, we can simplify the analysis of the\nasymptotic behavior to the case when all the opinions in the social network are\nsynchronized. Our goal is to emphasize the role of the trust given to the\nenvironmental signal in the asymptotic behavior of the opinion dynamics and\nimplicitly of the coupled system. To do that, we conduct a bifurcation analysis\nof the system around the origin when the trust parameter is varying. Specific\nconditions are presented for both pitchfork and Hopf bifurcation. Numerical\nillustration completes the theoretical findings.",
        "published": "2025-04-04T13:01:37+00:00"
    },
    {
        "title": "NeRFlex: Resource-aware Real-time High-quality Rendering of Complex Scenes on Mobile Devices",
        "authors": [
            "Zhe Wang",
            "Yifei Zhu"
        ],
        "summary": "Neural Radiance Fields (NeRF) is a cutting-edge neural network-based\ntechnique for novel view synthesis in 3D reconstruction. However, its\nsignificant computational demands pose challenges for deployment on mobile\ndevices. While mesh-based NeRF solutions have shown potential in achieving\nreal-time rendering on mobile platforms, they often fail to deliver\nhigh-quality reconstructions when rendering practical complex scenes.\nAdditionally, the non-negligible memory overhead caused by pre-computed\nintermediate results complicates their practical application. To overcome these\nchallenges, we present NeRFlex, a resource-aware, high-resolution, real-time\nrendering framework for complex scenes on mobile devices. NeRFlex integrates\nmobile NeRF rendering with multi-NeRF representations that decompose a scene\ninto multiple sub-scenes, each represented by an individual NeRF network.\nCrucially, NeRFlex considers both memory and computation constraints as\nfirst-class citizens and redesigns the reconstruction process accordingly.\nNeRFlex first designs a detail-oriented segmentation module to identify\nsub-scenes with high-frequency details. For each NeRF network, a lightweight\nprofiler, built on domain knowledge, is used to accurately map configurations\nto visual quality and memory usage. Based on these insights and the resource\nconstraints on mobile devices, NeRFlex presents a dynamic programming algorithm\nto efficiently determine configurations for all NeRF representations, despite\nthe NP-hardness of the original decision problem. Extensive experiments on\nreal-world datasets and mobile devices demonstrate that NeRFlex achieves\nreal-time, high-quality rendering on commercial mobile devices.",
        "published": "2025-04-04T12:53:33+00:00"
    },
    {
        "title": "QuinID: Enabling FDMA-Based Fully Parallel RFID with Frequency-Selective Antenna",
        "authors": [
            "Xin Na",
            "Jia Zhang",
            "Jiacheng Zhang",
            "Xiuzhen Guo",
            "Yang Zou",
            "Meng Jin",
            "Yimiao Sun",
            "Yunhao Liu",
            "Yuan He"
        ],
        "summary": "Parallelizing passive Radio Frequency Identification (RFID) reading is an\narguably crucial, yet unsolved challenge in modern IoT applications. Existing\napproaches remain limited to time-division operations and fail to read multiple\ntags simultaneously. In this paper, we introduce QuinID, the first\nfrequency-division multiple access (FDMA) RFID system to achieve fully parallel\nreading. We innovatively exploit the frequency selectivity of the tag antenna\nrather than a conventional digital FDMA, bypassing the power and circuitry\nconstraint of RFID tags. Specifically, we delicately design the\nfrequency-selective antenna based on surface acoustic wave (SAW) components to\nachieve extreme narrow-band response, so that QuinID tags (i.e., QuinTags)\noperate exclusively within their designated frequency bands. By carefully\ndesigning the matching network and canceling various interference, a customized\nQuinReader communicates simultaneously with multiple QuinTags across distinct\nbands. QuinID maintains high compatibility with commercial RFID systems and\npresents a tag cost of less than 10 cents. We implement a 5-band QuinID system\nand evaluate its performance under various settings. The results demonstrate a\nfivefold increase in read rate, reaching up to 5000 reads per second.",
        "published": "2025-04-04T12:49:10+00:00"
    },
    {
        "title": "On the rate of convergence of an over-parametrized deep neural network regression estimate learned by gradient descent",
        "authors": [
            "Michael Kohler"
        ],
        "summary": "Nonparametric regression with random design is considered.\n  The $L_2$ error with integration with respect to the design\n  measure is used as the error criterion.\n  An over-parametrized deep neural network\n  regression estimate\n  with logistic activation function\n  is defined, where all weights are learned\n  by gradient descent. It is shown that the estimate\n  achieves a nearly optimal rate of convergence in case\n  that the regression function is $(p,C)$--smooth.",
        "published": "2025-04-04T12:28:54+00:00"
    },
    {
        "title": "Leveraging Network Topology in a Two-way Competition for Influence in the Friedkin-Johnsen Model",
        "authors": [
            "Aashi Shrinate",
            "Twinkle Tripathy"
        ],
        "summary": "In this paper, we consider two stubborn agents who compete for `influence'\nover a strongly connected group of agents. This framework represents real-world\ncontests, such as competition among firms, two-party elections, and sports\nrivalries, among others. Considering stubbornness of agents to be an immutable\nproperty, we utilise the network topology alone to increase the influence of a\npreferred stubborn agent. We demonstrate this on a special class of strongly\nconnected networks by identifying the supporters of each of the stubborn agents\nin such networks. Thereafter, we present sufficient conditions under which a\nnetwork perturbation always increases the influence of the preferred stubborn\nagent. A key advantage of the proposed topology-based conditions is that they\nhold independent of the edge weights in the network. Most importantly, we\nassert that there exists a sequence of perturbations that can make the lesser\ninfluential stubborn agent more influential. Finally, we demonstrate our\nresults over the Sampson's Monastery dataset.",
        "published": "2025-04-04T12:15:19+00:00"
    },
    {
        "title": "BitHEP -- The Limits of Low-Precision ML in HEP",
        "authors": [
            "Claudius Krause",
            "Daohan Wang",
            "Ramon Winterhalder"
        ],
        "summary": "The increasing complexity of modern neural network architectures demands fast\nand memory-efficient implementations to mitigate computational bottlenecks. In\nthis work, we evaluate the recently proposed BitNet architecture in HEP\napplications, assessing its performance in classification, regression, and\ngenerative modeling tasks. Specifically, we investigate its suitability for\nquark-gluon discrimination, SMEFT parameter estimation, and detector\nsimulation, comparing its efficiency and accuracy to state-of-the-art methods.\nOur results show that while BitNet consistently performs competitively in\nclassification tasks, its performance in regression and generation varies with\nthe size and type of the network, highlighting key limitations and potential\nareas for improvement.",
        "published": "2025-04-04T11:57:59+00:00"
    },
    {
        "title": "MultiClear: Multimodal Soft Exoskeleton Glove for Transparent Object Grasping Assistance",
        "authors": [
            "Chen Hu",
            "Timothy Neate",
            "Shan Luo",
            "Letizia Gionfrida"
        ],
        "summary": "Grasping is a fundamental skill for interacting with the environment.\nHowever, this ability can be difficult for some (e.g. due to disability).\nWearable robotic solutions can enhance or restore hand function, and recent\nadvances have leveraged computer vision to improve grasping capabilities.\nHowever, grasping transparent objects remains challenging due to their poor\nvisual contrast and ambiguous depth cues. Furthermore, while multimodal control\nstrategies incorporating tactile and auditory feedback have been explored to\ngrasp transparent objects, the integration of vision with these modalities\nremains underdeveloped. This paper introduces MultiClear, a multimodal\nframework designed to enhance grasping assistance in a wearable soft\nexoskeleton glove for transparent objects by fusing RGB data, depth data, and\nauditory signals. The exoskeleton glove integrates a tendon-driven actuator\nwith an RGB-D camera and a built-in microphone. To achieve precise and adaptive\ncontrol, a hierarchical control architecture is proposed. For the proposed\nhierarchical control architecture, a high-level control layer provides\ncontextual awareness, a mid-level control layer processes multimodal sensory\ninputs, and a low-level control executes PID motor control for fine-tuned\ngrasping adjustments. The challenge of transparent object segmentation was\nmanaged by introducing a vision foundation model for zero-shot segmentation.\nThe proposed system achieves a Grasping Ability Score of 70.37%, demonstrating\nits effectiveness in transparent object manipulation.",
        "published": "2025-04-04T11:51:35+00:00"
    },
    {
        "title": "FLAIRBrainSeg: Fine-grained brain segmentation using FLAIR MRI only",
        "authors": [
            "Edern Le Bot",
            "R\u00e9mi Giraud",
            "Boris Mansencal",
            "Thomas Tourdias",
            "Jos\u00e8 V. Manjon",
            "Pierrick Coup\u00e9"
        ],
        "summary": "This paper introduces a novel method for brain segmentation using only FLAIR\nMRIs, specifically targeting cases where access to other imaging modalities is\nlimited. By leveraging existing automatic segmentation methods, we train a\nnetwork to approximate segmentations, typically obtained from T1-weighted MRIs.\nOur method, called FLAIRBrainSeg, produces segmentations of 132 structures and\nis robust to multiple sclerosis lesions. Experiments on both in-domain and\nout-of-domain datasets demonstrate that our method outperforms\nmodality-agnostic approaches based on image synthesis, the only currently\navailable alternative for performing brain parcellation using FLAIR MRI alone.\nThis technique holds promise for scenarios where T1-weighted MRIs are\nunavailable and offers a valuable alternative for clinicians and researchers in\nneed of reliable anatomical segmentation.",
        "published": "2025-04-04T11:47:18+00:00"
    },
    {
        "title": "Leggiero: Analog WiFi Backscatter with Payload Transparency",
        "authors": [
            "Xin Na",
            "Xiuzhen Guo",
            "Zihao Yu",
            "Jia Zhang",
            "Yuan He",
            "Yunhao Liu"
        ],
        "summary": "Backscatter is an enabling technology for battery-free sensing in today's\nArtificial Intelligence of Things (AIOT). Building a backscatter-based sensing\nsystem, however, is a daunting task, due to two obstacles: the unaffordable\npower consumption of the microprocessor and the coexistence with the ambient\ncarrier's traffic. In order to address the above issues, in this paper, we\npresent Leggiero, the first-of-its-kind analog WiFi backscatter with payload\ntransparency. Leveraging a specially designed circuit with a varactor diode,\nthis design avoids using a microprocessor to interface between the radio and\nthe sensor, and directly converts the analog sensor signal into the phase of RF\n(radio frequency) signal. By carefully designing the reference circuit on the\ntag and precisely locating the extra long training field (LTF) section of a\nWiFi packet, Leggiero embeds the analog phase value into the channel state\ninformation (CSI). A commodity WiFi receiver without hardware modification can\nsimultaneously decode the WiFi and the sensor data. We implement Leggiero\ndesign and evaluate its performance under varied settings. The results show\nthat the power consumption of the Leggiero tag (excluding the power of the\nperipheral sensor module) is 30uW at a sampling rate of 400Hz, which is 4.8*\nand 4* lower than the state-of-the-art WiFi backscatter schemes. The uplink\nthroughput of Leggiero is suficient to support a variety of sensing\napplications, while keeping the WiFi carrier's throughput performance\nunaffected.",
        "published": "2025-04-04T11:19:27+00:00"
    },
    {
        "title": "Bayesian LSTM for indoor temperature modeling",
        "authors": [
            "Emma Hannula",
            "Arttu H\u00e4kkinen",
            "Antti Solonen",
            "Felibe Uribe",
            "Jana de Wiljes",
            "Lassi Roininen"
        ],
        "summary": "Improving energy efficiency of building heating systems is essential for\nreducing global energy consumption and greenhouse gas emissions. Traditional\ncontrol methods in buildings rely on static heating curves based solely on\noutdoor temperature measurements, neglecting system state and free heat sources\nlike solar gain. Model predictive control (MPC) not only addresses these\nlimitations but further optimizes heating control by incorporating weather\nforecasts and system state predictions. However, current industrial MPC\nsolutions often use simplified physics-inspired models, which compromise\naccuracy for interpretability. While purely data-driven models offer better\npredictive performance, they face challenges like overfitting and lack of\ntransparency.\n  To bridge this gap, we propose a Bayesian Long Short-Term Memory (LSTM)\narchitecture for indoor temperature modeling. Our experiments across 100\nreal-world buildings demonstrate that the Bayesian LSTM outperforms an\nindustrial physics-based model in predictive accuracy, enabling potential for\nimproved energy efficiency and thermal comfort if deployed in heating MPC\nsolutions. Over deterministic black-box approaches, the Bayesian framework\nprovides additional advantages by improving generalization ability and allowing\ninterpretation of predictions via uncertainty quantification. This work\nadvances data-driven heating control by balancing predictive performance with\nthe transparency and reliability required for real-world heating MPC\napplications.",
        "published": "2025-04-04T11:07:23+00:00"
    },
    {
        "title": "Meta-DAN: towards an efficient prediction strategy for page-level handwritten text recognition",
        "authors": [
            "Denis Coquenet"
        ],
        "summary": "Recent advances in text recognition led to a paradigm shift for page-level\nrecognition, from multi-step segmentation-based approaches to end-to-end\nattention-based ones. However, the na\\\"ive character-level autoregressive\ndecoding process results in long prediction times: it requires several seconds\nto process a single page image on a modern GPU. We propose the Meta Document\nAttention Network (Meta-DAN) as a novel decoding strategy to reduce the\nprediction time while enabling a better context modeling. It relies on two main\ncomponents: windowed queries, to process several transformer queries\naltogether, enlarging the context modeling with near future; and multi-token\npredictions, whose goal is to predict several tokens per query instead of only\nthe next one. We evaluate the proposed approach on 10 full-page handwritten\ndatasets and demonstrate state-of-the-art results on average in terms of\ncharacter error rate. Source code and weights of trained models are available\nat https://github.com/FactoDeepLearning/meta_dan.",
        "published": "2025-04-04T11:06:09+00:00"
    },
    {
        "title": "Fast Thermal-Aware Chiplet Placement Assisted by Surrogate",
        "authors": [
            "Qinqin Zhang",
            "Xiaoyu Liang",
            "Ning Xu",
            "Yu Chen"
        ],
        "summary": "With the advent of the post-Moore era, the 2.5-D advanced package is a\npromising solution to sustain the development of very large-scale integrated\ncircuits. However, the thermal placement of chiplet, due to the high complexity\nof thermal simulation, is very challenging. In this paper, a surrogate-assisted\nsimulated annealing algorithm is proposed to simultaneously minimize both the\nwirelength and the maximum temperature of integrated chips. To alleviate the\ncomputational cost of thermal simulation, a radial basis function network is\nintroduced to approximate the thermal field, assisted by which the simulated\nannealing algorithm converges to the better placement in less time. Numerical\nresults demonstrate that the surrogate-assisted simulated annealing algorithm\nis competitive to the state-of-the-art thermal placement algorithms of chiplet,\nsuggesting its potential application in the agile design of 2.5D package chip.",
        "published": "2025-04-04T11:04:41+00:00"
    },
    {
        "title": "Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered Chatbots on the Web",
        "authors": [
            "Lars Krupp",
            "Daniel Gei\u00dfler",
            "Peter Hevesi",
            "Marco Hirsch",
            "Paul Lukowicz",
            "Jakob Karolus"
        ],
        "summary": "Integrated into websites, LLM-powered chatbots offer alternative means of\nnavigation and information retrieval, leading to a shift in how users access\ninformation on the web. Yet, predominantly closed-sourced solutions limit\nproliferation among web hosts and suffer from a lack of transparency with\nregard to implementation details and energy efficiency. In this work, we\npropose our openly available agent Talk2X leveraging an adapted\nretrieval-augmented generation approach (RAG) combined with an automatically\ngenerated vector database, benefiting energy efficiency. Talk2X's architecture\nis generalizable to arbitrary websites offering developers a ready to use tool\nfor integration. Using a mixed-methods approach, we evaluated Talk2X's\nusability by tasking users to acquire specific assets from an open science\nrepository. Talk2X significantly improved task completion time, correctness,\nand user experience supporting users in quickly pinpointing specific\ninformation as compared to standard user-website interaction. Our findings\ncontribute technical advancements to an ongoing paradigm shift of how we access\ninformation on the web.",
        "published": "2025-04-04T10:58:57+00:00"
    },
    {
        "title": "EOOD: Entropy-based Out-of-distribution Detection",
        "authors": [
            "Guide Yang",
            "Chao Hou",
            "Weilong Peng",
            "Xiang Fang",
            "Yongwei Nie",
            "Peican Zhu",
            "Keke Tang"
        ],
        "summary": "Deep neural networks (DNNs) often exhibit overconfidence when encountering\nout-of-distribution (OOD) samples, posing significant challenges for\ndeployment. Since DNNs are trained on in-distribution (ID) datasets, the\ninformation flow of ID samples through DNNs inevitably differs from that of OOD\nsamples. In this paper, we propose an Entropy-based Out-Of-distribution\nDetection (EOOD) framework. EOOD first identifies specific block where the\ninformation flow differences between ID and OOD samples are more pronounced,\nusing both ID and pseudo-OOD samples. It then calculates the conditional\nentropy on the selected block as the OOD confidence score. Comprehensive\nexperiments conducted across various ID and OOD settings demonstrate the\neffectiveness of EOOD in OOD detection and its superiority over\nstate-of-the-art methods.",
        "published": "2025-04-04T10:57:03+00:00"
    },
    {
        "title": "From Keypoints to Realism: A Realistic and Accurate Virtual Try-on Network from 2D Images",
        "authors": [
            "Maliheh Toozandehjani",
            "Ali Mousavi",
            "Reza Taheri"
        ],
        "summary": "The aim of image-based virtual try-on is to generate realistic images of\nindividuals wearing target garments, ensuring that the pose, body shape and\ncharacteristics of the target garment are accurately preserved. Existing\nmethods often fail to reproduce the fine details of target garments effectively\nand lack generalizability to new scenarios. In the proposed method, the\nperson's initial garment is completely removed. Subsequently, a precise warping\nis performed using the predicted keypoints to fully align the target garment\nwith the body structure and pose of the individual. Based on the warped\ngarment, a body segmentation map is more accurately predicted. Then, using an\nalignment-aware segment normalization, the misaligned areas between the warped\ngarment and the predicted garment region in the segmentation map are removed.\nFinally, the generator produces the final image with high visual quality,\nreconstructing the precise characteristics of the target garment, including its\noverall shape and texture. This approach emphasizes preserving garment\ncharacteristics and improving adaptability to various poses, providing better\ngeneralization for diverse applications.",
        "published": "2025-04-04T10:35:06+00:00"
    },
    {
        "title": "Probabilistic State Estimation of Timed Probabilistic Discrete Event Systems via Artificial Neural Networks [Draft Version]",
        "authors": [
            "Omar Amri",
            "Carla Seatzu",
            "Alessandro Giua",
            "Dimitri Lefebvre"
        ],
        "summary": "This paper is about the state estimation of timed probabilistic discrete\nevent systems. The main contribution is to propose general procedures for\ndeveloping state estimation approaches based on artificial neural networks. It\nis assumed that no formal model of the system exists but a data set is\navailable, which contains the history of the timed behaviour of the systems.\nThis dataset will be exploited to develop a neural network model that uses both\nlogical and temporal information gathered during the functioning of the system\nas inputs and provides the state probability vector as output. Two main\napproaches are successively proposed (i) state estimation of timed\nprobabilistic discrete event systems over observations: in this case the state\nestimate is reconstructed at the occurrence of each new observation; (ii) state\nestimation of timed probabilistic discrete event systems over time: in this\ncase the state estimate is reconstructed at each clock time increment. For each\napproach, the paper outlines the process of data preprocessing, model building\nand implementation. This paper not only proposes groundbreaking approaches but\nalso opens the door to further exploitation of artificial neural networks for\nthe benefit of discrete event systems.",
        "published": "2025-04-04T10:09:07+00:00"
    },
    {
        "title": "Stochastic ordering, attractiveness and couplings in non-conservative particle systems",
        "authors": [
            "Ra\u00fal Gouet",
            "F. Javier L\u00f3pez",
            "Gerardo Sanz"
        ],
        "summary": "We analyse the stochastic comparison of interacting particle systems allowing\nfor multiple arrivals, departures and non-conservative jumps of individuals\nbetween sites. That is, if $k$ individuals leave site $x$ for site $y$, a\npossibly different number $l$ arrive at destination. This setting includes new\nmodels, when compared to the conservative case, such as metapopulation models\nwith deaths during migrations. It implies a sharp increase of technical\ncomplexity, given the numerous changes to consider. Known results are\nsignificantly generalised, even in the conservative case, as no particular form\nof the transition rates is assumed.\n  We obtain necessary and sufficient conditions on the rates for the stochastic\ncomparison of the processes and prove their equivalence with the existence of\nan order-preserving Markovian coupling. As a corollary, we get necessary and\nsufficient conditions for the attractiveness of the processes. A salient\nfeature of our approach lies in the presentation of the coupling in terms of\nsolutions to network flow problems.\n  We illustrate the applicability of our results to a flexible family of\npopulation models described as interacting particle systems, with a range of\nparameters controlling births, deaths, catastrophes or migrations. We provide\nexplicit conditions on the parameters for the stochastic comparison and\nattractiveness of the models, showing their usefulness in studying their limit\nbehaviour. Additionally, we give three examples of constructing the coupling.",
        "published": "2025-04-04T10:09:07+00:00"
    },
    {
        "title": "Block Toeplitz Sparse Precision Matrix Estimation for Large-Scale Interval-Valued Time Series Forecasting",
        "authors": [
            "Wan Tian",
            "Zhongfeng Qin"
        ],
        "summary": "Modeling and forecasting interval-valued time series (ITS) have attracted\nconsiderable attention due to their growing presence in various contexts. To\nthe best of our knowledge, there have been no efforts to model large-scale ITS.\nIn this paper, we propose a feature extraction procedure for large-scale ITS,\nwhich involves key steps such as auto-segmentation and clustering, and feature\ntransfer learning. This procedure can be seamlessly integrated with any\nsuitable prediction models for forecasting purposes. Specifically, we transform\nthe automatic segmentation and clustering of ITS into the estimation of\nToeplitz sparse precision matrices and assignment set. The\nmajorization-minimization algorithm is employed to convert this highly\nnon-convex optimization problem into two subproblems. We derive efficient\ndynamic programming and alternating direction method to solve these two\nsubproblems alternately and establish their convergence properties. By\nemploying the Joint Recurrence Plot (JRP) to image subsequence and assigning a\nclass label to each cluster, an image dataset is constructed. Then, an\nappropriate neural network is chosen to train on this image dataset and used to\nextract features for the next step of forecasting. Real data applications\ndemonstrate that the proposed method can effectively obtain invariant\nrepresentations of the raw data and enhance forecasting performance.",
        "published": "2025-04-04T09:57:05+00:00"
    },
    {
        "title": "Adaptive Classification of Interval-Valued Time Series",
        "authors": [
            "Wan Tian",
            "Zhongfeng Qin"
        ],
        "summary": "In recent years, the modeling and analysis of interval-valued time series\nhave garnered significant attention in the fields of econometrics and\nstatistics. However, the existing literature primarily focuses on regression\ntasks while neglecting classification aspects. In this paper, we propose an\nadaptive approach for interval-valued time series classification. Specifically,\nwe represent interval-valued time series using convex combinations of upper and\nlower bounds of intervals and transform these representations into images based\non point-valued time series imaging methods. We utilize a fine-grained image\nclassification neural network to classify these images, to achieve the goal of\nclassifying the original interval-valued time series. This proposed method is\napplicable to both univariate and multivariate interval-valued time series. On\nthe optimization front, we treat the convex combination coefficients as\nlearnable parameters similar to the parameters of the neural network and\nprovide an efficient estimation method based on the alternating direction\nmethod of multipliers (ADMM). On the theoretical front, under specific\nconditions, we establish a margin-based multiclass generalization bound for\ngeneric CNNs composed of basic blocks involving convolution, pooling, and fully\nconnected layers. Through simulation studies and real data applications, we\nvalidate the effectiveness of the proposed method and compare its performance\nagainst a wide range of point-valued time series classification methods.",
        "published": "2025-04-04T09:52:40+00:00"
    },
    {
        "title": "Steerable Anatomical Shape Synthesis with Implicit Neural Representations",
        "authors": [
            "Bram de Wilde",
            "Max T. Rietberg",
            "Guillaume Lajoinie",
            "Jelmer M. Wolterink"
        ],
        "summary": "Generative modeling of anatomical structures plays a crucial role in virtual\nimaging trials, which allow researchers to perform studies without the costs\nand constraints inherent to in vivo and phantom studies. For clinical\nrelevance, generative models should allow targeted control to simulate specific\npatient populations rather than relying on purely random sampling. In this\nwork, we propose a steerable generative model based on implicit neural\nrepresentations. Implicit neural representations naturally support topology\nchanges, making them well-suited for anatomical structures with varying\ntopology, such as the thyroid. Our model learns a disentangled latent\nrepresentation, enabling fine-grained control over shape variations. Evaluation\nincludes reconstruction accuracy and anatomical plausibility. Our results\ndemonstrate that the proposed model achieves high-quality shape generation\nwhile enabling targeted anatomical modifications.",
        "published": "2025-04-04T09:49:00+00:00"
    },
    {
        "title": "Sim4EndoR: A Reinforcement Learning Centered Simulation Platform for Task Automation of Endovascular Robotics",
        "authors": [
            "Tianliang Yao",
            "Madaoji Ban",
            "Bo Lu",
            "Zhiqiang Pei",
            "Peng Qi"
        ],
        "summary": "Robotic-assisted percutaneous coronary intervention (PCI) holds considerable\npromise for elevating precision and safety in cardiovascular procedures.\nNevertheless, current systems heavily depend on human operators, resulting in\nvariability and the potential for human error. To tackle these challenges,\nSim4EndoR, an innovative reinforcement learning (RL) based simulation\nenvironment, is first introduced to bolster task-level autonomy in PCI. This\nplatform offers a comprehensive and risk-free environment for the development,\nevaluation, and refinement of potential autonomous systems, enhancing data\ncollection efficiency and minimizing the need for costly hardware trials. A\nnotable aspect of the groundbreaking Sim4EndoR is its reward function, which\ntakes into account the anatomical constraints of the vascular environment,\nutilizing the geometric characteristics of vessels to steer the learning\nprocess. By seamlessly integrating advanced physical simulations with neural\nnetwork-driven policy learning, Sim4EndoR fosters efficient sim-to-real\ntranslation, paving the way for safer, more consistent robotic interventions in\nclinical practice, ultimately improving patient outcomes.",
        "published": "2025-04-04T09:45:41+00:00"
    },
    {
        "title": "A model-free feature extraction procedure for interval-valued time series prediction",
        "authors": [
            "Wan Tian",
            "Zhongfeng Qin",
            "Tao Hu"
        ],
        "summary": "In this paper, we present a novel feature extraction procedure to predict\ninterval-valued time series by combing transfer learning and imaging\napproaches. Initially, we represent interval-valued time series using a\nbivariate point-valued time series, which serves as a representative form. We\nfirst transform each time series into images by employing various imaging\napproaches such as recurrence plot, gramian angular summation/difference field,\nand Markov transition field, and construct an image dataset by treating each\nimaging method's output as a separate class. Based on this dataset, we train\nseveral candidates for a feature extraction network (FEN), specifically ResNet\nwith varying layers. Then we choose the penultimate layer of the FEN to extract\nthe most relevant features from the transformed images. We integrate the\nextracted features into conventional predictive models to formulate the\ncorresponding prediction models. To formulate prediction, we integrate the\nextracted features into a regular prediction model. The proposed methods are\nevaluated based on the S\\&P 500 index and three data-generating processes\n(DGPs), and the experimental results demonstrate a notable improvement in\nprediction performance compared to existing methods.",
        "published": "2025-04-04T09:44:06+00:00"
    },
    {
        "title": "Roto-Translation Invariant Metrics on Position-Orientation Space",
        "authors": [
            "Gijs Bellaard",
            "Bart M. N. Smets"
        ],
        "summary": "Riemannian metrics on the position-orientation space M(3) that are\nroto-translation group SE(3) invariant play a key role in image analysis tasks\nlike enhancement, denoising, and segmentation. These metrics enable\nroto-translation equivariant algorithms, with the associated Riemannian\ndistance often used in implementation.\n  However, computing the Riemannian distance is costly, which makes it\nunsuitable in situations where constant recomputation is needed. We propose the\nmav (minimal angular velocity) distance, defined as the Riemannian length of a\ngeometrically meaningful curve, as a practical alternative.\n  We see an application of the mav distance in geometric deep learning. Namely,\nneural networks architectures such as PONITA, relies on geometric invariants to\ncreate their roto-translation equivariant model. The mav distance offers a\ntrainable invariant, with the parameters that determine the Riemannian metric\nacting as learnable weights.\n  In this paper we: 1) classify and parametrize all SE(3) invariant metrics on\nM(3), 2) describes how to efficiently calculate the mav distance, and 3)\ninvestigate if including the mav distance within PONITA can positively impact\nits accuracy in predicting molecular properties.",
        "published": "2025-04-04T09:36:11+00:00"
    },
    {
        "title": "Multi-Flow: Multi-View-Enriched Normalizing Flows for Industrial Anomaly Detection",
        "authors": [
            "Mathis Kruse",
            "Bodo Rosenhahn"
        ],
        "summary": "With more well-performing anomaly detection methods proposed, many of the\nsingle-view tasks have been solved to a relatively good degree. However,\nreal-world production scenarios often involve complex industrial products,\nwhose properties may not be fully captured by one single image. While\nnormalizing flow based approaches already work well in single-camera scenarios,\nthey currently do not make use of the priors in multi-view data. We aim to\nbridge this gap by using these flow-based models as a strong foundation and\npropose Multi-Flow, a novel multi-view anomaly detection method. Multi-Flow\nmakes use of a novel multi-view architecture, whose exact likelihood estimation\nis enhanced by fusing information across different views. For this, we propose\na new cross-view message-passing scheme, letting information flow between\nneighboring views. We empirically validate it on the real-world multi-view data\nset Real-IAD and reach a new state-of-the-art, surpassing current baselines in\nboth image-wise and sample-wise anomaly detection tasks.",
        "published": "2025-04-04T09:32:01+00:00"
    },
    {
        "title": "Universal Collection of Euclidean Invariants between Pairs of Position-Orientations",
        "authors": [
            "Gijs Bellaard",
            "Bart M. N. Smets",
            "Remco Duits"
        ],
        "summary": "Euclidean E(3) equivariant neural networks that employ scalar fields on\nposition-orientation space M(3) have been effectively applied to tasks such as\npredicting molecular dynamics and properties. To perform equivariant\nconvolutional-like operations in these architectures one needs Euclidean\ninvariant kernels on M(3) x M(3). In practice, a handcrafted collection of\ninvariants is selected, and this collection is then fed into multilayer\nperceptrons to parametrize the kernels. We rigorously describe an optimal\ncollection of 4 smooth scalar invariants on the whole of M(3) x M(3). With\noptimal we mean that the collection is independent and universal, meaning that\nall invariants are pertinent, and any invariant kernel is a function of them.\nWe evaluate two collections of invariants, one universal and one not, using the\nPONITA neural network architecture. Our experiments show that using a\ncollection of invariants that is universal positively impacts the accuracy of\nPONITA significantly.",
        "published": "2025-04-04T09:25:59+00:00"
    },
    {
        "title": "Offline and Distributional Reinforcement Learning for Wireless Communications",
        "authors": [
            "Eslam Eldeeb",
            "Hirley Alves"
        ],
        "summary": "The rapid growth of heterogeneous and massive wireless connectivity in 6G\nnetworks demands intelligent solutions to ensure scalability, reliability,\nprivacy, ultra-low latency, and effective control. Although artificial\nintelligence (AI) and machine learning (ML) have demonstrated their potential\nin this domain, traditional online reinforcement learning (RL) and deep RL\nmethods face limitations in real-time wireless networks. For instance, these\nmethods rely on online interaction with the environment, which might be\nunfeasible, costly, or unsafe. In addition, they cannot handle the inherent\nuncertainties in real-time wireless applications. We focus on offline and\ndistributional RL, two advanced RL techniques that can overcome these\nchallenges by training on static datasets and accounting for network\nuncertainties. We introduce a novel framework that combines offline and\ndistributional RL for wireless communication applications. Through case studies\non unmanned aerial vehicle (UAV) trajectory optimization and radio resource\nmanagement (RRM), we demonstrate that our proposed Conservative Quantile\nRegression (CQR) algorithm outperforms conventional RL approaches regarding\nconvergence speed and risk management. Finally, we discuss open challenges and\npotential future directions for applying these techniques in 6G networks,\npaving the way for safer and more efficient real-time wireless systems.",
        "published": "2025-04-04T09:24:39+00:00"
    },
    {
        "title": "Learning-Based Conformal Tube MPC for Safe Control in Interactive Multi-Agent Systems",
        "authors": [
            "Shuqi Wang",
            "Yue Gao",
            "Xiang Yin"
        ],
        "summary": "Safety assurance in multi-agent systems with coupled dynamics is a\nfundamental yet challenging problem, especially when agents exhibit uncertain\nand state-dependent behaviors. Classical robust control often assumes\nworst-case disturbances, leading to overly conservative actions. In this work,\nwe propose a learning-based framework that combines conformal prediction with\nmodel predictive control (MPC) to ensure probabilistic safety under\naction-level uncertainty. Unlike prior approaches that predict future states,\nwe directly model the control action of the uncontrollable agent as a\nstochastic function of the joint state, trained via neural networks and\ncalibrated using conformal prediction. This enables us to construct dynamic,\nprobabilistically guaranteed reachable tubes for the uncontrollable agent.\nThese tubes are then embedded into an MPC formulation to synthesize control\nactions for the controllable agent that ensure safe interactions over a finite\nplanning horizon. We provide formal stepwise and cumulative safety guarantees,\nand demonstrate the effectiveness of our approach through a pedestrian-vehicle\ninteraction scenario. Compared to baseline methods, our framework achieves\nhigher safety rates while maintaining high performance in terms of speed and\nresponsiveness.",
        "published": "2025-04-04T09:17:59+00:00"
    },
    {
        "title": "RWKVTTS: Yet another TTS based on RWKV-7",
        "authors": [
            "Lin yueyu",
            "Liu Xiao"
        ],
        "summary": "Human-AI interaction thrives on intuitive and efficient interfaces, among\nwhich voice stands out as a particularly natural and accessible modality.\nRecent advancements in transformer-based text-to-speech (TTS) systems, such as\nFish-Speech, CosyVoice, and MegaTTS 3, have delivered remarkable improvements\nin quality and realism, driving a significant evolution in the TTS domain. In\nthis paper, we introduce RWKV-7 \\cite{peng2025rwkv}, a cutting-edge RNN-based\narchitecture tailored for TTS applications. Unlike traditional transformer\nmodels, RWKV-7 leverages the strengths of recurrent neural networks to achieve\ngreater computational efficiency and scalability, while maintaining\nhigh-quality output. Our comprehensive benchmarks demonstrate that RWKV-7\noutperforms transformer-based models across multiple key metrics, including\nsynthesis speed, naturalness of speech, and resource efficiency. Furthermore,\nwe explore its adaptability to diverse linguistic contexts and low-resource\nenvironments, showcasing its potential to democratize TTS technology. These\nfindings position RWKV-7 as a powerful and innovative alternative, paving the\nway for more accessible and versatile voice synthesis solutions in real-world\napplications.Our code and weights are https://github.com/yynil/RWKVTTS,\nhttps://huggingface.co/spaces/RWKV-Red-Team",
        "published": "2025-04-04T09:17:20+00:00"
    },
    {
        "title": "Convergence and consensus analysis of a class of best-response opinion dynamics",
        "authors": [
            "Yuchen Xu",
            "Yi Han",
            "Chuanzhe Zhang",
            "Miao Wang",
            "Wenjun Mei"
        ],
        "summary": "Opinion dynamics aims to understand how individuals' opinions evolve through\nlocal interactions. Recently, opinion dynamics have been modeled as network\ngames, where individuals update their opinions in order to minimize the social\npressure caused by disagreeing with others. In this paper, we study a class of\nbest response opinion dynamics introduced by Mei et al., where a parameter\n$\\alpha > 0$ controls the marginal cost of opinion differences, bridging\nwell-known mechanisms such as the DeGroot model ($\\alpha = 2$) and the\nweighted-median model ($\\alpha = 1$). We conduct theoretical analysis on how\ndifferent values of $\\alpha$ affect the system's convergence and consensus\nbehavior. For the case when $\\alpha > 1$, corresponding to increasing marginal\ncosts, we establish the convergence of the dynamics and derive graph-theoretic\nconditions for consensus formation, which is proved to be similar to those in\nthe DeGroot model. When $\\alpha < 1$, we show via a counterexample that\nconvergence is not always guaranteed, and we provide sufficient conditions for\nconvergence and consensus. Additionally, numerical simulations on small-world\nnetworks reveal how network structure and $\\alpha$ together affect opinion\ndiversity.",
        "published": "2025-04-04T09:05:37+00:00"
    },
    {
        "title": "JanusDDG: A Thermodynamics-Compliant Model for Sequence-Based Protein Stability via Two-Fronts Multi-Head Attention",
        "authors": [
            "Guido Barducci",
            "Ivan Rossi",
            "Francesco Codic\u00e8",
            "Cesare Rollo",
            "Valeria Repetto",
            "Corrado Pancotti",
            "Virginia Iannibelli",
            "Tiziana Sanavia",
            "Piero Fariselli"
        ],
        "summary": "Understanding how residue variations affect protein stability is crucial for\ndesigning functional proteins and deciphering the molecular mechanisms\nunderlying disease-related mutations. Recent advances in protein language\nmodels (PLMs) have revolutionized computational protein analysis, enabling,\namong other things, more accurate predictions of mutational effects. In this\nwork, we introduce JanusDDG, a deep learning framework that leverages\nPLM-derived embeddings and a bidirectional cross-attention transformer\narchitecture to predict $\\Delta \\Delta G$ of single and multiple-residue\nmutations while simultaneously being constrained to respect fundamental\nthermodynamic properties, such as antisymmetry and transitivity. Unlike\nconventional self-attention, JanusDDG computes queries (Q) and values (V) as\nthe difference between wild-type and mutant embeddings, while keys (K)\nalternate between the two. This cross-interleaved attention mechanism enables\nthe model to capture mutation-induced perturbations while preserving essential\ncontextual information. Experimental results show that JanusDDG achieves\nstate-of-the-art performance in predicting $\\Delta \\Delta G$ from sequence\nalone, matching or exceeding the accuracy of structure-based methods for both\nsingle and multiple mutations.",
        "published": "2025-04-04T09:02:32+00:00"
    },
    {
        "title": "Resilience of the autocatalytic feedback loop for gene regulation",
        "authors": [
            "Daniele Proverbio",
            "Giulia Giordano"
        ],
        "summary": "Gene expression in response to stimuli is regulated by transcription factors\n(TFs) through feedback loop motifs, aimed at maintaining the desired TF\nconcentration despite uncertainties and perturbations. In this work, we\nconsider a stochastic model of the autocatalytic feedback loop for gene\nregulation and we probabilistically quantify its resilience, \\textit{i.e.}, its\nability to preserve the equilibrium associated with a prescribed concentration\nof TFs, and the corresponding basin of attraction, in the presence of noise. We\nshow that the formation of larger oligomers, corresponding to larger Hill\ncoefficients of the regulation function, and thus to sharper non-linearities,\nimproves the system resilience, even close to critical concentrations of TFs.\nOur formal results within a stochastic formulation relying on the Fokker-Planck\nequation are accompanied by numerical simulations.",
        "published": "2025-04-04T08:56:02+00:00"
    },
    {
        "title": "Verification of Autonomous Neural Car Control with KeYmaera X",
        "authors": [
            "Enguerrand Prebet",
            "Samuel Teuber",
            "Andr\u00e9 Platzer"
        ],
        "summary": "This article presents a formal model and formal safety proofs for the ABZ'25\ncase study in differential dynamic logic (dL). The case study considers an\nautonomous car driving on a highway avoiding collisions with neighbouring cars.\nUsing KeYmaera X's dL implementation, we prove absence of collision on an\ninfinite time horizon which ensures that safety is preserved independently of\ntrip length. The safety guarantees hold for time-varying reaction time and\nbrake force. Our dL model considers the single lane scenario with cars ahead or\nbehind. We demonstrate that dL with its tools is a rigorous foundation for\nruntime monitoring, shielding, and neural network verification. Doing so sheds\nlight on inconsistencies between the provided specification and simulation\nenvironment highway-env of the ABZ'25 study. We attempt to fix these\ninconsistencies and uncover numerous counterexamples which also indicate issues\nin the provided reinforcement learning environment.",
        "published": "2025-04-04T08:43:31+00:00"
    },
    {
        "title": "On-chip integrated light sources with MoS2/WSe2 moir\u00e9 superlattices at telecom wavelengths",
        "authors": [
            "Xingyu Huang",
            "Hanlin Fang",
            "Shima Kadkhodazadeh",
            "Monia Runge Nielsen",
            "Qiaoling Lin",
            "Zhipei Sun",
            "Martijn Wubs",
            "Sanshui Xiao"
        ],
        "summary": "On-chip integrated light sources are essential for photonic integrated\ncircuits, requiring waveguides to interface various components, from light\nsources to detectors. Two-dimensional (2D) transition metal dichalcogenide\n(TMD) heterostructures offer exceptional tunability and direct bandgaps,\nopening new avenues for on-chip light sources. However, a waveguide-integrated\nlight source based on 2D materials operating in the telecom windows has yet to\nbe realized. In this work, we demonstrate that the creation of a moir\\'e\nsuperlattice enables light emission in the optical fiber communication (OFC)\nO-band (1260-1360 nm) with brightness surpassing that of intralayer excitons in\nmonolayer MoTe2. Furthermore, we realize waveguide-integrated light sources\nemitting in the O-band by integrating these superlattices with asymmetric\nnanobeam cavities. This cavity design not only significantly enhances light\nemission but also improves the spectral purity of the single cavity mode.\nMoreover, the device output remains remarkably stable across varying pump power\nin an ambient environment, demonstrating excellent operational stability. The\ndevice performance remains unchanged over a nine-month measurement period,\nhighlighting its long-term stability. This work presents a new architecture for\non-chip light sources, advancing practical photonic applications.",
        "published": "2025-04-04T08:31:55+00:00"
    },
    {
        "title": "Linear Decomposition of the Majority Boolean Function using the Ones on Smaller Variables",
        "authors": [
            "Anupam Chattopadhyay",
            "Debjyoti Bhattacharjee",
            "Subhamoy Maitra"
        ],
        "summary": "A long-investigated problem in circuit complexity theory is to decompose an\n$n$-input or $n$-variable Majority Boolean function (call it $M_n$) using\n$k$-input ones ($M_k$), $k < n$, where the objective is to achieve the\ndecomposition using fewest $M_k$'s. An $\\mathcal{O}(n)$ decomposition for $M_n$\nhas been proposed recently with $k=3$. However, for an arbitrary value of $k$,\nno such construction exists even though there are several works reporting\ncontinual improvement of lower bounds, finally achieving an optimal lower bound\n$\\Omega(\\frac{n}{k}\\log k)$ as provided by Lecomte et. al., in CCC '22. In this\ndirection, here we propose two decomposition procedures for $M_n$, utilizing\ncounter trees and restricted partition functions, respectively. The\nconstruction technique based on counter tree requires $\\mathcal{O}(n)$ such\nmany $M_k$ functions, hence presenting a construction closest to the optimal\nlower bound, reported so far. The decomposition technique using restricted\npartition functions present a novel link between Majority Boolean function\nconstruction and elementary number theory. These decomposition techniques close\na gap in circuit complexity studies and are also useful for leveraging emerging\ncomputing technologies.",
        "published": "2025-04-04T08:22:43+00:00"
    },
    {
        "title": "Robot Localization Using a Learned Keypoint Detector and Descriptor with a Floor Camera and a Feature Rich Industrial Floor",
        "authors": [
            "Piet Br\u00f6mmel",
            "Dominik Br\u00e4mer",
            "Oliver Urbann",
            "Diana Kleingarn"
        ],
        "summary": "The localization of moving robots depends on the availability of good\nfeatures from the environment. Sensor systems like Lidar are popular, but\nunique features can also be extracted from images of the ground. This work\npresents the Keypoint Localization Framework (KOALA), which utilizes deep\nneural networks that extract sufficient features from an industrial floor for\naccurate localization without having readable markers. For this purpose, we use\na floor covering that can be produced as cheaply as common industrial floors.\nAlthough we do not use any filtering, prior, or temporal information, we can\nestimate our position in 75.7 % of all images with a mean position error of 2\ncm and a rotation error of 2.4 %. Thus, the robot kidnapping problem can be\nsolved with high precision in every frame, even while the robot is moving.\nFurthermore, we show that our framework with our detector and descriptor\ncombination is able to outperform comparable approaches.",
        "published": "2025-04-04T08:00:38+00:00"
    },
    {
        "title": "Adaptive Movement Sampling Physics-Informed Residual Network (AM-PIRN) for Solving Nonlinear Option Pricing models",
        "authors": [
            "Qinjiao Gao",
            "Zuowei Wang",
            "Ran Zhang",
            "Dongjiang Wang"
        ],
        "summary": "In this paper, we propose the Adaptive Movement Sampling Physics-Informed\nResidual Network (AM-PIRN) to address challenges in solving nonlinear option\npricing PDE models, where solutions often exhibit significant curvature or\nshock waves over time. The AM-PIRN architecture is designed to concurrently\nminimize PDE residuals and achieve high-fidelity option price approximations by\ndynamically redistributing training points based on evolving PDE residuals,\nwhile maintaining a fixed total number of points. To enhance stability and\ntraining efficiency, we integrate a ResNet backbone, replacing conventional\nfully connected neural networks used in Physics-Informed Neural Networks\n(PINNs). Numerical experiments across nonlinear option pricing models\ndemonstrate that AM-PIRN outperforms PINN, RAM-PINN, and WAM-PINN in both\nresolving PDE constraints and accurately estimating option prices. The method's\nadvantages are particularly pronounced in complex or multi-dimensional models,\nwhere its adaptive sampling and robust architecture effectively mitigate\nchallenges posed by sharp gradients and high nonlinearity.",
        "published": "2025-04-04T07:47:52+00:00"
    },
    {
        "title": "Decision SpikeFormer: Spike-Driven Transformer for Decision Making",
        "authors": [
            "Wei Huang",
            "Qinying Gu",
            "Nanyang Ye"
        ],
        "summary": "Offline reinforcement learning (RL) enables policy training solely on\npre-collected data, avoiding direct environment interaction - a crucial benefit\nfor energy-constrained embodied AI applications. Although Artificial Neural\nNetworks (ANN)-based methods perform well in offline RL, their high\ncomputational and energy demands motivate exploration of more efficient\nalternatives. Spiking Neural Networks (SNNs) show promise for such tasks, given\ntheir low power consumption. In this work, we introduce DSFormer, the first\nspike-driven transformer model designed to tackle offline RL via sequence\nmodeling. Unlike existing SNN transformers focused on spatial dimensions for\nvision tasks, we develop Temporal Spiking Self-Attention (TSSA) and Positional\nSpiking Self-Attention (PSSA) in DSFormer to capture the temporal and\npositional dependencies essential for sequence modeling in RL. Additionally, we\npropose Progressive Threshold-dependent Batch Normalization (PTBN), which\ncombines the benefits of LayerNorm and BatchNorm to preserve temporal\ndependencies while maintaining the spiking nature of SNNs. Comprehensive\nresults in the D4RL benchmark show DSFormer's superiority over both SNN and ANN\ncounterparts, achieving 78.4% energy savings, highlighting DSFormer's\nadvantages not only in energy efficiency but also in competitive performance.\nCode and models are public at https://wei-nijuan.github.io/DecisionSpikeFormer.",
        "published": "2025-04-04T07:42:36+00:00"
    },
    {
        "title": "Malware Detection in Docker Containers: An Image is Worth a Thousand Logs",
        "authors": [
            "Akis Nousias",
            "Efklidis Katsaros",
            "Evangelos Syrmos",
            "Panagiotis Radoglou-Grammatikis",
            "Thomas Lagkas",
            "Vasileios Argyriou",
            "Ioannis Moscholios",
            "Evangelos Markakis",
            "Sotirios Goudos",
            "Panagiotis Sarigiannidis"
        ],
        "summary": "Malware detection is increasingly challenged by evolving techniques like\nobfuscation and polymorphism, limiting the effectiveness of traditional\nmethods. Meanwhile, the widespread adoption of software containers has\nintroduced new security challenges, including the growing threat of malicious\nsoftware injection, where a container, once compromised, can serve as entry\npoint for further cyberattacks. In this work, we address these security issues\nby introducing a method to identify compromised containers through machine\nlearning analysis of their file systems. We cast the entire software containers\ninto large RGB images via their tarball representations, and propose to use\nestablished Convolutional Neural Network architectures on a streaming,\npatch-based manner. To support our experiments, we release the COSOCO\ndataset--the first of its kind--containing 3364 large-scale RGB images of\nbenign and compromised software containers at\nhttps://huggingface.co/datasets/k3ylabs/cosoco-image-dataset. Our method\ndetects more malware and achieves higher F1 and Recall scores than all\nindividual and ensembles of VirusTotal engines, demonstrating its effectiveness\nand setting a new standard for identifying malware-compromised software\ncontainers.",
        "published": "2025-04-04T07:38:16+00:00"
    },
    {
        "title": "Crash Time Matters: HybridMamba for Fine-Grained Temporal Localization in Traffic Surveillance Footage",
        "authors": [
            "Ibne Farabi Shihab",
            "Anuj Sharma"
        ],
        "summary": "Traffic crash detection in long-form surveillance videos is critical for\nemergency response and infrastructure planning but remains difficult due to the\nbrief and rare nature of crash events. We introduce HybridMamba, a novel\narchitecture that combines visual transformers with state-space temporal\nmodeling to achieve accurate crash time localization. Our method uses\nmulti-level token compression and hierarchical temporal processing to remain\ncomputationally efficient without sacrificing temporal resolution. Evaluated on\na large-scale dataset from the Iowa Department of Transportation, HybridMamba\nachieves a mean absolute error of 1.50 seconds, with 65.2 percent of\npredictions within one second of the ground truth. It outperforms recent\nvideo-language models such as TimeChat and VideoLLaMA2 by up to 2.8 seconds,\nwhile using significantly fewer parameters. Our results demonstrate strong\ngeneralization across videos ranging from 2 to 40 minutes in diverse\nconditions. HybridMamba offers a robust and efficient solution for fine-grained\ntemporal localization in traffic surveillance. The code will be released upon\npublication.",
        "published": "2025-04-04T07:35:11+00:00"
    },
    {
        "title": "Unlocking Neural Transparency: Jacobian Maps for Explainable AI in Alzheimer's Detection",
        "authors": [
            "Yasmine Mustafa",
            "Mohamed Elmahallawy",
            "Tie Luo"
        ],
        "summary": "Alzheimer's disease (AD) leads to progressive cognitive decline, making early\ndetection crucial for effective intervention. While deep learning models have\nshown high accuracy in AD diagnosis, their lack of interpretability limits\nclinical trust and adoption. This paper introduces a novel pre-model approach\nleveraging Jacobian Maps (JMs) within a multi-modal framework to enhance\nexplainability and trustworthiness in AD detection. By capturing localized\nbrain volume changes, JMs establish meaningful correlations between model\npredictions and well-known neuroanatomical biomarkers of AD. We validate JMs\nthrough experiments comparing a 3D CNN trained on JMs versus on traditional\npreprocessed data, which demonstrates superior accuracy. We also employ 3D\nGrad-CAM analysis to provide both visual and quantitative insights, further\nshowcasing improved interpretability and diagnostic reliability.",
        "published": "2025-04-04T07:24:52+00:00"
    },
    {
        "title": "A Robust Method for Fault Detection and Severity Estimation in Mechanical Vibration Data",
        "authors": [
            "Youngjae Jeon",
            "Eunho Heo",
            "Jinmo Lee",
            "Taewon Uhm",
            "Dongjin Lee"
        ],
        "summary": "This paper proposes a robust method for fault detection and severity\nestimation in multivariate time-series data to enhance predictive maintenance\nof mechanical systems. We use the Temporal Graph Convolutional Network (T-GCN)\nmodel to capture both spatial and temporal dependencies among variables. This\nenables accurate future state predictions under varying operational conditions.\nTo address the challenge of fluctuating anomaly scores that reduce fault\nseverity estimation accuracy, we introduce a novel fault severity index based\non the mean and standard deviation of anomaly scores. This generates a\ncontinuous and reliable severity measurement. We validate the proposed method\nusing two experimental datasets: an open IMS bearing dataset and data collected\nfrom a fanjet electric propulsion system. Results demonstrate that our method\nsignificantly reduces abrupt fluctuations and inconsistencies in anomaly\nscores. This provides a more dependable foundation for maintenance planning and\nrisk management in safety-critical applications.",
        "published": "2025-04-04T07:22:29+00:00"
    },
    {
        "title": "Electromyography-Based Gesture Recognition: Hierarchical Feature Extraction for Enhanced Spatial-Temporal Dynamics",
        "authors": [
            "Jungpil Shin",
            "Abu Saleh Musa Miah",
            "Sota Konnai",
            "Shu Hoshitaka",
            "Pankoo Kim"
        ],
        "summary": "Hand gesture recognition using multichannel surface electromyography (sEMG)\nis challenging due to unstable predictions and inefficient time-varying feature\nenhancement. To overcome the lack of signal based time-varying feature\nproblems, we propose a lightweight squeeze-excitation deep learning-based multi\nstream spatial temporal dynamics time-varying feature extraction approach to\nbuild an effective sEMG-based hand gesture recognition system. Each branch of\nthe proposed model was designed to extract hierarchical features, capturing\nboth global and detailed spatial-temporal relationships to ensure feature\neffectiveness. The first branch, utilizing a Bidirectional-TCN (Bi-TCN),\nfocuses on capturing long-term temporal dependencies by modelling past and\nfuture temporal contexts, providing a holistic view of gesture dynamics. The\nsecond branch, incorporating a 1D Convolutional layer, separable CNN, and\nSqueeze-and-Excitation (SE) block, efficiently extracts spatial-temporal\nfeatures while emphasizing critical feature channels, enhancing feature\nrelevance. The third branch, combining a Temporal Convolutional Network (TCN)\nand Bidirectional LSTM (BiLSTM), captures bidirectional temporal relationships\nand time-varying patterns. Outputs from all branches are fused using\nconcatenation to capture subtle variations in the data and then refined with a\nchannel attention module, selectively focusing on the most informative features\nwhile improving computational efficiency. The proposed model was tested on the\nNinapro DB2, DB4, and DB5 datasets, achieving accuracy rates of 96.41%, 92.40%,\nand 93.34%, respectively. These results demonstrate the capability of the\nsystem to handle complex sEMG dynamics, offering advancements in prosthetic\nlimb control and human-machine interface technologies with significant\nimplications for assistive technologies.",
        "published": "2025-04-04T07:11:12+00:00"
    },
    {
        "title": "Learning Lie Group Generators from Trajectories",
        "authors": [
            "Lifan Hu"
        ],
        "summary": "This work investigates the inverse problem of generator recovery in matrix\nLie groups from discretized trajectories. Let $G$ be a real matrix Lie group\nand $\\mathfrak{g} = \\text{Lie}(G)$ its corresponding Lie algebra. A smooth\ntrajectory $\\gamma($t$)$ generated by a fixed Lie algebra element $\\xi \\in\n\\mathfrak{g}$ follows the exponential flow $\\gamma($t$) = g_0 \\cdot \\exp(t\n\\xi)$. The central task addressed in this work is the reconstruction of such a\nlatent generator $\\xi$ from a discretized sequence of poses $ \\{g_0, g_1,\n\\dots, g_T\\} \\subset G$, sampled at uniform time intervals. This problem is\nformulated as a data-driven regression from normalized sequences of discrete\nLie algebra increments $\\log\\left(g_{t}^{-1} g_{t+1}\\right)$ to the constant\ngenerator $\\xi \\in \\mathfrak{g}$. A feedforward neural network is trained to\nlearn this mapping across several groups, including $\\text{SE(2)},\n\\text{SE(3)}, \\text{SO(3)}, and \\text{SL(2,$\\mathbb{R})$}$. It demonstrates\nstrong empirical accuracy under both clean and noisy conditions, which\nvalidates the viability of data-driven recovery of Lie group generators using\nshallow neural architectures. This is Lie-RL GitHub Repo\nhttps://github.com/Anormalm/LieRL-on-Trajectories. Feel free to make\nsuggestions and collaborations!",
        "published": "2025-04-04T07:08:59+00:00"
    },
    {
        "title": "From ChatGPT to DeepSeek AI: A Comprehensive Analysis of Evolution, Deviation, and Future Implications in AI-Language Models",
        "authors": [
            "Simrandeep Singh",
            "Shreya Bansal",
            "Abdulmotaleb El Saddik",
            "Mukesh Saini"
        ],
        "summary": "The rapid advancement of artificial intelligence (AI) has reshaped the field\nof natural language processing (NLP), with models like OpenAI ChatGPT and\nDeepSeek AI. Although ChatGPT established a strong foundation for\nconversational AI, DeepSeek AI introduces significant improvements in\narchitecture, performance, and ethical considerations. This paper presents a\ndetailed analysis of the evolution from ChatGPT to DeepSeek AI, highlighting\ntheir technical differences, practical applications, and broader implications\nfor AI development. To assess their capabilities, we conducted a case study\nusing a predefined set of multiple choice questions in various domains,\nevaluating the strengths and limitations of each model. By examining these\naspects, we provide valuable insight into the future trajectory of AI, its\npotential to transform industries, and key research directions for improving\nAI-driven language models.",
        "published": "2025-04-04T07:08:29+00:00"
    },
    {
        "title": "Structured Knowledge Accumulation: The Principle of Entropic Least Action in Forward-Only Neural Learning",
        "authors": [
            "Bouarfa Mahi Quantiota"
        ],
        "summary": "This paper aims to extend the Structured Knowledge Accumulation (SKA)\nframework recently proposed by \\cite{mahi2025ska}. We introduce two core\nconcepts: the Tensor Net function and the characteristic time property of\nneural learning. First, we reinterpret the learning rate as a time step in a\ncontinuous system. This transforms neural learning from discrete optimization\ninto continuous-time evolution. We show that learning dynamics remain\nconsistent when the product of learning rate and iteration steps stays\nconstant. This reveals a time-invariant behavior and identifies an intrinsic\ntimescale of the network. Second, we define the Tensor Net function as a\nmeasure that captures the relationship between decision probabilities, entropy\ngradients, and knowledge change. Additionally, we define its zero-crossing as\nthe equilibrium state between decision probabilities and entropy gradients. We\nshow that the convergence of entropy and knowledge flow provides a natural\nstopping condition, replacing arbitrary thresholds with an\ninformation-theoretic criterion. We also establish that SKA dynamics satisfy a\nvariational principle based on the Euler-Lagrange equation. These findings\nextend SKA into a continuous and self-organizing learning model. The framework\nlinks computational learning with physical systems that evolve by natural laws.\nBy understanding learning as a time-based process, we open new directions for\nbuilding efficient, robust, and biologically-inspired AI systems.",
        "published": "2025-04-04T07:00:27+00:00"
    },
    {
        "title": "PIONM: A Generalized Approach to Solving Density-Constrained Mean-Field Games Equilibrium under Modified Boundary Conditions",
        "authors": [
            "Jinwei Liu",
            "Wang Yao",
            "Xiao Zhang"
        ],
        "summary": "Neural network-based methods are effective for solving equilibria in\nMean-Field Games (MFGs), particularly in high-dimensional settings. However,\nsolving the coupled partial differential equations (PDEs) in MFGs limits their\napplicability since solving coupled PDEs is computationally expensive.\nAdditionally, modifying boundary conditions, such as the initial state\ndistribution or terminal value function, necessitates extensive retraining,\nreducing scalability. To address these challenges, we propose a generalized\nframework, PIONM (Physics-Informed Neural Operator NF-MKV Net), which leverages\nphysics-informed neural operators to solve MFGs equations. PIONM utilizes\nneural operators to compute MFGs equilibria for arbitrary boundary conditions.\nThe method encodes boundary conditions as input features and trains the model\nto align them with density evolution, modeled using discrete-time normalizing\nflows. Once trained, the algorithm efficiently computes the density\ndistribution at any time step for modified boundary condition, ensuring\nefficient adaptation to different boundary conditions in MFGs equilibria.\nUnlike traditional MFGs methods constrained by fixed coefficients, PIONM\nefficiently computes equilibria under varying boundary conditions, including\nobstacles, diffusion coefficients, initial densities, and terminal functions.\nPIONM can adapt to modified conditions while preserving density distribution\nconstraints, demonstrating superior scalability and generalization capabilities\ncompared to existing methods.",
        "published": "2025-04-04T06:46:09+00:00"
    },
    {
        "title": "A Runaway Electron Avalanche Surrogate for Partially Ionized Plasmas",
        "authors": [
            "Jonathan S. Arnaud",
            "Xian-Zhu Tang",
            "Christopher J. McDevitt"
        ],
        "summary": "A physics-constrained deep learning surrogate that predicts the exponential\ngrowth rate of runaway electrons (REs) for a plasma containing partially\nionized impurities is developed. Specifically, a physics-informed neural\nnetwork (PINN) that learns the adjoint of the relativistic Fokker Planck\nequation in steady-state is derived, enabling a rapid surrogate of the RE\navalanche for a broad range of plasma parameters, motivating a path towards an\nML-accelerated integrated description of a tokamak disruption. A steady-state\npower balance equation together with atomic physics data is embedded directly\ninto the PINN, thus limiting the PINN to train across physically consistent\ntemperatures and charge state distributions. This restricted training domain\nenables accurate predictions of the PINN while drastically reducing the\ncomputational cost of training the model. In addition, a novel closure for the\nrelativistic electron population used when evaluating the secondary source of\nREs is developed that enables improved accuracy compared to a\nRosenbluth-Putvinski source. The avalanche surrogate is verified against Monte\nCarlo simulations, where it is shown to accurately predict the RE avalanche\ngrowth rate across a broad range of plasma parameters encompassing distinct\ntokamak disruption scenarios.",
        "published": "2025-04-04T06:12:24+00:00"
    },
    {
        "title": "The strain-stiffening critical exponents in polymer networks and their universality",
        "authors": [
            "Zibin Zhang",
            "Eran Bouchbinder",
            "Edan Lerner"
        ],
        "summary": "Disordered athermal biopolymer materials, such as collagen networks that\nconstitute a major component in extracellular matrices and various connective\ntissues, are initially soft and compliant but stiffen dramatically under\nstrain. Such network materials are topologically sub-isostatic and feature\nstrong rigidity scale separation between the bending and stretching response of\nthe constituent polymer fibers. Recently, a comprehensive scaling theory of the\nathermal strain-stiffening phase transition has been developed, providing\npredictions for all critical exponents characterising the transition in terms\nof the distance to the critical strain and of the small rigidity scales ratio.\nHere, we employ large-scale computer simulations, at and away from criticality,\nto test the analytic predictions. We find that all numerical critical exponents\nare in quantitative agreement with the analytically-predicted ones. Moreover,\nwe find that all predicted exponents remain valid whether the driving strain is\nshear, i.e., volume-preserving, or dilation, and independent of the degree of\nthe network's sub-isostaticity, thus establishing the universality of the\nstrain-stiffening phase transition with respect to the symmetry of the driving\nstrain and the network's topology.",
        "published": "2025-04-04T06:08:21+00:00"
    },
    {
        "title": "A Survey of Quantum Transformers: Approaches, Advantages, Challenges, and Future Directions",
        "authors": [
            "Hui Zhang",
            "Qinglin Zhao"
        ],
        "summary": "Quantum Transformer models represent a significant research direction in\nquantum machine learning (QML), leveraging the parallelism and entanglement\nproperties of quantum computing to overcome the computational complexity and\nexpressive limitations of classical Transformers. Parameterized quantum circuit\n(PQC)-based Transformer models are the primary focus of current research,\nemploying PQCs to achieve varying degrees of quantumization, including\nstrategies such as QKV-only Quantum mapping, Quantum Pairwise Attention,\nQuantum Global Attention, and Quantum-Assisted Acceleration. These approaches\nare well-suited to Noisy Intermediate-Scale Quantum (NISQ) devices,\ndemonstrating potential in small-scale tasks to reduce complexity or enhance\nperformance. The strength of PQC-based methods lies in their compatibility with\nexisting quantum hardware, positioning them as the main pathway toward the\npractical implementation of quantum Transformers. However, these methods face\nchallenges such as limited scalability, the absence of standardized testing\nbenchmarks, and the \"barren plateau\" problem during training. As a\ncomplementary approach, Quantum Linear Algebra (QLA)-based Transformer models\nrely on future fault-tolerant quantum computing, utilizing techniques like\nblock-encoding and Quantum Singular Value Transformation (QSVT) to achieve\nefficient matrix operations and theoretically significant complexity\nreductions, though they remain in the theoretical exploration stage. Future\nresearch should prioritize optimizing PQC-based hybrid architectures and\nquantum global attention models, establishing unified evaluation frameworks,\nand addressing training difficulties, while also exploring hybrid PQC-QLA\napproaches to advance the development of quantum Transformers.",
        "published": "2025-04-04T05:40:18+00:00"
    },
    {
        "title": "Experimental Study on Time Series Analysis of Lower Limb Rehabilitation Exercise Data Driven by Novel Model Architecture and Large Models",
        "authors": [
            "Hengyu Lin"
        ],
        "summary": "This study investigates the application of novel model architectures and\nlarge-scale foundational models in temporal series analysis of lower limb\nrehabilitation motion data, aiming to leverage advancements in machine learning\nand artificial intelligence to empower active rehabilitation guidance\nstrategies for post-stroke patients in limb motor function recovery. Utilizing\nthe SIAT-LLMD dataset of lower limb movement data proposed by the Shenzhen\nInstitute of Advanced Technology, Chinese Academy of Sciences, we\nsystematically elucidate the implementation and analytical outcomes of the\ninnovative xLSTM architecture and the foundational model Lag-Llama in\nshort-term temporal prediction tasks involving joint kinematics and dynamics\nparameters. The research provides novel insights for AI-enabled medical\nrehabilitation applications, demonstrating the potential of cutting-edge model\narchitectures and large-scale models in rehabilitation medicine temporal\nprediction. These findings establish theoretical foundations for future\napplications of personalized rehabilitation regimens, offering significant\nimplications for the development of customized therapeutic interventions in\nclinical practice.",
        "published": "2025-04-04T05:40:13+00:00"
    },
    {
        "title": "An Intelligent and Privacy-Preserving Digital Twin Model for Aging-in-Place",
        "authors": [
            "Yongjie Wang",
            "Jonathan Cyril Leung",
            "Ming Chen",
            "Zhiwei Zeng",
            "Benny Toh Hsiang Tan",
            "Yang Qiu",
            "Zhiqi Shen"
        ],
        "summary": "The population of older adults is steadily increasing, with a strong\npreference for aging-in-place rather than moving to care facilities.\nConsequently, supporting this growing demographic has become a significant\nglobal challenge. However, facilitating successful aging-in-place is\nchallenging, requiring consideration of multiple factors such as data privacy,\nhealth status monitoring, and living environments to improve health outcomes.\nIn this paper, we propose an unobtrusive sensor system designed for\ninstallation in older adults' homes. Using data from the sensors, our system\nconstructs a digital twin, a virtual representation of events and activities\nthat occurred in the home. The system uses neural network models and decision\nrules to capture residents' activities and living environments. This digital\ntwin enables continuous health monitoring by providing actionable insights into\nresidents' well-being. Our system is designed to be low-cost and\nprivacy-preserving, with the aim of providing green and safe monitoring for the\nhealth of older adults. We have successfully deployed our system in two homes\nover a time period of two months, and our findings demonstrate the feasibility\nand effectiveness of digital twin technology in supporting independent living\nfor older adults. This study highlights that our system could revolutionize\nelder care by enabling personalized interventions, such as lifestyle\nadjustments, medical treatments, or modifications to the residential\nenvironment, to enhance health outcomes.",
        "published": "2025-04-04T05:37:08+00:00"
    },
    {
        "title": "An ADMM Algorithm for Structure Learning in Equilibrium Networks",
        "authors": [
            "Rohith Reddy Mada",
            "Rajasekhar Anguluri"
        ],
        "summary": "Learning the edge connectivity structure of networked systems from limited\ndata is a fundamental challenge in many critical infrastructure domains,\nincluding power, traffic, and finance. Such systems obey steady-state\nconservation laws: $x(t) = Ly(t)$, where $x(t) \\in \\mathbb{R}^p$ and $y(t) \\in\n\\mathbb{R}^p$ represent injected flows (inputs) and potentials (outputs),\nrespectively. The sparsity pattern of the $p\\times p$ Laplacian L encodes the\nunderlying edge structure. In a stochastic setting, the goal is to infer this\nsparsity pattern from zero-mean i.i.d. samples of y(t).\n  Recent work by Rayas et al. [1] has established statistical consistency\nresults for this learning problem by considering an $\\ell_1$-regularized\nmaximum likelihood estimator. However, their approach did not focus on\ndeveloping a scalable algorithm but relies on solving a convex program via the\nCVX package in Python. To address this gap, we propose an alternating direction\nmethod of multipliers (ADMM) algorithm. Our approach is simple, transparent,\nand computationally fast. A key contribution is demonstrating the role of a\nnon-symmetric algebraic Riccati equation in the primal step of ADMM. Numerical\nexperiments on a host of synthetic and benchmark networks, including power and\nwater systems, show that our method achieves high recovery accuracy.",
        "published": "2025-04-04T05:33:23+00:00"
    },
    {
        "title": "On the Connection Between Diffusion Models and Molecular Dynamics",
        "authors": [
            "Liam Harcombe",
            "Timothy T. Duignan"
        ],
        "summary": "Neural Network Potentials (NNPs) have emerged as a powerful tool for\nmodelling atomic interactions with high accuracy and computational efficiency.\nRecently, denoising diffusion models have shown promise in NNPs by training\nnetworks to remove noise added to stable configurations, eliminating the need\nfor force data during training. In this work, we explore the connection between\nnoise and forces by providing a new, simplified mathematical derivation of\ntheir relationship. We also demonstrate how a denoising model can be\nimplemented using a conventional MD software package interfaced with a standard\nNNP architecture. We demonstrate the approach by training a diffusion-based NNP\nto simulate a coarse-grained lithium chloride solution and employ data\nduplication to enhance model performance.",
        "published": "2025-04-04T05:32:38+00:00"
    },
    {
        "title": "Successive Antiferromagnetic Transition in the Frustrated Compound CeMgIn",
        "authors": [
            "Kou Onishi",
            "Hitoshi Sugawara",
            "Takahiro Sakurai",
            "Hitoshi Ohta",
            "Eiichi Matsuoka"
        ],
        "summary": "We report on the magnetic, transport, and thermal properties of the hexagonal\nZrNiAl-type compound CeMgIn with Ce atoms forming a distorted kagome network.\nThis compound exhibits successive antiferromagnetic transition at $T_\\text{N1}\n=$ 2.1 K, $T_\\text{N2} =$ 1.7 K, and possibly $T_\\text{N3} =$ 1.3 K. The\nelectrical resistivity exhibits a minimum at 11 K and a nonlogarithmic increase\nwith decreasing temperature down to $T_\\text{N2}$. We found that CeMgIn is the\nfirst ZrNiAl-type compound whose resistivity increase can be well explained by\nconsidering a model in which the electron-spin scattering is enhanced by the\nmagnetic frustration and the Ruderman-Kittel-Kasuya-Yosida interaction. These\nresults suggest that CeMgIn is a notable compound whose physical properties are\nstrongly affected by geometrical frustration. Since the Sommerfeld coefficient\nis 97 mJ/mol K$^2$, CeMgIn is classified as a moderate heavy-fermion compound.",
        "published": "2025-04-04T05:30:57+00:00"
    },
    {
        "title": "On Fundamental Limits for Fluid Antenna-assisted Integrated Sensing and Communications for Unsourced Random Access",
        "authors": [
            "Zhentian Zhang",
            "Kai-Kit Wong",
            "Jian Dang",
            "Zaichen Zhang",
            "Chan-Byoung Chae"
        ],
        "summary": "This paper investigates the unsourced random access (URA) problem for\nintegrated sensing and commutations (ISAC). Recent results reveal that\nconventional multiple access strategies for ISAC such as treating interference\nas noise (TIN) and time-division multiple access (TDMA) can be easily\noverwhelmed and fail to support the increasingly surging number of active\nusers. Hence, the unsourced ISAC (UNISAC) system model has emerged as a\npromising enabler for the future ISAC networks. To advance this work, we adopt\na more realistic channel model and propose to utilize fluid antenna system\n(FAS) for UNISAC. The achievable performance bound and floor of the proposed\nFAS-UNISAC are derived to validate the great potential. Our results demonstrate\nthat promising improvement on the available user volume and the sensing and\ncommunication capability can be obtained due to the spatial diversities\ninherent within fluid antenna.",
        "published": "2025-04-04T05:22:43+00:00"
    },
    {
        "title": "Throughput-Optimal Random Access: A Queueing-Theoretical Analysis for Learning-Based Access Design",
        "authors": [
            "Xinran Zhao",
            "Lin Dai"
        ],
        "summary": "Random access networks have long been observed to suffer from low throughput\nif nodes' access strategy is not properly designed. To improve the throughput\nperformance, learning-based approaches, with which each node learns from the\nobservations and experience to determine its own access strategy, have shown\nimmense potential, but are often designed empirically due to the lack of\ntheoretical guidance. As we will demonstrate in this paper, the\nqueueing-theoretical analysis can be leveraged as a powerful tool for optimal\ndesign of learning-based access. Specifically, based on a Multi-Armed-Bandit\n(MAB) framework, two random access schemes, MTOA-L with local rewards and\nMTOA-G with global rewards, are proposed for throughput optimization. Though\nboth can achieve the maximum throughput of 1, they have different short-term\nfairness performance. Through identifying the access strategies learned via\nMTOA-L and MTOA-G and feeding them into the proposed unified\nqueueing-theoretical framework, the throughput-fairness tradeoff of each is\ncharacterized and optimized by properly tuning the key parameters. The\ncomparison of the optimal tradeoffs shows that MTOA-G is much superior to\nMTOA-L especially when the number of nodes is large.",
        "published": "2025-04-04T05:08:33+00:00"
    },
    {
        "title": "REJEPA: A Novel Joint-Embedding Predictive Architecture for Efficient Remote Sensing Image Retrieval",
        "authors": [
            "Shabnam Choudhury",
            "Yash Salunkhe",
            "Sarthak Mehrotra",
            "Biplab Banerjee"
        ],
        "summary": "The rapid expansion of remote sensing image archives demands the development\nof strong and efficient techniques for content-based image retrieval (RS-CBIR).\nThis paper presents REJEPA (Retrieval with Joint-Embedding Predictive\nArchitecture), an innovative self-supervised framework designed for unimodal\nRS-CBIR. REJEPA utilises spatially distributed context token encoding to\nforecast abstract representations of target tokens, effectively capturing\nhigh-level semantic features and eliminating unnecessary pixel-level details.\nIn contrast to generative methods that focus on pixel reconstruction or\ncontrastive techniques that depend on negative pairs, REJEPA functions within\nfeature space, achieving a reduction in computational complexity of 40-60% when\ncompared to pixel-reconstruction baselines like Masked Autoencoders (MAE). To\nguarantee strong and varied representations, REJEPA incorporates\nVariance-Invariance-Covariance Regularisation (VICReg), which prevents encoder\ncollapse by promoting feature diversity and reducing redundancy. The method\ndemonstrates an estimated enhancement in retrieval accuracy of 5.1% on BEN-14K\n(S1), 7.4% on BEN-14K (S2), 6.0% on FMoW-RGB, and 10.1% on FMoW-Sentinel\ncompared to prominent SSL techniques, including CSMAE-SESD, Mask-VLM, SatMAE,\nScaleMAE, and SatMAE++, on extensive RS benchmarks BEN-14K (multispectral and\nSAR data), FMoW-RGB and FMoW-Sentinel. Through effective generalisation across\nsensor modalities, REJEPA establishes itself as a sensor-agnostic benchmark for\nefficient, scalable, and precise RS-CBIR, addressing challenges like varying\nresolutions, high object density, and complex backgrounds with computational\nefficiency.",
        "published": "2025-04-04T04:59:10+00:00"
    },
    {
        "title": "RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation",
        "authors": [
            "Hanbo Bi",
            "Yingchao Feng",
            "Boyuan Tong",
            "Mengyu Wang",
            "Haichen Yu",
            "Yongqiang Mao",
            "Hao Chang",
            "Wenhui Diao",
            "Peijin Wang",
            "Yue Yu",
            "Hanyang Peng",
            "Yehong Zhang",
            "Kun Fu",
            "Xian Sun"
        ],
        "summary": "The rapid advancement of foundation models has revolutionized visual\nrepresentation learning in a self-supervised manner. However, their application\nin remote sensing (RS) remains constrained by a fundamental gap: existing\nmodels predominantly handle single or limited modalities, overlooking the\ninherently multi-modal nature of RS observations. Optical, synthetic aperture\nradar (SAR), and multi-spectral data offer complementary insights that\nsignificantly reduce the inherent ambiguity and uncertainty in single-source\nanalysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS\nfoundation model with 14.7 billion parameters, pre-trained on 400 million\nmulti-modal RS images from nine satellites. RingMoE incorporates three key\ninnovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture\ncomprising modal-specialized, collaborative, and shared experts, effectively\nmodeling intra-modal knowledge while capturing cross-modal dependencies to\nmitigate conflicts between modal representations; (2) Physics-informed\nself-supervised learning, explicitly embedding sensor-specific radiometric\ncharacteristics into the pre-training objectives; (3) Dynamic expert pruning,\nenabling adaptive model compression from 14.7B to 1B parameters while\nmaintaining performance, facilitating efficient deployment in Earth observation\napplications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e.,\nclassification, detection, segmentation, tracking, change detection, and depth\nestimation), RingMoE outperforms existing foundation models and sets new SOTAs,\ndemonstrating remarkable adaptability from single-modal to multi-modal\nscenarios. Beyond theoretical progress, it has been deployed and trialed in\nmultiple sectors, including emergency response, land management, marine\nsciences, and urban planning.",
        "published": "2025-04-04T04:47:54+00:00"
    },
    {
        "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
        "authors": [
            "Yuxiang Zheng",
            "Dayuan Fu",
            "Xiangkun Hu",
            "Xiaojie Cai",
            "Lyumanshan Ye",
            "Pengrui Lu",
            "Pengfei Liu"
        ],
        "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.",
        "published": "2025-04-04T04:41:28+00:00"
    },
    {
        "title": "SpinHex: A low-crosstalk, spin-qubit architecture based on multi-electron couplers",
        "authors": [
            "Rub\u00e9n M. Otxoa",
            "Josu Etxezarreta Martinez",
            "Paul Schnabl",
            "Normann Mertig",
            "Charles Smith",
            "Frederico Martins"
        ],
        "summary": "Semiconductor spin qubits are an attractive quantum computing platform that\noffers long qubit coherence times and compatibility with existing semiconductor\nfabrication technology for scale up. Here, we propose a spin-qubit architecture\nbased on spinless multielectron quantum dots that act as low-crosstalk couplers\nbetween a two-dimensional arrangement of spin-qubits in a hexagonal lattice.\nThe multielectron couplers are controlled by voltage signals, which mediate\nfast Heisenberg exchange and thus enable coherent multi-qubit operations. For\nthe proposed architecture, we discuss the implementation of the rotated XZZX\nsurface code and numerically study its performance for a circuit-level noise\nmodel. We predict a threshold of $0.18\\%$ for the error rate of the entangling\ngates. We further evaluate the scalability of the proposed architecture and\npredict the need for $4480$ physical qubits per logical qubit with logical\nerror rates of $10^{-12}$ considering entangling gate fidelities of $99.99\\%$,\nresulting in a chip size of $2.6$cm$^2$ to host $10,000$ logical qubits.",
        "published": "2025-04-04T04:04:01+00:00"
    },
    {
        "title": "A Human Digital Twin Architecture for Knowledge-based Interactions and Context-Aware Conversations",
        "authors": [
            "Abdul Mannan Mohammed",
            "Azhar Ali Mohammad",
            "Jason A. Ortiz",
            "Carsten Neumann",
            "Grace Bochenek",
            "Dirk Reiners",
            "Carolina Cruz-Neira"
        ],
        "summary": "Recent developments in Artificial Intelligence (AI) and Machine Learning (ML)\nare creating new opportunities for Human-Autonomy Teaming (HAT) in tasks,\nmissions, and continuous coordinated activities. A major challenge is enabling\nhumans to maintain awareness and control over autonomous assets, while also\nbuilding trust and supporting shared contextual understanding. To address this,\nwe present a real-time Human Digital Twin (HDT) architecture that integrates\nLarge Language Models (LLMs) for knowledge reporting, answering, and\nrecommendation, embodied in a visual interface.\n  The system applies a metacognitive approach to enable personalized,\ncontext-aware responses aligned with the human teammate's expectations. The HDT\nacts as a visually and behaviorally realistic team member, integrated\nthroughout the mission lifecycle, from training to deployment to after-action\nreview. Our architecture includes speech recognition, context processing,\nAI-driven dialogue, emotion modeling, lip-syncing, and multimodal feedback. We\ndescribe the system design, performance metrics, and future development\ndirections for more adaptive and realistic HAT systems.",
        "published": "2025-04-04T03:56:26+00:00"
    },
    {
        "title": "Comparative Analysis of Unsupervised and Supervised Autoencoders for Nuclei Classification in Clear Cell Renal Cell Carcinoma Images",
        "authors": [
            "Fatemeh Javadian",
            "Zahra Aminparast",
            "Johannes Stegmaier",
            "Abin Jose"
        ],
        "summary": "This study explores the application of supervised and unsupervised\nautoencoders (AEs) to automate nuclei classification in clear cell renal cell\ncarcinoma (ccRCC) images, a diagnostic task traditionally reliant on subjective\nvisual grading by pathologists. We evaluate various AE architectures, including\nstandard AEs, contractive AEs (CAEs), and discriminative AEs (DAEs), as well as\na classifier-based discriminative AE (CDAE), optimized using the hyperparameter\ntuning tool Optuna. Bhattacharyya distance is selected from several metrics to\nassess class separability in the latent space, revealing challenges in\ndistinguishing adjacent grades using unsupervised models. CDAE, integrating a\nsupervised classifier branch, demonstrated superior performance in both latent\nspace separation and classification accuracy. Given that CDAE-CNN achieved\nnotable improvements in classification metrics, affirming the value of\nsupervised learning for class-specific feature extraction, F1 score was\nincorporated into the tuning process to optimize classification performance.\nResults show significant improvements in identifying aggressive ccRCC grades by\nleveraging the classification capability of AE through latent clustering\nfollowed by fine-grained classification. Our model outperforms the current\nstate of the art, CHR-Network, across all evaluated metrics. These findings\nsuggest that integrating a classifier branch in AEs, combined with neural\narchitecture search and contrastive learning, enhances grading automation in\nccRCC pathology, particularly in detecting aggressive tumor grades, and may\nimprove diagnostic accuracy.",
        "published": "2025-04-04T03:52:32+00:00"
    },
    {
        "title": "Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable",
        "authors": [
            "Xin Jin",
            "Simon Niklaus",
            "Zhoutong Zhang",
            "Zhihao Xia",
            "Chunle Guo",
            "Yuting Yang",
            "Jiawen Chen",
            "Chongyi Li"
        ],
        "summary": "Denoising is a crucial step in many video processing pipelines such as in\ninteractive editing, where high quality, speed, and user control are essential.\nWhile recent approaches achieve significant improvements in denoising quality\nby leveraging deep learning, they are prone to unexpected failures due to\ndiscrepancies between training data distributions and the wide variety of noise\npatterns found in real-world videos. These methods also tend to be slow and\nlack user control. In contrast, traditional denoising methods perform reliably\non in-the-wild videos and run relatively quickly on modern hardware. However,\nthey require manually tuning parameters for each input video, which is not only\ntedious but also requires skill. We bridge the gap between these two paradigms\nby proposing a differentiable denoising pipeline based on traditional methods.\nA neural network is then trained to predict the optimal denoising parameters\nfor each specific input, resulting in a robust and efficient approach that also\nsupports user control.",
        "published": "2025-04-04T03:03:23+00:00"
    },
    {
        "title": "DP-LET: An Efficient Spatio-Temporal Network Traffic Prediction Framework",
        "authors": [
            "Xintong Wang",
            "Haihan Nan",
            "Ruidong Li",
            "Huaming Wu"
        ],
        "summary": "Accurately predicting spatio-temporal network traffic is essential for\ndynamically managing computing resources in modern communication systems and\nminimizing energy consumption. Although spatio-temporal traffic prediction has\nreceived extensive research attention, further improvements in prediction\naccuracy and computational efficiency remain necessary. In particular, existing\ndecomposition-based methods or hybrid architectures often incur heavy overhead\nwhen capturing local and global feature correlations, necessitating novel\napproaches that optimize accuracy and complexity. In this paper, we propose an\nefficient spatio-temporal network traffic prediction framework, DP-LET, which\nconsists of a data processing module, a local feature enhancement module, and a\nTransformer-based prediction module. The data processing module is designed for\nhigh-efficiency denoising of network data and spatial decoupling. In contrast,\nthe local feature enhancement module leverages multiple Temporal Convolutional\nNetworks (TCNs) to capture fine-grained local features. Meanwhile, the\nprediction module utilizes a Transformer encoder to model long-term\ndependencies and assess feature relevance. A case study on real-world cellular\ntraffic prediction demonstrates the practicality of DP-LET, which maintains low\ncomputational complexity while achieving state-of-the-art performance,\nsignificantly reducing MSE by 31.8% and MAE by 23.1% compared to baseline\nmodels.",
        "published": "2025-04-04T02:52:43+00:00"
    },
    {
        "title": "FontGuard: A Robust Font Watermarking Approach Leveraging Deep Font Knowledge",
        "authors": [
            "Kahim Wong",
            "Jicheng Zhou",
            "Kemou Li",
            "Yain-Whar Si",
            "Xiaowei Wu",
            "Jiantao Zhou"
        ],
        "summary": "The proliferation of AI-generated content brings significant concerns on the\nforensic and security issues such as source tracing, copyright protection, etc,\nhighlighting the need for effective watermarking technologies. Font-based text\nwatermarking has emerged as an effective solution to embed information, which\ncould ensure copyright, traceability, and compliance of the generated text\ncontent. Existing font watermarking methods usually neglect essential font\nknowledge, which leads to watermarked fonts of low quality and limited\nembedding capacity. These methods are also vulnerable to real-world\ndistortions, low-resolution fonts, and inaccurate character segmentation. In\nthis paper, we introduce FontGuard, a novel font watermarking model that\nharnesses the capabilities of font models and language-guided contrastive\nlearning. Unlike previous methods that focus solely on the pixel-level\nalteration, FontGuard modifies fonts by altering hidden style features,\nresulting in better font quality upon watermark embedding. We also leverage the\nfont manifold to increase the embedding capacity of our proposed method by\ngenerating substantial font variants closely resembling the original font.\nFurthermore, in the decoder, we employ an image-text contrastive learning to\nreconstruct the embedded bits, which can achieve desirable robustness against\nvarious real-world transmission distortions. FontGuard outperforms\nstate-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under\nsynthetic, cross-media, and online social network distortions, respectively,\nwhile improving the visual quality by 52.7% in terms of LPIPS. Moreover,\nFontGuard uniquely allows the generation of watermarked fonts for unseen fonts\nwithout re-training the network. The code and dataset are available at\nhttps://github.com/KAHIMWONG/FontGuard.",
        "published": "2025-04-04T02:39:33+00:00"
    },
    {
        "title": "Distributed Resilience-Aware Control in Multi-Robot Networks",
        "authors": [
            "Haejoon Lee",
            "Dimitra Panagou"
        ],
        "summary": "Ensuring resilient consensus in multi-robot systems with misbehaving agents\nremains a challenge, as many existing network resilience properties are\ninherently combinatorial and globally defined. While previous works have\nproposed control laws to enhance or preserve resilience in multi-robot\nnetworks, they often assume a fixed topology with known resilience properties,\nor require global state knowledge. These assumptions may be impractical in\nphysically-constrained environments, where safety and resilience requirements\nare conflicting, or when misbehaving agents corrupt the shared information. In\nthis work, we propose a distributed control law that enables each robot to\nguarantee resilient consensus and safety during its navigation without fixed\ntopologies using only locally available information. To this end, we establish\na new sufficient condition for resilient consensus in time-varying networks\nbased on the degree of non-misbehaving or normal agents. Using this condition,\nwe design a Control Barrier Function (CBF)-based controller that guarantees\nresilient consensus and collision avoidance without requiring estimates of\nglobal state and/or control actions of all other robots. Finally, we validate\nour method through simulations.",
        "published": "2025-04-04T02:22:21+00:00"
    },
    {
        "title": "Graph Network Modeling Techniques for Visualizing Human Mobility Patterns",
        "authors": [
            "Sinjini Mitra",
            "Anuj Srivastava",
            "Avipsa Roy",
            "Pavan Turaga"
        ],
        "summary": "Human mobility analysis at urban-scale requires models to represent the\ncomplex nature of human movements, which in turn are affected by accessibility\nto nearby points of interest, underlying socioeconomic factors of a place, and\nlocal transport choices for people living in a geographic region. In this work,\nwe represent human mobility and the associated flow of movements as a grapyh.\nGraph-based approaches for mobility analysis are still in their early stages of\nadoption and are actively being researched. The challenges of graph-based\nmobility analysis are multifaceted - the lack of sufficiently high-quality data\nto represent flows at high spatial and teporal resolution whereas, limited\ncomputational resources to translate large voluments of mobility data into a\nnetwork structure, and scaling issues inherent in graph models etc. The current\nstudy develops a methodology by embedding graphs into a continuous space, which\nalleviates issues related to fast graph matching, graph time-series modeling,\nand visualization of mobility dynamics. Through experiments, we demonstrate how\nmobility data collected from taxicab trajectories could be transformed into\nnetwork structures and patterns of mobility flow changes, and can be used for\ndownstream tasks reporting approx 40% decrease in error on average in matched\ngraphs vs unmatched ones.",
        "published": "2025-04-04T02:21:44+00:00"
    },
    {
        "title": "Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin Lesion Segmentation",
        "authors": [
            "Xuanyu Liu",
            "Huiyun Yao",
            "Jinggui Gao",
            "Zhongyi Guo",
            "Xue Zhang",
            "Yulin Dong"
        ],
        "summary": "Background:Convolutional Neural Networks(CNN) and Vision Transformers(ViT)\nare the main techniques used in Medical image segmentation. However, CNN is\nlimited to local contextual information, and ViT's quadratic complexity results\nin significant computational costs. At the same time, equipping the model to\ndistinguish lesion boundaries with varying degrees of severity is also a\nchallenge encountered in skin lesion segmentation. Purpose:This research aims\nto optimize the balance between computational costs and long-range dependency\nmodelling and achieve excellent generalization across lesions with different\ndegrees of severity. Methods:we propose a lightweight U-shape network that\nutilizes Vision Fastformer with Fusion Mechanism (VFFM-UNet). We inherit the\nadvantages of Fastformer's additive attention mechanism, combining element-wise\nproduct and matrix product for comprehensive feature extraction and channel\nreduction to save computational costs. In order to accurately identify the\nlesion boundaries with varying degrees of severity, we designed Fusion\nMechanism including Multi-Granularity Fusion and Channel Fusion, which can\nprocess the feature maps in the granularity and channel levels to obtain\ndifferent contextual information. Results:Comprehensive experiments on the\nISIC2017, ISIC2018 and PH2 datasets demonstrate that VFFM-UNet outperforms\nexisting state-of-the-art models regarding parameter numbers, computational\ncomplexity and segmentation performance. In short, compared to MISSFormer, our\nmodel achieves superior segmentation performance while reducing parameter and\ncomputation costs by 101x and 15x, respectively. Conclusions:Both quantitative\nand qualitative analyses show that VFFM-UNet sets a new benchmark by reaching\nan ideal balance between parameter numbers, computational complexity, and\nsegmentation performance compared to existing state-of-the-art models.",
        "published": "2025-04-04T01:27:43+00:00"
    },
    {
        "title": "The challenge of the data in the SRCnetwork",
        "authors": [
            "Andrea Possenti"
        ],
        "summary": "At regime, SKAO is expected to provide the researchers with an annual amount\nof more than 700 hundred PB of data. The advanced analysis of all those data\nwill take place within a network (SRCnet) of so-called SKA Regional Centres,\nwhich, under the \"Findable, Accessible, Interoperable, and Reusable\" (FAIR)\nprinciples, will also take the responsibility for curating and archiving both\nthe Observatory Data Products and user-generated Advanced Data Products, as\nwell as for helping the researchers in the estimate of the needed computational\neffort at the proposal stage.\n  This contribution describes the status and the perspectives of this\ninternational network, with particular emphasis on the INAF-led Italian node\nwhich is part of that.",
        "published": "2025-04-04T01:19:30+00:00"
    },
    {
        "title": "Oscillatory Associative Memory with Exponential Capacity",
        "authors": [
            "Taosha Guo",
            "Arie Ogranovich",
            "Arvind R. Venkatakrishnan",
            "Madelyn R. Shapiro",
            "Francesco Bullo",
            "Fabio Pasqualetti"
        ],
        "summary": "The slowing of Moore's law and the increasing energy demands of machine\nlearning present critical challenges for both the hardware and machine learning\ncommunities, and drive the development of novel computing paradigms. Of\nparticular interest is the challenge of incorporating memory efficiently into\nthe learning process. Inspired by how human brains store and retrieve\ninformation, associative memory mechanisms provide a class of computational\nmethods that can store and retrieve patterns in a robust, energy-efficient\nmanner. Existing associative memory architectures, such as the celebrated\nHopfield model and oscillatory associative memory networks, store patterns as\nstable equilibria of network dynamics. However, the capacity (i.e. the number\nof patterns that a network can memorize normalized by their number of nodes) of\nexisting oscillatory models have been shown to decrease with the size of the\nnetwork, making them impractical for large-scale, real-world applications. In\nthis paper, we propose a novel associative memory architecture based on\nKuramoto oscillators. We show that the capacity of our associative memory\nnetwork increases exponentially with network size and features no spurious\nmemories. In addition, we present algorithms and numerical experiments to\nsupport these theoretical findings, providing guidelines for the hardware\nimplementation of the proposed associative memory networks.",
        "published": "2025-04-04T01:11:01+00:00"
    },
    {
        "title": "Scaling Open-Vocabulary Action Detection",
        "authors": [
            "Zhen Hao Sia",
            "Yogesh Singh Rawat"
        ],
        "summary": "In this work, we focus on scaling open-vocabulary action detection. Existing\napproaches for action detection are predominantly limited to closed-set\nscenarios and rely on complex, parameter-heavy architectures. Extending these\nmodels to the open-vocabulary setting poses two key challenges: (1) the lack of\nlarge-scale datasets with many action classes for robust training, and (2)\nparameter-heavy adaptations to a pretrained vision-language contrastive model\nto convert it for detection, risking overfitting the additional non-pretrained\nparameters to base action classes. Firstly, we introduce an encoder-only\nmultimodal model for video action detection, reducing the reliance on\nparameter-heavy additions for video action detection. Secondly, we introduce a\nsimple weakly supervised training strategy to exploit an existing closed-set\naction detection dataset for pretraining. Finally, we depart from the ill-posed\nbase-to-novel benchmark used by prior works in open-vocabulary action detection\nand devise a new benchmark to evaluate on existing closed-set action detection\ndatasets without ever using them for training, showing novel results to serve\nas baselines for future work.",
        "published": "2025-04-04T00:28:42+00:00"
    },
    {
        "title": "Post-processing for Fair Regression via Explainable SVD",
        "authors": [
            "Zhiqun Zuo",
            "Ding Zhu",
            "Mohammad Mahdi Khalili"
        ],
        "summary": "This paper presents a post-processing algorithm for training fair neural\nnetwork regression models that satisfy statistical parity, utilizing an\nexplainable singular value decomposition (SVD) of the weight matrix. We propose\na linear transformation of the weight matrix, whereby the singular values\nderived from the SVD of the transformed matrix directly correspond to the\ndifferences in the first and second moments of the output distributions across\ntwo groups. Consequently, we can convert the fairness constraints into\nconstraints on the singular values. We analytically solve the problem of\nfinding the optimal weights under these constraints. Experimental validation on\nvarious datasets demonstrates that our method achieves a similar or superior\nfairness-accuracy trade-off compared to the baselines without using the\nsensitive attribute at the inference time.",
        "published": "2025-04-04T00:10:01+00:00"
    },
    {
        "title": "Machine Learning-Based Detection and Analysis of Suspicious Activities in Bitcoin Wallet Transactions in the USA",
        "authors": [
            "Md Zahidul Islam",
            "Md Shahidul Islam",
            "Biswajit Chandra das",
            "Syed Ali Reza",
            "Proshanta Kumar Bhowmik",
            "Kanchon Kumar Bishnu",
            "Md Shafiqur Rahman",
            "Redoyan Chowdhury",
            "Laxmi Pant"
        ],
        "summary": "The dramatic adoption of Bitcoin and other cryptocurrencies in the USA has\nrevolutionized the financial landscape and provided unprecedented investment\nand transaction efficiency opportunities. The prime objective of this research\nproject is to develop machine learning algorithms capable of effectively\nidentifying and tracking suspicious activity in Bitcoin wallet transactions.\nWith high-tech analysis, the study aims to create a model with a feature for\nidentifying trends and outliers that can expose illicit activity. The current\nstudy specifically focuses on Bitcoin transaction information in America, with\na strong emphasis placed on the importance of knowing about the immediate\nenvironment in and through which such transactions pass through. The dataset is\ncomposed of in-depth Bitcoin wallet transactional information, including\nimportant factors such as transaction values, timestamps, network flows, and\naddresses for wallets. All entries in the dataset expose information about\nfinancial transactions between wallets, including received and sent\ntransactions, and such information is significant for analysis and trends that\ncan represent suspicious activity. This study deployed three accredited\nalgorithms, most notably, Logistic Regression, Random Forest, and Support\nVector Machines. In retrospect, Random Forest emerged as the best model with\nthe highest F1 Score, showcasing its ability to handle non-linear relationships\nin the data. Insights revealed significant patterns in wallet activity, such as\nthe correlation between unredeemed transactions and final balances. The\napplication of machine algorithms in tracking cryptocurrencies is a tool for\ncreating transparent and secure U.S. markets.",
        "published": "2025-04-04T00:07:32+00:00"
    },
    {
        "title": "Unlocking the AMD Neural Processing Unit for ML Training on the Client Using Bare-Metal-Programming Tools",
        "authors": [
            "Andr\u00e9 R\u00f6sti",
            "Michael Franz"
        ],
        "summary": "There has been a growing interest in executing machine learning (ML)\nworkloads on the client side for reasons of customizability, privacy,\nperformance, and availability. In response, hardware manufacturers have begun\nto incorporate so-called Neural Processing Units (NPUs) into their processors\nfor consumer devices. Such dedicated hardware optimizes both power efficiency\nand throughput for common machine learning tasks. AMD's NPU, part of their\nRyzen AI processors, is one of the first such accelerators integrated into a\nchip with an x86 processor. AMD supports bare-metal programming of their NPU\nrather than limiting programmers to pre-configured libraries.\n  In this paper, we explore the potential of using a bare-metal toolchain to\naccelerate the weight fine-tuning of a large language model, GPT-2, entirely on\nthe client side using the AMD NPU. Fine-tuning on the edge allows for private\ncustomization of a model to a specific use case. To the best of our knowledge,\nthis is the first time such an accelerator has been used to perform training on\nthe client side. We offload time-intensive matrix multiplication operations\nfrom the CPU onto the NPU, achieving a speedup of over 2.8x for these\noperations. This improves end-to-end performance of the model in terms of\nthroughput (1.7x and 1.2x speedup in FLOPS/s on mains and battery power,\nrespectively) and energy efficiency (1.4x improvement in FLOPS/Ws on battery\npower). We detail our implementation approach and present an in-depth\nexploration of the NPU hardware and bare-metal tool-flow.",
        "published": "2025-04-03T23:28:57+00:00"
    },
    {
        "title": "Integrating Identity-Based Identification against Adaptive Adversaries in Federated Learning",
        "authors": [
            "Jakub Kacper Szelag",
            "Ji-Jian Chin",
            "Lauren Ansell",
            "Sook-Chin Yip"
        ],
        "summary": "Federated Learning (FL) has recently emerged as a promising paradigm for\nprivacy-preserving, distributed machine learning. However, FL systems face\nsignificant security threats, particularly from adaptive adversaries capable of\nmodifying their attack strategies to evade detection. One such threat is the\npresence of Reconnecting Malicious Clients (RMCs), which exploit FLs open\nconnectivity by reconnecting to the system with modified attack strategies. To\naddress this vulnerability, we propose integration of Identity-Based\nIdentification (IBI) as a security measure within FL environments. By\nleveraging IBI, we enable FL systems to authenticate clients based on\ncryptographic identity schemes, effectively preventing previously disconnected\nmalicious clients from re-entering the system. Our approach is implemented\nusing the TNC-IBI (Tan-Ng-Chin) scheme over elliptic curves to ensure\ncomputational efficiency, particularly in resource-constrained environments\nlike Internet of Things (IoT). Experimental results demonstrate that\nintegrating IBI with secure aggregation algorithms, such as Krum and Trimmed\nMean, significantly improves FL robustness by mitigating the impact of RMCs. We\nfurther discuss the broader implications of IBI in FL security, highlighting\nresearch directions for adaptive adversary detection, reputation-based\nmechanisms, and the applicability of identity-based cryptographic frameworks in\ndecentralized FL architectures. Our findings advocate for a holistic approach\nto FL security, emphasizing the necessity of proactive defence strategies\nagainst evolving adaptive adversarial threats.",
        "published": "2025-04-03T22:58:27+00:00"
    },
    {
        "title": "AD-GPT: Large Language Models in Alzheimer's Disease",
        "authors": [
            "Ziyu Liu",
            "Lintao Tang",
            "Zeliang Sun",
            "Zhengliang Liu",
            "Yanjun Lyu",
            "Wei Ruan",
            "Yangshuang Xu",
            "Liang Shan",
            "Jiyoon Shin",
            "Xiaohe Chen",
            "Dajiang Zhu",
            "Tianming Liu",
            "Rongjie Liu",
            "Chao Huang"
        ],
        "summary": "Large language models (LLMs) have emerged as powerful tools for medical\ninformation retrieval, yet their accuracy and depth remain limited in\nspecialized domains such as Alzheimer's disease (AD), a growing global health\nchallenge. To address this gap, we introduce AD-GPT, a domain-specific\ngenerative pre-trained transformer designed to enhance the retrieval and\nanalysis of AD-related genetic and neurobiological information. AD-GPT\nintegrates diverse biomedical data sources, including potential AD-associated\ngenes, molecular genetic information, and key gene variants linked to brain\nregions. We develop a stacked LLM architecture combining Llama3 and BERT,\noptimized for four critical tasks in AD research: (1) genetic information\nretrieval, (2) gene-brain region relationship assessment, (3) gene-AD\nrelationship analysis, and (4) brain region-AD relationship mapping.\nComparative evaluations against state-of-the-art LLMs demonstrate AD-GPT's\nsuperior precision and reliability across these tasks, underscoring its\npotential as a robust and specialized AI tool for advancing AD research and\nbiomarker discovery.",
        "published": "2025-04-03T22:49:10+00:00"
    },
    {
        "title": "Moving Target Defense Against Adversarial False Data Injection Attacks In Power Grids",
        "authors": [
            "Yexiang Chen",
            "Subhash Lakshminarayana",
            "H. Vincent Poor"
        ],
        "summary": "Machine learning (ML)-based detectors have been shown to be effective in\ndetecting stealthy false data injection attacks (FDIAs) that can bypass\nconventional bad data detectors (BDDs) in power systems. However, ML models are\nalso vulnerable to adversarial attacks. A sophisticated perturbation signal\nadded to the original BDD-bypassing FDIA can conceal the attack from ML-based\ndetectors. In this paper, we develop a moving target defense (MTD) strategy to\ndefend against adversarial FDIAs in power grids. We first develop an\nMTD-strengthened deep neural network (DNN) model, which deploys a pool of DNN\nmodels rather than a single static model that cooperate to detect the\nadversarial attack jointly. The MTD model pool introduces randomness to the ML\nmodel's decision boundary, thereby making the adversarial attacks detectable.\nFurthermore, to increase the effectiveness of the MTD strategy and reduce the\ncomputational costs associated with developing the MTD model pool, we combine\nthis approach with the physics-based MTD, which involves dynamically perturbing\nthe transmission line reactance and retraining the DNN-based detector to adapt\nto the new system topology. Simulations conducted on IEEE test bus systems\ndemonstrate that the MTD-strengthened DNN achieves up to 94.2% accuracy in\ndetecting adversarial FDIAs. When combined with a physics-based MTD, the\ndetection accuracy surpasses 99%, while significantly reducing the\ncomputational costs of updating the DNN models. This approach requires only\nmoderate perturbations to transmission line reactances, resulting in minimal\nincreases in OPF cost.",
        "published": "2025-04-03T22:33:44+00:00"
    },
    {
        "title": "Connected components in networks with higher-order interactions",
        "authors": [
            "Gyeong-Gyun Ha",
            "Izaak Neri",
            "Alessia Annibale"
        ],
        "summary": "We address the problem of defining connected components in hypergraphs, which\nare models for systems with higher-order interactions. For graphs with dyadic\ninteractions, connected components are defined in terms of paths connecting\nnodes along the graph. However, defining connected components in hypergraphs is\na more involved problem, as one needs to consider the higher-order nature of\nthe interactions associated with the hyperedge. Higher-order interactions can\nbe taken into consideration through a logic associated with the hyperedges, two\nexamples being OR-logic and AND-logic; these logical operations can be\nconsidered two limiting cases corresponding to non-cooperative and fully\ncooperative interactions, respectively. In this paper we show how connected\ncomponents can be defined in hypergraphs with OR or AND logic. While OR-logic\nand AND-logic provide the same connected components for nondirected\nhypergraphs, for directed hypergraphs the strongly connected component of\nAND-logic is a subset of the OR-logic strongly connected component.\nInterestingly, higher-order interactions change the general topological\nproperties of connected components in directed hypergraphs. Notably, while for\ndirected graphs the strongly connected component is the intersection of its in-\nand out-component, in hypergraphs with AND-logic the intersection of in- and\nout-component does not equal the strongly connected component. We develop a\ntheory for the fraction of nodes that are part of the largest connected\ncomponent and through comparison with real-world data we show that\ndegree-cardinality correlations play a significant role.",
        "published": "2025-04-03T22:21:22+00:00"
    },
    {
        "title": "Simulation of Atomic Layer Deposition with a Quantum Computer",
        "authors": [
            "Evgeny Plekhanov",
            "Georgia Prokopiou",
            "Michal Krompiec",
            "Viktor Radovic",
            "Pierre Desjardins",
            "Pluton Pullumbi",
            "David Mu\u00f1oz Ramo"
        ],
        "summary": "In this work, we present the study of an atomic layer deposition (ALD) of\nzirconium by means of a quantum computation on an emulator representing the\nfeatures of an architecture based on qubits implemented on carbon nanotubes.\nALD process control is key in several technological applications such as\nspintronics, catalysis and renewable energy storage. We first derive a large\nab-initio model of the precursor molecule approaching the infinite hydroxylated\nsilicon (100) surface. In particular, we optimize geometry in three\nconfigurations: reactants, transition state and products. Subsequently, we\nderive an effective small cluster model for each state. Atomic valence active\nspace (AVAS) transformation is then performed on these small clusters, leading\nto an effective qubit Hamiltonian, which is solved using the Variational\nQuantum Eigensolver (VQE) algorithm. We study the convergence of the reaction\nactivation barrier with respect to the active space size and benchmark quantum\ncalculations on a noiseless emulator and on an emulator representing a carbon\nnanotube qubit architecture, including an appropriate noise model and\npost-selection error mitigation. These calculations reveal an excellent\nagreement between the two emulation modes. Our VQE calculations provide the\nmulti-configurational corrections to the single determinant DFT and HF states\nand pave the way for the routine quantum calculations of ALD reactions.",
        "published": "2025-04-03T22:01:00+00:00"
    },
    {
        "title": "Push-Grasp Policy Learning Using Equivariant Models and Grasp Score Optimization",
        "authors": [
            "Boce Hu",
            "Heng Tian",
            "Dian Wang",
            "Haojie Huang",
            "Xupeng Zhu",
            "Robin Walters",
            "Robert Platt"
        ],
        "summary": "Goal-conditioned robotic grasping in cluttered environments remains a\nchallenging problem due to occlusions caused by surrounding objects, which\nprevent direct access to the target object. A promising solution to mitigate\nthis issue is combining pushing and grasping policies, enabling active\nrearrangement of the scene to facilitate target retrieval. However, existing\nmethods often overlook the rich geometric structures inherent in such tasks,\nthus limiting their effectiveness in complex, heavily cluttered scenarios. To\naddress this, we propose the Equivariant Push-Grasp Network, a novel framework\nfor joint pushing and grasping policy learning. Our contributions are twofold:\n(1) leveraging SE(2)-equivariance to improve both pushing and grasping\nperformance and (2) a grasp score optimization-based training strategy that\nsimplifies the joint learning process. Experimental results show that our\nmethod improves grasp success rates by 49% in simulation and by 35% in\nreal-world scenarios compared to strong baselines, representing a significant\nadvancement in push-grasp policy learning.",
        "published": "2025-04-03T22:00:35+00:00"
    },
    {
        "title": "Cooperative Inference for Real-Time 3D Human Pose Estimation in Multi-Device Edge Networks",
        "authors": [
            "Hyun-Ho Choi",
            "Kangsoo Kim",
            "Ki-Ho Lee",
            "Kisong Lee"
        ],
        "summary": "Accurate and real-time three-dimensional (3D) pose estimation is challenging\nin resource-constrained and dynamic environments owing to its high\ncomputational complexity. To address this issue, this study proposes a novel\ncooperative inference method for real-time 3D human pose estimation in mobile\nedge computing (MEC) networks. In the proposed method, multiple end devices\nequipped with lightweight inference models employ dual confidence thresholds to\nfilter ambiguous images. Only the filtered images are offloaded to an edge\nserver with a more powerful inference model for re-evaluation, thereby\nimproving the estimation accuracy under computational and communication\nconstraints. We numerically analyze the performance of the proposed inference\nmethod in terms of the inference accuracy and end-to-end delay and formulate a\njoint optimization problem to derive the optimal confidence thresholds and\ntransmission time for each device, with the objective of minimizing the mean\nper-joint position error (MPJPE) while satisfying the required end-to-end delay\nconstraint. To solve this problem, we demonstrate that minimizing the MPJPE is\nequivalent to maximizing the sum of the inference accuracies for all devices,\ndecompose the problem into manageable subproblems, and present a low-complexity\noptimization algorithm to obtain a near-optimal solution. The experimental\nresults show that a trade-off exists between the MPJPE and end-to-end delay\ndepending on the confidence thresholds. Furthermore, the results confirm that\nthe proposed cooperative inference method achieves a significant reduction in\nthe MPJPE through the optimal selection of confidence thresholds and\ntransmission times, while consistently satisfying the end-to-end delay\nrequirement in various MEC environments.",
        "published": "2025-04-03T21:58:29+00:00"
    },
    {
        "title": "Extending CREAMT: Leveraging Large Language Models for Literary Translation Post-Editing",
        "authors": [
            "Antonio Castaldo",
            "Sheila Castilho",
            "Joss Moorkens",
            "Johanna Monti"
        ],
        "summary": "Post-editing machine translation (MT) for creative texts, such as literature,\nrequires balancing efficiency with the preservation of creativity and style.\nWhile neural MT systems struggle with these challenges, large language models\n(LLMs) offer improved capabilities for context-aware and creative translation.\nThis study evaluates the feasibility of post-editing literary translations\ngenerated by LLMs. Using a custom research tool, we collaborated with\nprofessional literary translators to analyze editing time, quality, and\ncreativity. Our results indicate that post-editing LLM-generated translations\nsignificantly reduces editing time compared to human translation while\nmaintaining a similar level of creativity. The minimal difference in creativity\nbetween PE and MT, combined with substantial productivity gains, suggests that\nLLMs may effectively support literary translators working with high-resource\nlanguages.",
        "published": "2025-04-03T21:48:09+00:00"
    },
    {
        "title": "Sliced Wasserstein Discrepancy in Disentangling Representation and Adaptation Networks for Unsupervised Domain Adaptation",
        "authors": [
            "Joel Sol",
            "Shadi Alijani",
            "Homayoun Najjaran"
        ],
        "summary": "This paper introduces DRANet-SWD, an extension of existing work that\ndisentangles content and style representations of images for unsupervised\ndomain adaptation (UDA). The approach builds upon DRANet by incorporating the\nsliced Wasserstein discrepancy (SWD) as a style loss instead of the traditional\nGram matrix loss. The potential advantages of SWD over the Gram matrix loss for\ncapturing style variations in domain adaptation are investigated. Experiments\nusing digit classification datasets and driving scenario segmentation validate\nthe method, demonstrating that DRANet-SWD enhances performance. Results\nindicate that SWD provides a more robust statistical comparison of feature\ndistributions, leading to better style adaptation. These findings highlight the\neffectiveness of SWD in refining feature alignment and improving domain\nadaptation tasks across these benchmarks. Our code can be found here.",
        "published": "2025-04-03T21:43:47+00:00"
    },
    {
        "title": "How to Adapt Control Barrier Functions? A Learning-Based Approach with Applications to a VTOL Quadplane",
        "authors": [
            "Taekyung Kim",
            "Randal W. Beard",
            "Dimitra Panagou"
        ],
        "summary": "In this paper, we present a novel theoretical framework for online adaptation\nof Control Barrier Function (CBF) parameters, i.e., of the class K functions\nincluded in the CBF condition, under input constraints. We introduce the\nconcept of locally validated CBF parameters, which are adapted online to\nguarantee finite-horizon safety, based on conditions derived from Nagumo's\ntheorem and tangent cone analysis. To identify these parameters online, we\nintegrate a learning-based approach with an uncertainty-aware verification\nprocess that account for both epistemic and aleatoric uncertainties inherent in\nneural network predictions. Our method is demonstrated on a VTOL quadplane\nmodel during challenging transition and landing maneuvers, showcasing enhanced\nperformance while maintaining safety.",
        "published": "2025-04-03T21:32:32+00:00"
    },
    {
        "title": "Low Rank Factorizations are Indirect Encodings for Deep Neuroevolution",
        "authors": [
            "Jack Garbus",
            "Jordan Pollack"
        ],
        "summary": "Deep neuroevolution is a highly scalable alternative to reinforcement\nlearning due to its unique ability to encode network updates in a small number\nof bytes. Recent insights from traditional deep learning indicate\nhigh-dimensional models possess intrinsic, low-rank structure. In this work, we\nintroduce low-rank, factorized neuroevolution: an indirect encoding through\nwhich we can search a small space of low-rank factors that enforce underlying\nstructure across a network's weights. We compare our approach with\nnon-factorized networks of similar and smaller size to understand how much\nperformance can be attributed to the smaller search space. We evaluate our\nmethod on a language modeling task using transformers, as well as continuous\nand discrete vision-based reinforcement learning tasks. Our study shows that\nlow-rank, factorized neuroevolution outperforms or is competitive with\nnon-factorized neuroevolution, performing notably well on language modeling.\nOur results also suggest deleterious factorized mutations have a stronger\nnegative impact on performance than deleterious non-factorized mutations, which\nsignificantly reduces the runtime on environments with early termination for\nbad performers. More broadly, these results show how we can use insights from\nbackpropgation-based methods to enhance neuroevolution",
        "published": "2025-04-03T21:31:48+00:00"
    },
    {
        "title": "Building functional and mechanistic models of cortical computation based on canonical cell type connectivity",
        "authors": [
            "Arno Granier",
            "Katharina A Wilmes",
            "Mihai A Petrovici",
            "Walter Senn"
        ],
        "summary": "Neuronal circuits of the cerebral cortex are the structural basis of\nmammalian cognition. The same qualitative components and connectivity motifs\nare repeated across functionally specialized cortical areas and mammalian\nspecies, suggesting a single underlying algorithmic motif. Here, we propose a\nperspective on current knowledge of the cortical structure, from which we\nextract two core principles for computational modeling. The first principle is\nthat cortical cell types fulfill distinct computational roles. The second\nprinciple is that cortical connectivity can be efficiently characterized by\nonly a few canonical blueprints of connectivity between cell types. Starting\nwith these two foundational principles, we outline a general framework for\nbuilding functional and mechanistic models of cortical circuits.",
        "published": "2025-04-03T21:08:12+00:00"
    },
    {
        "title": "Ontologies in Design: How Imagining a Tree Reveals Possibilites and Assumptions in Large Language Models",
        "authors": [
            "Nava Haghighi",
            "Sunny Yu",
            "James Landay",
            "Daniela Rosner"
        ],
        "summary": "Amid the recent uptake of Generative AI, sociotechnical scholars and critics\nhave traced a multitude of resulting harms, with analyses largely focused on\nvalues and axiology (e.g., bias). While value-based analyses are crucial, we\nargue that ontologies -- concerning what we allow ourselves to think or talk\nabout -- is a vital but under-recognized dimension in analyzing these systems.\nProposing a need for a practice-based engagement with ontologies, we offer four\norientations for considering ontologies in design: pluralism, groundedness,\nliveliness, and enactment. We share examples of potentialities that are opened\nup through these orientations across the entire LLM development pipeline by\nconducting two ontological analyses: examining the responses of four LLM-based\nchatbots in a prompting exercise, and analyzing the architecture of an\nLLM-based agent simulation. We conclude by sharing opportunities and\nlimitations of working with ontologies in the design and development of\nsociotechnical systems.",
        "published": "2025-04-03T21:04:36+00:00"
    },
    {
        "title": "Deep Reinforcement Learning via Object-Centric Attention",
        "authors": [
            "Jannis Bl\u00fcml",
            "Cedric Derstroff",
            "Bjarne Gregori",
            "Elisabeth Dillies",
            "Quentin Delfosse",
            "Kristian Kersting"
        ],
        "summary": "Deep reinforcement learning agents, trained on raw pixel inputs, often fail\nto generalize beyond their training environments, relying on spurious\ncorrelations and irrelevant background details. To address this issue,\nobject-centric agents have recently emerged. However, they require different\nrepresentations tailored to the task specifications. Contrary to deep agents,\nno single object-centric architecture can be applied to any environment.\nInspired by principles of cognitive science and Occam's Razor, we introduce\nObject-Centric Attention via Masking (OCCAM), which selectively preserves\ntask-relevant entities while filtering out irrelevant visual information.\nSpecifically, OCCAM takes advantage of the object-centric inductive bias.\nEmpirical evaluations on Atari benchmarks demonstrate that OCCAM significantly\nimproves robustness to novel perturbations and reduces sample complexity while\nshowing similar or improved performance compared to conventional pixel-based\nRL. These results suggest that structured abstraction can enhance\ngeneralization without requiring explicit symbolic representations or\ndomain-specific object extraction pipelines.",
        "published": "2025-04-03T20:48:27+00:00"
    },
    {
        "title": "Emotion Recognition Using Convolutional Neural Networks",
        "authors": [
            "Shaoyuan Xu",
            "Yang Cheng",
            "Qian Lin",
            "Jan P. Allebach"
        ],
        "summary": "Emotion has an important role in daily life, as it helps people better\ncommunicate with and understand each other more efficiently. Facial expressions\ncan be classified into 7 categories: angry, disgust, fear, happy, neutral, sad\nand surprise. How to detect and recognize these seven emotions has become a\npopular topic in the past decade. In this paper, we develop an emotion\nrecognition system that can apply emotion recognition on both still images and\nreal-time videos by using deep learning.\n  We build our own emotion recognition classification and regression system\nfrom scratch, which includes dataset collection, data preprocessing , model\ntraining and testing. Given a certain image or a real-time video, our system is\nable to show the classification and regression results for all of the 7\nemotions. The proposed system is tested on 2 different datasets, and achieved\nan accuracy of over 80\\%. Moreover, the result obtained from real-time testing\nproves the feasibility of implementing convolutional neural networks in real\ntime to detect emotions accurately and efficiently.",
        "published": "2025-04-03T20:08:32+00:00"
    },
    {
        "title": "Autonomy Architectures for Safe Planning in Unknown Environments Under Budget Constraints",
        "authors": [
            "Daniel M. Cherenson",
            "Devansh R. Agrawal",
            "Dimitra Panagou"
        ],
        "summary": "Mission planning can often be formulated as a constrained control problem\nunder multiple path constraints (i.e., safety constraints) and budget\nconstraints (i.e., resource expenditure constraints). In a priori unknown\nenvironments, verifying that an offline solution will satisfy the constraints\nfor all time can be difficult, if not impossible. Our contributions are as\nfollows: 1) We propose an online method, building on our previous work\n\"gatekeeper\", to guarantee safety and satisfy budget constraints of the system\ntrajectory at all times throughout a mission. 2) Next, we prove that our\nalgorithm is recursively feasible and correct. 3) Finally, instead of using a\nheuristically designed backup controller, we propose a sampling-based method to\nconstruct backup trajectories that both minimize resource expenditure and reach\nbudget renewal sets, in which path constraints are satisfied and the\nconstrained resources are renewed. We demonstrate our approach in simulation\nwith a fixed-wing UAV in a GNSS-denied environment with a budget constraint on\nlocalization error that can be renewed at visual landmarks.",
        "published": "2025-04-03T19:46:45+00:00"
    },
    {
        "title": "Anomaly Detection in Time Series Data Using Reinforcement Learning, Variational Autoencoder, and Active Learning",
        "authors": [
            "Bahareh Golchin",
            "Banafsheh Rekabdar"
        ],
        "summary": "A novel approach to detecting anomalies in time series data is presented in\nthis paper. This approach is pivotal in domains such as data centers, sensor\nnetworks, and finance. Traditional methods often struggle with manual parameter\ntuning and cannot adapt to new anomaly types. Our method overcomes these\nlimitations by integrating Deep Reinforcement Learning (DRL) with a Variational\nAutoencoder (VAE) and Active Learning. By incorporating a Long Short-Term\nMemory (LSTM) network, our approach models sequential data and its dependencies\neffectively, allowing for the detection of new anomaly classes with minimal\nlabeled data. Our innovative DRL- VAE and Active Learning combination\nsignificantly improves existing methods, as shown by our evaluations on\nreal-world datasets, enhancing anomaly detection techniques and advancing time\nseries analysis.",
        "published": "2025-04-03T19:41:52+00:00"
    },
    {
        "title": "Route Recommendations for Traffic Management Under Learned Partial Driver Compliance",
        "authors": [
            "Heeseung Bang",
            "Jung-Hoon Cho",
            "Cathy Wu",
            "Andreas A. Malikopoulos"
        ],
        "summary": "In this paper, we aim to mitigate congestion in traffic management systems by\nguiding travelers along system-optimal (SO) routes. However, we recognize that\nmost theoretical approaches assume perfect driver compliance, which often does\nnot reflect reality, as drivers tend to deviate from recommendations to fulfill\ntheir personal objectives. Therefore, we propose a route recommendation\nframework that explicitly learns partial driver compliance and optimizes\ntraffic flow under realistic adherence. We first compute an SO edge flow\nthrough flow optimization techniques. Next, we train a compliance model based\non historical driver decisions to capture individual responses to our\nrecommendations. Finally, we formulate a stochastic optimization problem that\nminimizes the gap between the target SO flow and the realized flow under\nconditions of imperfect adherence. Our simulations conducted on a grid network\nreveal that our approach significantly reduces travel time compared to baseline\nstrategies, demonstrating the practical advantage of incorporating learned\ncompliance into traffic management.",
        "published": "2025-04-03T19:31:16+00:00"
    },
    {
        "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept Mechanistic Interpretability Study via Activation Patching",
        "authors": [
            "Nooshin Bahador"
        ],
        "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
        "published": "2025-04-03T18:54:50+00:00"
    },
    {
        "title": "Improved Compact Genetic Algorithms with Efficient Caching",
        "authors": [
            "Prasanta Dutta",
            "Anirban Mukhopadhyay"
        ],
        "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
        "published": "2025-04-03T18:47:26+00:00"
    },
    {
        "title": "QID: Efficient Query-Informed ViTs in Data-Scarce Regimes for OCR-free Visual Document Understanding",
        "authors": [
            "Binh M. Le",
            "Shaoyuan Xu",
            "Jinmiao Fu",
            "Zhishen Huang",
            "Moyan Li",
            "Yanhui Guo",
            "Hongdong Li",
            "Sameera Ramasinghe",
            "Bryan Wang"
        ],
        "summary": "In Visual Document Understanding (VDU) tasks, fine-tuning a pre-trained\nVision-Language Model (VLM) with new datasets often falls short in optimizing\nthe vision encoder to identify query-specific regions in text-rich document\nimages. Existing methods that directly inject queries into model layers by\nmodifying the network architecture often struggle to adapt to new datasets with\nlimited annotations. To address this, we introduce QID, a novel, streamlined,\narchitecture-preserving approach that integrates query embeddings into the\nvision encoder, leading to notable performance gains, particularly in\ndata-scarce fine-tuning scenarios. Specifically, our approach introduces a\ndual-module framework: a query-aware module that generates a unique query\nvector to precisely guide the model's focus, as well as a query-agnostic module\nthat captures the positional relationships among tokens, ensuring robust\nspatial understanding. Notably, both modules operate independently of the\nvision attention blocks, facilitating targeted learning of query embeddings and\nenhancing visual semantic identification. Experiments with OCR-free VLMs across\nmultiple datasets demonstrate significant performance improvements using our\nmethod, especially in handling text-rich documents in data-scarce environments.",
        "published": "2025-04-03T18:47:16+00:00"
    },
    {
        "title": "Cultural Learning-Based Culture Adaptation of Language Models",
        "authors": [
            "Chen Cecilia Liu",
            "Anna Korhonen",
            "Iryna Gurevych"
        ],
        "summary": "Adapting large language models (LLMs) to diverse cultural values is a\nchallenging task, as existing LLMs often reflect the values of specific groups\nby default, and potentially causing harm to others. In this paper, we present\nCLCA, a novel framework for enhancing LLM alignment with cultural values based\non cultural learning. The framework leverages simulated social interactions to\ngenerate conversations in which LLMs engage in role-playing within culturally\nadapted social scenarios, capturing implicit cultural norms for model\nfine-tuning. CLCA improves cultural value alignment across various model\narchitectures measured using World Value Survey data, demonstrating the\neffectiveness of our proposed approach. Our results provide early evidence that\nunderstanding intent and social interactions can enhance cultural value\nadaptation in LLMs, highlighting the promise of training approaches based on\ncultural learning.",
        "published": "2025-04-03T18:16:26+00:00"
    },
    {
        "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning",
        "authors": [
            "Xianwei Zhuang",
            "Yuxin Xie",
            "Yufan Deng",
            "Dongchao Yang",
            "Liming Liang",
            "Jinghan Ru",
            "Yuguo Yin",
            "Yuexian Zou"
        ],
        "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
        "published": "2025-04-03T18:06:28+00:00"
    },
    {
        "title": "Graph Attention for Heterogeneous Graphs with Positional Encoding",
        "authors": [
            "Nikhil Shivakumar Nayak"
        ],
        "summary": "Graph Neural Networks (GNNs) have emerged as the de facto standard for\nmodeling graph data, with attention mechanisms and transformers significantly\nenhancing their performance on graph-based tasks. Despite these advancements,\nthe performance of GNNs on heterogeneous graphs often remains complex, with\nnetworks generally underperforming compared to their homogeneous counterparts.\nThis work benchmarks various GNN architectures to identify the most effective\nmethods for heterogeneous graphs, with a particular focus on node\nclassification and link prediction. Our findings reveal that graph attention\nnetworks excel in these tasks. As a main contribution, we explore enhancements\nto these attention networks by integrating positional encodings for node\nembeddings. This involves utilizing the full Laplacian spectrum to accurately\ncapture both the relative and absolute positions of each node within the graph,\nfurther enhancing performance on downstream tasks such as node classification\nand link prediction.",
        "published": "2025-04-03T18:00:02+00:00"
    },
    {
        "title": "Do Two AI Scientists Agree?",
        "authors": [
            "Xinghong Fu",
            "Ziming Liu",
            "Max Tegmark"
        ],
        "summary": "When two AI models are trained on the same scientific task, do they learn the\nsame theory or two different theories? Throughout history of science, we have\nwitnessed the rise and fall of theories driven by experimental validation or\nfalsification: many theories may co-exist when experimental data is lacking,\nbut the space of survived theories become more constrained with more\nexperimental data becoming available. We show the same story is true for AI\nscientists. With increasingly more systems provided in training data, AI\nscientists tend to converge in the theories they learned, although sometimes\nthey form distinct groups corresponding to different theories. To\nmechanistically interpret what theories AI scientists learn and quantify their\nagreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI\nScientists, trained on standard problems in physics, aggregating training\nresults across many seeds simulating the different configurations of AI\nscientists. Our findings suggests for AI scientists switch from learning a\nHamiltonian theory in simple setups to a Lagrangian formulation when more\ncomplex systems are introduced. We also observe strong seed dependence of the\ntraining dynamics and final learned weights, controlling the rise and fall of\nrelevant theories. We finally demonstrate that not only can our neural networks\naid interpretability, it can also be applied to higher dimensional problems.",
        "published": "2025-04-03T17:58:44+00:00"
    },
    {
        "title": "GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings",
        "authors": [
            "Yuexi Du",
            "Jiazhen Zhang",
            "Nicha C. Dvornek",
            "John A. Onofrey"
        ],
        "summary": "Symmetry, where certain features remain invariant under geometric\ntransformations, can often serve as a powerful prior in designing convolutional\nneural networks (CNNs). While conventional CNNs inherently support\ntranslational equivariance, extending this property to rotation and reflection\nhas proven challenging, often forcing a compromise between equivariance,\nefficiency, and information loss. In this work, we introduce Gaussian Mixture\nRing Convolution (GMR-Conv), an efficient convolution kernel that smooths\nradial symmetry using a mixture of Gaussian-weighted rings. This design\nmitigates discretization errors of circular kernels, thereby preserving robust\nrotation and reflection equivariance without incurring computational overhead.\nWe further optimize both the space and speed efficiency of GMR-Conv via a novel\nparameterization and computation strategy, allowing larger kernels at an\nacceptable cost. Extensive experiments on eight classification and one\nsegmentation datasets demonstrate that GMR-Conv not only matches conventional\nCNNs' performance but can also surpass it in applications with orientation-less\ndata. GMR-Conv is also proven to be more robust and efficient than the\nstate-of-the-art equivariant learning methods. Our work provides inspiring\nempirical evidence that carefully applied radial symmetry can alleviate the\nchallenges of information loss, marking a promising advance in equivariant\nnetwork architectures. The code is available at\nhttps://github.com/XYPB/GMR-Conv.",
        "published": "2025-04-03T17:58:18+00:00"
    },
    {
        "title": "Logarithmic entanglement lightcone from eigenstate correlations in the many-body localised phase",
        "authors": [
            "Ratul Thakur",
            "Bikram Pain",
            "Sthitadhi Roy"
        ],
        "summary": "We investigate the operator entanglement of the time-evolution operator\nthrough the framework of eigenstate correlations. Focusing on strongly\ndisordered quantum many-body systems in the many-body localised (MBL) regime,\nwe analyse the operator entanglement across various spatiotemporal cuts,\nrevealing the logarithmic lightcone of entanglement spreading. We demonstrate\nthat this logarithmic lightcone arises directly from a hierarchy of\nenergyscales and lengthscales encoded in eigenstate correlations. By\ncharacterising the statistics of these hierarchical scales, we develop a\nmicroscopic theory for the spatiotemporal structure of entanglement spreading\nin MBL systems -- without invoking phenomenological constructs such as\n$\\ell$-bits. This approach reveals the fundamental connection between\neigenstate correlations and the emergent entanglement structure in MBL systems.",
        "published": "2025-04-03T17:57:10+00:00"
    },
    {
        "title": "F-ViTA: Foundation Model Guided Visible to Thermal Translation",
        "authors": [
            "Jay N. Paranjape",
            "Celso de Melo",
            "Vishal M. Patel"
        ],
        "summary": "Thermal imaging is crucial for scene understanding, particularly in low-light\nand nighttime conditions. However, collecting large thermal datasets is costly\nand labor-intensive due to the specialized equipment required for infrared\nimage capture. To address this challenge, researchers have explored\nvisible-to-thermal image translation. Most existing methods rely on Generative\nAdversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a\nstyle transfer problem. As a result, these approaches attempt to learn both the\nmodality distribution shift and underlying physical principles from limited\ntraining data. In this paper, we propose F-ViTA, a novel approach that\nleverages the general world knowledge embedded in foundation models to guide\nthe diffusion process for improved translation. Specifically, we condition an\nInstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation\nmodels such as SAM and Grounded DINO. This allows the model to learn meaningful\ncorrelations between scene objects and their thermal signatures in infrared\nimagery. Extensive experiments on five public datasets demonstrate that F-ViTA\noutperforms state-of-the-art (SOTA) methods. Furthermore, our model generalizes\nwell to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared\n(LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the\nsame visible image. Code: https://github.com/JayParanjape/F-ViTA/tree/master.",
        "published": "2025-04-03T17:47:06+00:00"
    },
    {
        "title": "MENA: Multimodal Epistemic Network Analysis for Visualizing Competencies and Emotions",
        "authors": [
            "Behdokht Kiafar",
            "Pavan Uttej Ravva",
            "Asif Ahmmed Joy",
            "Salam Daher",
            "Roghayeh Leila Barmaki"
        ],
        "summary": "The need to improve geriatric care quality presents a challenge that requires\ninsights from stakeholders. While simulated trainings can boost competencies,\nextracting meaningful insights from these practices to enhance simulation\neffectiveness remains a challenge. In this study, we introduce Multimodal\nEpistemic Network Analysis (MENA), a novel framework for analyzing caregiver\nattitudes and emotions in an Augmented Reality setting and exploring how the\nawareness of a virtual geriatric patient (VGP) impacts these aspects. MENA\nenhances the capabilities of Epistemic Network Analysis by detecting positive\nemotions, enabling visualization and analysis of complex relationships between\ncaregiving competencies and emotions in dynamic caregiving practices. The\nframework provides visual representations that demonstrate how participants\nprovided more supportive care and engaged more effectively in person-centered\ncaregiving with aware VGP. This method could be applicable in any setting that\ndepends on dynamic interpersonal interactions, as it visualizes connections\nbetween key elements using network graphs and enables the direct comparison of\nmultiple networks, thereby broadening its implications across various fields.",
        "published": "2025-04-03T17:40:49+00:00"
    },
    {
        "title": "Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets",
        "authors": [
            "Chuning Zhu",
            "Raymond Yu",
            "Siyuan Feng",
            "Benjamin Burchfiel",
            "Paarth Shah",
            "Abhishek Gupta"
        ],
        "summary": "Imitation learning has emerged as a promising approach towards building\ngeneralist robots. However, scaling imitation learning for large robot\nfoundation models remains challenging due to its reliance on high-quality\nexpert demonstrations. Meanwhile, large amounts of video data depicting a wide\nrange of environments and diverse behaviors are readily available. This data\nprovides a rich source of information about real-world dynamics and\nagent-environment interactions. Leveraging this data directly for imitation\nlearning, however, has proven difficult due to the lack of action annotation\nrequired for most contemporary methods. In this work, we present Unified World\nModels (UWM), a framework that allows for leveraging both video and action data\nfor policy learning. Specifically, a UWM integrates an action diffusion process\nand a video diffusion process within a unified transformer architecture, where\nindependent diffusion timesteps govern each modality. We show that by simply\ncontrolling each diffusion timestep, UWM can flexibly represent a policy, a\nforward dynamics, an inverse dynamics, and a video generator. Through simulated\nand real-world experiments, we show that: (1) UWM enables effective pretraining\non large-scale multitask robot datasets with both dynamics and action\npredictions, resulting in more generalizable and robust policies than imitation\nlearning, (2) UWM naturally facilitates learning from action-free video data\nthrough independent control of modality-specific diffusion timesteps, further\nimproving the performance of finetuned policies. Our results suggest that UWM\noffers a promising step toward harnessing large, heterogeneous datasets for\nscalable robot learning, and provides a simple unification between the often\ndisparate paradigms of imitation learning and world modeling. Videos and code\nare available at https://weirdlabuw.github.io/uwm/.",
        "published": "2025-04-03T17:38:59+00:00"
    },
    {
        "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation",
        "authors": [
            "Zhiyuan Yan",
            "Junyan Ye",
            "Weijia Li",
            "Zilong Huang",
            "Shenghai Yuan",
            "Xiangyang He",
            "Kaiqing Lin",
            "Jun He",
            "Conghui He",
            "Li Yuan"
        ],
        "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.",
        "published": "2025-04-03T17:23:16+00:00"
    },
    {
        "title": "Towards Green AI-Native Networks: Evaluation of Neural Circuit Policy for Estimating Energy Consumption of Base Stations",
        "authors": [
            "Selim Ickin",
            "Shruti Bothe",
            "Aman Raparia",
            "Nitin Khanna",
            "Erik Sanders"
        ],
        "summary": "Optimization of radio hardware and AI-based network management software yield\nsignificant energy savings in radio access networks. The execution of\nunderlying Machine Learning (ML) models, which enable energy savings through\nrecommended actions, may require additional compute and energy, highlighting\nthe opportunity to explore and adopt accurate and energy-efficient ML\ntechnologies. This work evaluates the novel use of sparsely structured Neural\nCircuit Policies (NCPs) in a use case to estimate the energy consumption of\nbase stations. Sparsity in ML models yields reduced memory, computation and\nenergy demand, hence facilitating a low-cost and scalable solution. We also\nevaluate the generalization capability of NCPs in comparison to traditional and\nwidely used ML models such as Long Short Term Memory (LSTM), via quantifying\ntheir sensitivity to varying model hyper-parameters (HPs). NCPs demonstrated a\nclear reduction in computational overhead and energy consumption. Moreover,\nresults indicated that the NCPs are robust to varying HPs such as number of\nepochs and neurons in each layer, making them a suitable option to ease model\nmanagement and to reduce energy consumption in Machine Learning Operations\n(MLOps) in telecommunications.",
        "published": "2025-04-03T17:22:39+00:00"
    },
    {
        "title": "Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition",
        "authors": [
            "Vincent Gbouna Zakka",
            "Luis J. Manso",
            "Zhuangzhuang Dai"
        ],
        "summary": "Human activity recognition is increasingly vital for supporting independent\nliving, particularly for the elderly and those in need of assistance. Domestic\nservice robots with monitoring capabilities can enhance safety and provide\nessential support. Although image-based methods have advanced considerably in\nthe past decade, their adoption remains limited by concerns over privacy and\nsensitivity to low-light or dark conditions. As an alternative, millimetre-wave\n(mmWave) radar can produce point cloud data which is privacy-preserving.\nHowever, processing the sparse and noisy point clouds remains a long-standing\nchallenge. While graph-based methods and attention mechanisms show promise,\nthey predominantly rely on \"fixed\" kernels; kernels that are applied uniformly\nacross all neighbourhoods, highlighting the need for adaptive approaches that\ncan dynamically adjust their kernels to the specific geometry of each local\nneighbourhood in point cloud data. To overcome this limitation, we introduce an\nadaptive approach within the graph convolutional framework. Instead of a single\nshared weight function, our Multi-Head Adaptive Kernel (MAK) module generates\nmultiple dynamic kernels, each capturing different aspects of the local feature\nspace. By progressively refining local features while maintaining global\nspatial context, our method enables convolution kernels to adapt to varying\nlocal features. Experimental results on benchmark datasets confirm the\neffectiveness of our approach, achieving state-of-the-art performance in human\nactivity recognition. Our source code is made publicly available at:\nhttps://github.com/Gbouna/MAK-GCN",
        "published": "2025-04-03T17:19:20+00:00"
    },
    {
        "title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
        "authors": [
            "Andres Algaba",
            "Vincent Holst",
            "Floriano Tori",
            "Melika Mobini",
            "Brecht Verbeken",
            "Sylvia Wenmackers",
            "Vincent Ginis"
        ],
        "summary": "The spread of scientific knowledge depends on how researchers discover and\ncite previous work. The adoption of large language models (LLMs) in the\nscientific research process introduces a new layer to these citation practices.\nHowever, it remains unclear to what extent LLMs align with human citation\npractices, how they perform across domains, and may influence citation\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\nin citations by consistently favoring highly cited papers when generating\nreferences. This pattern persists across scientific domains despite significant\nfield-specific variations in existence rates, which refer to the proportion of\ngenerated references that match existing records in external bibliometric\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\nwe find that LLM recommendations diverge from traditional citation patterns by\npreferring more recent references with shorter titles and fewer authors.\nEmphasizing their content-level relevance, the generated references are\nsemantically aligned with the content of each paper at levels comparable to the\nground truth references and display similar network effects while reducing\nauthor self-citations. These findings illustrate how LLMs may reshape citation\npractices and influence the trajectory of scientific discovery by reflecting\nand amplifying established trends. As LLMs become more integrated into the\nscientific research process, it is important to understand their role in\nshaping how scientific communities discover and build upon prior work.",
        "published": "2025-04-03T17:04:56+00:00"
    },
    {
        "title": "CanonNet: Canonical Ordering and Curvature Learning for Point Cloud Analysis",
        "authors": [
            "Benjy Friedmann",
            "Michael Werman"
        ],
        "summary": "Point cloud processing poses two fundamental challenges: establishing\nconsistent point ordering and effectively learning fine-grained geometric\nfeatures. Current architectures rely on complex operations that limit\nexpressivity while struggling to capture detailed surface geometry. We present\nCanonNet, a lightweight neural network composed of two complementary\ncomponents: (1) a preprocessing pipeline that creates a canonical point\nordering and orientation, and (2) a geometric learning framework where networks\nlearn from synthetic surfaces with precise curvature values. This modular\napproach eliminates the need for complex transformation-invariant architectures\nwhile effectively capturing local geometric properties. Our experiments\ndemonstrate state-of-the-art performance in curvature estimation and\ncompetitive results in geometric descriptor tasks with significantly fewer\nparameters (\\textbf{100X}) than comparable methods. CanonNet's efficiency makes\nit particularly suitable for real-world applications where computational\nresources are limited, demonstrating that mathematical preprocessing can\neffectively complement neural architectures for point cloud analysis. The code\nfor the project is publicly available\n\\hyperlink{https://benjyfri.github.io/CanonNet/}{https://benjyfri.github.io/CanonNet/}.",
        "published": "2025-04-03T16:58:57+00:00"
    },
    {
        "title": "Echoes of the hidden: Uncovering coordination beyond network structure",
        "authors": [
            "Shahar Somin",
            "Tom Cohen",
            "Jeremy Kepner",
            "Alex Pentland"
        ],
        "summary": "The study of connectivity and coordination has drawn increasing attention in\nrecent decades due to their central role in driving markets, shaping societal\ndynamics, and influencing biological systems. Traditionally, observable\nconnections, such as phone calls, financial transactions, or social media\nconnections, have been used to infer coordination and connectivity. However,\nincomplete, encrypted, or fragmented data, alongside the ubiquity of\ncommunication platforms and deliberate obfuscation, often leave many real-world\nconnections hidden. In this study, we demonstrate that coordinating individuals\nexhibit shared bursty activity patterns, enabling their detection even when\nobservable links between them are sparse or entirely absent. We further propose\na generative model based on the network of networks formalism to account for\nthe mechanisms driving this collaborative burstiness, attributing it to shock\npropagation across networks rather than isolated individual behavior. Model\nsimulations demonstrate that when observable connection density is below 70\\%,\nburstiness significantly improves coordination detection compared to\nstate-of-the-art temporal and structural methods. This work provides a new\nperspective on community and coordination dynamics, advancing both theoretical\nunderstanding and practical detection. By laying the foundation for identifying\nhidden connections beyond observable network structures, it enables detection\nacross different platforms, alongside enhancing system behavior understanding,\ninformed decision-making, and risk mitigation.",
        "published": "2025-04-03T16:49:58+00:00"
    },
    {
        "title": "Learning dynamics on the picosecond timescale in a superconducting synapse structure",
        "authors": [
            "Ken Segall",
            "Leon Nichols",
            "Will Friend",
            "Steven B. Kaplan"
        ],
        "summary": "Conventional Artificial Intelligence (AI) systems are running into\nlimitations in terms of training time and energy. Following the principles of\nthe human brain, spiking neural networks trained with unsupervised learning\noffer a faster, more energy-efficient alternative. However, the dynamics of\nspiking, learning, and forgetting become more complicated in such schemes. Here\nwe study a superconducting electronics implementation of a learning synapse and\nexperimentally measure its spiking dynamics. By pulsing the system with a\nsuperconducting neuron, we show that a superconducting inductor can dynamically\nhold the synaptic weight with updates due to learning and forgetting. Learning\ncan be stopped by slowing down the arrival time of the post-synaptic pulse, in\naccordance with the Spike-Timing Dependent Plasticity paradigm. We find\nexcellent agreement with circuit simulations, and by fitting the turn-on of the\npulsing frequency, we confirm a learning time of 16.1 +/- 1 ps. The power\ndissipation in the learning part of the synapse is less than one attojoule per\nlearning event. This leads to the possibility of an extremely fast and\nenergy-efficient learning processor.",
        "published": "2025-04-03T16:46:19+00:00"
    },
    {
        "title": "Atrial constitutive neural networks",
        "authors": [
            "Mathias Peirlinck",
            "Kevin Linka",
            "Ellen Kuhl"
        ],
        "summary": "This work presents a novel approach for characterizing the mechanical\nbehavior of atrial tissue using constitutive neural networks. Based on\nexperimental biaxial tensile test data of healthy human atria, we automatically\ndiscover the most appropriate constitutive material model, thereby overcoming\nthe limitations of traditional, pre-defined models. This approach offers a new\nperspective on modeling atrial mechanics and is a significant step towards\nimproved simulation and prediction of cardiac health.",
        "published": "2025-04-03T16:35:49+00:00"
    },
    {
        "title": "GEOPARD: Geometric Pretraining for Articulation Prediction in 3D Shapes",
        "authors": [
            "Pradyumn Goyal",
            "Dmitry Petrov",
            "Sheldon Andrews",
            "Yizhak Ben-Shabat",
            "Hsueh-Ti Derek Liu",
            "Evangelos Kalogerakis"
        ],
        "summary": "We present GEOPARD, a transformer-based architecture for predicting\narticulation from a single static snapshot of a 3D shape. The key idea of our\nmethod is a pretraining strategy that allows our transformer to learn plausible\ncandidate articulations for 3D shapes based on a geometric-driven search\nwithout manual articulation annotation. The search automatically discovers\nphysically valid part motions that do not cause detachments or collisions with\nother shape parts. Our experiments indicate that this geometric pretraining\nstrategy, along with carefully designed choices in our transformer\narchitecture, yields state-of-the-art results in articulation inference in the\nPartNet-Mobility dataset.",
        "published": "2025-04-03T16:35:17+00:00"
    },
    {
        "title": "RBT4DNN: Requirements-based Testing of Neural Networks",
        "authors": [
            "Nusrat Jahan Mozumder",
            "Felipe Toledo",
            "Swaroopa Dola",
            "Matthew B. Dwyer"
        ],
        "summary": "Deep neural network (DNN) testing is crucial for the reliability and safety\nof critical systems, where failures can have severe consequences. Although\nvarious techniques have been developed to create robustness test suites,\nrequirements-based testing for DNNs remains largely unexplored - yet such tests\nare recognized as an essential component of software validation of critical\nsystems. In this work, we propose a requirements-based test suite generation\nmethod that uses structured natural language requirements formulated in a\nsemantic feature space to create test suites by prompting text-conditional\nlatent diffusion models with the requirement precondition and then using the\nassociated postcondition to define a test oracle to judge outputs of the DNN\nunder test. We investigate the approach using fine-tuned variants of\npre-trained generative models. Our experiments on the MNIST, CelebA-HQ,\nImageNet, and autonomous car driving datasets demonstrate that the generated\ntest suites are realistic, diverse, consistent with preconditions, and capable\nof revealing faults.",
        "published": "2025-04-03T16:24:49+00:00"
    },
    {
        "title": "Monitored Fluctuating Hydrodynamics",
        "authors": [
            "Sarang Gopalakrishnan",
            "Ewan McCulloch",
            "Romain Vasseur"
        ],
        "summary": "We introduce a hydrodynamic framework for describing monitored classical\nstochastic processes. We study the conditional ensembles for these monitored\nprocesses -- i.e., we compute spacetime correlation functions conditioned on a\nfixed, typical measurement record. In the presence of global symmetries we show\nthat these conditional ensembles can undergo measurement-induced ``sharpening''\nphase transitions as a function of the monitoring rate; moreover, even weak\nmonitoring can give rise to novel critical phases, derived entirely from a\nclassical perspective. We give a simple hydrodynamic derivation of the known\ncharge-sharpening transition for diffusive many-body quantum systems. We show\nthat although the unmonitored symmetric and asymmetric exclusion processes are\nin different universality classes of transport, their conditional ensembles\nflow to the same fixed point with emergent relativistic invariance under\nmonitoring. On the other hand, weakly monitored systems with non-Abelian\nsymmetries enter a novel strongly coupled fixed point with non-trivial\ndynamical exponent, which we characterize. Our formalism naturally accounts for\nmonitoring general observables, such as currents or density gradients, and\nallows for a direct calculation of information-theoretic diagnostics of\nsharpening transitions, including the Shannon entropy of the measurement\nrecord.",
        "published": "2025-04-03T16:19:18+00:00"
    },
    {
        "title": "HQViT: Hybrid Quantum Vision Transformer for Image Classification",
        "authors": [
            "Hui Zhang",
            "Qinglin Zhao",
            "Mengchu Zhou",
            "Li Feng"
        ],
        "summary": "Transformer-based architectures have revolutionized the landscape of deep\nlearning. In computer vision domain, Vision Transformer demonstrates remarkable\nperformance on par with or even surpassing that of convolutional neural\nnetworks. However, the quadratic computational complexity of its self-attention\nmechanism poses challenges for classical computing, making model training with\nhigh-dimensional input data, e.g., images, particularly expensive. To address\nsuch limitations, we propose a Hybrid Quantum Vision Transformer (HQViT), that\nleverages the principles of quantum computing to accelerate model training\nwhile enhancing model performance. HQViT introduces whole-image processing with\namplitude encoding to better preserve global image information without\nadditional positional encoding. By leveraging quantum computation on the most\ncritical steps and selectively handling other components in a classical way, we\nlower the cost of quantum resources for HQViT. The qubit requirement is\nminimized to $O(log_2N)$ and the number of parameterized quantum gates is only\n$O(log_2d)$, making it well-suited for Noisy Intermediate-Scale Quantum\ndevices. By offloading the computationally intensive attention coefficient\nmatrix calculation to the quantum framework, HQViT reduces the classical\ncomputational load by $O(T^2d)$. Extensive experiments across various computer\nvision datasets demonstrate that HQViT outperforms existing models, achieving a\nmaximum improvement of up to $10.9\\%$ (on the MNIST 10-classification task)\nover the state of the art. This work highlights the great potential to combine\nquantum and classical computing to cope with complex image classification\ntasks.",
        "published": "2025-04-03T16:13:34+00:00"
    },
    {
        "title": "Autonomous Human-Robot Interaction via Operator Imitation",
        "authors": [
            "Sammy Christen",
            "David M\u00fcller",
            "Agon Serifi",
            "Ruben Grandia",
            "Georg Wiedebach",
            "Michael A. Hopkins",
            "Espen Knoop",
            "Moritz B\u00e4cher"
        ],
        "summary": "Teleoperated robotic characters can perform expressive interactions with\nhumans, relying on the operators' experience and social intuition. In this\nwork, we propose to create autonomous interactive robots, by training a model\nto imitate operator data. Our model is trained on a dataset of human-robot\ninteractions, where an expert operator is asked to vary the interactions and\nmood of the robot, while the operator commands as well as the pose of the human\nand robot are recorded. Our approach learns to predict continuous operator\ncommands through a diffusion process and discrete commands through a\nclassifier, all unified within a single transformer architecture. We evaluate\nthe resulting model in simulation and with a user study on the real system. We\nshow that our method enables simple autonomous human-robot interactions that\nare comparable to the expert-operator baseline, and that users can recognize\nthe different robot moods as generated by our model. Finally, we demonstrate a\nzero-shot transfer of our model onto a different robotic platform with the same\noperator interface.",
        "published": "2025-04-03T16:06:44+00:00"
    },
    {
        "title": "Dynamic Directional Routing of Freight in the Physical Internet",
        "authors": [
            "Sahrish Jaleel Shaikh",
            "Praveen Muthukrishnan",
            "Yijun Lai",
            "Benoit Montreuil"
        ],
        "summary": "The Physical Internet (PI) envisions an interconnected, modular, and\ndynamically managed logistics system inspired by the Digital Internet. It\nenables open-access networks where shipments traverse a hyperconnected system\nof hubs, adjusting routes based on real-time conditions. A key challenge in\nscalable and adaptive freight movement is routing determining how shipments\nnavigate the network to balance service levels, consolidation, and\nadaptability. This paper introduces directional routing, a dynamic approach\nthat flexibly adjusts shipment paths, optimizing efficiency and consolidation\nusing real-time logistics data. Unlike shortest-path routing, which follows\nfixed routes, directional routing dynamically selects feasible next-hop hubs\nbased on network conditions, consolidation opportunities, and service level\nconstraints. It consists of two phases: area discovery, which identifies\ncandidate hubs, and node selection, which determines the next hub based on\nreal-time parameters. This paper advances the area discovery phase by\nintroducing a Reduced Search Space Breadth-First Search (RSS-BFS) method to\nsystematically identify feasible routing areas while balancing service levels\nand consolidation. The proposed approach enhances network fluidity,\nscalability, and adaptability in PI-based logistics, advancing autonomous and\nsustainable freight movement.",
        "published": "2025-04-03T16:05:03+00:00"
    },
    {
        "title": "Clustering in a preferential attachment network with triangles",
        "authors": [
            "Angelica Pachon",
            "Robin Stephenson"
        ],
        "summary": "We study a generalization of the affine preferential attachment model where\ntriangles are randomly added to the graph. We show that the model exhibits an\nasymptotically power-law degree distribution with adjustable parameter\n$\\gamma\\in (1,\\infty)$, and positive clustering. However, the clustering\nbehaviour depends on how it is measured. With high probability, the average\nlocal clustering coefficient remains positive, independently of $\\gamma$,\nwhereas the expectation of the global clustering coefficient does not vanish\nonly when $\\gamma>3$.",
        "published": "2025-04-03T16:02:27+00:00"
    },
    {
        "title": "Impact of a Blockchain-based Universal Basic Income Pilot: The case of Circles UBI currency",
        "authors": [
            "Alessandro Longo",
            "Teodoro Criscione",
            "Julio Linares",
            "Sowelu Avanzo"
        ],
        "summary": "Circles UBI is a blockchain-based Community Currency System (CCS) that has\nbeen active in Berlin (Germany) since October 2021. The Circles Coop, which\nlaunched the project in 2021, was shut down in December 2023. In this paper, we\nshow the results of a survey carried out between October and November 2023. The\nrespondents were twenty-five individuals involved in various ways in the\nCircles' network. The main emerging narrative points out how their\nparticipation was deeply motivated by their identification with the values and\nideals of the Circles community. Among them, we selected five profiles that\nstood for their difference in type and degree of involvement. Finally, we\nreport some stories of economic linkages that suggest a positive externality in\nadopting a local community currency. To our knowledge, this is the first\nqualitative study of a universal basic income designed as a community currency\nand adopting blockchain technology. This pilot project was a remarkable\nexperiment for its adopted advanced technological and social innovations. In\nfact, as far as we know, the integration of basic income and local currency\nfeatures has been experimented with only in two other cases (Maric\\'a, Brazil\nand Barcelona, Spain) and none of them adopted a decentralized ledger system.\nIn this work, we try to outline strengths and weaknesses that emerged after\nabout two years of activity. For this reason, future researchers and activists\ninterested in this field will find valuable information.",
        "published": "2025-04-03T15:53:37+00:00"
    },
    {
        "title": "Web3DB: Web 3.0 RDBMS for Individual Data Ownership",
        "authors": [
            "Shankha Shubhra Mukherjee",
            "Wenyi Tang",
            "Gustavo Prado Fenzi Aniceto",
            "Jake Chandler",
            "WenZhan Song",
            "Taeho Jung"
        ],
        "summary": "This paper introduces Web3DB, a decentralized relational database management\nsystem (RDBMS) designed to align with the principles of Web 3.0, addressing\ncritical shortcomings of traditional centralized DBMS, such as data privacy,\nsecurity vulnerabilities, and single points of failure. Several similar systems\nhave been proposed, but they are not compatible with the legacy systems based\non RDBMS. Motivated by the necessity for enhanced data sovereignty and the\ndecentralization of data control, Web3DB leverages blockchain technology for\nfine-grained access control and utilizes decentralized data storage. This\nsystem leverages a novel, modular architecture that contributes to enhanced\nflexibility, scalability, and user-centric functionality. Central to the Web3DB\ninnovation is its decentralized query execution, which uses cryptographic\nsortition and blockchain verification to ensure secure and fair query\nprocessing across network nodes. The motivation for integrating relational\ndatabases within decentralized DBMS primarily stems from the need to combine\nthe robustness and ease of use of relational database structures with the\nbenefits of decentralization. This paper outlines the architecture of Web3DB,\nits practical implementation, and the system's ability to support SQL-like\noperations on relational data, manage multi-tenancy, and facilitate open data\nsharing, setting new standards for decentralized databases in the Web 3.0 era.",
        "published": "2025-04-03T15:52:39+00:00"
    },
    {
        "title": "A Study on Adversarial Robustness of Discriminative Prototypical Learning",
        "authors": [
            "Ramin Zarei Sabzevar",
            "Hamed Mohammadzadeh",
            "Tahmineh Tavakoli",
            "Ahad Harati"
        ],
        "summary": "Deep neural networks demonstrate significant vulnerability to adversarial\nperturbations, posing risks for critical applications. Current adversarial\ntraining methods predominantly focus on robustness against attacks without\nexplicitly leveraging geometric structures in the latent space, usually\nresulting in reduced accuracy on the original clean data. To address these\nissues, we propose a novel adversarial training framework named Adversarial\nDeep Positive-Negative Prototypes (Adv-DPNP), which integrates disriminative\nprototype-based learning with adversarial training. Adv-DPNP uses unified class\nprototypes serving dual roles as classifier weights and robust anchors,\nenhancing both intra-class compactness and inter-class separation in the latent\nspace. Moreover, a novel dual-branch training mechanism maintains stable\nprototypes by updating them exclusively with clean data; while the feature\nextractor layers are learned using both clean and adversarial data to remain\ninvariant against adversarial perturbations. In addition, our approach utilizes\na composite loss function combining positive prototype alignment, negative\nprototype repulsion, and consistency regularization to further enhance\ndiscrimination, adversarial robustness, and clean accuracy. Extensive\nexperiments conducted on standard benchmark datasets confirm the effectiveness\nof Adv-DPNP compared to state-of-the-art methods, achieving higher clean\naccuracy and competitive robustness under adversarial perturbations and common\ncorruptions. Our code is available at https://github.com/fum-rpl/adv-dpnp",
        "published": "2025-04-03T15:42:58+00:00"
    },
    {
        "title": "SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions",
        "authors": [
            "Shengrui XU",
            "Tianchi Lu",
            "Zikun Wang",
            "Jixiu Zhai",
            "Jingwan Wang"
        ],
        "summary": "Protein-Protein Interaction (PPI) prediction is a key task in uncovering\ncellular functional networks and disease mechanisms. However, traditional\nexperimental methods are time-consuming and costly, and existing computational\nmodels face challenges in cross-modal feature fusion, robustness, and\nfalse-negative suppression. In this paper, we propose a novel supervised\ncontrastive multimodal framework, SCMPPI, for PPI prediction. By integrating\nprotein sequence features (AAC, DPC, CKSAAP-ESMC) with PPI network topology\ninformation (Node2Vec graph embedding), and combining an improved supervised\ncontrastive learning strategy, SCMPPI significantly enhances PPI prediction\nperformance. For the PPI task, SCMPPI introduces a negative sample filtering\nmechanism and modifies the contrastive loss function, effectively optimizing\nmultimodal features. Experiments on eight benchmark datasets, including yeast,\nhuman, and H.pylori, show that SCMPPI outperforms existing state-of-the-art\nmethods (such as DF-PPI and TAGPPI) in key metrics such as accuracy ( 98.01%)\nand AUC (99.62%), and demonstrates strong generalization in cross-species\nprediction (AUC > 99% on multi-species datasets). Furthermore, SCMPPI has been\nsuccessfully applied to CD9 networks, the Wnt pathway, and cancer-specific\nnetworks, providing a reliable tool for disease target discovery. This\nframework also offers a new paradigm for multimodal biological information\nfusion and contrastive learning in collaborative optimization for various\ncombined predictions.",
        "published": "2025-04-03T15:34:02+00:00"
    },
    {
        "title": "Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation",
        "authors": [
            "Xingguang Zhang",
            "Nicholas Chimitt",
            "Xijun Wang",
            "Yu Yuan",
            "Stanley H. Chan"
        ],
        "summary": "Atmospheric turbulence is a major source of image degradation in long-range\nimaging systems. Although numerous deep learning-based turbulence mitigation\n(TM) methods have been proposed, many are slow, memory-hungry, and do not\ngeneralize well. In the spatial domain, methods based on convolutional\noperators have a limited receptive field, so they cannot handle a large spatial\ndependency required by turbulence. In the temporal domain, methods relying on\nself-attention can, in theory, leverage the lucky effects of turbulence, but\ntheir quadratic complexity makes it difficult to scale to many frames.\nTraditional recurrent aggregation methods face parallelization challenges.\n  In this paper, we present a new TM method based on two concepts: (1) A\nturbulence mitigation network based on the Selective State Space Model\n(MambaTM). MambaTM provides a global receptive field in each layer across\nspatial and temporal dimensions while maintaining linear computational\ncomplexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state\nspace model. Unlike classical Zernike-based representations of phase\ndistortion, the new LPD map uniquely captures the actual effects of turbulence,\nsignificantly improving the model's capability to estimate degradation by\nreducing the ill-posedness. Our proposed method exceeds current\nstate-of-the-art networks on various synthetic and real-world TM benchmarks\nwith significantly faster inference speed. The code is available at\nhttp://github.com/xg416/MambaTM.",
        "published": "2025-04-03T15:33:18+00:00"
    },
    {
        "title": "GPTQv2: Efficient Finetuning-Free Quantization for Asymmetric Calibration",
        "authors": [
            "Yuhang Li",
            "Ruokai Yin",
            "Donghyun Lee",
            "Shiting Xiao",
            "Priyadarshini Panda"
        ],
        "summary": "We introduce GPTQv2, a novel finetuning-free quantization method for\ncompressing large-scale transformer architectures. Unlike the previous GPTQ\nmethod, which independently calibrates each layer, we always match the\nquantized layer's output to the exact output in the full-precision model,\nresulting in a scheme that we call asymmetric calibration. Such a scheme can\neffectively reduce the quantization error accumulated in previous layers. We\nanalyze this problem using optimal brain compression to derive a close-formed\nsolution. The new solution explicitly minimizes the quantization error as well\nas the accumulated asymmetry error. Furthermore, we utilize various techniques\nto parallelize the solution calculation, including channel parallelization,\nneuron decomposition, and Cholesky reformulation for matrix fusion. As a\nresult, GPTQv2 is easy to implement, simply using 20 more lines of code than\nGPTQ but improving its performance under low-bit quantization. Remarkably, on a\nsingle GPU, we quantize a 405B language transformer as well as EVA-02 the rank\nfirst vision transformer that achieves 90% pretraining Imagenet accuracy. Code\nis available at github.com/Intelligent-Computing-Lab-Yale/GPTQv2.",
        "published": "2025-04-03T15:30:43+00:00"
    },
    {
        "title": "Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication using DRL",
        "authors": [
            "Achilles Kiwanuka Machumilane",
            "Alberto Gotta",
            "Pietro Cassar\u00e0"
        ],
        "summary": "Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted\nnext-generation wireless networks is critical for mobility management and\nensuring UAV safety and ubiquitous connectivity, especially in dense urban\nenvironments with street canyons and tall buildings. Traditional statistical\nand model-based techniques have been successfully used for path optimization in\ncommunication networks. However, when dynamic channel propagation\ncharacteristics such as line-of-sight (LOS), interference, handover, and\nsignal-to-interference and noise ratio (SINR) are included in path\noptimization, statistical and model-based path planning solutions become\nobsolete since they cannot adapt to the dynamic and time-varying wireless\nchannels, especially in the mmWave bands. In this paper, we propose a novel\nmodel-free actor-critic deep reinforcement learning (AC-DRL) framework for path\noptimization in UAV-assisted 5G mmWave wireless networks, which combines four\nimportant aspects of UAV communication: \\textit{flight time, handover,\nconnectivity and SINR}. We train an AC-RL agent that enables a UAV connected to\na gNB to determine the optimal path to a desired destination in the shortest\npossible time with minimal gNB handover, while maintaining connectivity and the\nhighest possible SINR. We train our model with data from a powerful ray tracing\ntool called Wireless InSite, which uses 3D images of the propagation\nenvironment and provides data that closely resembles the real propagation\nenvironment. The simulation results show that our system has superior\nperformance in tracking high SINR compared to other selected RL algorithms.",
        "published": "2025-04-03T15:28:04+00:00"
    },
    {
        "title": "STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability",
        "authors": [
            "Iv\u00e1n Sevillano-Garc\u00eda",
            "Juli\u00e1n Luengo",
            "Francisco Herrera"
        ],
        "summary": "Out-of-Distribution (OOD) detection is a critical task in machine learning,\nparticularly in safety-sensitive applications where model failures can have\nserious consequences. However, current OOD detection methods often suffer from\nrestrictive distributional assumptions, limited scalability, and a lack of\ninterpretability. To address these challenges, we propose STOOD-X, a two-stage\nmethodology that combines a Statistical nonparametric Test for OOD Detection\nwith eXplainability enhancements. In the first stage, STOOD-X uses\nfeature-space distances and a Wilcoxon-Mann-Whitney test to identify OOD\nsamples without assuming a specific feature distribution. In the second stage,\nit generates user-friendly, concept-based visual explanations that reveal the\nfeatures driving each decision, aligning with the BLUE XAI paradigm. Through\nextensive experiments on benchmark datasets and multiple architectures, STOOD-X\nachieves competitive performance against state-of-the-art post hoc OOD\ndetectors, particularly in high-dimensional and complex settings. In addition,\nits explainability framework enables human oversight, bias detection, and model\ndebugging, fostering trust and collaboration between humans and AI systems. The\nSTOOD-X methodology therefore offers a robust, explainable, and scalable\nsolution for real-world OOD detection tasks.",
        "published": "2025-04-03T15:26:03+00:00"
    },
    {
        "title": "Morpheus: Benchmarking Physical Reasoning of Video Generative Models with Real Physical Experiments",
        "authors": [
            "Chenyu Zhang",
            "Daniil Cherniavskii",
            "Andrii Zadaianchuk",
            "Antonios Tragoudaras",
            "Antonios Vozikis",
            "Thijmen Nijdam",
            "Derck W. E. Prinzhorn",
            "Mark Bodracska",
            "Nicu Sebe",
            "Efstratios Gavves"
        ],
        "summary": "Recent advances in image and video generation raise hopes that these models\npossess world modeling capabilities, the ability to generate realistic,\nphysically plausible videos. This could revolutionize applications in robotics,\nautonomous driving, and scientific simulation. However, before treating these\nmodels as world models, we must ask: Do they adhere to physical conservation\nlaws? To answer this, we introduce Morpheus, a benchmark for evaluating video\ngeneration models on physical reasoning. It features 80 real-world videos\ncapturing physical phenomena, guided by conservation laws. Since artificial\ngenerations lack ground truth, we assess physical plausibility using\nphysics-informed metrics evaluated with respect to infallible conservation laws\nknown per physical setting, leveraging advances in physics-informed neural\nnetworks and vision-language foundation models. Our findings reveal that even\nwith advanced prompting and video conditioning, current models struggle to\nencode physical principles despite generating aesthetically pleasing videos.\nAll data, leaderboard, and code are open-sourced at our project page.",
        "published": "2025-04-03T15:21:17+00:00"
    },
    {
        "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
        "authors": [
            "Maciej Besta",
            "Lorenzo Paleari",
            "Jia Hao Andrea Jiang",
            "Robert Gerstenberger",
            "You Wu",
            "Patrick Iff",
            "Ales Kubicek",
            "Piotr Nyczyk",
            "Diana Khimey",
            "J\u00f3n Gunnar Hannesson",
            "Grzegorz Kwa\u015bniewski",
            "Marcin Copik",
            "Hubert Niewiadomski",
            "Torsten Hoefler"
        ],
        "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants.",
        "published": "2025-04-03T15:11:55+00:00"
    },
    {
        "title": "Two-Stage nnU-Net for Automatic Multi-class Bi-Atrial Segmentation from LGE-MRIs",
        "authors": [
            "Y. On",
            "C. Galazis",
            "C. Chiu",
            "M. Varela"
        ],
        "summary": "Late gadolinium enhancement magnetic resonance imaging (LGE-MRI) is used to\nvisualise atrial fibrosis and scars, providing important information for\npersonalised atrial fibrillation (AF) treatments. Since manual analysis and\ndelineations of these images can be both labour-intensive and subject to\nvariability, we develop an automatic pipeline to perform segmentation of the\nleft atrial (LA) cavity, the right atrial (RA) cavity, and the wall of both\natria on LGE-MRI. Our method is based on a two-stage nnU-Net architecture,\ncombining 2D and 3D convolutional networks, and incorporates adaptive histogram\nequalisation to improve tissue contrast in the input images and morphological\noperations on the output segmentation maps. We achieve Dice similarity\ncoefficients of 0.92 +/- 0.03, 0.93 +/- 0.03, 0.71 +/- 0.05 and 95% Hausdorff\ndistances of (3.89 +/- 6.67) mm, (4.42 +/- 1.66) mm and (3.94 +/- 1.83) mm for\nLA, RA, and wall, respectively. The accurate delineation of the LA, RA and the\nmyocardial wall is the first step in analysing atrial structure in\ncardiovascular patients, especially those with AF. This can allow clinicians to\nprovide adequate and personalised treatment plans in a timely manner.",
        "published": "2025-04-03T15:08:33+00:00"
    },
    {
        "title": "Compositionality Unlocks Deep Interpretable Models",
        "authors": [
            "Thomas Dooms",
            "Ward Gauderis",
            "Geraint A. Wiggins",
            "Jose Oramas"
        ],
        "summary": "We propose $\\chi$-net, an intrinsically interpretable architecture combining\nthe compositional multilinear structure of tensor networks with the\nexpressivity and efficiency of deep neural networks. $\\chi$-nets retain equal\naccuracy compared to their baseline counterparts. Our novel, efficient\ndiagonalisation algorithm, ODT, reveals linear low-rank structure in a\nmultilayer SVHN model. We leverage this toward formal weight-based\ninterpretability and model compression.",
        "published": "2025-04-03T15:07:54+00:00"
    },
    {
        "title": "MiLo: Efficient Quantized MoE Inference with Mixture of Low-Rank Compensators",
        "authors": [
            "Beichen Huang",
            "Yueming Yuan",
            "Zelei Shao",
            "Minjia Zhang"
        ],
        "summary": "A critical approach for efficiently deploying Mixture-of-Experts (MoE) models\nwith massive parameters is quantization. However, state-of-the-art MoE models\nsuffer from non-negligible accuracy loss with extreme quantization, such as\nunder 4 bits. To address this, we introduce MiLo, a novel method that augments\nhighly quantized MoEs with a mixture of low-rank compensators. These\ncompensators consume only a small amount of additional memory but significantly\nrecover accuracy loss from extreme quantization. MiLo also identifies that\nMoEmodels exhibit distinctive characteristics across weights due to their\nhybrid dense-sparse architectures, and employs adaptive rank selection policies\nalong with iterative optimizations to close the accuracy gap. MiLo does not\nrely on calibration data, allowing it to generalize to different MoE models and\ndatasets without overfitting to a calibration set. To avoid the hardware\ninefficiencies of extreme quantization, such as 3-bit, MiLo develops Tensor\nCore-friendly 3-bit kernels, enabling measured latency speedups on 3-bit\nquantized MoE models. Our evaluation shows that MiLo outperforms existing\nmethods on SoTA MoE models across various tasks.",
        "published": "2025-04-03T14:54:17+00:00"
    },
    {
        "title": "SymDQN: Symbolic Knowledge and Reasoning in Neural Network-based Reinforcement Learning",
        "authors": [
            "Ivo Amador",
            "Nina Gierasimczuk"
        ],
        "summary": "We propose a learning architecture that allows symbolic control and guidance\nin reinforcement learning with deep neural networks. We introduce SymDQN, a\nnovel modular approach that augments the existing Dueling Deep Q-Networks\n(DuelDQN) architecture with modules based on the neuro-symbolic framework of\nLogic Tensor Networks (LTNs). The modules guide action policy learning and\nallow reinforcement learning agents to display behaviour consistent with\nreasoning about the environment. Our experiment is an ablation study performed\non the modules. It is conducted in a reinforcement learning environment of a\n5x5 grid navigated by an agent that encounters various shapes, each associated\nwith a given reward. The underlying DuelDQN attempts to learn the optimal\nbehaviour of the agent in this environment, while the modules facilitate shape\nrecognition and reward prediction. We show that our architecture significantly\nimproves learning, both in terms of performance and the precision of the agent.\nThe modularity of SymDQN allows reflecting on the intricacies and complexities\nof combining neural and symbolic approaches in reinforcement learning.",
        "published": "2025-04-03T14:51:11+00:00"
    },
    {
        "title": "The Markov approximation of the periodic multivariate Poisson autoregression",
        "authors": [
            "Mahmoud Khabou",
            "Edward A. K. Cohen",
            "Almut E. D. Veraart"
        ],
        "summary": "This paper introduces a periodic multivariate Poisson autoregression with\npotentially infinite memory, with a special focus on the network setting. Using\ncontraction techniques, we study the stability of such a process and provide\nupper bounds on how fast it reaches the periodically stationary regime. We then\npropose a computationally efficient Markov approximation using the properties\nof the exponential function and a density result. Furthermore, we prove the\nstrong consistency of the maximum likelihood estimator for the Markov\napproximation and empirically test its robustness in the case of\nmisspecification. Our model is applied to the prediction of weekly Rotavirus\ncases in Berlin, demonstrating superior performance compared to the existing\nPNAR model.",
        "published": "2025-04-03T14:48:11+00:00"
    },
    {
        "title": "Controlled Social Learning: Altruism vs. Bias",
        "authors": [
            "Raghu Arghal",
            "Kevin He",
            "Shirin Saeedi Bidokhti",
            "Saswati Sarkar"
        ],
        "summary": "We introduce a model of controlled sequential social learning in which a\nplanner may pay a cost to adjust the private information structure of agents.\nThe planner may seek to induce correct actions that are consistent with an\nunknown true state of the world (altruistic planner) or to induce a specific\naction the planner prefers (biased planner). Our framework presents a new\noptimization problem for social learning that combines dynamic programming with\ndecentralized action choices and Bayesian belief updates. This sheds light on\npractical policy questions, such as how the socially optimal level of ad\npersonalization changes according to current beliefs or how a political\ncampaign may selectively illuminate or obfuscate the winning potential of its\ncandidate among voters. We then prove the convexity of the value function and\ncharacterize the optimal policies of altruistic and biased planners, which\nattain desired tradeoffs between the costs they incur and the payoffs they earn\nfrom the choices they induce in the agents. Even for a planner who has\nequivalent knowledge to an individual, cannot lie or cherry-pick information,\nand is fully observable, we demonstrate that it is possible to dramatically\ninfluence social welfare in both positive and negative directions.",
        "published": "2025-04-03T14:45:24+00:00"
    },
    {
        "title": "Adaptive Frequency Enhancement Network for Remote Sensing Image Semantic Segmentation",
        "authors": [
            "Feng Gao",
            "Miao Fu",
            "Jingchao Cao",
            "Junyu Dong",
            "Qian Du"
        ],
        "summary": "Semantic segmentation of high-resolution remote sensing images plays a\ncrucial role in land-use monitoring and urban planning. Recent remarkable\nprogress in deep learning-based methods makes it possible to generate\nsatisfactory segmentation results. However, existing methods still face\nchallenges in adapting network parameters to various land cover distributions\nand enhancing the interaction between spatial and frequency domain features. To\naddress these challenges, we propose the Adaptive Frequency Enhancement Network\n(AFENet), which integrates two key components: the Adaptive Frequency and\nSpatial feature Interaction Module (AFSIM) and the Selective feature Fusion\nModule (SFM). AFSIM dynamically separates and modulates high- and low-frequency\nfeatures according to the content of the input image. It adaptively generates\ntwo masks to separate high- and low-frequency components, therefore providing\noptimal details and contextual supplementary information for ground object\nfeature representation. SFM selectively fuses global context and local detailed\nfeatures to enhance the network's representation capability. Hence, the\ninteractions between frequency and spatial features are further enhanced.\nExtensive experiments on three publicly available datasets demonstrate that the\nproposed AFENet outperforms state-of-the-art methods. In addition, we also\nvalidate the effectiveness of AFSIM and SFM in managing diverse land cover\ntypes and complex scenarios. Our codes are available at\nhttps://github.com/oucailab/AFENet.",
        "published": "2025-04-03T14:42:49+00:00"
    },
    {
        "title": "Utilizing 5G NR SSB Blocks for Passive Detection and Localization of Low-Altitude Drones",
        "authors": [
            "Palatip Jopanya",
            "Diana P. M. Osorio"
        ],
        "summary": "With the exponential growth of the unmanned aerial vehicle (UAV) industry and\na broad range of applications expected to appear in the coming years, the\nemployment of traditional radar systems is becoming increasingly cumbersome for\nUAV supervision. Motivated by this emerging challenge, this paper investigates\nthe feasibility of employing integrated sensing and communication (ISAC)\nsystems implemented over current and future wireless networks to perform this\ntask. We propose a sensing mechanism based on the synchronization signal block\n(SSB) in the fifth-generation (5G) standard that performs sensing in a passive\nbistatic setting. By assuming planar arrays at the sensing nodes and according\nto the 5G standard, we consider that the SSB signal is sent in a grid of\northogonal beams that are multiplexed in time, with some of them pointing\ntoward a surveillance region where low-altitude drones can be flying. The\nCramer-Rao Bound (CRB) is derived as the theoretical bound for range and\nvelocity estimation. Our results demonstrate the potential of employing SSB\nsignals for UAV-like target localization at low SNR.",
        "published": "2025-04-03T14:36:11+00:00"
    },
    {
        "title": "Reservoir Computing: A New Paradigm for Neural Networks",
        "authors": [
            "Felix Grezes"
        ],
        "summary": "A Literature Review of Reservoir Computing.\n  Even before Artificial Intelligence was its own field of computational\nscience, humanity has tried to mimic the activity of the human brain. In the\nearly 1940s the first artificial neuron models were created as purely\nmathematical concepts. Over the years, ideas from neuroscience and computer\nscience were used to develop the modern Neural Network. The interest in these\nmodels rose quickly but fell when they failed to be successfully applied to\npractical applications, and rose again in the late 2000s with the drastic\nincrease in computing power, notably in the field of natural language\nprocessing, for example with the state-of-the-art speech recognizer making\nheavy use of deep neural networks.\n  Recurrent Neural Networks (RNNs), a class of neural networks with cycles in\nthe network, exacerbates the difficulties of traditional neural nets. Slow\nconvergence limiting the use to small networks, and difficulty to train through\ngradient-descent methods because of the recurrent dynamics have hindered\nresearch on RNNs, yet their biological plausibility and their capability to\nmodel dynamical systems over simple functions makes then interesting for\ncomputational researchers.\n  Reservoir Computing emerges as a solution to these problems that RNNs\ntraditionally face. Promising to be both theoretically sound and\ncomputationally fast, Reservoir Computing has already been applied successfully\nto numerous fields: natural language processing, computational biology and\nneuroscience, robotics, even physics. This survey will explore the history and\nappeal of both traditional feed-forward and recurrent neural networks, before\ndescribing the theory and models of this new reservoir computing paradigm.\nFinally recent papers using reservoir computing in a variety of scientific\nfields will be reviewed.",
        "published": "2025-04-03T14:34:51+00:00"
    },
    {
        "title": "Medium Access for Push-Pull Data Transmission in 6G Wireless Systems",
        "authors": [
            "Shashi Raj Pandey",
            "Fabio Saggese",
            "Junya Shiraishi",
            "Federico Chiariotti",
            "Petar Popovski"
        ],
        "summary": "Medium access in 5G systems was tailored to accommodate diverse traffic\nclasses through network resource slicing. 6G wireless systems are expected to\nbe significantly reliant on Artificial Intelligence (AI), leading to\ndata-driven and goal-oriented communication. This leads to augmentation of the\ndesign space for Medium Access Control (MAC) protocols, which is the focus of\nthis article. We introduce a taxonomy based on push-based and pull-based\ncommunication, which is useful to categorize both the legacy and the AI-driven\naccess schemes. We provide MAC protocol design guidelines for pull- and\npush-based communication in terms of goal-oriented criteria, such as timing and\ndata relevance. We articulate a framework for co-existence between pull and\npush-based communications in 6G systems, combining their advantages. We\nhighlight the design principles and main tradeoffs, as well as the\narchitectural considerations for integrating these designs in Open-Radio Access\nNetwork (O-RAN) and 6G systems.",
        "published": "2025-04-03T14:34:38+00:00"
    },
    {
        "title": "Data-Driven Design of 3GPP Handover Parameters with Bayesian Optimization and Transfer Learning",
        "authors": [
            "Mohamed Benzaghta",
            "Sahar Ammar",
            "David L\u00f3pez-P\u00e9rez",
            "Basem Shihada",
            "Giovanni Geraci"
        ],
        "summary": "Mobility management in dense cellular networks is challenging due to varying\nuser speeds and deployment conditions. Traditional 3GPP handover (HO) schemes,\nrelying on fixed A3-offset and time-to-trigger (TTT) parameters, struggle to\nbalance radio link failures (RLFs) and ping-pongs. We propose a data-driven HO\noptimization framework based on high-dimensional Bayesian optimization (HD-BO)\nand enhanced with transfer learning to reduce training time and improve\ngeneralization across different user speeds. Evaluations on a real-world\ndeployment show that HD-BO outperforms 3GPP set-1 and set-5 benchmarks, while\ntransfer learning enables rapid adaptation without loss in performance. This\nhighlights the potential of data-driven, site-specific mobility management in\nlarge-scale networks.",
        "published": "2025-04-03T14:31:20+00:00"
    },
    {
        "title": "A Scalable Synthesis Algorithm for Reversible Functions",
        "authors": [
            "Moein Sarvaghad-Moghaddam",
            "Morteza Saheb Zamani",
            "Mehdi Sedighi"
        ],
        "summary": "Reversible computation is an emerging technology that has gained significant\nattention due to its critical role in quantum circuit synthesis and low-power\ndesign. This paper introduces a transformation-based method for exact synthesis\nof reversible circuits. The proposed approach utilizes a modified\nQuine-McCluskey algorithm to eliminate input-output discrepancies in the truth\ntable, transforming the permutation matrix into an identity matrix.\nFurthermore, a novel search space reduction technique is presented which,\ncombined with the primary method, enables the synthesis algorithm to handle\nhigh-input reversible functions. This approach combines the influence of\nmultiple control qubits on a target qubit, evaluating their collective impact.\nThis aggregation can decrease the control qubit count within quantum gates.\nConsequently, it proves beneficial for applications like surface code error\ncorrection architectures as well as current Noisy Intermediate-Scale Quantum\n(NISQ) hardwares. Experimental results demonstrate significant improvements\nover the state-of-the-art exact synthesis methods, achieving up to 99%\nimprovements in terms of the number of levels of T-gates.",
        "published": "2025-04-03T14:29:33+00:00"
    },
    {
        "title": "Incorporating the ChEES Criterion into Sequential Monte Carlo Samplers",
        "authors": [
            "Andrew Millard",
            "Joshua Murphy",
            "Daniel Frisch",
            "Simon Maskell"
        ],
        "summary": "Markov chain Monte Carlo (MCMC) methods are a powerful but computationally\nexpensive way of performing non-parametric Bayesian inference. MCMC proposals\nwhich utilise gradients, such as Hamiltonian Monte Carlo (HMC), can better\nexplore the parameter space of interest if the additional hyper-parameters are\nchosen well. The No-U-Turn Sampler (NUTS) is a variant of HMC which is\nextremely effective at selecting these hyper-parameters but is slow to run and\nis not suited to GPU architectures. An alternative to NUTS, Change in the\nEstimator of the Expected Square HMC (ChEES-HMC) was shown not only to run\nfaster than NUTS on GPU but also sample from posteriors more efficiently.\nSequential Monte Carlo (SMC) samplers are another sampling method which instead\noutput weighted samples from the posterior. They are very amenable to\nparallelisation and therefore being run on GPUs while having additional\nflexibility in their choice of proposal over MCMC. We incorporate (ChEEs-HMC)\nas a proposal into SMC samplers and demonstrate competitive but faster\nperformance than NUTS on a number of tasks.",
        "published": "2025-04-03T14:25:19+00:00"
    },
    {
        "title": "Towards Computation- and Communication-efficient Computational Pathology",
        "authors": [
            "Chu Han",
            "Bingchao Zhao",
            "Jiatai Lin",
            "Shanshan Lyu",
            "Longfei Wang",
            "Tianpeng Deng",
            "Cheng Lu",
            "Changhong Liang",
            "Hannah Y. Wen",
            "Xiaojing Guo",
            "Zhenwei Shi",
            "Zaiyi Liu"
        ],
        "summary": "Despite the impressive performance across a wide range of applications,\ncurrent computational pathology models face significant diagnostic efficiency\nchallenges due to their reliance on high-magnification whole-slide image\nanalysis. This limitation severely compromises their clinical utility,\nespecially in time-sensitive diagnostic scenarios and situations requiring\nefficient data transfer. To address these issues, we present a novel\ncomputation- and communication-efficient framework called Magnification-Aligned\nGlobal-Local Transformer (MAGA-GLTrans). Our approach significantly reduces\ncomputational time, file transfer requirements, and storage overhead by\nenabling effective analysis using low-magnification inputs rather than\nhigh-magnification ones. The key innovation lies in our proposed magnification\nalignment (MAGA) mechanism, which employs self-supervised learning to bridge\nthe information gap between low and high magnification levels by effectively\naligning their feature representations. Through extensive evaluation across\nvarious fundamental CPath tasks, MAGA-GLTrans demonstrates state-of-the-art\nclassification performance while achieving remarkable efficiency gains: up to\n10.7 times reduction in computational time and over 20 times reduction in file\ntransfer and storage requirements. Furthermore, we highlight the versatility of\nour MAGA framework through two significant extensions: (1) its applicability as\na feature extractor to enhance the efficiency of any CPath architecture, and\n(2) its compatibility with existing foundation models and\nhistopathology-specific encoders, enabling them to process low-magnification\ninputs with minimal information loss. These advancements position MAGA-GLTrans\nas a particularly promising solution for time-sensitive applications,\nespecially in the context of intraoperative frozen section diagnosis where both\naccuracy and efficiency are paramount.",
        "published": "2025-04-03T14:25:19+00:00"
    },
    {
        "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning",
        "authors": [
            "Leonardo Iurada",
            "Marco Ciccone",
            "Tatiana Tommasi"
        ],
        "summary": "Task arithmetic has emerged as a promising approach for editing models by\nrepresenting task-specific knowledge as composable task vectors. However,\nexisting methods rely on network linearization to derive task vectors, leading\nto computational bottlenecks during training and inference. Moreover,\nlinearization alone does not ensure weight disentanglement, the key property\nthat enables conflict-free composition of task vectors. To address this, we\npropose TaLoS which allows to build sparse task vectors with minimal\ninterference without requiring explicit linearization and sharing information\nacross tasks. We find that pre-trained models contain a subset of parameters\nwith consistently low gradient sensitivity across tasks, and that sparsely\nupdating only these parameters allows for promoting weight disentanglement\nduring fine-tuning. Our experiments prove that TaLoS improves training and\ninference efficiency while outperforming current methods in task addition and\nnegation. By enabling modular parameter editing, our approach fosters practical\ndeployment of adaptable foundation models in real-world applications.",
        "published": "2025-04-03T14:20:06+00:00"
    },
    {
        "title": "A Hybrid Similarity-Aware Graph Neural Network with Transformer for Node Classification",
        "authors": [
            "Aman Singh",
            "Shahid Shafi Dar",
            "Ranveer Singh",
            "Nagendra Kumar"
        ],
        "summary": "Node classification has gained significant importance in graph deep learning\nwith real-world applications such as recommendation systems, drug discovery,\nand citation networks. Graph Convolutional Networks and Graph Transformers have\nachieved superior performance in node classification tasks. However, the key\nconcern with Graph Convolutional Networks is over-squashing, which limits their\nability to capture long-range dependencies in the network. Additionally, Graph\nTransformers face scalability challenges, making it difficult to process large\ngraphs efficiently. To address this, we propose a novel framework, A Hybrid\nSImilarity-Aware Graph Neural Network with Transformer for Node Classification\n(SIGNNet), which capitalizes on local and global structural information,\nenhances the model's capability to effectively capture fine-grained\nrelationships and broader contextual patterns within the graph structure. The\nproposed method leverages Graph Convolutional Networks alongside a score-based\nmechanism to effectively capture local and global node interactions while\naddressing the limitations of over-squashing. Our proposed method employs a\nnovel Personalized PageRank-based node sampling method to address scalability\nissues by generating subgraphs of nodes. Additionally, SIGNNet incorporates a\nnovel attention mechanism, Structure-Aware Multi-Head Attention (SA-MHA), which\nintegrates node structural information for informed attention weighting,\nenabling the model to prioritize nodes based on topological significance.\nExtensive experiments demonstrate the significant improvements achieved by the\nproposed method over existing state-of-the-art methods, with average accuracy\ngains of 6.03%, 5.47%, 4.78%, 19.10%, 19.61%, 7.22%, 19.54%, and 14.94% on\nCora, Citeseer, CS, Wisconsin, Texas, Actor, Cornell and Chameleon datasets,\nrespectively.",
        "published": "2025-04-03T14:14:37+00:00"
    },
    {
        "title": "UAV-Assisted 5G Networks: Mobility-Aware 3D Trajectory Optimization and Resource Allocation for Dynamic Environments",
        "authors": [
            "Asad Mahmood",
            "Thang X. Vu",
            "Wali Ullah Khan",
            "Symeon Chatzinotas",
            "Bj\u00f6rn Ottersten"
        ],
        "summary": "This paper proposes a framework for robust design of UAV-assisted wireless\nnetworks that combine 3D trajectory optimization with user mobility prediction\nto address dynamic resource allocation challenges. We proposed a sparse\nsecond-order prediction model for real-time user tracking coupled with\nheuristic user clustering to balance service quality and computational\ncomplexity. The joint optimization problem is formulated to maximize the\nminimum rate. It is then decomposed into user association, 3D trajectory\ndesign, and resource allocation subproblems, which are solved iteratively via\nsuccessive convex approximation (SCA). Extensive simulations demonstrate: (1)\nnear-optimal performance with $\\epsilon \\approx 0.67\\%$ deviation from\nupper-bound solutions, (2) $16\\%$ higher minimum rates for distant users\ncompared to non-predictive 3D designs, and (3) $10-30\\%$ faster outage\nmitigation than time-division benchmarks. The framework's adaptive speed\ncontrol enables precise mobile user tracking while maintaining energy\nefficiency under constrained flight time. Results demonstrate superior\nrobustness in edge-coverage scenarios, making it particularly suitable for\n$5G/6G$ networks.",
        "published": "2025-04-03T14:12:58+00:00"
    },
    {
        "title": "Learning Geometrically-Informed Lyapunov Functions with Deep Diffeomorphic RBF Networks",
        "authors": [
            "Samuel Tesfazgi",
            "Leonhard Sprandl",
            "Sandra Hirche"
        ],
        "summary": "The practical deployment of learning-based autonomous systems would greatly\nbenefit from tools that flexibly obtain safety guarantees in the form of\ncertificate functions from data. While the geometrical properties of such\ncertificate functions are well understood, synthesizing them using machine\nlearning techniques still remains a challenge. To mitigate this issue, we\npropose a diffeomorphic function learning framework where prior structural\nknowledge of the desired output is encoded in the geometry of a simple\nsurrogate function, which is subsequently augmented through an expressive,\ntopology-preserving state-space transformation. Thereby, we achieve an indirect\nfunction approximation framework that is guaranteed to remain in the desired\nhypothesis space. To this end, we introduce a novel approach to construct\ndiffeomorphic maps based on RBF networks, which facilitate precise, local\ntransformations around data. Finally, we demonstrate our approach by learning\ndiffeomorphic Lyapunov functions from real-world data and apply our method to\ndifferent attractor systems.",
        "published": "2025-04-03T14:09:17+00:00"
    },
    {
        "title": "Graphs are everywhere -- Psst! In Music Recommendation too",
        "authors": [
            "Bharani Jayakumar",
            "Orkun \u00d6zo\u011flu"
        ],
        "summary": "In recent years, graphs have gained prominence across various domains,\nespecially in recommendation systems. Within the realm of music recommendation,\ngraphs play a crucial role in enhancing genre-based recommendations by\nintegrating Mel-Frequency Cepstral Coefficients (MFCC) with advanced graph\nembeddings. This study explores the efficacy of Graph Convolutional Networks\n(GCN), GraphSAGE, and Graph Transformer (GT) models in learning embeddings that\neffectively capture intricate relationships between music items and genres\nrepresented within graph structures. Through comprehensive empirical\nevaluations on diverse real-world music datasets, our findings consistently\ndemonstrate that these graph-based approaches outperform traditional methods\nthat rely solely on MFCC features or collaborative filtering techniques.\nSpecifically, the graph-enhanced models achieve notably higher accuracy in\npredicting genre-specific preferences and offering relevant music suggestions\nto users. These results underscore the effectiveness of utilizing graph\nembeddings to enrich feature representations and exploit latent associations\nwithin music data, thereby illustrating their potential to advance the\ncapabilities of personalized and context-aware music recommendation systems.\nKeywords: graphs, recommendation systems, neural networks, MFCC",
        "published": "2025-04-03T14:00:52+00:00"
    },
    {
        "title": "State-Space Model Inspired Multiple-Input Multiple-Output Spiking Neurons",
        "authors": [
            "Sanja Karilanova",
            "Subhrakanti Dey",
            "Ay\u00e7a \u00d6z\u00e7elikkale"
        ],
        "summary": "In spiking neural networks (SNNs), the main unit of information processing is\nthe neuron with an internal state. The internal state generates an output spike\nbased on its component associated with the membrane potential. This spike is\nthen communicated to other neurons in the network. Here, we propose a general\nmultiple-input multiple-output (MIMO) spiking neuron model that goes beyond\nthis traditional single-input single-output (SISO) model in the SNN literature.\nOur proposed framework is based on interpreting the neurons as state-space\nmodels (SSMs) with linear state evolutions and non-linear spiking activation\nfunctions. We illustrate the trade-offs among various parameters of the\nproposed SSM-inspired neuron model, such as the number of hidden neuron states,\nthe number of input and output channels, including single-input multiple-output\n(SIMO) and multiple-input single-output (MISO) models. We show that for SNNs\nwith a small number of neurons with large internal state spaces, significant\nperformance gains may be obtained by increasing the number of output channels\nof a neuron. In particular, a network with spiking neurons with multiple-output\nchannels may achieve the same level of accuracy with the baseline with the\ncontinuous-valued communications on the same reference network architecture.",
        "published": "2025-04-03T13:55:11+00:00"
    },
    {
        "title": "Deep learning for music generation. Four approaches and their comparative evaluation",
        "authors": [
            "Razvan Paroiu",
            "Stefan Trausan-Matu"
        ],
        "summary": "This paper introduces four different artificial intelligence algorithms for\nmusic generation and aims to compare these methods not only based on the\naesthetic quality of the generated music but also on their suitability for\nspecific applications. The first set of melodies is produced by a slightly\nmodified visual transformer neural network that is used as a language model.\nThe second set of melodies is generated by combining chat sonification with a\nclassic transformer neural network (the same method of music generation is\npresented in a previous research), the third set of melodies is generated by\ncombining the Schillinger rhythm theory together with a classic transformer\nneural network, and the fourth set of melodies is generated using GPT3\ntransformer provided by OpenAI. A comparative analysis is performed on the\nmelodies generated by these approaches and the results indicate that\nsignificant differences can be observed between them and regarding the\naesthetic value of them, GPT3 produced the most pleasing melodies, and the\nnewly introduced Schillinger method proved to generate better sounding music\nthan previous sonification methods.",
        "published": "2025-04-03T13:51:07+00:00"
    },
    {
        "title": "Bridging the Gap between Gaussian Diffusion Models and Universal Quantization for Image Compression",
        "authors": [
            "Lucas Relic",
            "Roberto Azevedo",
            "Yang Zhang",
            "Markus Gross",
            "Christopher Schroers"
        ],
        "summary": "Generative neural image compression supports data representation at extremely\nlow bitrate, synthesizing details at the client and consistently producing\nhighly realistic images. By leveraging the similarities between quantization\nerror and additive noise, diffusion-based generative image compression codecs\ncan be built using a latent diffusion model to \"denoise\" the artifacts\nintroduced by quantization. However, we identify three critical gaps in\nprevious approaches following this paradigm (namely, the noise level, noise\ntype, and discretization gaps) that result in the quantized data falling out of\nthe data distribution known by the diffusion model. In this work, we propose a\nnovel quantization-based forward diffusion process with theoretical foundations\nthat tackles all three aforementioned gaps. We achieve this through universal\nquantization with a carefully tailored quantization schedule and a diffusion\nmodel trained with uniform noise. Compared to previous work, our proposal\nproduces consistently realistic and detailed reconstructions, even at very low\nbitrates. In such a regime, we achieve the best rate-distortion-realism\nperformance, outperforming previous related works.",
        "published": "2025-04-03T13:42:19+00:00"
    },
    {
        "title": "Fluorine production in He-burning regions of massive stars during cosmic history",
        "authors": [
            "Sophie Tsiatsiou",
            "Georges Meynet",
            "Eoin Farrell",
            "Yutaka Hirai",
            "Arthur Choplin",
            "Yves Sibony",
            "S\u00e9bastien Martinet",
            "Rafael Guer\u00e7o",
            "Verne Smith",
            "Katia Cunha",
            "St\u00e9phane Goriely",
            "Marcel Arnould",
            "Jos\u00e9 G. Fern\u00e1ndez-Trincado",
            "Sylvia Ekstr\u00f6m"
        ],
        "summary": "The origin of fluorine is still a debated question. AGB stars synthesise this\nelement and likely contribute significantly to its synthesis in the present-day\nUniverse. However, it is not clear whether other sources contribute, especially\nin the early Universe. We discuss variations of the surface abundances of\nfluorine coming from our massive star models and compare them with available\npresent-day observations. We compute the contribution of massive stars in\nproducing 19F over metallicities covering the whole cosmic history. We used\nmodels in the mass range of 9Msol < Mini < 300Msol at metallicities from Pop\nIII up to super-solar while accounting for the required nuclear network to\nfollow the evolution of 19F during the core H- and He-burning phases. Results\nfrom models with and without rotational mixing are presented. We find that\nrotating models predict a slight depletion of fluorine at their surface at the\nend of the MS phase. In more advanced evolutionary phases, only models with an\ninitial mass larger than 25Msol at metallicities Z > 0.014 show phases where\nthe abundance of fluorine is enhanced. This occurs when the star is a WR star\nof the WC type. WC stars can show surface abundances of fluorine ten times\nlarger than their initial abundance. However, we obtained that the winds of\nmassive stars at metallicities larger than Z=0.006 do not significantly\ncontribute to fluorine production, confirming previous findings. In contrast,\nvery metal-poor rapidly rotating massive star models may be important sources\nof fluorine through the mass expelled at the time of their SN explosion.\nObservations of WC stars at solar or super-solar metallicities may provide very\ninteresting indications on the nuclear pathways that lead to fluorine\nproduction in massive stars. The possibility of observing fluorine-rich CEMPs\nis also a way to put constrains in present models at very low metallicities.",
        "published": "2025-04-03T13:36:04+00:00"
    },
    {
        "title": "Local Flow Estimation at the top of the Earth's Core using Physics Informed Neural Networks",
        "authors": [
            "Naomi Shakespeare-Rees",
            "Philip W. Livermore",
            "Christopher J. Davies",
            "Hannah F. Rogers",
            "William J. Brown",
            "Ciar\u00e1n D. Beggan",
            "Christopher C. Finlay"
        ],
        "summary": "The Earth's main geomagnetic field arises from the constant motion of the\nfluid outer core. By assuming that the field changes are advection-dominated,\nthe fluid motion at the core surface can be related to the secular variation of\nthe geomagnetic field. The majority of existing core flow models are global,\nshowing features such as an eccentric planetary gyre, with some evidence of\nrapid regional changes. By construction, the flow defined at any location by\nsuch a model depends on all magnetic field variations across the entire\ncore-mantle boundary making it challenging to interpret local structures in the\nflow as due to specific local changes in magnetic field. Here we present an\nalternative strategy in which we construct regional flow models that rely only\non local secular changes. We use a novel technique based on machine learning\ntermed Physics-Informed Neural Networks (PINNs), in which we seek a regional\nflow model that simultaneously fits both the local magnetic field variation and\ndynamical conditions assumed satisfied by the flow. Although we present results\nusing the Tangentially Geostrophic flow constraint, we set out a modelling\nframework for which the physics constraint can be easily changed by altering a\nsingle line of code. After validating the PINN-based method on synthetic flows,\nwe apply our method to the CHAOS-8.1 geomagnetic field model, itself based on\ndata from Swarm. Constructing a global mosaic of regional flows, we reproduce\nthe planetary gyre, providing independent evidence that the strong secular\nchanges at high latitude and in equatorial regions are part of the same global\nfeature. Our models also corroborate regional changes in core flows over the\nlast decade. Furthermore, our models endorse the existence of a dynamic high\nlatitude jet, which began accelerating around 2005 but has been weakening since\n2017.",
        "published": "2025-04-03T13:27:45+00:00"
    },
    {
        "title": "MAD: A Magnitude And Direction Policy Parametrization for Stability Constrained Reinforcement Learning",
        "authors": [
            "Luca Furieri",
            "Sucheth Shenoy",
            "Danilo Saccani",
            "Andrea Martin",
            "Giancarlo Ferrari Trecate"
        ],
        "summary": "We introduce magnitude and direction (MAD) policies, a policy\nparameterization for reinforcement learning (RL) that preserves Lp closed-loop\nstability for nonlinear dynamical systems. Although complete in their ability\nto describe all stabilizing controllers, methods based on nonlinear Youla and\nsystem-level synthesis are significantly affected by the difficulty of\nparameterizing Lp-stable operators. In contrast, MAD policies introduce\nexplicit feedback on state-dependent features - a key element behind the\nsuccess of RL pipelines - without compromising closed-loop stability. This is\nachieved by describing the magnitude of the control input with a\ndisturbance-feedback Lp-stable operator, while selecting its direction based on\nstate-dependent features through a universal function approximator. We further\ncharacterize the robust stability properties of MAD policies under model\nmismatch. Unlike existing disturbance-feedback policy parameterizations, MAD\npolicies introduce state-feedback components compatible with model-free RL\npipelines, ensuring closed-loop stability without requiring model information\nbeyond open-loop stability. Numerical experiments show that MAD policies\ntrained with deep deterministic policy gradient (DDPG) methods generalize to\nunseen scenarios, matching the performance of standard neural network policies\nwhile guaranteeing closed-loop stability by design.",
        "published": "2025-04-03T13:26:26+00:00"
    },
    {
        "title": "Digital Twins for Internet of Battlespace Things (IoBT) Coalitions",
        "authors": [
            "Athanasios Gkelias",
            "Patrick J. Baker",
            "Kin K. Leung",
            "Olwen Worthington",
            "Christopher R. Melville"
        ],
        "summary": "This paper presents a new framework for integrating Digital Twins (DTs)\nwithin Internet of battlespace Things (IoBT) coalitions. We introduce a novel\nthree-tier architecture that enables efficient coordination and management of\nDT models across coalition partners while addressing key challenges in\ninteroperability, security, and resource allocation. The architecture comprises\nspecialized controllers at each tier: Digital Twin Coalition Partner (DTCP)\ncontrollers managing individual coalition partners' DT resources, a central\nDigital Twin Coalition(DTC) controller orchestrating cross-partner\ncoordination, and Digital Twin Coalition Mission (DTCP) controllers handling\nmission-specific DT interactions. We propose a hybrid approach for DT model\nplacement across edge devices, tactical nodes, and cloud infrastructure,\noptimizing performance while maintaining security and accessibility. The\narchitecture leverages software-defined networking principles for dynamic\nresource allocation and slice management, enabling efficient sharing of\ncomputational and network resources between DT operations and primary IoBT\nfunctions. Our proposed framework aims to provide a robust foundation for\ndeploying and managing Digital Twins in coalition warfare, enhancing\nsituational awareness, decision-making capabilities, and operational\neffectiveness while ensuring secure and interoperable operations across diverse\ncoalition partners.",
        "published": "2025-04-03T13:23:44+00:00"
    },
    {
        "title": "Leveraging LLM For Synchronizing Information Across Multilingual Tables",
        "authors": [
            "Siddharth Khincha",
            "Tushar Kataria",
            "Ankita Anand",
            "Dan Roth",
            "Vivek Gupta"
        ],
        "summary": "The vast amount of online information today poses challenges for non-English\nspeakers, as much of it is concentrated in high-resource languages such as\nEnglish and French. Wikipedia reflects this imbalance, with content in\nlow-resource languages frequently outdated or incomplete. Recent research has\nsought to improve cross-language synchronization of Wikipedia tables using\nrule-based methods. These approaches can be effective, but they struggle with\ncomplexity and generalization. This paper explores large language models (LLMs)\nfor multilingual information synchronization, using zero-shot prompting as a\nscalable solution. We introduce the Information Updation dataset, simulating\nthe real-world process of updating outdated Wikipedia tables, and evaluate LLM\nperformance. Our findings reveal that single-prompt approaches often produce\nsuboptimal results, prompting us to introduce a task decomposition strategy\nthat enhances coherence and accuracy. Our proposed method outperforms existing\nbaselines, particularly in Information Updation (1.79%) and Information\nAddition (20.58%), highlighting the model strength in dynamically updating and\nenriching data across architectures.",
        "published": "2025-04-03T13:15:18+00:00"
    },
    {
        "title": "Noise Calibration and Spatial-Frequency Interactive Network for STEM Image Enhancement",
        "authors": [
            "Hesong Li",
            "Ziqi Wu",
            "Ruiwen Shao",
            "Tao Zhang",
            "Ying Fu"
        ],
        "summary": "Scanning Transmission Electron Microscopy (STEM) enables the observation of\natomic arrangements at sub-angstrom resolution, allowing for atomically\nresolved analysis of the physical and chemical properties of materials.\nHowever, due to the effects of noise, electron beam damage, sample thickness,\netc, obtaining satisfactory atomic-level images is often challenging. Enhancing\nSTEM images can reveal clearer structural details of materials. Nonetheless,\nexisting STEM image enhancement methods usually overlook unique features in the\nfrequency domain, and existing datasets lack realism and generality. To resolve\nthese issues, in this paper, we develop noise calibration, data synthesis, and\nenhancement methods for STEM images. We first present a STEM noise calibration\nmethod, which is used to synthesize more realistic STEM images. The parameters\nof background noise, scan noise, and pointwise noise are obtained by\nstatistical analysis and fitting of real STEM images containing atoms. Then we\nuse these parameters to develop a more general dataset that considers both\nregular and random atomic arrangements and includes both HAADF and BF mode\nimages. Finally, we design a spatial-frequency interactive network for STEM\nimage enhancement, which can explore the information in the frequency domain\nformed by the periodicity of atomic arrangement. Experimental results show that\nour data is closer to real STEM images and achieves better enhancement\nperformances together with our network. Code will be available at\nhttps://github.com/HeasonLee/SFIN}{https://github.com/HeasonLee/SFIN.",
        "published": "2025-04-03T13:11:57+00:00"
    },
    {
        "title": "Enhanced Permeability Estimation in Microporous Rocks Using a Hybrid Macropore-Darcy Approach",
        "authors": [
            "Dmytro Petrovskyy",
            "Julien Maes",
            "Hannah P. Menke",
            "Muhammad Ali",
            "Abdul H. Mazeli",
            "Muhammad Z. Kashim",
            "Zainol A. A. Bakar",
            "Kamaljit Singh"
        ],
        "summary": "This study presents a novel workflow for constructing hybrid macropore-Darcy\nmodels from micro-CT images of microporous rocks. In our approach, macropore\nnetworks are extracted using established methods, while the microporosity is\ncharacterised through segmented phase classification and incorporated into the\nmodel as Darcy cells. Effectively, Darcy cells capture the micro scale\nconnectivity variations that are missing in the macroscopic networks. This dual\nentity model thus incorporates both the conventional macroscopic pore structure\nand the critical flow pathways present in the under-resolved microporous\nregions. The proposed workflow is rigorously validated by comparing the\npermeability estimates with direct numerical simulation (DNS) results and\nexperimental measurements. Our findings demonstrate that this hybrid approach\nreliably reproduces fluid flow behaviour in complex porous media while\nsignificantly reducing computational demands, offering a promising tool for\nadvanced groundwater modelling and water resource management.",
        "published": "2025-04-03T13:05:31+00:00"
    },
    {
        "title": "Probabilistic Pontryagin's Maximum Principle for Continuous-Time Model-Based Reinforcement Learning",
        "authors": [
            "David Leeftink",
            "\u00c7a\u011fatay Y\u0131ld\u0131z",
            "Steffen Ridderbusch",
            "Max Hinne",
            "Marcel van Gerven"
        ],
        "summary": "Without exact knowledge of the true system dynamics, optimal control of\nnon-linear continuous-time systems requires careful treatment of epistemic\nuncertainty. In this work, we propose a probabilistic extension to Pontryagin's\nmaximum principle by minimizing the mean Hamiltonian with respect to epistemic\nuncertainty. We show minimization of the mean Hamiltonian is a necessary\noptimality condition when optimizing the mean cost, and propose a multiple\nshooting numerical method scalable to large-scale probabilistic dynamical\nmodels, including ensemble neural ordinary differential equations. Comparisons\nagainst state-of-the-art methods in online and offline model-based\nreinforcement learning tasks show that our probabilistic Hamiltonian\nformulation leads to reduced trial costs in offline settings and achieves\ncompetitive performance in online scenarios. By bridging optimal control and\nreinforcement learning, our approach offers a principled and practical\nframework for controlling uncertain systems with learned dynamics.",
        "published": "2025-04-03T12:51:20+00:00"
    },
    {
        "title": "A 3D-1D-0D Multiscale Model of the Neuro-Glial-Vascular Unit for Synaptic and Vascular Dynamics in the Dorsal Vagal Complex",
        "authors": [
            "Alexander Hermann",
            "Tobias K\u00f6ppl",
            "Andreas Wagner",
            "Arman Shojaei",
            "Barbara Wohlmuth",
            "Roland Aydin",
            "Christian J. Cyron",
            "Roustem Miftahof"
        ],
        "summary": "Cerebral blood flow regulation is critical for brain function, and its\ndisruption is implicated in various neurological disorders. Many existing\nmodels do not fully capture the complex, multiscale interactions among neuronal\nactivity, astrocytic signaling, and vascular dynamics--especially in key\nbrainstem regions. In this work, we present a 3D-1D-0D multiscale computational\nframework for modeling the neuro-glial-vascular unit (NGVU) in the dorsal vagal\ncomplex (DVC). Our approach integrates a quadripartite synapse model--which\nrepresents the interplay among excitatory and inhibitory neurons, astrocytes,\nand vascular smooth muscle cells--with a hierarchical description of vascular\ndynamics that couples a three-dimensional microcirculatory network with a\none-dimensional macrocirculatory representation and a zero-dimensional synaptic\ncomponent. By linking neuronal spiking, astrocytic calcium and gliotransmitter\nsignaling, and vascular tone regulation, our model reproduces key features of\nfunctional hyperemia and elucidates the feedback loops that help maintain\ncerebral blood flow. Simulation results demonstrate that neurotransmitter\nrelease triggers astrocytic responses that modulate vessel radius to optimize\noxygen and nutrient delivery. This integrated framework, to our knowledge the\nfirst model to combine these elements for the NGVU in the DVC, provides a\nrobust and modular platform for future investigations into the pathophysiology\nof cerebral blood flow regulation and its role in autonomic control, including\nthe regulation of stomach function.",
        "published": "2025-04-03T12:42:18+00:00"
    },
    {
        "title": "A Sensorimotor Vision Transformer",
        "authors": [
            "Konrad Gadzicki",
            "Kerstin Schill",
            "Christoph Zetzsche"
        ],
        "summary": "This paper presents the Sensorimotor Transformer (SMT), a vision model\ninspired by human saccadic eye movements that prioritize high-saliency regions\nin visual input to enhance computational efficiency and reduce memory\nconsumption. Unlike traditional models that process all image patches\nuniformly, SMT identifies and selects the most salient patches based on\nintrinsic two-dimensional (i2D) features, such as corners and occlusions, which\nare known to convey high-information content and align with human fixation\npatterns. The SMT architecture uses this biological principle to leverage\nvision transformers to process only the most informative patches, allowing for\na substantial reduction in memory usage that scales with the sequence length of\nselected patches. This approach aligns with visual neuroscience findings,\nsuggesting that the human visual system optimizes information gathering through\nselective, spatially dynamic focus. Experimental evaluations on Imagenet-1k\ndemonstrate that SMT achieves competitive top-1 accuracy while significantly\nreducing memory consumption and computational complexity, particularly when a\nlimited number of patches is used. This work introduces a saccade-like\nselection mechanism into transformer-based vision models, offering an efficient\nalternative for image analysis and providing new insights into biologically\nmotivated architectures for resource-constrained applications.",
        "published": "2025-04-03T12:37:44+00:00"
    },
    {
        "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
        "authors": [
            "Vincenzo Petrolo",
            "Flavia Guella",
            "Michele Caon",
            "Pasquale Davide Schiavone",
            "Guido Masera",
            "Maurizio Martina"
        ],
        "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
        "published": "2025-04-03T12:36:01+00:00"
    },
    {
        "title": "RAFFLE: Active learning accelerated interface structure prediction",
        "authors": [
            "Ned Thaddeus Taylor",
            "Joe Pitfield",
            "Francis Huw Davies",
            "Steven Paul Hepplestone"
        ],
        "summary": "Interfaces between materials play a crucial role in the performance of most\ndevices. However, predicting the structure of a material interface is\ncomputationally demanding due to the vast configuration space, which requires\nevaluating an unfeasibly large number of highly complex structures. We\nintroduce RAFFLE, a software package designed to efficiently explore low-energy\ninterface configurations between any two crystals. RAFFLE leverages physical\ninsights and genetic algorithms to intelligently sample the configuration\nspace, using dynamically evolving 2-, 3-, and 4-body distribution functions as\ngeneralised structural descriptors. These descriptors are iteratively updated\nthrough active learning, which inform atom placement strategies. RAFFLE's\neffectiveness is demonstrated across a diverse set of systems, including bulk\nmaterials, intercalation structures, and interfaces. When tested on bulk\naluminium and MoS$_2$, it successfully identifies known ground-state and\nhigh-pressure phases. Applied to intercalation systems, it predicts stable\nintercalant phases. For Si|Ge interfaces, RAFFLE identifies intermixing as a\nstrain compensation mechanism, generating reconstructions that are more stable\nthan abrupt interfaces. By accelerating interface structure prediction, RAFFLE\noffers a powerful tool for materials discovery, enabling efficient exploration\nof complex configuration spaces.",
        "published": "2025-04-03T12:33:31+00:00"
    },
    {
        "title": "SelfMedHPM: Self Pre-training With Hard Patches Mining Masked Autoencoders For Medical Image Segmentation",
        "authors": [
            "Yunhao Lv",
            "Lingyu Chen",
            "Jian Wang",
            "Yangxi Li",
            "Fang Chen"
        ],
        "summary": "In recent years, deep learning methods such as convolutional neural network\n(CNN) and transformers have made significant progress in CT multi-organ\nsegmentation. However, CT multi-organ segmentation methods based on masked\nimage modeling (MIM) are very limited. There are already methods using MAE for\nCT multi-organ segmentation task, we believe that the existing methods do not\nidentify the most difficult areas to reconstruct. To this end, we propose a MIM\nself-training framework with hard patches mining masked autoencoders for CT\nmulti-organ segmentation tasks (selfMedHPM). The method performs ViT\nself-pretraining on the training set of the target data and introduces an\nauxiliary loss predictor, which first predicts the patch loss and determines\nthe location of the next mask. SelfMedHPM implementation is better than various\ncompetitive methods in abdominal CT multi-organ segmentation and body CT\nmulti-organ segmentation. We have validated the performance of our method on\nthe Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for abdomen\nmult-organ segmentation and the SinoMed Whole Body (SMWB) dataset for body\nmulti-organ segmentation tasks.",
        "published": "2025-04-03T12:28:21+00:00"
    },
    {
        "title": "Denoising medium resolution stellar spectra with U-Net convolutional neural networks",
        "authors": [
            "Bal\u00e1zs P\u00e1l",
            "L\u00e1szl\u00f3 Dobos"
        ],
        "summary": "We investigated the use of a U-Net convolutional neural network for denoising\nsimulated medium-resolution spectroscopic observations of stars. Simulated\nspectra were generated under realistic observational conditions resembling the\nSubaru Prime Focus Spectrograph (PFS). We found that our U-Net model\neffectively captured spectral features, achieving an average relative error of\naround $1\\%$ across a broad range of stellar parameters, despite a limited\ntraining set of only $1000$ observations and a relatively short training\nperiod. Although U-Net did not reach the performance previously demonstrated by\nfully-connected denoising autoencoders (DAEs) consisting of dense layers and\ntrained extensively on larger datasets, it outperformed dense networks trained\nunder similarly constrained conditions. These results indicate that the U-Net\narchitecture offers rapid, robust feature learning and may be particularly\nadvantageous in scenarios involving initial denoising, subsequently refined by\nmore accurate, but otherwise slower deep-learning models.",
        "published": "2025-04-03T12:27:14+00:00"
    },
    {
        "title": "Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework",
        "authors": [
            "Christian Alexander Holz",
            "Christian Bader",
            "Markus Enzweiler",
            "Matthias Dr\u00fcppel"
        ],
        "summary": "This paper presents novel Machine Learning (ML) methodologies for\nMulti-Object Tracking (MOT), specifically designed to meet the increasing\ncomplexity and precision demands of Advanced Driver Assistance Systems (ADAS).\nWe introduce three Neural Network (NN) models that address key challenges in\nMOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii)\nthe Single-Association Network (SANT) for mapping individual Sensor Object (SO)\nto existing tracks, and (iii) the Multi-Association Network (MANTa) for\nassociating multiple SOs to multiple tracks. These models are seamlessly\nintegrated into a traditional Kalman Filter (KF) framework, maintaining the\nsystem's modularity by replacing relevant components without disrupting the\noverall architecture. Importantly, all three networks are designed to be run in\na realtime, embedded environment. Each network contains less than 50k trainable\nparameters. Our evaluation, conducted on the public KITTI tracking dataset,\ndemonstrates significant improvements in tracking performance. SPENT reduces\nthe Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT\nand MANTa achieve up to 95% accuracy in sensor object-to-track assignments.\nThese results underscore the effectiveness of incorporating task-specific NNs\ninto traditional tracking systems, boosting performance and robustness while\npreserving modularity, maintainability, and interpretability.",
        "published": "2025-04-03T12:13:38+00:00"
    },
    {
        "title": "MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields",
        "authors": [
            "Yash Kulthe",
            "Andrew Gilbert",
            "John Collomosse"
        ],
        "summary": "We present MultiNeRF, a 3D watermarking method that embeds multiple uniquely\nkeyed watermarks within images rendered by a single Neural Radiance Field\n(NeRF) model, whilst maintaining high visual quality. Our approach extends the\nTensoRF NeRF model by incorporating a dedicated watermark grid alongside the\nexisting geometry and appearance grids. This extension ensures higher watermark\ncapacity without entangling watermark signals with scene content. We propose a\nFiLM-based conditional modulation mechanism that dynamically activates\nwatermarks based on input identifiers, allowing multiple independent watermarks\nto be embedded and extracted without requiring model retraining. MultiNeRF is\nvalidated on the NeRF-Synthetic and LLFF datasets, with statistically\nsignificant improvements in robust capacity without compromising rendering\nquality. By generalizing single-watermark NeRF methods into a flexible\nmulti-watermarking framework, MultiNeRF provides a scalable solution for 3D\ncontent. attribution.",
        "published": "2025-04-03T12:06:04+00:00"
    },
    {
        "title": "Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay",
        "authors": [
            "Hussain Jagirdar",
            "Rukma Talwadker",
            "Aditya Pareek",
            "Pulkit Agrawal",
            "Tridib Mukherjee"
        ],
        "summary": "Multi-variate Time Series (MTS) forecasting has made large strides (with very\nnegligible errors) through recent advancements in neural networks, e.g.,\nTransformers. However, in critical situations like predicting gaming\noverindulgence that affects one's mental well-being; an accurate forecast\nwithout a contributing evidence (explanation) is irrelevant. Hence, it becomes\nimportant that the forecasts are Interpretable - intermediate representation of\nthe forecasted trajectory is comprehensible; as well as Explainable - attentive\ninput features and events are accessible for a personalized and timely\nintervention of players at risk. While the contributing state of the art\nresearch on interpretability primarily focuses on temporally-smooth\nsingle-process driven time series data, our online multi-player gameplay data\ndemonstrates intractable temporal randomness due to intrinsic orthogonality\nbetween player's game outcome and their intent to engage further. We introduce\na novel deep Actionable Forecasting Network (AFN), which addresses the\ninter-dependent challenges associated with three exclusive objectives - 1)\nforecasting accuracy; 2) smooth comprehensible trajectory and 3) explanations\nvia multi-dimensional input features while tackling the challenges introduced\nby our non-smooth temporal data, together in one single solution. AFN\nestablishes a \\it{new benchmark} via: (i) achieving 25% improvement on the MSE\nof the forecasts on player data in comparison to the SOM-VAE based SOTA\nnetworks; (ii) attributing unfavourable progression of a player's time series\nto a specific future time step(s), with the premise of eliminating near-future\noverindulgent player volume by over 18% with player specific actionable inputs\nfeature(s) and (iii) proactively detecting over 23% (100% jump from SOTA) of\nthe to-be overindulgent, players on an average, 4 weeks in advance.",
        "published": "2025-04-03T11:49:24+00:00"
    },
    {
        "title": "APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers",
        "authors": [
            "Zhuguanyu Wu",
            "Jiayi Zhang",
            "Jiaxin Chen",
            "Jinyang Guo",
            "Di Huang",
            "Yunhong Wang"
        ],
        "summary": "Vision Transformers (ViTs) have become one of the most commonly used\nbackbones for vision tasks. Despite their remarkable performance, they often\nsuffer significant accuracy drops when quantized for practical deployment,\nparticularly by post-training quantization (PTQ) under ultra-low bits.\nRecently, reconstruction-based PTQ methods have shown promising performance in\nquantizing Convolutional Neural Networks (CNNs). However, they fail when\napplied to ViTs, primarily due to the inaccurate estimation of output\nimportance and the substantial accuracy degradation in quantizing post-GELU\nactivations. To address these issues, we propose \\textbf{APHQ-ViT}, a novel PTQ\napproach based on importance estimation with Average Perturbation Hessian\n(APH). Specifically, we first thoroughly analyze the current approximation\napproaches with Hessian loss, and propose an improved average perturbation\nHessian loss. To deal with the quantization of the post-GELU activations, we\ndesign an MLP Reconstruction (MR) method by replacing the GELU function in MLP\nwith ReLU and reconstructing it by the APH loss on a small unlabeled\ncalibration set. Extensive experiments demonstrate that APHQ-ViT using linear\nquantizers outperforms existing PTQ methods by substantial margins in 3-bit and\n4-bit across different vision tasks. The source code is available at\nhttps://github.com/GoatWu/APHQ-ViT.",
        "published": "2025-04-03T11:48:56+00:00"
    },
    {
        "title": "Secrecy Performance of a Keyhole-based Multi-user System with Multiple Eavesdroppers",
        "authors": [
            "Parwez Alam",
            "Ankit Dubey",
            "Jules M. Moualeu",
            "Telex M. N. Ngatched",
            "Chinmoy Kundu"
        ],
        "summary": "This paper investigates the secrecy performance of a keyhole-aided multi-user\ncommunication network in the presence of multiple eavesdroppers. The\ncommunication happens through the same keyhole for legitimate users and\neavesdroppers. In this context, the secrecy performance is evaluated for a user\nscheduling technique by obtaining the exact closed-form expression of secrecy\noutage probability (SOP). Further, a simplified asymptotic SOP expression is\nderived assuming high signal-to-noise ratio (SNR) scenario for a better\nunderstanding of the impact of system parameters. The effect of the keyhole\nparameters, number of users, number of eavesdroppers, and threshold secrecy\nrate on the SOP performance are also investigated for the considered system\nmodel. In the high-SNR regime, the asymptotic SOP saturates to a constant value\nand does not depend on the keyhole parameter and the channel parameter of the\nsource-to-keyhole channel.",
        "published": "2025-04-03T11:38:08+00:00"
    },
    {
        "title": "Direction switchable single-photon emitter using a Rydberg polariton",
        "authors": [
            "Changcheng Li",
            "Xiao-Feng Shi",
            "Yuechun Jiao",
            "Jiuheng Yang",
            "Jingxu Bai",
            "C. Stuart Adams",
            "Suotang Jia",
            "Jianming Zhao"
        ],
        "summary": "All-optical redirection or routing of single photons is essential for quantum\nnetworks. Although studied in various systems both in theory and experiment,\nthe redirection of single photons with many output ports, compatible with\nlarge-scale photonic circuits, still needs to be explored. Here, we demonstrate\na direction switchable single-photon emitter using a Rydberg polariton. The\nRydberg component of the stored photon is changed using a stimulated Raman\ntransition with a specific intermediate state. By adjusting the direction of\nthe retrieval laser, we can redirect the emitted photon into a rich variety of\nalternative modes. Building upon this scheme, we propose a quantum routing of\nsingle photons with \\textit{N} output channels and unity routing efficiency. In\naddition, the protocol reduces the effect of motional dephasing increasing the\nphoton lifetime to $>10~\\mu$s ($>20$ times photon processing time), enabling\nfunctional quantum devices based on Rydberg polaritons.",
        "published": "2025-04-03T11:35:36+00:00"
    },
    {
        "title": "An Overview of Josephson Junctions Based QPUs",
        "authors": [
            "Omid Mohebi",
            "Alireza Hesam Mohseni"
        ],
        "summary": "Quantum processing units (QPUs) based on superconducting Josephson junctions\npromise significant advances in quantum computing. However, they face critical\nchallenges. Decoherence, scalability limitations, and error correction overhead\nhinder practical, fault-tolerant implementations. This paper investigates these\nissues by exploring both fundamental quantum phenomena and practical\nengineering challenges. We analyze key quantum mechanical principles such as\nsuperposition, entanglement, and decoherence that govern the behavior of\nsuperconducting qubits. We also discuss quantum tunneling, Cooper pair\nformation, and the operational mechanics of Josephson junctions in detail.\nAdditionally, we present a comparative analysis with alternative architectures,\nincluding ion trap and photonic systems. This comparison highlights the unique\nadvantages and trade-offs of Josephson junction-based QPUs. Our findings\nemphasize the critical role of material innovations and optimized control\ntechniques. These advances are essential for mitigating noise and decoherence\nand for realizing robust, scalable quantum computing.",
        "published": "2025-04-03T11:26:52+00:00"
    },
    {
        "title": "On Word-of-Mouth and Private-Prior Sequential Social Learning",
        "authors": [
            "Andrea Da Col",
            "Cristian R. Rojas",
            "Vikram Krishnamurthy"
        ],
        "summary": "Social learning provides a fundamental framework in economics and social\nsciences for studying interactions among rational agents who observe each\nother's actions but lack direct access to individual beliefs. This paper\ninvestigates a specific social learning paradigm known as Word-of-Mouth (WoM),\nwhere a series of agents seeks to estimate the state of a dynamical system. The\nfirst agent receives noisy measurements of the state, while each subsequent\nagent relies solely on a degraded version of her predecessor's estimate. A\ndefining feature of WoM is that the final agent's belief is publicly broadcast\nand adopted by all agents, in place of their own. We analyze this setting both\ntheoretically and through numerical simulations, showing that some agents\nbenefit from using the public belief broadcast by the last agent, while others\nsuffer from performance deterioration.",
        "published": "2025-04-03T11:20:53+00:00"
    },
    {
        "title": "Semiconductor Wafer Map Defect Classification with Tiny Vision Transformers",
        "authors": [
            "Faisal Mohammad",
            "Duksan Ryu"
        ],
        "summary": "Semiconductor wafer defect classification is critical for ensuring high\nprecision and yield in manufacturing. Traditional CNN-based models often\nstruggle with class imbalances and recognition of the multiple overlapping\ndefect types in wafer maps. To address these challenges, we propose ViT-Tiny, a\nlightweight Vision Transformer (ViT) framework optimized for wafer defect\nclassification. Trained on the WM-38k dataset. ViT-Tiny outperforms its\nViT-Base counterpart and state-of-the-art (SOTA) models, such as MSF-Trans and\nCNN-based architectures. Through extensive ablation studies, we determine that\na patch size of 16 provides optimal performance. ViT-Tiny achieves an F1-score\nof 98.4%, surpassing MSF-Trans by 2.94% in four-defect classification,\nimproving recall by 2.86% in two-defect classification, and increasing\nprecision by 3.13% in three-defect classification. Additionally, it\ndemonstrates enhanced robustness under limited labeled data conditions, making\nit a computationally efficient and reliable solution for real-world\nsemiconductor defect detection.",
        "published": "2025-04-03T11:18:00+00:00"
    },
    {
        "title": "Industrial Internet Robot Collaboration System and Edge Computing Optimization",
        "authors": [
            "Qian Zuo",
            "Dajun Tao",
            "Tian Qi",
            "Jieyi Xie",
            "Zijie Zhou",
            "Zhen Tian",
            "Yu Mingyu"
        ],
        "summary": "In a complex environment, for a mobile robot to safely and collision - free\navoid all obstacles, it poses high requirements for its intelligence level.\nGiven that the information such as the position and geometric characteristics\nof obstacles is random, the control parameters of the robot, such as velocity\nand angular velocity, are also prone to random deviations. To address this\nissue in the framework of the Industrial Internet Robot Collaboration System,\nthis paper proposes a global path control scheme for mobile robots based on\ndeep learning. First of all, the dynamic equation of the mobile robot is\nestablished. According to the linear velocity and angular velocity of the\nmobile robot, its motion behaviors are divided into obstacle - avoidance\nbehavior, target - turning behavior, and target approaching behavior.\nSubsequently, the neural network method in deep learning is used to build a\nglobal path planning model for the robot. On this basis, a fuzzy controller is\ndesigned with the help of a fuzzy control algorithm to correct the deviations\nthat occur during path planning, thereby achieving optimized control of the\nrobot's global path. In addition, considering edge computing optimization, the\nproposed model can process local data at the edge device, reducing the\ncommunication burden between the robot and the central server, and improving\nthe real time performance of path planning. The experimental results show that\nfor the mobile robot controlled by the research method in this paper, the\ndeviation distance of the path angle is within 5 cm, the deviation convergence\ncan be completed within 10 ms, and the planned path is shorter. This indicates\nthat the proposed scheme can effectively improve the global path planning\nability of mobile robots in the industrial Internet environment and promote the\ncollaborative operation of robots through edge computing optimization.",
        "published": "2025-04-03T11:15:10+00:00"
    },
    {
        "title": "The Self-Learning Agent with a Progressive Neural Network Integrated Transformer",
        "authors": [
            "Ajay Sivakumar",
            "Shalini",
            "Vasantha Raj",
            "Sebastian Sylvester"
        ],
        "summary": "This paper introduces a self-learning agent that integrates LLaMA 3.2 with a\nProgressive Neural Network (PNN) for continual learning in conversational AI\nand code generation. The framework dynamically collects data, fine-tunes tasks\nwith minimal samples, and leverages Meta-Learning for rapid adaptation. LoRA\noptimizes fine-tuning, while Elastic Weight Consolidation (EWC) enhances\nknowledge retention. Experimental results demonstrate improved adaptability and\nmemory stability, positioning this approach as a scalable step toward\nArtificial General Intelligence (AGI).",
        "published": "2025-04-03T11:13:31+00:00"
    },
    {
        "title": "Hybrid neural network method of a multilayer perceptron and autoencoder for the \u03b1-particle preformation factor in \u03b1-decay theory",
        "authors": [
            "Jiaqi Luo",
            "Yang Xu",
            "Xiaolong Li",
            "Junxiang Wang",
            "Yangjie Zhang",
            "Jungang Deng",
            "Fang Zhang",
            "Nana Ma"
        ],
        "summary": "The preformation factor quantifies the probability of {\\alpha} particles\npreforming on the surface of the parent nucleus in decay theory and is closely\nrelated to the study of {\\alpha} clustering structure. In this work, a\nmultilayer perceptron and autoencoder (MLP + AE) hybrid neural network method\nis introduced to extract preformation factors within the generalized liquid\ndrop model and experimental data. A K-fold cross validation method is also\nadopted. The accuracy of the preformation factor calculated by this improved\nneural network is comparable to the results of the empirical formula. MLP + AE\ncan effectively capture the linear relationship between the logarithm of the\npreformation factor and the square root of the ratio of the decay energy,\nfurther verifying that Geiger-Nuttall law can deal with preformation factor.\nThe extracted preformation probability of isotope and isotone chains show\ndifferent trends near the magic number, and in addition, an odd-even staggering\neffect appears. This means that the preformation factors are affected by closed\nshells and unpaired nucleons. Therefore the preformation factors can provide\nnuclear structure information. Furthermore, for 41 new nuclides, the half-lives\nintroduced with the preformation factors reproduce the experimental values as\nexpected. Finally, the preformation factors and {\\alpha}-decay half-lives of Z\n= 119 and 120 superheavy nuclei are predicted.",
        "published": "2025-04-03T11:08:49+00:00"
    },
    {
        "title": "Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging",
        "authors": [
            "Kyungmin Choi",
            "JaKeoung Koo",
            "Stephen McLaughlin",
            "Abderrahim Halimi"
        ],
        "summary": "Single-photon Lidar imaging offers a significant advantage in 3D imaging due\nto its high resolution and long-range capabilities, however it is challenging\nto apply in noisy environments with multiple targets per pixel. To tackle these\nchallenges, several methods have been proposed. Statistical methods demonstrate\ninterpretability on the inferred parameters, but they are often limited in\ntheir ability to handle complex scenes. Deep learning-based methods have shown\nsuperior performance in terms of accuracy and robustness, but they lack\ninterpretability or they are limited to a single-peak per pixel. In this paper,\nwe propose a deep unrolling algorithm for dual-peak single-photon Lidar\nimaging. We introduce a hierarchical Bayesian model for multiple targets and\npropose a neural network that unrolls the underlying statistical method. To\nsupport multiple targets, we adopt a dual depth maps representation and exploit\ngeometric deep learning to extract features from the point cloud. The proposed\nmethod takes advantages of statistical methods and learning-based methods in\nterms of accuracy and quantifying uncertainty. The experimental results on\nsynthetic and real data demonstrate the competitive performance when compared\nto existing methods, while also providing uncertainty information.",
        "published": "2025-04-03T10:57:26+00:00"
    },
    {
        "title": "Hierarchical Policy-Gradient Reinforcement Learning for Multi-Agent Shepherding Control of Non-Cohesive Targets",
        "authors": [
            "Stefano Covone",
            "Italo Napolitano",
            "Francesco De Lellis",
            "Mario di Bernardo"
        ],
        "summary": "We propose a decentralized reinforcement learning solution for multi-agent\nshepherding of non-cohesive targets using policy-gradient methods. Our\narchitecture integrates target-selection with target-driving through Proximal\nPolicy Optimization, overcoming discrete-action constraints of previous Deep\nQ-Network approaches and enabling smoother agent trajectories. This model-free\nframework effectively solves the shepherding problem without prior dynamics\nknowledge. Experiments demonstrate our method's effectiveness and scalability\nwith increased target numbers and limited sensing capabilities.",
        "published": "2025-04-03T10:56:57+00:00"
    },
    {
        "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
        "authors": [
            "Xiaofeng Han",
            "Shunpeng Chen",
            "Zenghuang Fu",
            "Zhe Feng",
            "Lue Fan",
            "Dong An",
            "Changwei Wang",
            "Li Guo",
            "Weiliang Meng",
            "Xiaopeng Zhang",
            "Rongtao Xu",
            "Shibiao Xu"
        ],
        "summary": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We systematically review the\napplications of multimodal fusion in key robotic vision tasks, including\nsemantic scene understanding, simultaneous localization and mapping (SLAM), 3D\nobject detection, navigation and localization, and robot manipulation. We\ncompare VLMs based on large language models (LLMs) with traditional multimodal\nfusion methods, analyzing their advantages, limitations, and synergies.\nAdditionally, we conduct an in-depth analysis of commonly used datasets,\nevaluating their applicability and challenges in real-world robotic scenarios.\nFurthermore, we identify critical research challenges such as cross-modal\nalignment, efficient fusion strategies, real-time deployment, and domain\nadaptation, and propose future research directions, including self-supervised\nlearning for robust multimodal representations, transformer-based fusion\narchitectures, and scalable multimodal frameworks. Through a comprehensive\nreview, comparative analysis, and forward-looking discussion, we provide a\nvaluable reference for advancing multimodal perception and interaction in\nrobotic vision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV.",
        "published": "2025-04-03T10:53:07+00:00"
    },
    {
        "title": "Advancing Air Quality Monitoring: TinyML-Based Real-Time Ozone Prediction with Cost-Effective Edge Devices",
        "authors": [
            "Huam Ming Ken",
            "Mehran Behjati"
        ],
        "summary": "The escalation of urban air pollution necessitates innovative solutions for\nreal-time air quality monitoring and prediction. This paper introduces a novel\nTinyML-based system designed to predict ozone concentration in real-time. The\nsystem employs an Arduino Nano 33 BLE Sense microcontroller equipped with an\nMQ7 sensor for carbon monoxide (CO) detection and built-in sensors for\ntemperature and pressure measurements. The data, sourced from a Kaggle dataset\non air quality parameters from India, underwent thorough cleaning and\npreprocessing. Model training and evaluation were performed using Edge Impulse,\nconsidering various combinations of input parameters (CO, temperature, and\npressure). The optimal model, incorporating all three variables, achieved a\nmean squared error (MSE) of 0.03 and an R-squared value of 0.95, indicating\nhigh predictive accuracy. The regression model was deployed on the\nmicrocontroller via the Arduino IDE, showcasing robust real-time performance.\nSensitivity analysis identified CO levels as the most critical predictor of\nozone concentration, followed by pressure and temperature. The system's\nlow-cost and low-power design makes it suitable for widespread implementation,\nparticularly in resource-constrained settings. This TinyML approach provides\nprecise real-time predictions of ozone levels, enabling prompt responses to\npollution events and enhancing public health protection.",
        "published": "2025-04-03T10:48:24+00:00"
    },
    {
        "title": "Adaptive path planning for efficient object search by UAVs in agricultural fields",
        "authors": [
            "Rick van Essen",
            "Eldert van Henten",
            "Lammert Kooistra",
            "Gert Kootstra"
        ],
        "summary": "This paper presents an adaptive path planner for object search in\nagricultural fields using UAVs. The path planner uses a high-altitude coverage\nflight path and plans additional low-altitude inspections when the detection\nnetwork is uncertain. The path planner was evaluated in an offline simulation\nenvironment containing real-world images. We trained a YOLOv8 detection network\nto detect artificial plants placed in grass fields to showcase the potential of\nour path planner. We evaluated the effect of different detection certainty\nmeasures, optimized the path planning parameters, investigated the effects of\nlocalization errors and different numbers of objects in the field. The YOLOv8\ndetection confidence worked best to differentiate between true and false\npositive detections and was therefore used in the adaptive planner. The optimal\nparameters of the path planner depended on the distribution of objects in the\nfield, when the objects were uniformly distributed, more low-altitude\ninspections were needed compared to a non-uniform distribution of objects,\nresulting in a longer path length. The adaptive planner proved to be robust\nagainst localization uncertainty. When increasing the number of objects, the\nflight path length increased, especially when the objects were uniformly\ndistributed. When the objects were non-uniformly distributed, the adaptive path\nplanner yielded a shorter path than a low-altitude coverage path, even with\nhigh number of objects. Overall, the presented adaptive path planner allowed to\nfind non-uniformly distributed objects in a field faster than a coverage path\nplanner and resulted in a compatible detection accuracy. The path planner is\nmade available at https://github.com/wur-abe/uav_adaptive_planner.",
        "published": "2025-04-03T10:47:31+00:00"
    },
    {
        "title": "Enhancing Air Quality Monitoring: A Brief Review of Federated Learning Advances",
        "authors": [
            "Sara Yarham",
            "Mehran Behjati"
        ],
        "summary": "Monitoring air quality and environmental conditions is crucial for public\nhealth and effective urban planning. Current environmental monitoring\napproaches often rely on centralized data collection and processing, which pose\nsignificant privacy, security, and scalability challenges. Federated Learning\n(FL) offers a promising solution to these limitations by enabling collaborative\nmodel training across multiple devices without sharing raw data. This\ndecentralized approach addresses privacy concerns while still leveraging\ndistributed data sources. This paper provides a comprehensive review of FL\napplications in air quality and environmental monitoring, emphasizing its\neffectiveness in predicting pollutants and managing environmental data.\nHowever, the paper also identifies key limitations of FL when applied in this\ndomain, including challenges such as communication overhead, infrastructure\ndemands, generalizability issues, computational complexity, and security\nvulnerabilities. For instance, communication overhead, caused by the frequent\nexchange of model updates between local devices and central servers, is a\nnotable challenge. To address this, future research should focus on optimizing\ncommunication protocols and reducing the frequency of updates to lessen the\nburden on network resources. Additionally, the paper suggests further research\ndirections to refine FL frameworks and enhance their applicability in\nreal-world environmental monitoring scenarios. By synthesizing findings from\nexisting studies, this paper highlights the potential of FL to improve air\nquality management while maintaining data privacy and security, and it provides\nvaluable insights for future developments in the field.",
        "published": "2025-04-03T10:36:02+00:00"
    },
    {
        "title": "Finite steady-state current defies non-Hermitian many-body localization",
        "authors": [
            "Pietro Brighi",
            "Marko Ljubotina",
            "Federico Roccati",
            "Federico Balducci"
        ],
        "summary": "Non-Hermitian many-body localization (NH MBL) has emerged as a possible\nscenario for stable localization in open systems, as suggested by spectral\nindicators identifying a putative transition for finite system sizes.\n  In this work, we shift the focus to dynamical probes, specifically the\nsteady-state spin current, to investigate transport properties in a disordered,\nnon-Hermitian XXZ spin chain. Through exact diagonalization for small systems\nand tensor-network methods for larger chains, we demonstrate that the\nsteady-state current remains finite and decays exponentially with disorder\nstrength, showing no evidence of a transition up to disorder values far beyond\nthe previously claimed critical point. Our results reveal a stark discrepancy\nbetween spectral indicators, which suggest localization, and transport\nbehavior, which indicates delocalization. This highlights the importance of\ndynamical observables in characterizing NH MBL and suggests that traditional\nspectral measures may not fully capture the physics of non-Hermitian systems.\n  Additionally, we observe a non-commutativity of limits in system size and\ntime, further complicating the interpretation of finite-size studies. These\nfindings challenge the existence of NH MBL in the studied model and underscore\nthe need for alternative approaches to understand localization in non-Hermitian\nsettings.",
        "published": "2025-04-03T10:24:39+00:00"
    },
    {
        "title": "A Physics-Informed Meta-Learning Framework for the Continuous Solution of Parametric PDEs on Arbitrary Geometries",
        "authors": [
            "Reza Najian Asl",
            "Yusuke Yamazaki",
            "Kianoosh Taghikhani",
            "Mayu Muramatsu",
            "Markus Apel",
            "Shahed Rezaei"
        ],
        "summary": "In this work, we introduce implicit Finite Operator Learning (iFOL) for the\ncontinuous and parametric solution of partial differential equations (PDEs) on\narbitrary geometries. We propose a physics-informed encoder-decoder network to\nestablish the mapping between continuous parameter and solution spaces. The\ndecoder constructs the parametric solution field by leveraging an implicit\nneural field network conditioned on a latent or feature code. Instance-specific\ncodes are derived through a PDE encoding process based on the second-order\nmeta-learning technique. In training and inference, a physics-informed loss\nfunction is minimized during the PDE encoding and decoding. iFOL expresses the\nloss function in an energy or weighted residual form and evaluates it using\ndiscrete residuals derived from standard numerical PDE methods. This approach\nresults in the backpropagation of discrete residuals during both training and\ninference.\n  iFOL features several key properties: (1) its unique loss formulation\neliminates the need for the conventional encode-process-decode pipeline\npreviously used in operator learning with conditional neural fields for PDEs;\n(2) it not only provides accurate parametric and continuous fields but also\ndelivers solution-to-parameter gradients without requiring additional loss\nterms or sensitivity analysis; (3) it can effectively capture sharp\ndiscontinuities in the solution; and (4) it removes constraints on the geometry\nand mesh, making it applicable to arbitrary geometries and spatial sampling\n(zero-shot super-resolution capability). We critically assess these features\nand analyze the network's ability to generalize to unseen samples across both\nstationary and transient PDEs. The overall performance of the proposed method\nis promising, demonstrating its applicability to a range of challenging\nproblems in computational mechanics.",
        "published": "2025-04-03T10:24:00+00:00"
    },
    {
        "title": "QPanda3: A High-Performance Software-Hardware Collaborative Framework for Large-Scale Quantum-Classical Computing Integration",
        "authors": [
            "Tianrui Zou",
            "Yuan Fang",
            "Jing Wang",
            "Menghan Dou",
            "Jun Fu",
            "ZiQiang Zhao",
            "ShuBin Zhao",
            "Lei Yu",
            "Dongyi Zhao",
            "Zhaoyun Chen",
            "Guoping Guo"
        ],
        "summary": "QPanda3 is a high-performance quantum programming framework that enhances\nquantum computing efficiency through optimized circuit compilation, an advanced\ninstruction stream format (OriginBIS), and hardware-aware execution strategies.\nThese engineering optimizations significantly improve both processing speed and\nsystem performance, addressing key challenges in the NISQ era. A core\ninnovation, OriginBIS, accelerates encoding speeds by up to 86.9x compared to\nOpenQASM 2.0, while decoding is 35.6x faster, leading to more efficient data\nhandling, reduced memory overhead, and improved communication efficiency. This\ndirectly enhances the execution of quantum circuits, making large-scale quantum\nsimulations more feasible. Comprehensive benchmarking demonstrates QPanda3's\nsuperior performance: quantum circuit construction is 20.7x faster, execution\nspeeds improve by 3.4x, and transpilation efficiency increases by 14.97x over\nQiskit. Notably, in compiling a 118-qubit W-state circuit on a 2D-grid\ntopology, QPanda3 achieves an unprecedented 869.9x speedup, underscoring its\nability to handle complex quantum workloads at scale. By combining high-speed\nquantum processing with a modular and extensible software architecture, QPanda3\nprovides a practical bridge between today's NISQ devices and future\nfault-tolerant quantum computing. It facilitates real-world applications in\nfinancial modeling, materials science, and combinatorial optimization, while\nits robust and scalable design supports industrial adoption and cloud-based\ndeployment.",
        "published": "2025-04-03T10:20:16+00:00"
    },
    {
        "title": "Taylor Series-Inspired Local Structure Fitting Network for Few-shot Point Cloud Semantic Segmentation",
        "authors": [
            "Changshuo Wang",
            "Shuting He",
            "Xiang Fang",
            "Meiqing Wu",
            "Siew-Kei Lam",
            "Prayag Tiwari"
        ],
        "summary": "Few-shot point cloud semantic segmentation aims to accurately segment\n\"unseen\" new categories in point cloud scenes using limited labeled data.\nHowever, pretraining-based methods not only introduce excessive time overhead\nbut also overlook the local structure representation among irregular point\nclouds. To address these issues, we propose a pretraining-free local structure\nfitting network for few-shot point cloud semantic segmentation, named\nTaylorSeg. Specifically, inspired by Taylor series, we treat the local\nstructure representation of irregular point clouds as a polynomial fitting\nproblem and propose a novel local structure fitting convolution, called\nTaylorConv. This convolution learns the low-order basic information and\nhigh-order refined information of point clouds from explicit encoding of local\ngeometric structures. Then, using TaylorConv as the basic component, we\nconstruct two variants of TaylorSeg: a non-parametric TaylorSeg-NN and a\nparametric TaylorSeg-PN. The former can achieve performance comparable to\nexisting parametric models without pretraining. For the latter, we equip it\nwith an Adaptive Push-Pull (APP) module to mitigate the feature distribution\ndifferences between the query set and the support set. Extensive experiments\nvalidate the effectiveness of the proposed method. Notably, under the 2-way\n1-shot setting, TaylorSeg-PN achieves improvements of +2.28% and +4.37% mIoU on\nthe S3DIS and ScanNet datasets respectively, compared to the previous\nstate-of-the-art methods. Our code is available at\nhttps://github.com/changshuowang/TaylorSeg.",
        "published": "2025-04-03T10:19:06+00:00"
    },
    {
        "title": "Towards Learning-Augmented Peer-to-Peer Networks: Self-Stabilizing Graph Linearization with Untrusted Advice",
        "authors": [
            "Vijeth Aradhya",
            "Christian Scheideler"
        ],
        "summary": "Distributed peer-to-peer systems are widely popular due to their\ndecentralized nature, which ensures that no peer is critical for the\nfunctionality of the system. However, fully decentralized solutions are usually\nmuch harder to design, and tend to have a much higher overhead compared to\ncentralized approaches, where the peers are connected to a powerful server. On\nthe other hand, centralized approaches have a single point of failure. Thus, is\nthere some way to combine their advantages without inheriting their\ndisadvantages? To that end, we consider a supervised peer-to-peer approach\nwhere the peers can ask a potentially unreliable supervisor for advice. This is\nin line with the increasingly popular algorithmic paradigm called algorithms\nwith predictions or learning-augmented algorithms, but we are the first to\nconsider it in the context of peer-to-peer networks.\n  Specifically, we design self-stabilizing algorithms for the fundamental\nproblem of distributed graph linearization, where peers are supposed to recover\nthe \"sorted line\" network from any initial network after a transient fault.\nWith the help of the supervisor, peers can recover the sorted line network in\n$O(\\log n)$ time, if the advice is correct; otherwise, the algorithm retains\nits original recovery time (i.e., without any supervisor). A crucial challenge\nthat we overcome is to correctly compose multiple self-stabilizing algorithms,\nthat is, one that processes and exploits the advice, and another that does not\nrely on the advice at all. Our key technical contributions combine ideas from\nthe fields of overlay networks and proof-labeling schemes. Finally, we give a\nmatching lower bound of $\\Omega(\\log n)$ for the recovery time of any algorithm\nif the advice can be corrupted, where $n$ is the network size.",
        "published": "2025-04-03T10:07:41+00:00"
    },
    {
        "title": "Revolutionizing Medical Data Transmission with IoMT: A Comprehensive Survey of Wireless Communication Solutions and Future Directions",
        "authors": [
            "Jiasi Zhou",
            "Yanjing Sun",
            "Chintha Tellambura"
        ],
        "summary": "Traditional hospital-based medical examination methods face unprecedented\nchallenges due to the aging global population. The Internet of Medical Things\n(IoMT), an advanced extension of the Internet of Things (IoT) tailored for the\nmedical field, offers a transformative solution for delivering medical care.\nIoMT consists of interconnected medical devices that collect and transmit\npatients' vital signs online. This data can be analyzed to identify potential\nhealth issues, support medical decision-making, enhance patient outcomes, and\nstreamline healthcare operations. Additionally, IoMT helps individuals make\ninformed decisions about their health and fitness. There is a natural synergy\nwith emerging communication technologies to ensure the secure and timely\ntransmission of medical data. This paper presents the first comprehensive\ntutorial on cutting-edge IoMT research focusing on wireless communication-based\nsolutions. It introduces a systematic three-tier framework to analyze IoMT\nnetworks and identify application scenarios. The paper examines the medical\ndata transmission process, including intra-wireless Body Area Networks (WBAN),\ninter-WBAN, and beyond-WBAN communications. It also discusses the challenges of\nimplementing IoMT applications, such as the longevity of biosensors, co-channel\ninterference management, information security, and data processing delays.\nProposed solutions to these challenges are explored from a wireless\ncommunication perspective, and future research directions are outlined. The\nsurvey concludes with a summary of key findings and insights.",
        "published": "2025-04-03T10:00:42+00:00"
    },
    {
        "title": "Improved universal approximation with neural networks studied via affine-invariant subspaces of $L_2(\\mathbb{R}^n)$",
        "authors": [
            "Cornelia Schneider",
            "Samuel Probst"
        ],
        "summary": "We show that there are no non-trivial closed subspaces of $L_2(\\mathbb{R}^n)$\nthat are invariant under invertible affine transformations. We apply this\nresult to neural networks showing that any nonzero $L_2(\\mathbb{R})$ function\nis an adequate activation function in a one hidden layer neural network in\norder to approximate every function in $L_2(\\mathbb{R})$ with any desired\naccuracy. This generalizes the universal approximation properties of neural\nnetworks in $L_2(\\mathbb{R})$ related to Wiener's Tauberian Theorems. Our\nresults extend to the spaces $L_p(\\mathbb{R})$ with $p>1$.",
        "published": "2025-04-03T10:00:40+00:00"
    },
    {
        "title": "Language-Integrated Recursive Queries",
        "authors": [
            "Anna Herlihy",
            "Anastasia Ailamaki",
            "Martin Odersky",
            "Amir Shaikhha"
        ],
        "summary": "Performance-critical industrial applications, including large-scale program,\nnetwork, and distributed system analyses, rely on fixed-point computations. The\nintroduction of recursive common table expressions (CTEs) using the WITH\nRECURSIVE keyword in SQL:1999 extended the ability of relational database\nsystems to handle fixed-point computations, unlocking significant performance\nadvantages by allowing computation to move closer to the data. Yet with\nrecursion, SQL becomes a Turing-complete programming language and, with that,\nunrecoverable safety and correctness risks. SQL itself lacks a fixed semantics,\nas the SQL specification is written in natural language, full of ambiguities\nthat database vendors resolve in divergent ways. As a result, reasoning about\nthe correctness of recursive SQL programs must rely on isolated mathematical\nproperties of queries rather than wrestling a unified formal model out of a\nlanguage with notoriously inconsistent semantics. To address these challenges,\nwe propose a calculus that automatically derives mathematical properties from\nembedded recursive queries and, depending on the database backend, rejects\nqueries that may lead to the three classes of recursive query errors - database\nerrors, incorrect results, and non-termination. We introduce TyQL, a practical\nimplementation in Scala for safe, recursive language-integrated query. Using\nNamed-Tuples and type-level pattern matching, TyQL ensures query portability\nand safety, showing no performance penalty compared to raw SQL strings while\nunlocking a three-orders-of-magnitude speedup over non-recursive SQL queries.",
        "published": "2025-04-03T09:58:52+00:00"
    },
    {
        "title": "OmniTalker: Real-Time Text-Driven Talking Head Generation with In-Context Audio-Visual Style Replication",
        "authors": [
            "Zhongjian Wang",
            "Peng Zhang",
            "Jinwei Qi",
            "Guangyuan Wang Sheng Xu",
            "Bang Zhang",
            "Liefeng Bo"
        ],
        "summary": "Recent years have witnessed remarkable advances in talking head generation,\nowing to its potential to revolutionize the human-AI interaction from text\ninterfaces into realistic video chats. However, research on text-driven talking\nheads remains underexplored, with existing methods predominantly adopting a\ncascaded pipeline that combines TTS systems with audio-driven talking head\nmodels. This conventional pipeline not only introduces system complexity and\nlatency overhead but also fundamentally suffers from asynchronous audiovisual\noutput and stylistic discrepancies between generated speech and visual\nexpressions. To address these limitations, we introduce OmniTalker, an\nend-to-end unified framework that simultaneously generates synchronized speech\nand talking head videos from text and reference video in real-time zero-shot\nscenarios, while preserving both speech style and facial styles. The framework\nemploys a dual-branch diffusion transformer architecture: the audio branch\nsynthesizes mel-spectrograms from text, while the visual branch predicts\nfine-grained head poses and facial dynamics. To bridge modalities, we introduce\na novel audio-visual fusion module that integrates cross-modal information to\nensure temporal synchronization and stylistic coherence between audio and\nvisual outputs. Furthermore, our in-context reference learning module\neffectively captures both speech and facial style characteristics from a single\nreference video without introducing an extra style extracting module. To the\nbest of our knowledge, OmniTalker presents the first unified framework that\njointly models speech style and facial style in a zero-shot setting, achieving\nreal-time inference speed of 25 FPS. Extensive experiments demonstrate that our\nmethod surpasses existing approaches in generation quality, particularly\nexcelling in style preservation and audio-video synchronization.",
        "published": "2025-04-03T09:48:13+00:00"
    },
    {
        "title": "How Artificial Intelligence Leads to Knowledge Why: An Inquiry Inspired by Aristotle's Posterior Analytics",
        "authors": [
            "Guus Eelink",
            "Kilian R\u00fcckschlo\u00df",
            "Felix Weitk\u00e4mper"
        ],
        "summary": "Bayesian networks and causal models provide frameworks for handling queries\nabout external interventions and counterfactuals, enabling tasks that go beyond\nwhat probability distributions alone can address. While these formalisms are\noften informally described as capturing causal knowledge, there is a lack of a\nformal theory characterizing the type of knowledge required to predict the\neffects of external interventions. This work introduces the theoretical\nframework of causal systems to clarify Aristotle's distinction between\nknowledge that and knowledge why within artificial intelligence. By\ninterpreting existing artificial intelligence technologies as causal systems,\nit investigates the corresponding types of knowledge. Furthermore, it argues\nthat predicting the effects of external interventions is feasible only with\nknowledge why, providing a more precise understanding of the knowledge\nnecessary for such tasks.",
        "published": "2025-04-03T09:37:05+00:00"
    },
    {
        "title": "On learning racing policies with reinforcement learning",
        "authors": [
            "Grzegorz Czechmanowski",
            "Jan W\u0119grzynowski",
            "Piotr Kicki",
            "Krzysztof Walas"
        ],
        "summary": "Fully autonomous vehicles promise enhanced safety and efficiency. However,\nensuring reliable operation in challenging corner cases requires control\nalgorithms capable of performing at the vehicle limits. We address this\nrequirement by considering the task of autonomous racing and propose solving it\nby learning a racing policy using Reinforcement Learning (RL). Our approach\nleverages domain randomization, actuator dynamics modeling, and policy\narchitecture design to enable reliable and safe zero-shot deployment on a real\nplatform. Evaluated on the F1TENTH race car, our RL policy not only surpasses a\nstate-of-the-art Model Predictive Control (MPC), but, to the best of our\nknowledge, also represents the first instance of an RL policy outperforming\nexpert human drivers in RC racing. This work identifies the key factors driving\nthis performance improvement, providing critical insights for the design of\nrobust RL-based control strategies for autonomous vehicles.",
        "published": "2025-04-03T09:21:48+00:00"
    },
    {
        "title": "Hyperspectral Remote Sensing Images Salient Object Detection: The First Benchmark Dataset and Baseline",
        "authors": [
            "Peifu Liu",
            "Huiyan Bai",
            "Tingfa Xu",
            "Jihui Wang",
            "Huan Chen",
            "Jianan Li"
        ],
        "summary": "The objective of hyperspectral remote sensing image salient object detection\n(HRSI-SOD) is to identify objects or regions that exhibit distinct spectrum\ncontrasts with the background. This area holds significant promise for\npractical applications; however, progress has been limited by a notable\nscarcity of dedicated datasets and methodologies. To bridge this gap and\nstimulate further research, we introduce the first HRSI-SOD dataset, termed\nHRSSD, which includes 704 hyperspectral images and 5327 pixel-level annotated\nsalient objects. The HRSSD dataset poses substantial challenges for salient\nobject detection algorithms due to large scale variation, diverse\nforeground-background relations, and multi-salient objects. Additionally, we\npropose an innovative and efficient baseline model for HRSI-SOD, termed the\nDeep Spectral Saliency Network (DSSN). The core of DSSN is the Cross-level\nSaliency Assessment Block, which performs pixel-wise attention and evaluates\nthe contributions of multi-scale similarity maps at each spatial location,\neffectively reducing erroneous responses in cluttered regions and emphasizes\nsalient regions across scales. Additionally, the High-resolution Fusion Module\ncombines bottom-up fusion strategy and learned spatial upsampling to leverage\nthe strengths of multi-scale saliency maps, ensuring accurate localization of\nsmall objects. Experiments on the HRSSD dataset robustly validate the\nsuperiority of DSSN, underscoring the critical need for specialized datasets\nand methodologies in this domain. Further evaluations on the HSOD-BIT and\nHS-SOD datasets demonstrate the generalizability of the proposed method. The\ndataset and source code are publicly available at\nhttps://github.com/laprf/HRSSD.",
        "published": "2025-04-03T09:12:42+00:00"
    },
    {
        "title": "F5R-TTS: Improving Flow Matching based Text-to-Speech with Group Relative Policy Optimization",
        "authors": [
            "Xiaohui Sun",
            "Ruitong Xiao",
            "Jianye Mo",
            "Bowen Wu",
            "Qun Yu",
            "Baoxun Wang"
        ],
        "summary": "We present F5R-TTS, a novel text-to-speech (TTS) system that integrates\nGradient Reward Policy Optimization (GRPO) into a flow-matching based\narchitecture. By reformulating the deterministic outputs of flow-matching TTS\ninto probabilistic Gaussian distributions, our approach enables seamless\nintegration of reinforcement learning algorithms. During pretraining, we train\na probabilistically reformulated flow-matching based model which is derived\nfrom F5-TTS with an open-source dataset. In the subsequent reinforcement\nlearning (RL) phase, we employ a GRPO-driven enhancement stage that leverages\ndual reward metrics: word error rate (WER) computed via automatic speech\nrecognition and speaker similarity (SIM) assessed by verification models.\nExperimental results on zero-shot voice cloning demonstrate that F5R-TTS\nachieves significant improvements in both speech intelligibility (relatively\n29.5\\% WER reduction) and speaker similarity (relatively 4.6\\% SIM score\nincrease) compared to conventional flow-matching based TTS systems. Audio\nsamples are available at https://frontierlabs.github.io/F5R.",
        "published": "2025-04-03T08:57:15+00:00"
    },
    {
        "title": "Lifecycle Management of Trustworthy AI Models in 6G Networks: The REASON Approach",
        "authors": [
            "Juan Parra-Ullauri",
            "Xueqing Zhou",
            "Shadi Moazzeni",
            "Rasheed Hussain",
            "Xenofon Vasilakos",
            "Yulei Wu",
            "Renjith Baby",
            "M M Hassan Mahmud",
            "Gabriele Incorvaia",
            "Darryl Hond",
            "Hamid Asgari",
            "Andrea Tassi",
            "Daniel Warren",
            "Dimitra Simeonidou"
        ],
        "summary": "Artificial Intelligence (AI) is expected to play a key role in 6G networks\nincluding optimising system management, operation, and evolution. This requires\nsystematic lifecycle management of AI models, ensuring their impact on services\nand stakeholders is continuously monitored. While current 6G initiatives\nintroduce AI, they often fall short in addressing end-to-end intelligence and\ncrucial aspects like trust, transparency, privacy, and verifiability.\nTrustworthy AI is vital, especially for critical infrastructures like 6G. This\npaper introduces the REASON approach for holistically addressing AI's native\nintegration and trustworthiness in future 6G networks. The approach comprises\nAI Orchestration (AIO) for model lifecycle management, Cognition (COG) for\nperformance evaluation and explanation, and AI Monitoring (AIM) for tracking\nand feedback. Digital Twin (DT) technology is leveraged to facilitate real-time\nmonitoring and scenario testing, which are essential for AIO, COG, and AIM. We\ndemonstrate this approach through an AI-enabled xAPP use case, leveraging a DT\nplatform to validate, explain, and deploy trustworthy AI models.",
        "published": "2025-04-03T08:56:29+00:00"
    },
    {
        "title": "EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling",
        "authors": [
            "Hao Yin",
            "Shi Guo",
            "Xu Jia",
            "Xudong XU",
            "Lu Zhang",
            "Si Liu",
            "Dong Wang",
            "Huchuan Lu",
            "Tianfan Xue"
        ],
        "summary": "When sound waves hit an object, they induce vibrations that produce\nhigh-frequency and subtle visual changes, which can be used for recovering the\nsound. Early studies always encounter trade-offs related to sampling rate,\nbandwidth, field of view, and the simplicity of the optical path. Recent\nadvances in event camera hardware show good potential for its application in\nvisual sound recovery, because of its superior ability in capturing\nhigh-frequency signals. However, existing event-based vibration recovery\nmethods are still sub-optimal for sound recovery. In this work, we propose a\nnovel pipeline for non-contact sound recovery, fully utilizing spatial-temporal\ninformation from the event stream. We first generate a large training set using\na novel simulation pipeline. Then we designed a network that leverages the\nsparsity of events to capture spatial information and uses Mamba to model\nlong-term temporal information. Lastly, we train a spatial aggregation block to\naggregate information from different locations to further improve signal\nquality. To capture event signals caused by sound waves, we also designed an\nimaging system using a laser matrix to enhance the gradient and collected\nmultiple data sequences for testing. Experimental results on synthetic and\nreal-world data demonstrate the effectiveness of our method.",
        "published": "2025-04-03T08:51:17+00:00"
    },
    {
        "title": "Marine Saliency Segmenter: Object-Focused Conditional Diffusion with Region-Level Semantic Knowledge Distillation",
        "authors": [
            "Laibin Chang",
            "Yunke Wang",
            "JiaXing Huang",
            "Longxiang Deng",
            "Bo Du",
            "Chang Xu"
        ],
        "summary": "Marine Saliency Segmentation (MSS) plays a pivotal role in various\nvision-based marine exploration tasks. However, existing marine segmentation\ntechniques face the dilemma of object mislocalization and imprecise boundaries\ndue to the complex underwater environment. Meanwhile, despite the impressive\nperformance of diffusion models in visual segmentation, there remains potential\nto further leverage contextual semantics to enhance feature learning of\nregion-level salient objects, thereby improving segmentation outcomes. Building\non this insight, we propose DiffMSS, a novel marine saliency segmenter based on\nthe diffusion model, which utilizes semantic knowledge distillation to guide\nthe segmentation of marine salient objects. Specifically, we design a\nregion-word similarity matching mechanism to identify salient terms at the word\nlevel from the text descriptions. These high-level semantic features guide the\nconditional feature learning network in generating salient and accurate\ndiffusion conditions with semantic knowledge distillation. To further refine\nthe segmentation of fine-grained structures in unique marine organisms, we\ndevelop the dedicated consensus deterministic sampling to suppress\noverconfident missegmentations. Comprehensive experiments demonstrate the\nsuperior performance of DiffMSS over state-of-the-art methods in both\nquantitative and qualitative evaluations.",
        "published": "2025-04-03T08:31:36+00:00"
    },
    {
        "title": "Steiner Traveling Salesman Problem with Quantum Annealing",
        "authors": [
            "Alessia Ciacco",
            "Francesca Guerriero",
            "Eneko Osaba"
        ],
        "summary": "The Steiner Traveling Salesman Problem (STSP) is a variant of the classical\nTraveling Salesman Problem. The STSP involves incorporating steiner nodes,\nwhich are extra nodes not originally part of the required visit set but that\ncan be added to the route to enhance the overall solution and minimize the\ntotal travel cost. Given the NP-hard nature of the STSP, we propose a quantum\napproach to address it. Specifically, we employ quantum annealing using\nD-Wave's hardware to explore its potential for solving this problem. To enhance\ncomputational feasibility, we develop a preprocessing method that effectively\nreduces the network size. Our experimental results demonstrate that this\nreduction technique significantly decreases the problem complexity, making the\nQuadratic Unconstrained Binary Optimization formulation, the standard input for\nquantum annealers, better suited for existing quantum hardware. Furthermore,\nthe results highlight the potential of quantum annealing as a promising and\ninnovative approach for solving the STSP.",
        "published": "2025-04-03T08:29:57+00:00"
    },
    {
        "title": "Exploring energy consumption of AI frameworks on a 64-core RV64 Server CPU",
        "authors": [
            "Giulio Malenza",
            "Francesco Targa",
            "Adriano Marques Garcia",
            "Marco Aldinucci",
            "Robert Birke"
        ],
        "summary": "In today's era of rapid technological advancement, artificial intelligence\n(AI) applications require large-scale, high-performance, and data-intensive\ncomputations, leading to significant energy demands. Addressing this challenge\nnecessitates a combined approach involving both hardware and software\ninnovations. Hardware manufacturers are developing new, efficient, and\nspecialized solutions, with the RISC-V architecture emerging as a prominent\nplayer due to its open, extensible, and energy-efficient instruction set\narchitecture (ISA). Simultaneously, software developers are creating new\nalgorithms and frameworks, yet their energy efficiency often remains unclear.\nIn this study, we conduct a comprehensive benchmark analysis of machine\nlearning (ML) applications on the 64-core SOPHON SG2042 RISC-V architecture. We\nspecifically analyze the energy consumption of deep learning inference models\nacross three leading AI frameworks: PyTorch, ONNX Runtime, and TensorFlow. Our\nfindings show that frameworks using the XNNPACK back-end, such as ONNX Runtime\nand TensorFlow, consume less energy compared to PyTorch, which is compiled with\nthe native OpenBLAS back-end.",
        "published": "2025-04-03T08:27:10+00:00"
    },
    {
        "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
        "authors": [
            "Kim Sung-Bin",
            "Jeongsoo Choi",
            "Puyuan Peng",
            "Joon Son Chung",
            "Tae-Hyun Oh",
            "David Harwath"
        ],
        "summary": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that\nsynthesizes high-quality speech from text and facial cues. This task has broad\napplications in filmmaking, multimedia creation, and assisting voice-impaired\nindividuals. Building on the success of Neural Codec Language Models (NCLMs)\nfor speech synthesis, our method extends their capabilities by incorporating\nvideo features, ensuring that synthesized speech is time-synchronized and\nexpressively aligned with facial movements while preserving natural prosody. To\ninject visual cues, we design adapters to align facial features with the NCLM\ntoken space and introduce audio-visual fusion layers to merge audio-visual\ninformation within the NCLM framework. Additionally, we curate CelebV-Dub, a\nnew dataset of expressive, real-world videos specifically designed for\nautomated video dubbing. Extensive experiments show that our model achieves\nhigh-quality, intelligible, and natural speech synthesis with accurate lip\nsynchronization, outperforming existing methods in human perception and\nperforming favorably in objective evaluations. We also adapt VoiceCraft-Dub for\nthe video-to-speech task, demonstrating its versatility for various\napplications.",
        "published": "2025-04-03T08:24:47+00:00"
    },
    {
        "title": "Reinforcement Learning for Solving the Pricing Problem in Column Generation: Applications to Vehicle Routing",
        "authors": [
            "Abdo Abouelrous",
            "Laurens Bliek",
            "Adriana F. Gabor",
            "Yaoxin Wu",
            "Yingqian Zhang"
        ],
        "summary": "In this paper, we address the problem of Column Generation (CG) using\nReinforcement Learning (RL). Specifically, we use a RL model based on the\nattention-mechanism architecture to find the columns with most negative reduced\ncost in the Pricing Problem (PP). Unlike previous Machine Learning (ML)\napplications for CG, our model deploys an end-to-end mechanism as it\nindependently solves the pricing problem without the help of any heuristic. We\nconsider a variant of Vehicle Routing Problem (VRP) as a case study for our\nmethod. Through a set of experiments where our method is compared against a\nDynamic Programming (DP)-based heuristic for solving the PP, we show that our\nmethod solves the linear relaxation up to a reasonable objective gap within 9%\nin significantly shorter running times, up to over 300 times faster for\ninstances with 100 customers.",
        "published": "2025-04-03T08:22:19+00:00"
    },
    {
        "title": "HPGN: Hybrid Priors-Guided Network for Compressed Low-Light Image Enhancement",
        "authors": [
            "Hantang Li",
            "Jinhua Hao",
            "Lei Xiong",
            "Shuyuan Zhu"
        ],
        "summary": "In practical applications, conventional methods generate large volumes of\nlow-light images that require compression for efficient storage and\ntransmission. However, most existing methods either disregard the removal of\npotential compression artifacts during the enhancement process or fail to\nestablish a unified framework for joint task enhancement of images with varying\ncompression qualities. To solve this problem, we propose the hybrid\npriors-guided network (HPGN), which enhances compressed low-light images by\nintegrating both compression and illumination priors. Our approach fully\nutilizes the JPEG quality factor (QF) and DCT quantization matrix (QM) to guide\nthe design of efficient joint task plug-and-play modules. Additionally, we\nemploy a random QF generation strategy to guide model training, enabling a\nsingle model to enhance images across different compression levels.\nExperimental results confirm the superiority of our proposed method.",
        "published": "2025-04-03T08:06:24+00:00"
    },
    {
        "title": "Quantum Key Distribution over Complex Networks",
        "authors": [
            "Luca Mariani",
            "Raja Yehia",
            "Carlos Pascual-Garc\u00eda",
            "Federico Centrone",
            "Jasper van der Kolk",
            "M. \u00c1ngeles Serrano",
            "Antonio Ac\u00edn"
        ],
        "summary": "There exist several initiatives worldwide to deploy quantum key distribution\n(QKD) over existing fibre networks and achieve quantum-safe security at large\nscales. To understand the overall QKD network performance, it is required to\ntransition from the analysis of individual links, as done so far, to the\ncharacterization of the network as a whole. In this work, we undertake this\nstudy by embedding QKD protocols on complex networks, which correctly model the\nexisting fiber networks. We focus on networks with trusted nodes and on\ncontinuous-variable (CV) schemes, which have much higher key rates than their\ndiscrete-variable (DV) counterparts. In the effective CV network, however, many\nof the unique properties of complex networks, such as small-worldness and the\npresence of hubs, are lost due to the fast decay of the key rate with physical\ndistance for CV systems. These properties can be restored when considering a\nhybrid network consisting of both CV and DV protocols, achieving at the same\ntime high average rate and inter-connectivity. Our work opens the path to the\nstudy of QKD complex networks in existing infrastructures.",
        "published": "2025-04-03T08:05:44+00:00"
    },
    {
        "title": "SHapley Estimated Explanation (SHEP): A Fast Post-Hoc Attribution Method for Interpreting Intelligent Fault Diagnosis",
        "authors": [
            "Qian Chen",
            "Xingjian Dong",
            "Zhike Peng",
            "Guang Meng"
        ],
        "summary": "Despite significant progress in intelligent fault diagnosis (IFD), the lack\nof interpretability remains a critical barrier to practical industrial\napplications, driving the growth of interpretability research in IFD. Post-hoc\ninterpretability has gained popularity due to its ability to preserve network\nflexibility and scalability without modifying model structures. However, these\nmethods often yield suboptimal time-domain explanations. Recently, combining\ndomain transform with SHAP has improved interpretability by extending\nexplanations to more informative domains. Nonetheless, the computational\nexpense of SHAP, exacerbated by increased dimensions from domain transforms,\nremains a major challenge. To address this, we propose patch-wise attribution\nand SHapley Estimated Explanation (SHEP). Patch-wise attribution reduces\nfeature dimensions at the cost of explanation granularity, while SHEP\nsimplifies subset enumeration to approximate SHAP, reducing complexity from\nexponential to linear. Together, these methods significantly enhance SHAP's\ncomputational efficiency, providing feasibility for real-time interpretation in\nmonitoring tasks. Extensive experiments confirm SHEP's efficiency,\ninterpretability, and reliability in approximating SHAP. Additionally, with\nopen-source code, SHEP has the potential to serve as a benchmark for post-hoc\ninterpretability in IFD. The code is available on\nhttps://github.com/ChenQian0618/SHEP.",
        "published": "2025-04-03T07:56:07+00:00"
    },
    {
        "title": "SProBench: Stream Processing Benchmark for High Performance Computing Infrastructure",
        "authors": [
            "Apurv Deepak Kulkarni",
            "Siavash Ghiasvand"
        ],
        "summary": "Recent advancements in data stream processing frameworks have improved\nreal-time data handling, however, scalability remains a significant challenge\naffecting throughput and latency. While studies have explored this issue on\nlocal machines and cloud clusters, research on modern high performance\ncomputing (HPC) infrastructures is yet limited due to the lack of scalable\nmeasurement tools. This work presents SProBench, a novel benchmark suite\ndesigned to evaluate the performance of data stream processing frameworks in\nlarge-scale computing systems. Building on best practices, SProBench\nincorporates a modular architecture, offers native support for SLURM-based\nclusters, and seamlessly integrates with popular stream processing frameworks\nsuch as Apache Flink, Apache Spark Streaming, and Apache Kafka Streams.\nExperiments conducted on HPC clusters demonstrate its exceptional scalability,\ndelivering throughput that surpasses existing benchmarks by more than tenfold.\nThe distinctive features of SProBench, including complete customization\noptions, built-in automated experiment management tools, seamless\ninteroperability, and an open-source license, distinguish it as an innovative\nbenchmark suite tailored to meet the needs of modern data stream processing\nframeworks.",
        "published": "2025-04-03T07:54:49+00:00"
    },
    {
        "title": "Low-cost Embedded Breathing Rate Determination Using 802.15.4z IR-UWB Hardware for Remote Healthcare",
        "authors": [
            "Anton Lambrecht",
            "Stijn Luchie",
            "Jaron Fontaine",
            "Ben Van Herbruggen",
            "Adnan Shahid",
            "Eli De Poorter"
        ],
        "summary": "Respiratory diseases account for a significant portion of global mortality.\nAffordable and early detection is an effective way of addressing these\nailments. To this end, a low-cost commercial off-the-shelf (COTS), IEEE\n802.15.4z standard compliant impulse-radio ultra-wideband (IR-UWB) radar system\nis exploited to estimate human respiration rates. We propose a convolutional\nneural network (CNN) to predict breathing rates from ultra-wideband (UWB)\nchannel impulse response (CIR) data, and compare its performance with other\nrule-based algorithms. The study uses a diverse dataset of 16 individuals,\nincorporating various real-life environments to evaluate system robustness.\nResults show that the CNN achieves a mean absolute error (MAE) of 1.73 breaths\nper minute (BPM) in unseen situations, significantly outperforming rule-based\nmethods (3.40 BPM). By incorporating calibration data from other individuals in\nthe unseen situations, the error is further reduced to 0.84 BPM. In addition,\nthis work evaluates the feasibility of running the pipeline on a low-cost\nembedded device. Applying 8-bit quantization to both the weights and\ninput/ouput tensors, reduces memory requirements by 67% and inference time by\n64% with only a 3% increase in MAE. As a result, we show it is feasible to\ndeploy the algorithm on an nRF52840 system-on-chip (SoC) requiring only 46 KB\nof memory and operating with an inference time of only 192 ms. Once deployed,\nthe system can last up to 268 days without recharging using a 20 000 mAh\nbattery pack. For breathing monitoring in bed, the sampling rate can be\nlowered, extending battery life to 313 days, making the solution highly\nefficient for real-world, low-cost deployments.",
        "published": "2025-04-03T07:54:25+00:00"
    },
    {
        "title": "Brightness Perceiving for Recursive Low-Light Image Enhancement",
        "authors": [
            "Haodian Wang",
            "Long Peng",
            "Yuejin Sun",
            "Zengyu Wan",
            "Yang Wang",
            "Yang Cao"
        ],
        "summary": "Due to the wide dynamic range in real low-light scenes, there will be large\ndifferences in the degree of contrast degradation and detail blurring of\ncaptured images, making it difficult for existing end-to-end methods to enhance\nlow-light images to normal exposure. To address the above issue, we decompose\nlow-light image enhancement into a recursive enhancement task and propose a\nbrightness-perceiving-based recursive enhancement framework for high dynamic\nrange low-light image enhancement. Specifically, our recursive enhancement\nframework consists of two parallel sub-networks: Adaptive Contrast and Texture\nenhancement network (ACT-Net) and Brightness Perception network (BP-Net). The\nACT-Net is proposed to adaptively enhance image contrast and details under the\nguidance of the brightness adjustment branch and gradient adjustment branch,\nwhich are proposed to perceive the degradation degree of contrast and details\nin low-light images. To adaptively enhance images captured under different\nbrightness levels, BP-Net is proposed to control the recursive enhancement\ntimes of ACT-Net by exploring the image brightness distribution properties.\nFinally, in order to coordinate ACT-Net and BP-Net, we design a novel\nunsupervised training strategy to facilitate the training procedure. To further\nvalidate the effectiveness of the proposed method, we construct a new dataset\nwith a broader brightness distribution by mixing three low-light datasets.\nCompared with eleven existing representative methods, the proposed method\nachieves new SOTA performance on six reference and no reference metrics.\nSpecifically, the proposed method improves the PSNR by 0.9 dB compared to the\nexisting SOTA method.",
        "published": "2025-04-03T07:53:33+00:00"
    },
    {
        "title": "Optical and magnetic response by design in GaAs quantum dots",
        "authors": [
            "Christian Schimpf",
            "Ailton J. Garcia Jr.",
            "Zhe X. Koong",
            "Giang N. Nguyen",
            "Lukas L. Niekamp",
            "Martin Hayhurst Appel",
            "Ahmed Hassanen",
            "James Waller",
            "Yusuf Karli",
            "Saimon Philipe Covre da Silva",
            "Julian Ritzmann",
            "Hans-Georg Babin",
            "Andreas D. Wieck",
            "Anton Pishchagin",
            "Nico Margaria",
            "Ti-Huong Au",
            "Sebastien Bossier",
            "Martina Morassi",
            "Aristide Lemaitre",
            "Pascale Senellart",
            "Niccolo Somaschi",
            "Arne Ludwig",
            "Richard Warburton",
            "Mete Atat\u00fcre",
            "Armando Rastelli",
            "Micha\u0142 Gawe\u0142czyk",
            "Dorian Gangloff"
        ],
        "summary": "Quantum networking technologies use spin qubits and their interface to single\nphotons as core components of a network node. This necessitates the ability to\nco-design the magnetic- and optical-dipole response of a quantum system. These\nproperties are notoriously difficult to design in many solid-state systems,\nwhere spin-orbit coupling and the crystalline environment for each qubit create\ninhomogeneity of electronic g-factors and optically active states. Here, we\nshow that GaAs quantum dots (QDs) obtained via the quasi-strain-free local\ndroplet etching epitaxy growth method provide spin and optical properties\npredictable from assuming the highest possible QD symmetry. Our measurements of\nelectron and hole g-tensors and of transition dipole moment orientations for\ncharged excitons agree with our predictions from a multiband k.p simulation\nconstrained only by a single atomic-force-microscopy reconstruction of QD\nmorphology. This agreement is verified across multiple wavelength-specific\ngrowth runs at different facilities within the range of 730 nm to 790 nm for\nthe exciton emission. Remarkably, our measurements and simulations track the\nin-plane electron g-factors through a zero-crossing from -0.1 to 0.3 and linear\noptical dipole moment orientations fully determined by an external magnetic\nfield. The robustness of our results demonstrates the capability to design -\nprior to growth - the properties of a spin qubit and its tunable optical\ninterface best adapted to a target magnetic and photonic environment with\ndirect application for high-quality spin-photon entanglement.",
        "published": "2025-04-03T07:43:01+00:00"
    },
    {
        "title": "Liquid Neural Networks: Next-Generation AI for Telecom from First Principles",
        "authors": [
            "Fenghao Zhu",
            "Xinquan Wang",
            "Chen Zhu",
            "Chongwen Huang"
        ],
        "summary": "Artificial intelligence (AI) has emerged as a transformative technology with\nimmense potential to reshape the next-generation of wireless networks. By\nleveraging advanced algorithms and machine learning techniques, AI offers\nunprecedented capabilities in optimizing network performance, enhancing data\nprocessing efficiency, and enabling smarter decision-making processes. However,\nexisting AI solutions face significant challenges in terms of robustness and\ninterpretability. Specifically, current AI models exhibit substantial\nperformance degradation in dynamic environments with varying data\ndistributions, and the black-box nature of these algorithms raises concerns\nregarding safety, transparency, and fairness. This presents a major challenge\nin integrating AI into practical communication systems. Recently, a novel type\nof neural network, known as the liquid neural networks (LNNs), has been\ndesigned from first principles to address these issues. In this paper, we\nexplore the potential of LNNs in telecommunications. First, we illustrate the\nmechanisms of LNNs and highlight their unique advantages over traditional\nnetworks. Then we unveil the opportunities that LNNs bring to future wireless\nnetworks. Furthermore, we discuss the challenges and design directions for the\nimplementation of LNNs. Finally, we summarize the performance of LNNs in two\ncase studies.",
        "published": "2025-04-03T07:41:04+00:00"
    },
    {
        "title": "Toward General and Robust LLM-enhanced Text-attributed Graph Learning",
        "authors": [
            "Zihao Zhang",
            "Xunkai Li",
            "Rong-Hua Li",
            "Bing Zhou",
            "Zhenjun Li",
            "Guoren Wang"
        ],
        "summary": "Recent advancements in Large Language Models (LLMs) and the proliferation of\nText-Attributed Graphs (TAGs) across various domains have positioned\nLLM-enhanced TAG learning as a critical research area. By utilizing rich graph\ndescriptions, this paradigm leverages LLMs to generate high-quality embeddings,\nthereby enhancing the representational capacity of Graph Neural Networks\n(GNNs). However, the field faces significant challenges: (1) the absence of a\nunified framework to systematize the diverse optimization perspectives arising\nfrom the complex interactions between LLMs and GNNs, and (2) the lack of a\nrobust method capable of handling real-world TAGs, which often suffer from\ntexts and edge sparsity, leading to suboptimal performance.\n  To address these challenges, we propose UltraTAG, a unified pipeline for\nLLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and\ndomain-adaptive framework that not only organizes existing methodologies but\nalso paves the way for future advancements in the field. Building on this\nframework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed\nto tackle the inherent sparsity issues in real-world TAGs. UltraTAG-S employs\nLLM-based text propagation and text augmentation to mitigate text sparsity,\nwhile leveraging LLM-augmented node selection techniques based on PageRank and\nedge reconfiguration strategies to address edge sparsity. Our extensive\nexperiments demonstrate that UltraTAG-S significantly outperforms existing\nbaselines, achieving improvements of 2.12\\% and 17.47\\% in ideal and sparse\nsettings, respectively. Moreover, as the data sparsity ratio increases, the\nperformance improvement of UltraTAG-S also rises, which underscores the\neffectiveness and robustness of UltraTAG-S.",
        "published": "2025-04-03T07:24:18+00:00"
    },
    {
        "title": "Stability of complex communities: A perspective from discrete-time dynamics",
        "authors": [
            "Shuaiying Wang",
            "Yuguang Yang",
            "Aming Li"
        ],
        "summary": "Understanding the stability of complex communities is a central focus in\necology, many important theoretical advancements have been made to identify\ndrivers of ecological stability. However, previous results often rely on the\ncontinuous-time dynamics, assuming that species have overlapping generations.\nIn contrast, numerous real-world communities consist of species with\nnon-overlapping generations, whose quantitative behavior can only be precisely\nrepresented by discrete-time dynamics rather than continuous ones. Here, we\ndevelop a theoretical framework and propose a metric to quantify the stability\nof complex communities characterized by non-overlapping generations and diverse\ninteraction types. In stark contrast to existing results for overlapping\ngenerations, we find that increasing self-regulation strength first stabilizes\nand then destabilizes complex communities. This pattern is further confirmed in\nboth exploitative (E. aerogenes, P. aurantiaca, P. chlororaphis, P.\ncitronellolis) and competitive (P. putida, P. veroni, S. marcescens) soil\nmicrobial communities. Moreover, we show that communities with diverse\ninteraction types become the most stable, which is corroborated by empirical\nmouse microbial networks. Furthermore, we reveal that the prevalence of weak\ninteractions can stabilize communities, which is consistent with findings from\nexisting microbial experiments. Our analyses of complex communities with\nnon-overlapping generations provide a more comprehensive understanding of\necological stability and informs practical strategies for ecological\nrestoration and control.",
        "published": "2025-04-03T07:10:52+00:00"
    },
    {
        "title": "Distributed Log-driven Anomaly Detection System based on Evolving Decision Making",
        "authors": [
            "Zhuoran Tan",
            "Qiyuan Wang",
            "Christos Anagnostopoulos",
            "Shameem P. Parambath",
            "Jeremy Singer",
            "Sam Temple"
        ],
        "summary": "Effective anomaly detection from logs is crucial for enhancing cybersecurity\ndefenses by enabling the early identification of threats. Despite advances in\nanomaly detection, existing systems often fall short in areas such as\npost-detection validation, scalability, and effective maintenance. These\nlimitations not only hinder the detection of new threats but also impair\noverall system performance. To address these challenges, we propose CEDLog, a\nnovel practical framework that integrates Elastic Weight Consolidation (EWC)\nfor continual learning and implements distributed computing for scalable\nprocessing by integrating Apache Airflow and Dask. In CEDLog, anomalies are\ndetected through the synthesis of Multi-layer Perceptron (MLP) and Graph\nConvolutional Networks (GCNs) using critical features present in event logs.\nThrough comparisons with update strategies on large-scale datasets, we\ndemonstrate the strengths of CEDLog, showcasing efficient updates and low false\npositives",
        "published": "2025-04-03T06:50:30+00:00"
    },
    {
        "title": "On shallow feedforward neural networks with inputs from a topological space",
        "authors": [
            "Vugar Ismailov"
        ],
        "summary": "We study feedforward neural networks with inputs from a topological space\n(TFNNs). We prove a universal approximation theorem for shallow TFNNs, which\ndemonstrates their capacity to approximate any continuous function defined on\nthis topological space. As an application, we obtain an approximative version\nof Kolmogorov's superposition theorem for compact metric spaces.",
        "published": "2025-04-03T06:48:46+00:00"
    },
    {
        "title": "Causal Self-supervised Pretrained Frontend with Predictive Code for Speech Separation",
        "authors": [
            "Wupeng Wang",
            "Zexu Pan",
            "Xinke Li",
            "Shuai Wang",
            "Haizhou Li"
        ],
        "summary": "Speech separation (SS) seeks to disentangle a multi-talker speech mixture\ninto single-talker speech streams. Although SS can be generally achieved using\noffline methods, such a processing paradigm is not suitable for real-time\nstreaming applications. Causal separation models, which rely only on past and\npresent information, offer a promising solution for real-time streaming.\nHowever, these models typically suffer from notable performance degradation due\nto the absence of future context. In this paper, we introduce a novel frontend\nthat is designed to mitigate the mismatch between training and run-time\ninference by implicitly incorporating future information into causal models\nthrough predictive patterns. The pretrained frontend employs a transformer\ndecoder network with a causal convolutional encoder as the backbone and is\npretrained in a self-supervised manner with two innovative pretext tasks:\nautoregressive hybrid prediction and contextual knowledge distillation. These\ntasks enable the model to capture predictive patterns directly from mixtures in\na self-supervised manner. The pretrained frontend subsequently serves as a\nfeature extractor to generate high-quality predictive patterns. Comprehensive\nevaluations on synthetic and real-world datasets validated the effectiveness of\nthe proposed pretrained frontend.",
        "published": "2025-04-03T06:18:30+00:00"
    },
    {
        "title": "Asymmetric graph alignment and the phase transition for asymmetric tree correlation testing",
        "authors": [
            "Jakob Maier",
            "Laurent Massouli\u00e9"
        ],
        "summary": "Graph alignment - identifying node correspondences between two graphs - is a\nfundamental problem with applications in network analysis, biology, and privacy\nresearch. While substantial progress has been made in aligning correlated\nErd\\H{o}s-R\\'enyi graphs under symmetric settings, real-world networks often\nexhibit asymmetry in both node numbers and edge densities. In this work, we\nintroduce a novel framework for asymmetric correlated Erd\\H{o}s-R\\'enyi graphs,\ngeneralizing existing models to account for these asymmetries. We conduct a\nrigorous theoretical analysis of graph alignment in the sparse regime, where\nlocal neighborhoods exhibit tree-like structures. Our approach leverages tree\ncorrelation testing as the central tool in our polynomial-time algorithm,\nMPAlign, which achieves one-sided partial alignment under certain conditions.\n  A key contribution of our work is characterizing these conditions under which\nasymmetric tree correlation testing is feasible: If two correlated graphs $G$\nand $G'$ have average degrees $\\lambda s$ and $\\lambda s'$ respectively, where\n$\\lambda$ is their common density and $s,s'$ are marginal correlation\nparameters, their tree neighborhoods can be aligned if $ss' > \\alpha$, where\n$\\alpha$ denotes Otter's constant and $\\lambda$ is supposed large enough. The\nfeasibility of this tree comparison problem undergoes a sharp phase transition\nsince $ss' \\leq \\alpha$ implies its impossibility. These new results on tree\ncorrelation testing allow us to solve a class of random subgraph isomorphism\nproblems, resolving an open problem in the field.",
        "published": "2025-04-03T06:10:00+00:00"
    },
    {
        "title": "SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks",
        "authors": [
            "Xinyu Luo",
            "Kecheng Chen",
            "Pao-Sheng Vincent Sun",
            "Chris Xing Tian",
            "Arindam Basu",
            "Haoliang Li"
        ],
        "summary": "Spiking Neural Networks (SNNs), as a biologically plausible alternative to\nArtificial Neural Networks (ANNs), have demonstrated advantages in terms of\nenergy efficiency, temporal processing, and biological plausibility. However,\nSNNs are highly sensitive to distribution shifts, which can significantly\ndegrade their performance in real-world scenarios. Traditional test-time\nadaptation (TTA) methods designed for ANNs often fail to address the unique\ncomputational dynamics of SNNs, such as sparsity and temporal spiking behavior.\nTo address these challenges, we propose $\\textbf{SP}$ike-$\\textbf{A}$ware\n$\\textbf{C}$onsistency $\\textbf{E}$nhancement (SPACE), the first source-free\nand single-instance TTA method specifically designed for SNNs. SPACE leverages\nthe inherent spike dynamics of SNNs to maximize the consistency of\nspike-behavior-based local feature maps across augmented versions of a single\ntest sample, enabling robust adaptation without requiring source data. We\nevaluate SPACE on multiple datasets, including CIFAR-10-C, CIFAR-100-C,\nTiny-ImageNet-C and DVS Gesture-C. Furthermore, SPACE demonstrates strong\ngeneralization across different model architectures, achieving consistent\nperformance improvements on both VGG9 and ResNet11. Experimental results show\nthat SPACE outperforms state-of-the-art methods, highlighting its effectiveness\nand robustness in real-world settings.",
        "published": "2025-04-03T06:05:05+00:00"
    },
    {
        "title": "Flow State: Humans Enabling AI Systems to Program Themselves",
        "authors": [
            "Helena Zhang",
            "Jakobi Haskell",
            "Yosef Frost"
        ],
        "summary": "Compound AI systems, orchestrating multiple AI components and external APIs,\nare increasingly vital but face challenges in managing complexity, handling\nambiguity, and enabling effective development workflows. Existing frameworks\noften introduce significant overhead, implicit complexity, or restrictive\nabstractions, hindering maintainability and iterative refinement, especially in\nHuman-AI collaborative settings. We argue that overcoming these hurdles\nrequires a foundational architecture prioritizing structural clarity and\nexplicit control. To this end, we introduce Pocketflow, a platform centered on\nHuman-AI co-design, enabled by Pocketflow. Pocketflow is a Python framework\nbuilt upon a deliberately minimal yet synergistic set of core abstractions:\nmodular Nodes with a strict lifecycle, declarative Flow orchestration, native\nhierarchical nesting (Flow-as-Node), and explicit action-based conditional\nlogic. This unique combination provides a robust, vendor-agnostic foundation\nwith very little code that demonstrably reduces overhead while offering the\nexpressiveness needed for complex patterns like agentic workflows and RAG.\nComplemented by Pocket AI, an assistant leveraging this structure for system\ndesign, Pocketflow provides an effective environment for iteratively\nprototyping, refining, and deploying the adaptable, scalable AI systems\ndemanded by modern enterprises.",
        "published": "2025-04-03T05:25:46+00:00"
    },
    {
        "title": "Ga$_2$O$_3$ TCAD Mobility Parameter Calibration using Simulation Augmented Machine Learning with Physics Informed Neural Network",
        "authors": [
            "Le Minh Long Nguyen",
            "Edric Ong",
            "Matthew Eng",
            "Yuhao Zhang",
            "Hiu Yung Wong"
        ],
        "summary": "In this paper, we demonstrate the possibility of performing automatic\nTechnology Computer-Aided-Design (TCAD) parameter calibration using machine\nlearning, verified with experimental data. The machine only needs to be trained\nby TCAD data. Schottky Barrier Diode (SBD) fabricated with emerging\nultra-wide-bandgap material, Gallium Oxide (Ga$_2$O$_3$), is measured and its\ncurrent-voltage (IV) is used for Ga$_2$O$_3$ Philips Unified Mobility (PhuMob)\nmodel parameters, effective anode workfunction, and ambient temperature\nextraction (7 parameters). A machine comprised of an autoencoder (AE) and a\nneural network (NN) (AE-NN) is used. Ga$_2$O$_3$ PhuMob parameters are\nextracted from the noisy experimental curves. TCAD simulation with the\nextracted parameters shows that the quality of the parameters is as good as an\nexpert's calibration at the pre-turned-on regime but not in the on-state\nregime. By using a simple physics-informed neural network (PINN) (AE-PINN), the\nmachine performs as well as the human expert in all regimes.",
        "published": "2025-04-03T05:09:43+00:00"
    },
    {
        "title": "LLM-Guided Evolution: An Autonomous Model Optimization for Object Detection",
        "authors": [
            "YiMing Yu",
            "Jason Zutty"
        ],
        "summary": "In machine learning, Neural Architecture Search (NAS) requires domain\nknowledge of model design and a large amount of trial-and-error to achieve\npromising performance. Meanwhile, evolutionary algorithms have traditionally\nrelied on fixed rules and pre-defined building blocks. The Large Language Model\n(LLM)-Guided Evolution (GE) framework transformed this approach by\nincorporating LLMs to directly modify model source code for image\nclassification algorithms on CIFAR data and intelligently guide mutations and\ncrossovers. A key element of LLM-GE is the \"Evolution of Thought\" (EoT)\ntechnique, which establishes feedback loops, allowing LLMs to refine their\ndecisions iteratively based on how previous operations performed. In this\nstudy, we perform NAS for object detection by improving LLM-GE to modify the\narchitecture of You Only Look Once (YOLO) models to enhance performance on the\nKITTI dataset. Our approach intelligently adjusts the design and settings of\nYOLO to find the optimal algorithms against objective such as detection\naccuracy and speed. We show that LLM-GE produced variants with significant\nperformance improvements, such as an increase in Mean Average Precision from\n92.5% to 94.5%. This result highlights the flexibility and effectiveness of\nLLM-GE on real-world challenges, offering a novel paradigm for automated\nmachine learning that combines LLM-driven reasoning with evolutionary\nstrategies.",
        "published": "2025-04-03T05:06:06+00:00"
    },
    {
        "title": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation",
        "authors": [
            "Amit Rand",
            "Hadi Ibrahim"
        ],
        "summary": "Medical imaging, particularly X-ray analysis, often involves detecting\nmultiple conditions simultaneously within a single scan, making multi-label\nclassification crucial for real-world clinical applications. We present the\nMedical X-ray Attention (MXA) block, a novel attention mechanism tailored\nspecifically to address the unique challenges of X-ray abnormality detection.\nThe MXA block enhances traditional Multi-Head Self Attention (MHSA) by\nintegrating a specialized module that efficiently captures both detailed local\ninformation and broader global context. To the best of our knowledge, this is\nthe first work to propose a task-specific attention mechanism for diagnosing\nchest X-rays, as well as to attempt multi-label classification using an\nEfficient Vision Transformer (EfficientViT). By embedding the MXA block within\nthe EfficientViT architecture and employing knowledge distillation, our\nproposed model significantly improves performance on the CheXpert dataset, a\nwidely used benchmark for multi-label chest X-ray abnormality detection. Our\napproach achieves an area under the curve (AUC) of 0.85, an absolute\nimprovement of 0.19 compared to our baseline model's AUC of 0.66, corresponding\nto a substantial approximate 233% relative improvement over random guessing\n(AUC = 0.5).",
        "published": "2025-04-03T04:55:42+00:00"
    },
    {
        "title": "Enhancing Customer Contact Efficiency with Graph Neural Networks in Credit Card Fraud Detection Workflow",
        "authors": [
            "Menghao Huo",
            "Kuan Lu",
            "Qiang Zhu",
            "Zhenrui Chen"
        ],
        "summary": "Credit card fraud has been a persistent issue since the last century, causing\nsignificant financial losses to the industry. The most effective way to prevent\nfraud is by contacting customers to verify suspicious transactions. However,\nwhile these systems are designed to detect fraudulent activity, they often\nmistakenly flag legitimate transactions, leading to unnecessary declines that\ndisrupt the user experience and erode customer trust. Frequent false positives\ncan frustrate customers, resulting in dissatisfaction, increased complaints,\nand a diminished sense of security. To address these limitations, we propose a\nfraud detection framework incorporating Relational Graph Convolutional Networks\n(RGCN) to enhance the accuracy and efficiency of identifying fraudulent\ntransactions. By leveraging the relational structure of transaction data, our\nmodel reduces the need for direct customer confirmation while maintaining high\ndetection performance. Our experiments are conducted using the IBM credit card\ntransaction dataset to evaluate the effectiveness of this approach.",
        "published": "2025-04-03T04:50:45+00:00"
    },
    {
        "title": "Generative Classifier for Domain Generalization",
        "authors": [
            "Shaocong Long",
            "Qianyu Zhou",
            "Xiangtai Li",
            "Chenhao Ying",
            "Yunhai Tong",
            "Lizhuang Ma",
            "Yuan Luo",
            "Dacheng Tao"
        ],
        "summary": "Domain generalization (DG) aims to improve the generalizability of computer\nvision models toward distribution shifts. The mainstream DG methods focus on\nlearning domain invariance, however, such methods overlook the potential\ninherent in domain-specific information. While the prevailing practice of\ndiscriminative linear classifier has been tailored to domain-invariant\nfeatures, it struggles when confronted with diverse domain-specific\ninformation, e.g., intra-class shifts, that exhibits multi-modality. To address\nthese issues, we explore the theoretical implications of relying on domain\ninvariance, revealing the crucial role of domain-specific information in\nmitigating the target risk for DG. Drawing from these insights, we propose\nGenerative Classifier-driven Domain Generalization (GCDG), introducing a\ngenerative paradigm for the DG classifier based on Gaussian Mixture Models\n(GMMs) for each class across domains. GCDG consists of three key modules:\nHeterogeneity Learning Classifier~(HLC), Spurious Correlation Blocking~(SCB),\nand Diverse Component Balancing~(DCB). Concretely, HLC attempts to model the\nfeature distributions and thereby capture valuable domain-specific information\nvia GMMs. SCB identifies the neural units containing spurious correlations and\nperturbs them, mitigating the risk of HLC learning spurious patterns.\nMeanwhile, DCB ensures a balanced contribution of components in HLC, preventing\nthe underestimation or neglect of critical components. In this way, GCDG excels\nin capturing the nuances of domain-specific information characterized by\ndiverse distributions. GCDG demonstrates the potential to reduce the target\nrisk and encourage flat minima, improving the generalizability. Extensive\nexperiments show GCDG's comparable performance on five DG benchmarks and one\nface anti-spoofing dataset, seamlessly integrating into existing DG methods\nwith consistent improvements.",
        "published": "2025-04-03T04:38:33+00:00"
    },
    {
        "title": "Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with LLM-Powered Assistance",
        "authors": [
            "Bo Yuan",
            "Yulin Chen",
            "Yin Zhang",
            "Wei Jiang"
        ],
        "summary": "Learning from noisy labels (LNL) is a challenge that arises in many\nreal-world scenarios where collected training data can contain incorrect or\ncorrupted labels. Most existing solutions identify noisy labels and adopt\nactive learning to query human experts on them for denoising. In the era of\nlarge language models (LLMs), although we can reduce the human effort to\nimprove these methods, their performances are still subject to accurately\nseparating the clean and noisy samples from noisy data. In this paper, we\npropose an innovative collaborative learning framework NoiseAL based on active\nlearning to combine LLMs and small models (SMs) for learning from noisy labels.\nDuring collaborative training, we first adopt two SMs to form a co-prediction\nnetwork and propose a dynamic-enhanced threshold strategy to divide the noisy\ndata into different subsets, then select the clean and noisy samples from these\nsubsets to feed the active annotator LLMs to rectify noisy samples. Finally, we\nemploy different optimization objectives to conquer subsets with different\ndegrees of label noises. Extensive experiments on synthetic and real-world\nnoise datasets further demonstrate the superiority of our framework over\nstate-of-the-art baselines.",
        "published": "2025-04-03T04:36:39+00:00"
    },
    {
        "title": "Efficient Computation of Hyper-triangles on Hypergraphs",
        "authors": [
            "Haozhe Yin",
            "Kai Wang",
            "Wenjie Zhang",
            "Ying Zhang",
            "Ruijia Wu",
            "Xuemin Lin"
        ],
        "summary": "Hypergraphs, which use hyperedges to capture groupwise interactions among\ndifferent entities, have gained increasing attention recently for their\nversatility in effectively modeling real-world networks. In this paper, we\nstudy the problem of computing hyper-triangles (formed by three fully-connected\nhyperedges), which is a basic structural unit in hypergraphs. Although existing\napproaches can be adopted to compute hyper-triangles by exhaustively examining\nhyperedge combinations, they overlook the structural characteristics\ndistinguishing different hyper-triangle patterns. Consequently, these\napproaches lack specificity in computing particular hyper-triangle patterns and\nexhibit low efficiency. In this paper, we unveil a new formation pathway for\nhyper-triangles, transitioning from hyperedges to hyperwedges before assembling\ninto hyper-triangles, and classify hyper-triangle patterns based on\nhyperwedges. Leveraging this insight, we introduce a two-step framework to\nreduce the redundant checking of hyperedge combinations. Under this framework,\nwe propose efficient algorithms for computing a specific pattern of\nhyper-triangles. Approximate algorithms are also devised to support estimated\ncounting scenarios. Furthermore, we introduce a fine-grained hypergraph\nclustering coefficient measurement that can reflect diverse properties of\nhypergraphs based on different hyper-triangle patterns. Extensive experimental\nevaluations conducted on 11 real-world datasets validate the effectiveness and\nefficiency of our proposed techniques.",
        "published": "2025-04-03T04:32:37+00:00"
    },
    {
        "title": "MinkOcc: Towards real-time label-efficient semantic occupancy prediction",
        "authors": [
            "Samuel Sze",
            "Daniele De Martini",
            "Lars Kunze"
        ],
        "summary": "Developing 3D semantic occupancy prediction models often relies on dense 3D\nannotations for supervised learning, a process that is both labor and\nresource-intensive, underscoring the need for label-efficient or even\nlabel-free approaches. To address this, we introduce MinkOcc, a multi-modal 3D\nsemantic occupancy prediction framework for cameras and LiDARs that proposes a\ntwo-step semi-supervised training procedure. Here, a small dataset of\nexplicitly 3D annotations warm-starts the training process; then, the\nsupervision is continued by simpler-to-annotate accumulated LiDAR sweeps and\nimages -- semantically labelled through vision foundational models. MinkOcc\neffectively utilizes these sensor-rich supervisory cues and reduces reliance on\nmanual labeling by 90\\% while maintaining competitive accuracy. In addition,\nthe proposed model incorporates information from LiDAR and camera data through\nearly fusion and leverages sparse convolution networks for real-time\nprediction. With its efficiency in both supervision and computation, we aim to\nextend MinkOcc beyond curated datasets, enabling broader real-world deployment\nof 3D semantic occupancy prediction in autonomous driving.",
        "published": "2025-04-03T04:31:56+00:00"
    },
    {
        "title": "MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning in Assistive Driving Perception",
        "authors": [
            "Wenzhuo Liu",
            "Wenshuo Wang",
            "Yicheng Qiao",
            "Qiannan Guo",
            "Jiayin Zhu",
            "Pengfei Li",
            "Zilong Chen",
            "Huiming Yang",
            "Zhiwei Li",
            "Lening Wang",
            "Tiao Tan",
            "Huaping Liu"
        ],
        "summary": "Advanced driver assistance systems require a comprehensive understanding of\nthe driver's mental/physical state and traffic context but existing works often\nneglect the potential benefits of joint learning between these tasks. This\npaper proposes MMTL-UniAD, a unified multi-modal multi-task learning framework\nthat simultaneously recognizes driver behavior (e.g., looking around, talking),\ndriver emotion (e.g., anxiety, happiness), vehicle behavior (e.g., parking,\nturning), and traffic context (e.g., traffic jam, traffic smooth). A key\nchallenge is avoiding negative transfer between tasks, which can impair\nlearning performance. To address this, we introduce two key components into the\nframework: one is the multi-axis region attention network to extract global\ncontext-sensitive features, and the other is the dual-branch multimodal\nembedding to learn multimodal embeddings from both task-shared and\ntask-specific features. The former uses a multi-attention mechanism to extract\ntask-relevant features, mitigating negative transfer caused by task-unrelated\nfeatures. The latter employs a dual-branch structure to adaptively adjust\ntask-shared and task-specific parameters, enhancing cross-task knowledge\ntransfer while reducing task conflicts. We assess MMTL-UniAD on the AIDE\ndataset, using a series of ablation studies, and show that it outperforms\nstate-of-the-art methods across all four tasks. The code is available on\nhttps://github.com/Wenzhuo-Liu/MMTL-UniAD.",
        "published": "2025-04-03T04:23:27+00:00"
    },
    {
        "title": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism",
        "authors": [
            "Ruidong Zhu",
            "Ziheng Jiang",
            "Chao Jin",
            "Peng Wu",
            "Cesar A. Stuardo",
            "Dongyang Wang",
            "Xinlei Zhang",
            "Huaping Zhou",
            "Haoran Wei",
            "Yang Cheng",
            "Jianzhe Xiao",
            "Xinyi Zhang",
            "Lingjun Liu",
            "Haibin Lin",
            "Li-Wen Chang",
            "Jianxi Ye",
            "Xiao Yu",
            "Xuanzhe Liu",
            "Xin Jin",
            "Xin Liu"
        ],
        "summary": "Mixture-of-Experts (MoE) showcases tremendous potential to scale large\nlanguage models (LLMs) with enhanced performance and reduced computational\ncomplexity. However, its sparsely activated architecture shifts feed-forward\nnetworks (FFNs) from being compute-intensive to memory-intensive during\ninference, leading to substantially lower GPU utilization and increased\noperational costs. We present MegaScale-Infer, an efficient and cost-effective\nsystem for serving large-scale MoE models. MegaScale-Infer disaggregates\nattention and FFN modules within each model layer, enabling independent\nscaling, tailored parallelism strategies, and heterogeneous deployment for both\nmodules. To fully exploit disaggregation in the presence of MoE's sparsity,\nMegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a\nrequest batch into micro-batches and shuttles them between attention and FFNs\nfor inference. Combined with distinct model parallelism for each module,\nMegaScale-Infer effectively hides communication overhead and maximizes GPU\nutilization. To adapt to disaggregated attention and FFN modules and minimize\ndata transmission overhead (e.g., token dispatch), MegaScale-Infer provides a\nhigh-performance M2N communication library that eliminates unnecessary\nGPU-to-CPU data copies, group initialization overhead, and GPU synchronization.\nExperimental results indicate that MegaScale-Infer achieves up to 1.90x higher\nper-GPU throughput than state-of-the-art solutions.",
        "published": "2025-04-03T04:20:44+00:00"
    },
    {
        "title": "Implicit Neural Differential Model for Spatiotemporal Dynamics",
        "authors": [
            "Deepak Akhare",
            "Pan Du",
            "Tengfei Luo",
            "Jian-Xun Wang"
        ],
        "summary": "Hybrid neural-physics modeling frameworks through differentiable programming\nhave emerged as powerful tools in scientific machine learning, enabling the\nintegration of known physics with data-driven learning to improve prediction\naccuracy and generalizability. However, most existing hybrid frameworks rely on\nexplicit recurrent formulations, which suffer from numerical instability and\nerror accumulation during long-horizon forecasting. In this work, we introduce\nIm-PiNDiff, a novel implicit physics-integrated neural differentiable solver\nfor stable and accurate modeling of spatiotemporal dynamics. Inspired by deep\nequilibrium models, Im-PiNDiff advances the state using implicit fixed-point\nlayers, enabling robust long-term simulation while remaining fully end-to-end\ndifferentiable. To enable scalable training, we introduce a hybrid gradient\npropagation strategy that integrates adjoint-state methods with reverse-mode\nautomatic differentiation. This approach eliminates the need to store\nintermediate solver states and decouples memory complexity from the number of\nsolver iterations, significantly reducing training overhead. We further\nincorporate checkpointing techniques to manage memory in long-horizon rollouts.\nNumerical experiments on various spatiotemporal PDE systems, including\nadvection-diffusion processes, Burgers' dynamics, and multi-physics chemical\nvapor infiltration processes, demonstrate that Im-PiNDiff achieves superior\npredictive performance, enhanced numerical stability, and substantial\nreductions in memory and runtime cost relative to explicit and naive implicit\nbaselines. This work provides a principled, efficient, and scalable framework\nfor hybrid neural-physics modeling.",
        "published": "2025-04-03T04:07:18+00:00"
    },
    {
        "title": "In-situ three-dimensional strain engineering of solid-state quantum emitters in photonic structures towards scalable quantum networks",
        "authors": [
            "Yan Chen",
            "Xueshi Li",
            "Shunfa Liu",
            "Jiawei Yang",
            "Yuming Wei",
            "Kaili Xiong",
            "Yangpeng Wang",
            "Jiawei Wang",
            "Pingxing Chen",
            "Xiao Li",
            "Chaofan Zhang",
            "Ying Yu",
            "Tian Jiang",
            "Jin Liu"
        ],
        "summary": "Solid-state quantum emitters are pivotal for modern photonic quantum\ntechnology, yet their inherent spectral inhomogeneity imposes a critical\nchallenge in pursuing scalable quantum network. Here, we develop a\ncryogenic-compatible strain-engineering platform based on a\npolydimethylsiloxane (PDMS) stamp that is not obviously working properly at\ncryogenic temperature. In-situ three-dimensional (3D) strain control is\nachieved for quantum dots (QDs) embedded in photonic nanostructures. The\ncompliant PDMS enables independent tuning of emission energy and elimination of\nfine structure splitting (FSS) of single QDs, as demonstrated by a 7 meV\nspectral shift with a near-vanishing FSS in circular Bragg resonators and an\nunprecedented 15 meV tuning range in the micropillar. The PDMS-based 3D\nstrain-engineering platform, compatible with diverse photonic structures at\ncryogenic temperature, provides a powerful and versatile tool for exploring\nfundamental strain-related physics and advancing integrated photonic quantum\ntechnology.",
        "published": "2025-04-03T04:00:25+00:00"
    },
    {
        "title": "Stock Price Prediction Using Triple Barrier Labeling and Raw OHLCV Data: Evidence from Korean Markets",
        "authors": [
            "Sungwoo Kang"
        ],
        "summary": "This paper demonstrates that deep learning models trained on raw OHLCV\n(open-high-low-close-volume) data can achieve comparable performance to\ntraditional machine learning (ML) models using technical indicators for stock\nprice prediction in Korean markets. While previous studies have emphasized the\nimportance of technical indicators and feature engineering, we show that a\nsimple LSTM network trained on raw OHLCV data alone can match the performance\nof sophisticated ML models that incorporate technical indicators. Using a\ndataset of Korean stocks from 2006 to 2024, we optimize the triple barrier\nlabeling parameters to achieve balanced label proportions with a 29-day window\nand 9\\% barriers. Our experiments reveal that LSTM networks achieve similar\nperformance to traditional machine learning models like XGBoost, despite using\nonly raw OHLCV data without any technical indicators. Furthermore, we identify\nthat the optimal window size varies with model hidden size, with a\nconfiguration of window size 100 and hidden size 8 yielding the best\nperformance. Additionally, our results confirm that using full OHLCV data\nprovides better predictive accuracy compared to using only close price or close\nprice with volume. These findings challenge conventional approaches to feature\nengineering in financial forecasting and suggest that simpler approaches\nfocusing on raw data and appropriate model selection may be more effective than\ncomplex feature engineering strategies.",
        "published": "2025-04-03T03:30:50+00:00"
    },
    {
        "title": "CRC-SGAD: Conformal Risk Control for Supervised Graph Anomaly Detection",
        "authors": [
            "Songran Bai",
            "Xiaolong Zheng",
            "Daniel Dajun Zeng"
        ],
        "summary": "Graph Anomaly Detection (GAD) is critical in security-sensitive domains, yet\nfaces reliability challenges: miscalibrated confidence estimation\n(underconfidence in normal nodes, overconfidence in anomalies), adversarial\nvulnerability of derived confidence score under structural perturbations, and\nlimited efficacy of conventional calibration methods for sparse anomaly\npatterns. Thus we propose CRC-SGAD, a framework integrating statistical risk\ncontrol into GAD via two innovations: (1) A Dual-Threshold Conformal Risk\nControl mechanism that provides theoretically guaranteed bounds for both False\nNegative Rate (FNR) and False Positive Rate (FPR) through providing prediction\nsets; (2) A Subgraph-aware Spectral Graph Neural Calibrator (SSGNC) that\noptimizes node representations through adaptive spectral filtering while\nreducing the size of prediction sets via hybrid loss optimization. Experiments\non four datasets and five GAD models demonstrate statistically significant\nimprovements in FNR and FPR control and prediction set size. CRC-SGAD\nestablishes a paradigm for statistically rigorous anomaly detection in\ngraph-structured security applications.",
        "published": "2025-04-03T03:27:49+00:00"
    },
    {
        "title": "Super diffusive length dependent thermal conductivity in one-dimensional materials with structural defects: longitudinal to transverse phonon scattering leads to $\u03ba\\propto L^{1/3}$ law",
        "authors": [
            "Alexander L. Burin"
        ],
        "summary": "Structural defects in one-dimensional heat conductors couple longitudinal\n(stretching) and transverse (bending) vibrations. This coupling results in the\nscattering of longitudinal phonons to transverse phonons and backwards. We show\nthat the decay rate of longitudinal phonons due to this scattering scales with\ntheir frequencies as $\\omega^{3/2}$ within the long wavelength limit ($\\omega\n\\rightarrow 0$), which is more efficient scattering compared to the\ntraditionally considered Rayleigh scattering within the longitudinal band\n($\\omega^2$). This scattering results in temperature independent thermal\nconductivity depending on the size as $\\kappa \\propto L^{1/3}$ for sufficiently\nlong materials. This predicted length dependence is observed in nanowires,\nthough the temperature dependence is seen there possibly because of deviations\nfrom pure one-dimensional behavior. The significant effect of interaction of\nlongitudinal phonons with transverse phonons is consistent with the earlier\nobservations of a substantial suppression of thermal energy transport by kinks,\nobviously leading to such interaction, though anharmonic interaction can also\nbe significant.",
        "published": "2025-04-03T03:08:54+00:00"
    },
    {
        "title": "Learning and Improving Backgammon Strategy",
        "authors": [
            "Gregory R. Galperin"
        ],
        "summary": "A novel approach to learning is presented, combining features of on-line and\noff-line methods to achieve considerable performance in the task of learning a\nbackgammon value function in a process that exploits the processing power of\nparallel supercomputers. The off-line methods comprise a set of techniques for\nparallelizing neural network training and $TD(\\lambda)$ reinforcement learning;\nhere Monte-Carlo ``Rollouts'' are introduced as a massively parallel on-line\npolicy improvement technique which applies resources to the decision points\nencountered during the search of the game tree to further augment the learned\nvalue function estimate. A level of play roughly as good as, or possibly better\nthan, the current champion human and computer backgammon players has been\nachieved in a short period of learning.",
        "published": "2025-04-03T02:27:22+00:00"
    },
    {
        "title": "Comparative Analysis of Distributed Caching Algorithms: Performance Metrics and Implementation Considerations",
        "authors": [
            "Helen Mayer",
            "James Richards"
        ],
        "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
        "published": "2025-04-03T02:24:21+00:00"
    },
    {
        "title": "From short-sighted to far-sighted: A comparative study of recursive machine learning approaches for open quantum systems",
        "authors": [
            "Arif Ullah"
        ],
        "summary": "Accurately modeling open quantum system dynamics is crucial for advancing\nquantum technologies, yet traditional methods struggle to balance accuracy and\nefficiency. Machine learning (ML) provides a promising alternative,\nparticularly through recursive models that predict system evolution based on\npast history. While these models have shown success in predicting single\nobservables, their effectiveness in more complex tasks, such as forecasting the\nfull reduced density matrix (RDM), remains unclear. We extend history-based\nrecursive ML approaches to complex quantum systems, comparing four\nphysics-informed neural network (PINN) architectures: (i) single-RDM-predicting\nPINN (SR-PINN), (ii) SR-PINN with simulation parameters (PSR-PINN), (iii)\nmulti-RDMs-predicting PINN (MR-PINN), and (iv) MR-PINN with simulation\nparameters (PMR-PINN). These models are applied to the spin-boson (SB) model\nand the Fenna-Matthews-Olson (FMO) complex. Our results show that SR-PINN and\nPSR-PINN, constrained by a narrow history window, fail to capture complex\nquantum evolution, leading to unstable long-term predictions, especially in\nnonlinear and highly correlated dynamics. In contrast, MR-PINN and PMR-PINN\nimprove accuracy by extending the forecast horizon, incorporating long-range\ncorrelations, and reducing error propagation. Surprisingly, explicitly\nincluding simulation parameters such as temperature and reorganization energy\nin PSR-PINN and PMR-PINN does not consistently enhance accuracy and can even\nreduce performance, suggesting that these effects are already encoded in the\nRDM evolution. These findings highlight the limitations of short-sighted\nrecursive forecasting and demonstrate the superior stability and accuracy of\nfar-sighted approaches for long-term predictions.",
        "published": "2025-04-03T02:14:47+00:00"
    },
    {
        "title": "Image Coding for Machines via Feature-Preserving Rate-Distortion Optimization",
        "authors": [
            "Samuel Fern\u00e1ndez-Mendui\u00f1a",
            "Eduardo Pavez",
            "Antonio Ortega"
        ],
        "summary": "Many images and videos are primarily processed by computer vision algorithms,\ninvolving only occasional human inspection. When this content requires\ncompression before processing, e.g., in distributed applications, coding\nmethods must optimize for both visual quality and downstream task performance.\nWe first show that, given the features obtained from the original and the\ndecoded images, an approach to reduce the effect of compression on a task loss\nis to perform rate-distortion optimization (RDO) using the distance between\nfeatures as a distortion metric. However, optimizing directly such a\nrate-distortion trade-off requires an iterative workflow of encoding, decoding,\nand feature evaluation for each coding parameter, which is computationally\nimpractical. We address this problem by simplifying the RDO formulation to make\nthe distortion term computable using block-based encoders. We first apply\nTaylor's expansion to the feature extractor, recasting the feature distance as\na quadratic metric with the Jacobian matrix of the neural network. Then, we\nreplace the linearized metric with a block-wise approximation, which we call\ninput-dependent squared error (IDSE). To reduce computational complexity, we\napproximate IDSE using Jacobian sketches. The resulting loss can be evaluated\nblock-wise in the transform domain and combined with the sum of squared errors\n(SSE) to address both visual quality and computer vision performance.\nSimulations with AVC across multiple feature extractors and downstream neural\nnetworks show up to 10% bit-rate savings for the same computer vision accuracy\ncompared to RDO based on SSE, with no decoder complexity overhead and just a 7%\nencoder complexity increase.",
        "published": "2025-04-03T02:11:26+00:00"
    },
    {
        "title": "Comparative Analysis of Deepfake Detection Models: New Approaches and Perspectives",
        "authors": [
            "Matheus Martins Batista"
        ],
        "summary": "The growing threat posed by deepfake videos, capable of manipulating\nrealities and disseminating misinformation, drives the urgent need for\neffective detection methods. This work investigates and compares different\napproaches for identifying deepfakes, focusing on the GenConViT model and its\nperformance relative to other architectures present in the DeepfakeBenchmark.\nTo contextualize the research, the social and legal impacts of deepfakes are\naddressed, as well as the technical fundamentals of their creation and\ndetection, including digital image processing, machine learning, and artificial\nneural networks, with emphasis on Convolutional Neural Networks (CNNs),\nGenerative Adversarial Networks (GANs), and Transformers. The performance\nevaluation of the models was conducted using relevant metrics and new datasets\nestablished in the literature, such as WildDeep-fake and DeepSpeak, aiming to\nidentify the most effective tools in the battle against misinformation and\nmedia manipulation. The obtained results indicated that GenConViT, after\nfine-tuning, exhibited superior performance in terms of accuracy (93.82%) and\ngeneralization capacity, surpassing other architectures in the\nDeepfakeBenchmark on the DeepSpeak dataset. This study contributes to the\nadvancement of deepfake detection techniques, offering contributions to the\ndevelopment of more robust and effective solutions against the dissemination of\nfalse information.",
        "published": "2025-04-03T02:10:27+00:00"
    },
    {
        "title": "FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention",
        "authors": [
            "Huangliang Dai",
            "Shixun Wu",
            "Hairui Zhao",
            "Jiajun Huang",
            "Zizhe Jian",
            "Yue Zhu",
            "Haiyang Hu",
            "Zizhong Chen"
        ],
        "summary": "Transformer models leverage self-attention mechanisms to capture complex\ndependencies, demonstrating exceptional performance in various applications.\nHowever, the long-duration high-load computations required for model inference\nimpose stringent reliability demands on the computing platform, as soft errors\nthat occur during execution can significantly degrade model performance.\nExisting fault tolerance methods protect each operation separately using\ndecoupled kernels, incurring substantial computational and memory overhead. In\nthis paper, we propose a novel error-resilient framework for Transformer\nmodels, integrating end-to-end fault tolerant attention (EFTA) to improve\ninference reliability against soft errors. Our approach enables error detection\nand correction within a fully fused attention kernel, reducing redundant data\naccess and thereby mitigating memory faults. To further enhance error coverage\nand reduce overhead, we design a hybrid fault tolerance scheme tailored for the\nEFTA, introducing for the first time: 1) architecture-aware algorithm-based\nfault tolerance (ABFT) using tensor checksum, which minimizes inter-thread\ncommunication overhead on tensor cores during error detection; 2) selective\nneuron value restriction, which selectively applies adaptive fault tolerance\nconstraints to neuron values, balancing error coverage and overhead; 3) unified\nverification, reusing checksums to streamline multiple computation steps into a\nsingle verification process. Experimental results show that EFTA achieves up to\n7.56x speedup over traditional methods with an average fault tolerance overhead\nof 13.9%.",
        "published": "2025-04-03T02:05:08+00:00"
    },
    {
        "title": "Mid-Infrared Imaging Spectroscopy of N2O Solid Simulating the haze of trans-Neptunian objects",
        "authors": [
            "Daiki Takama",
            "Ryoichi Koga",
            "Shohei Negishi",
            "Biao Zhao",
            "Yuan Li",
            "Yasuhiro Hirahara",
            "Fumiyuki Ito"
        ],
        "summary": "\\ Nitrous oxide (N$_2$O) ice is likely to exist in trans-Neptunian objects\nsuch as Pluto and Triton, potentially formed through ultraviolet (UV) radiation\nfrom the Sun or cosmic ray irradiation of N$_2$ and CO ices. However, the\nmid-infrared spectral characteristics of N$_2$O ice in higher temperature\nregions (90-110 K), changes in mid-infrared spectra during UV irradiation, and\nthe chemical network of nitrogen oxide (N$_x$O$_y$) ices remain insufficiently\nunderstood. This study aims to elucidate these aspects through in-situ\nmid-infrared spectral measurements of cryogenic particles using two-dimensional\nimaging Fourier transform infrared spectroscopy.\n  Spectroscopic imaging confirmed strong absorption at 7.75 $\\mu$m (N$_2$O\n$\\nu_1$ vibrational mode), with weaker vibrational modes observed at 8.60\n$\\mu$m (N$_2$O 2$\\nu_2$), 7.27 $\\mu$m (N$_2$O torsion), and 5.29 $\\mu$m (N$_2$O\n$\\nu_1$+$\\nu_2$). Annealing experiments simulating high-temperature conditions\ndemonstrated that all vibrational modes irreversibly intensified with\nincreasing temperature, indicating progressive crystallization. New spectral\nfeatures appeared at approximately 12 $\\mu$m and 14 $\\mu$m at the condensed\nsample.\n  N$_2$O ice was exposed to ultraviolet radiation (190-340 nm) using a D$_2$\nlamp for 8.5 hours to investigate spectral changes during UV irradiation. After\n60-90 minutes of irradiation, all N$_2$O vibrational modes disappeared, while\nabsorption intensities of various nitrogen oxides, including NO, NO$_2$,\nN$_2$O$_3$, and O$_3$ increased. Beyond 180 minutes, vibrational modes of\nmultiple nitrogen oxide ices exhibited intensity variations across different\nwavelengths, corresponding to other species such as cis-(NO)$_2$, N$_2$O$_4$,\nand N$_2$O$_5$.",
        "published": "2025-04-03T02:03:25+00:00"
    },
    {
        "title": "LLM-Augmented Graph Neural Recommenders: Integrating User Reviews",
        "authors": [
            "Hiroki Kanezashi",
            "Toyotaro Suzumura",
            "Cade Reid",
            "Md Mostafizur Rahman",
            "Yu Hirate"
        ],
        "summary": "Recommender systems increasingly aim to combine signals from both user\nreviews and purchase (or other interaction) behaviors. While user-written\ncomments provide explicit insights about preferences, merging these textual\nrepresentations from large language models (LLMs) with graph-based embeddings\nof user actions remains a challenging task. In this work, we propose a\nframework that employs both a Graph Neural Network (GNN)-based model and an LLM\nto produce review-aware representations, preserving review semantics while\nmitigating textual noise. Our approach utilizes a hybrid objective that\nbalances user-item interactions against text-derived features, ensuring that\nuser's both behavioral and linguistic signals are effectively captured. We\nevaluate this method on multiple datasets from diverse application domains,\ndemonstrating consistent improvements over a baseline GNN-based recommender\nmodel. Notably, our model achieves significant gains in recommendation accuracy\nwhen review data is sparse or unevenly distributed. These findings highlight\nthe importance of integrating LLM-driven textual feedback with GNN-derived user\nbehavioral patterns to develop robust, context-aware recommender systems.",
        "published": "2025-04-03T00:40:09+00:00"
    },
    {
        "title": "A User-Tunable Machine Learning Framework for Step-Wise Synthesis Planning",
        "authors": [
            "Shivesh Prakash",
            "Hans-Arno Jacobsen",
            "Viki Kumar Prasad"
        ],
        "summary": "We introduce MHNpath, a machine learning-driven retrosynthetic tool designed\nfor computer-aided synthesis planning. Leveraging modern Hopfield networks and\nnovel comparative metrics, MHNpath efficiently prioritizes reaction templates,\nimproving the scalability and accuracy of retrosynthetic predictions. The tool\nincorporates a tunable scoring system that allows users to prioritize pathways\nbased on cost, reaction temperature, and toxicity, thereby facilitating the\ndesign of greener and cost-effective reaction routes. We demonstrate its\neffectiveness through case studies involving complex molecules from\nChemByDesign, showcasing its ability to predict novel synthetic and enzymatic\npathways. Furthermore, we benchmark MHNpath against existing frameworks,\nreplicating experimentally validated \"gold-standard\" pathways from PaRoutes.\nOur case studies reveal that the tool can generate shorter, cheaper,\nmoderate-temperature routes employing green solvents, as exemplified by\ncompounds such as dronabinol, arformoterol, and lupinine.",
        "published": "2025-04-03T00:23:21+00:00"
    },
    {
        "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
        "authors": [
            "Lele Cao"
        ],
        "summary": "Advances in AI-generated content have led to wide adoption of large language\nmodels, diffusion-based visual generators, and synthetic audio tools. However,\nthese developments raise critical concerns about misinformation, copyright\ninfringement, security threats, and the erosion of public trust. In this paper,\nwe explore an extensive range of methods designed to detect and mitigate\nAI-generated textual, visual, and audio content. We begin by discussing\nmotivations and potential impacts associated with AI-based content generation,\nincluding real-world risks and ethical dilemmas. We then outline detection\ntechniques spanning observation-based strategies, linguistic and statistical\nanalysis, model-based pipelines, watermarking and fingerprinting, as well as\nemergent ensemble approaches. We also present new perspectives on robustness,\nadaptation to rapidly improving generative architectures, and the critical role\nof human-in-the-loop verification. By surveying state-of-the-art research and\nhighlighting case studies in academic, journalistic, legal, and industrial\ncontexts, this paper aims to inform robust solutions and policymaking. We\nconclude by discussing open challenges, including adversarial transformations,\ndomain generalization, and ethical concerns, thereby offering a holistic guide\nfor researchers, practitioners, and regulators to preserve content authenticity\nin the face of increasingly sophisticated AI-generated media.",
        "published": "2025-04-02T23:27:55+00:00"
    },
    {
        "title": "Who Should Set the Standards? Analysing Censored Arabic Content on Facebook during the Palestine-Israel Conflict",
        "authors": [
            "Walid Magdy",
            "Hamdy Mubarak",
            "Joni Salminen"
        ],
        "summary": "Nascent research on human-computer interaction concerns itself with fairness\nof content moderation systems. Designing globally applicable content moderation\nsystems requires considering historical, cultural, and socio-technical factors.\nInspired by this line of work, we investigate Arab users' perception of\nFacebook's moderation practices. We collect a set of 448 deleted Arabic posts,\nand we ask Arab annotators to evaluate these posts based on (a) Facebook\nCommunity Standards (FBCS) and (b) their personal opinion. Each post was judged\nby 10 annotators to account for subjectivity. Our analysis shows a clear gap\nbetween the Arabs' understanding of the FBCS and how Facebook implements these\nstandards. The study highlights a need for discussion on the moderation\nguidelines on social media platforms about who decides the moderation\nguidelines, how these guidelines are interpreted, and how well they represent\nthe views of marginalised user communities.",
        "published": "2025-04-02T23:22:13+00:00"
    },
    {
        "title": "FastFlow: Early Yet Robust Network Flow Classification using the Minimal Number of Time-Series Packets",
        "authors": [
            "Rushi Jayeshkumar Babaria",
            "Minzhao Lyu",
            "Gustavo Batista",
            "Vijay Sivaraman"
        ],
        "summary": "Network traffic classification is of great importance for network operators\nin their daily routines, such as analyzing the usage patterns of multimedia\napplications and optimizing network configurations. Internet service providers\n(ISPs) that operate high-speed links expect network flow classifiers to\naccurately classify flows early, using the minimal number of necessary initial\npackets per flow. These classifiers must also be robust to packet sequence\ndisorders in candidate flows and capable of detecting unseen flow types that\nare not within the existing classification scope, which are not well achieved\nby existing methods. In this paper, we develop FastFlow, a time-series flow\nclassification method that accurately classifies network flows as one of the\nknown types or the unknown type, which dynamically selects the minimal number\nof packets to balance accuracy and efficiency. Toward the objectives, we first\ndevelop a flow representation process that converts packet streams at both\nper-packet and per-slot granularity for precise packet statistics with\nrobustness to packet sequence disorders. Second, we develop a sequential\ndecision-based classification model that leverages LSTM architecture trained\nwith reinforcement learning. Our model makes dynamic decisions on the minimal\nnumber of time-series data points per flow for the confident classification as\none of the known flow types or an unknown one. We evaluated our method on\npublic datasets and demonstrated its superior performance in early and accurate\nflow classification. Deployment insights on the classification of over 22.9\nmillion flows across seven application types and 33 content providers in a\ncampus network over one week are discussed, showing that FastFlow requires an\naverage of only 8.37 packets and 0.5 seconds to classify the application type\nof a flow with over 91% accuracy and over 96% accuracy for the content\nproviders.",
        "published": "2025-04-02T23:17:14+00:00"
    },
    {
        "title": "HQCC: A Hybrid Quantum-Classical Classifier with Adaptive Structure",
        "authors": [
            "Ren-Xin Zhao",
            "Xinze Tong",
            "Shi Wang"
        ],
        "summary": "Parameterized Quantum Circuits (PQCs) with fixed structures severely degrade\nthe performance of Quantum Machine Learning (QML). To address this, a Hybrid\nQuantum-Classical Classifier (HQCC) is proposed. It opens a practical way to\nadvance QML in the Noisy Intermediate-Scale Quantum (NISQ) era by adaptively\noptimizing the PQC through a Long Short-Term Memory (LSTM) driven dynamic\ncircuit generator, utilizing a local quantum filter for scalable feature\nextraction, and exploiting architectural plasticity to balance the entanglement\ndepth and noise robustness. We realize the HQCC on the TensorCircuit platform\nand run simulations on the MNIST and Fashion MNIST datasets, achieving up to\n97.12\\% accuracy on MNIST and outperforming several alternative methods.",
        "published": "2025-04-02T22:49:00+00:00"
    },
    {
        "title": "Neural Style Transfer for Synthesising a Dataset of Ancient Egyptian Hieroglyphs",
        "authors": [
            "Lewis Matheson Creed"
        ],
        "summary": "The limited availability of training data for low-resource languages makes\napplying machine learning techniques challenging. Ancient Egyptian is one such\nlanguage with few resources. However, innovative applications of data\naugmentation methods, such as Neural Style Transfer, could overcome these\nbarriers. This paper presents a novel method for generating datasets of ancient\nEgyptian hieroglyphs by applying NST to a digital typeface. Experimental\nresults found that image classification models trained on NST-generated\nexamples and photographs demonstrate equal performance and transferability to\nreal unseen images of hieroglyphs.",
        "published": "2025-04-02T22:30:45+00:00"
    },
    {
        "title": "Toward a Sustainable Low-Altitude Economy: A Survey of Energy-Efficient RIS-UAV Networks",
        "authors": [
            "Manzoor Ahmed",
            "Aized Amin Soofi",
            "Feroz Khan",
            "Salman Raza",
            "Wali Ullah Khan",
            "Lina Su",
            "Fang Xu",
            "Zhu Han"
        ],
        "summary": "The integration of RIS into UAV networks presents a transformative solution\nfor achieving energy-efficient and reliable communication, particularly within\nthe rapidly expanding low-altitude economy (LAE). As UAVs facilitate diverse\naerial services-spanning logistics to smart surveillance-their limited energy\nreserves create significant challenges. RIS effectively addresses this issue by\ndynamically shaping the wireless environment to enhance signal quality, reduce\npower consumption, and extend UAV operation time, thus enabling sustainable and\nscalable deployment across various LAE applications. This survey provides a\ncomprehensive review of RIS-assisted UAV networks, focusing on energy-efficient\ndesign within LAE applications. We begin by introducing the fundamentals of\nRIS, covering its operational modes, deployment architectures, and roles in\nboth terrestrial and aerial environments. Next, advanced EE-driven strategies\nfor integrating RIS and UAVs. Techniques such as trajectory optimization, power\ncontrol, beamforming, and dynamic resource management are examined. Emphasis is\nplaced on collaborative solutions that incorporate UAV-mounted RIS, wireless\nenergy harvesting (EH), and intelligent scheduling frameworks. We further\ncategorize RIS-enabled schemes based on key performance objectives relevant to\nLAE scenarios. These objectives include sum rate maximization, coverage\nextension, QoS guarantees, secrecy rate improvement, latency reduction, and age\nof information (AoI) minimization. The survey also delves into RIS-UAV synergy\nwith emerging technologies like MEC, NOMA, V2X communication, and WPT. These\ntechnologies are crucial to the LAE ecosystem. Finally, we outline open\nresearch challenges and future directions, emphasizing the critical role of\nenergy-aware, RIS-enhanced UAV networks in shaping scalable, sustainable, and\nintelligent infrastructures within the LAE.",
        "published": "2025-04-02T22:22:23+00:00"
    },
    {
        "title": "UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting",
        "authors": [
            "Jaehoon Choi",
            "Dongki Jung",
            "Yonghan Lee",
            "Sungmin Eum",
            "Dinesh Manocha",
            "Heesung Kwon"
        ],
        "summary": "We present UAVTwin, a method for creating digital twins from real-world\nenvironments and facilitating data augmentation for training downstream models\nembedded in unmanned aerial vehicles (UAVs). Specifically, our approach focuses\non synthesizing foreground components, such as various human instances in\nmotion within complex scene backgrounds, from UAV perspectives. This is\nachieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing\nbackgrounds along with controllable synthetic human models that display diverse\nappearances and actions in multiple poses. To the best of our knowledge,\nUAVTwin is the first approach for UAV-based perception that is capable of\ngenerating high-fidelity digital twins based on 3DGS. The proposed work\nsignificantly enhances downstream models through data augmentation for\nreal-world environments with multiple dynamic objects and significant\nappearance variations-both of which typically introduce artifacts in 3DGS-based\nmodeling. To tackle these challenges, we propose a novel appearance modeling\nstrategy and a mask refinement module to enhance the training of 3D Gaussian\nSplatting. We demonstrate the high quality of neural rendering by achieving a\n1.23 dB improvement in PSNR compared to recent methods. Furthermore, we\nvalidate the effectiveness of data augmentation by showing a 2.5% to 13.7%\nimprovement in mAP for the human detection task.",
        "published": "2025-04-02T22:17:30+00:00"
    },
    {
        "title": "Niche Dynamics in Complex Online Community Ecosystems",
        "authors": [
            "Nathan TeBlunthuis"
        ],
        "summary": "Online communities are important organizational forms where members socialize\nand share information. Curiously, different online communities often overlap\nconsiderably in topic and membership. Recent research has investigated\ncompetition and mutualism among overlapping online communities through the lens\nof organizational ecology; however, it has not accounted for how the nonlinear\ndynamics of online attention may lead to episodic competition and mutualism.\nNeither has it explored the origins of competition and mutualism in the\nprocesses by which online communities select or adapt to their niches. This\npaper presents a large-scale study of 8,806 Reddit communities belonging to\n1,919 clusters of high user overlap over a 5-year period. The method uses\nnonlinear time series methods to infer bursty, often short-lived ecological\ndynamics. Results reveal that mutualism episodes are longer lived and slightly\nmore frequent than competition episodes. Next, it tests whether online\ncommunities find their niches by specializing to avoid competition using panel\nregression models. It finds that competitive ecological interactions lead to\ndecreasing topic and user overlaps; however, changes that decrease such niche\noverlaps do not lead to mutualism. The discussion proposes that future designs\nmay enable online community ecosystem management by informing online community\nleaders to organize \"spin-off\" communities or via feeds and recommendations.",
        "published": "2025-04-02T21:56:46+00:00"
    },
    {
        "title": "OmniCellTOSG: The First Cell Text-Omic Signaling Graphs Dataset for Joint LLM and GNN Modeling",
        "authors": [
            "Heming Zhang",
            "Tim Xu",
            "Dekang Cao",
            "Shunning Liang",
            "Lars Schimmelpfennig",
            "Levi Kaster",
            "Di Huang",
            "Carlos Cruchaga",
            "Guangfu Li",
            "Michael Province",
            "Yixin Chen",
            "Philip Payne",
            "Fuhai Li"
        ],
        "summary": "Complex cell signaling systems -- governed by varying protein abundances and\ninteractions -- generate diverse cell types across organs. These systems evolve\nunder influences such as age, sex, diet, environmental exposures, and diseases,\nmaking them challenging to decode given the involvement of tens of thousands of\ngenes and proteins. Recently, hundreds of millions of single-cell omics data\nhave provided a robust foundation for understanding these signaling networks\nwithin various cell subpopulations and conditions. Inspired by the success of\nlarge foundation models (for example, large language models and large vision\nmodels) pre-trained on massive datasets, we introduce OmniCellTOSG, the first\ndataset of cell text-omic signaling graphs (TOSGs). Each TOSG represents the\nsignaling network of an individual or meta-cell and is labeled with information\nsuch as organ, disease, sex, age, and cell subtype. OmniCellTOSG offers two key\ncontributions. First, it introduces a novel graph model that integrates\nhuman-readable annotations -- such as biological functions, cellular locations,\nsignaling pathways, related diseases, and drugs -- with quantitative gene and\nprotein abundance data, enabling graph reasoning to decode cell signaling. This\napproach calls for new joint models combining large language models and graph\nneural networks. Second, the dataset is built from single-cell RNA sequencing\ndata of approximately 120 million cells from diverse tissues and conditions\n(healthy and diseased) and is fully compatible with PyTorch. This facilitates\nthe development of innovative cell signaling models that could transform\nresearch in life sciences, healthcare, and precision medicine. The OmniCellTOSG\ndataset is continuously expanding and will be updated regularly. The dataset\nand code are available at https://github.com/FuhaiLiAiLab/OmniCellTOSG.",
        "published": "2025-04-02T21:47:58+00:00"
    },
    {
        "title": "LL4G: Self-Supervised Dynamic Optimization for Graph-Based Personality Detection",
        "authors": [
            "Lingzhi Shen",
            "Yunfei Long",
            "Xiaohao Cai",
            "Guanming Chen",
            "Yuhan Wang",
            "Imran Razzak",
            "Shoaib Jameel"
        ],
        "summary": "Graph-based personality detection constructs graph structures from textual\ndata, particularly social media posts. Current methods often struggle with\nsparse or noisy data and rely on static graphs, limiting their ability to\ncapture dynamic changes between nodes and relationships. This paper introduces\nLL4G, a self-supervised framework leveraging large language models (LLMs) to\noptimize graph neural networks (GNNs). LLMs extract rich semantic features to\ngenerate node representations and to infer explicit and implicit relationships.\nThe graph structure adaptively adds nodes and edges based on input data,\ncontinuously optimizing itself. The GNN then uses these optimized\nrepresentations for joint training on node reconstruction, edge prediction, and\ncontrastive learning tasks. This integration of semantic and structural\ninformation generates robust personality profiles. Experimental results on\nKaggle and Pandora datasets show LL4G outperforms state-of-the-art models.",
        "published": "2025-04-02T21:46:30+00:00"
    },
    {
        "title": "UAC: Uncertainty-Aware Calibration of Neural Networks for Gesture Detection",
        "authors": [
            "Farida Al Haddad",
            "Yuxin Wang",
            "Malcolm Mielle"
        ],
        "summary": "Artificial intelligence has the potential to impact safety and efficiency in\nsafety-critical domains such as construction, manufacturing, and healthcare.\nFor example, using sensor data from wearable devices, such as inertial\nmeasurement units (IMUs), human gestures can be detected while maintaining\nprivacy, thereby ensuring that safety protocols are followed. However, strict\nsafety requirements in these domains have limited the adoption of AI, since\naccurate calibration of predicted probabilities and robustness against\nout-of-distribution (OOD) data is necessary.\n  This paper proposes UAC (Uncertainty-Aware Calibration), a novel two-step\nmethod to address these challenges in IMU-based gesture recognition. First, we\npresent an uncertainty-aware gesture network architecture that predicts both\ngesture probabilities and their associated uncertainties from IMU data. This\nuncertainty is then used to calibrate the probabilities of each potential\ngesture. Second, an entropy-weighted expectation of predictions over multiple\nIMU data windows is used to improve accuracy while maintaining correct\ncalibration.\n  Our method is evaluated using three publicly available IMU datasets for\ngesture detection and is compared to three state-of-the-art calibration methods\nfor neural networks: temperature scaling, entropy maximization, and Laplace\napproximation. UAC outperforms existing methods, achieving improved accuracy\nand calibration in both OOD and in-distribution scenarios. Moreover, we find\nthat, unlike our method, none of the state-of-the-art methods significantly\nimprove the calibration of IMU-based gesture recognition models. In conclusion,\nour work highlights the advantages of uncertainty-aware calibration of neural\nnetworks, demonstrating improvements in both calibration and accuracy for\ngesture detection using IMU data.",
        "published": "2025-04-02T21:40:01+00:00"
    },
    {
        "title": "Robust Channel Estimation for Optical Wireless Communications Using Neural Network",
        "authors": [
            "Dianxin Luan",
            "John Thompson"
        ],
        "summary": "Optical Wireless Communication (OWC) has gained significant attention due to\nits high-speed data transmission and throughput. Optical wireless channels are\noften assumed to be flat, but we evaluate frequency selective channels to\nconsider high data rate optical wireless or very dispersive environments. To\naddress this for optical scenarios, this paper presents a robust channel\nestimation framework with low-complexity to mitigate frequency-selective\neffects, then to improve system reliability and performance. This channel\nestimation framework contains a neural network that can estimate general\noptical wireless channels without prior channel information about the\nenvironment. Based on this estimate and the corresponding delay spread, one of\nseveral candidate offline-trained neural networks will be activated to predict\nthis channel. Simulation results demonstrate that the proposed method has\nimproved and robust normalized mean square error (NMSE) and bit error rate\n(BER) performance compared to conventional estimation methods while maintaining\ncomputational efficiency. These findings highlight the potential of neural\nnetwork solutions in enhancing the performance of OWC systems under indoor\nchannel conditions.",
        "published": "2025-04-02T21:16:34+00:00"
    },
    {
        "title": "Base Station Certificate and Multi-Factor Authentication for Cellular Radio Control Communication Security",
        "authors": [
            "Sourav Purification",
            "Simeon Wuthier",
            "Jinoh Kim",
            "Ikkyun Kim",
            "Sang-Yoon Chang"
        ],
        "summary": "Current cellular networking remains vulnerable to malicious fake base\nstations due to the lack of base station authentication mechanism or even a key\nto enable authentication. We design and build a base station certificate\n(certifying the base station's public key and location) and a multi-factor\nauthentication (making use of the certificate and the information transmitted\nin the online radio control communications) to secure the authenticity and\nmessage integrity of the base station control communications. We advance beyond\nthe state-of-the-art research by introducing greater authentication factors\n(and analyzing their individual security properties and benefits), and by using\nblockchain to deliver the base station digital certificate offline (enabling\ngreater key length or security strength and computational or networking\nefficiency). We design the certificate construction, delivery, and the\nmulti-factor authentication use on the user equipment. The user verification\ninvolves multiple factors verified through the ledger database, the location\nsensing (GPS in our implementation), and the cryptographic signature\nverification of the cellular control communication (SIB1 broadcasting). We\nanalyze our scheme's security, performance, and the fit to the existing\nstandardized networking protocols. Our work involves the implementation of\nbuilding on X.509 certificate (adapted), smart contract-based blockchain,\n5G-standardized RRC control communications, and software-defined radios. Our\nanalyses show that our scheme effectively defends against more security threats\nand can enable stronger security, i.e., ECDSA with greater key lengths.\nFurthermore, our scheme enables computing and energy to be more than three\ntimes efficient than the previous research on the mobile user equipment.",
        "published": "2025-04-02T21:12:29+00:00"
    },
    {
        "title": "Towards Enabling Learning for Time-Varying finite horizon Sequential Decision-Making Problems*",
        "authors": [
            "Dhananjay Tiwari",
            "Salar Basiri",
            "Srinivasa Salapaka"
        ],
        "summary": "Parameterized Sequential Decision Making (Para-SDM) framework models a wide\narray of network design applications spanning supply-chain, transportation, and\nsensor networks. These problems entail sequential multi-stage optimization\ncharacterized by states, control actions, and cost functions dependent on\ndesignable parameters. The challenge is to determine both the sequential\ndecision policy and parameters simultaneously to minimize cumulative stagewise\ncosts. Many Para-SDM problems are NP-hard and often necessitate time-varying\npolicies. Existing algorithms tackling finite-horizon time-varying Para-SDM\nproblems struggle with scalability when faced with a large number of states.\nConversely, the sole algorithm addressing infinite-horizon Para-SDM assumes\ntime (stage)-invariance, yielding stationary policies. However, this approach\nproves scalable for time-invariant problems by leveraging deep neural networks\nto learn optimal stage-invariant state-action value functions, enabling\nhandling of large-scale scenarios. This article proposes a novel approach that\nreinterprets finite-horizon, time-varying Para-SDM problems as equivalent\ntime-invariant problems through topography lifting. Our method achieves nearly\nidentical results to the time-varying solution while exhibiting improved\nperformance times in various simulations, notably in the small cell network\nproblem. This fresh perspective on Para-SDM problems expands the scope of\naddressable issues and holds promise for future scalability through the\nintegration of learning methods.",
        "published": "2025-04-02T21:05:51+00:00"
    },
    {
        "title": "Achieving Unanimous Consensus in Decision Making Using Multi-Agents",
        "authors": [
            "Apurba Pokharel",
            "Ram Dantu",
            "Shakila Zaman",
            "Sirisha Talapuru",
            "Vinh Quach"
        ],
        "summary": "Blockchain consensus mechanisms have relied on algorithms such as\nProof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality\nand integrity. However, these approaches struggle with adaptability for\ndecision-making where the opinions of each matter rather than reaching an\nagreement based on honest majority or weighted consensus. This paper introduces\na novel deliberation-based consensus mechanism where Large Language Models\n(LLMs) act as rational agents engaging in structured discussions to reach a\nunanimous consensus. By leveraging graded consensus and a multi-round\ndeliberation process, our approach ensures both unanimous consensus for\ndefinitive problems and graded confidence for prioritized decisions and\npolicies. We provide a formalization of our system and use it to show that the\nproperties of blockchains: consistency, agreement, liveness, and determinism\nare maintained. Moreover, experimental results demonstrate our system's\nfeasibility, showcasing how our deliberation method's convergence, block\nproperties, and accuracy enable decision-making on blockchain networks. We also\naddress key challenges with this novel approach such as degeneration of\nthoughts, hallucinations, malicious models and nodes, resource consumption, and\nscalability.",
        "published": "2025-04-02T21:02:54+00:00"
    },
    {
        "title": "Reinsuring AI: Energy, Agriculture, Finance & Medicine as Precedents for Scalable Governance of Frontier Artificial Intelligence",
        "authors": [
            "Nicholas Stetler"
        ],
        "summary": "The governance of frontier artificial intelligence (AI) systems--particularly\nthose capable of catastrophic misuse or systemic failure--requires\ninstitutional structures that are robust, adaptive, and innovation-preserving.\nThis paper proposes a novel framework for governing such high-stakes models\nthrough a three-tiered insurance architecture: (1) mandatory private liability\ninsurance for frontier model developers; (2) an industry-administered risk pool\nto absorb recurring, non-catastrophic losses; and (3) federally backed\nreinsurance for tail-risk events. Drawing from historical precedents in nuclear\nenergy (Price-Anderson), terrorism risk (TRIA), agricultural crop insurance,\nflood reinsurance, and medical malpractice, the proposal shows how the federal\ngovernment can stabilize private AI insurance markets without resorting to\nbrittle regulation or predictive licensing regimes. The structure aligns\nincentives between AI developers and downstream stakeholders, transforms safety\npractices into insurable standards, and enables modular oversight through\nadaptive eligibility criteria. By focusing on risk-transfer mechanisms rather\nthan prescriptive rules, this framework seeks to render AI safety a structural\nfeature of the innovation ecosystem itself--integrated into capital markets,\nnot external to them. The paper concludes with a legal and administrative\nfeasibility analysis, proposing avenues for statutory authorization and agency\nplacement within existing federal structures.",
        "published": "2025-04-02T21:02:19+00:00"
    },
    {
        "title": "Fractal Patterns in Discrete Laplacians: Iterative Construction on 2D Square Lattices",
        "authors": [
            "Ma\u0142gorzata Nowak-K\u0119pczyk"
        ],
        "summary": "We investigate the iterative construction of discrete Laplacians on 2D square\nlattices, revealing emergent fractal-like patterns shaped by modular\narithmetic. While classical 2222-style iterations reproduce known structures\nsuch as the Sierpinski triangle, our alternating binary-ternary (2322-style)\nprocess produces a novel class of aperiodic figures. These display low density\nvariance, minimal connectivity loss, and non-repetitive organization\nreminiscent of Dekking's sequences. Fourier and autocorrelation analyses\nconfirm their quasi-periodic nature, suggesting applications in self-assembly,\nsensor networks, and biological modeling. The findings open new paths toward\nstructured randomness and fractal dynamics in discrete systems.\n  These findings also open avenues for exploring higher-dimensional Laplacian\nconstructions and their implications in quasicrystals, aperiodic tilings, and\nstochastic processes.",
        "published": "2025-04-02T21:02:06+00:00"
    },
    {
        "title": "Graph Analytics for Cyber-Physical System Resilience Quantification",
        "authors": [
            "Romain Dagnas",
            "Michel Barbeau",
            "Joaquin Garcia-Alfaro",
            "Reda Yaich"
        ],
        "summary": "Critical infrastructures integrate a wide range of smart technologies and\nbecome highly connected to the cyber world. This is especially true for\nCyber-Physical Systems (CPSs), which integrate hardware and software\ncomponents. Despite the advantages of smart infrastructures, they remain\nvulnerable to cyberattacks. This work focuses on the cyber resilience of CPSs.\nWe propose a methodology based on knowledge graph modeling and graph analytics\nto quantify the resilience potential of complex systems by using a multilayered\nmodel based on knowledge graphs. Our methodology also allows us to identify\ncritical points. These critical points are components or functions of an\narchitecture that can generate critical failures if attacked. Thus, identifying\nthem can help enhance resilience and avoid cascading effects. We use the SWaT\n(Secure Water Treatment) testbed as a use case to achieve this objective. This\nsystem mimics the actual behavior of a water treatment station in Singapore. We\nmodel three resilient designs of SWaT according to our multilayered model. We\nconduct a resilience assessment based on three relevant metrics used in graph\nanalytics. We compare the results obtained with each metric and discuss their\naccuracy in identifying critical points. We perform an experimentation analysis\nbased on the knowledge gained by a cyber adversary about the system\narchitecture. We show that the most resilient SWaT design has the necessary\npotential to bounce back and absorb the attacks. We discuss our results and\nconclude this work by providing further research axes.",
        "published": "2025-04-02T20:43:40+00:00"
    },
    {
        "title": "Vectorised Parallel in Time methods for low-order discretizations with application to Porous Media problems",
        "authors": [
            "Christian Engwer",
            "Alexander Schell",
            "Nils-Arne Dreier"
        ],
        "summary": "High order methods have shown great potential to overcome performance issues\nof simulations of partial differential equations (PDEs) on modern hardware,\nstill many users stick to low-order, matrixbased simulations, in particular in\nporous media applications. Heterogeneous coefficients and low regularity of the\nsolution are reasons not to employ high order discretizations. We present a new\napproach for the simulation of instationary PDEs that allows to partially\nmitigate the performance problems. By reformulating the original problem we\nderive a parallel in time time integrator that increases the arithmetic\nintensity and introduces additional structure into the problem. By this it\nhelps accelerate matrix-based simulations on modern hardware architectures.\nBased on a system for multiple time steps we will formulate a matrix equation\nthat can be solved using vectorised solvers like Block Krylov methods. The\nstructure of this approach makes it applicable for a wide range of linear and\nnonlinear problems. In our numerical experiments we present some first results\nfor three different PDEs, a linear convection-diffusion equation, a nonlinear\ndiffusion-reaction equation and a realistic example based on the Richards'\nequation.",
        "published": "2025-04-02T20:26:22+00:00"
    },
    {
        "title": "An Integrated Transportation Network and Power Grid Simulation Approach for Assessing Environmental Impact of Electric Vehicles",
        "authors": [
            "Diana Wallison",
            "Jessica Wert",
            "Farnaz Safdarian",
            "Komal Shetye",
            "Thomas J. Overbye",
            "Jonathan M. Snodgrass",
            "Yanzhi Xu"
        ],
        "summary": "This study develops an integrated approach that includes EV charging and\npower generation to assess the complex cross-sector interactions of vehicle\nelectrification and its environmental impact. The charging load from on-road EV\noperation is developed based on a regional-level transportation simulation and\ncharging behavior simulation, considering different EV penetration levels,\ncongestion levels, and charging strategies. The emissions from EGUs are\nestimated from a dispatch study in a power grid simulation using the charging\nload as a major input. A case study of Austin, Texas is performed to quantify\nthe environmental impact of EV adoption on both on-road and EGU emission\nsources at the regional level. The results demonstrate the range of emission\nimpact under a combination of factors.",
        "published": "2025-04-02T19:54:27+00:00"
    },
    {
        "title": "Perturbations and Phase Transitions in Swarm Optimization Algorithms",
        "authors": [
            "Tom\u00e1\u0161 Vantuch",
            "Ivan Zelinka",
            "Andrew Adamatzky",
            "Norbert Marwan"
        ],
        "summary": "Natural systems often exhibit chaotic behavior in their space-time evolution.\nSystems transiting between chaos and order manifest a potential to compute, as\nshown with cellular automata and artificial neural networks. We demonstrate\nthat swarm optimization algorithms also exhibit transitions from chaos,\nanalogous to a motion of gas molecules, when particles explore solution space\ndisorderly, to order, when particles follow a leader, similar to molecules\npropagating along diffusion gradients in liquid solutions of reagents. We\nanalyze these `phase-like' transitions in swarm optimization algorithms using\nrecurrence quantification analysis and Lempel-Ziv complexity estimation. We\ndemonstrate that converging iterations of the optimization algorithms are\nstatistically different from non-converging ones in a view of applied chaos,\ncomplexity and predictability estimating indicators.\n  An identification of a key factor responsible for the intensity of their\nphase transition is the main contribution of this paper. We examined an\noptimization as a process with three variable factors -- an algorithm, number\ngenerator and optimization function. More than 9.000 executions of the\noptimization algorithm revealed that the nature of an applied algorithm itself\nis the main source of the phase transitions. Some of the algorithms exhibit\nlarger transition-shifting behavior while others perform rather\ntransition-steady computing. These findings might be important for future\nextensions of these algorithms.",
        "published": "2025-04-02T19:49:19+00:00"
    },
    {
        "title": "Distributed Resource Allocation for Human-Autonomy Teaming under Coupled Constraints",
        "authors": [
            "Yichen Yao",
            "Ryan Mbagna Nanko",
            "Yue Wang",
            "Xuan Wang"
        ],
        "summary": "This paper studies the optimal resource allocation problem within a\nmulti-agent network composed of both autonomous agents and humans. The main\nchallenge lies in the globally coupled constraints that link the decisions of\nautonomous agents with those of humans. To address this, we propose a\nreformulation that transforms these coupled constraints into decoupled local\nconstraints defined over the system's communication graph. Building on this\nreformulation and incorporating a human response model that captures\nhuman-robot interactions while accounting for individual preferences and\nbiases, we develop a fully distributed algorithm. This algorithm guides the\nstates of the autonomous agents to equilibrium points which, when combined with\nthe human responses, yield a globally optimal resource allocation. We provide\nboth theoretical analysis and numerical simulations to validate the\neffectiveness of the proposed approach.",
        "published": "2025-04-02T19:46:41+00:00"
    },
    {
        "title": "Near-Peak Spectrum of Gravitational Waves from Collapsing Domain Walls",
        "authors": [
            "Bryce Cyr",
            "Steven Cotterill",
            "Richard Battye"
        ],
        "summary": "Cosmological domain walls appear in many well-motivated extensions to the\nstandard model of particle physics. If produced, they quickly enter into a\nself-similar scaling regime, where they are capable of efficiently sourcing a\nstochastic background of gravitational waves. In order to avoid a cosmological\ncatastrophe, they must also decay before their enormous energy densities can\nhave adverse effects on background dynamics. Here, we provide a suite of\nlattice simulations to comprehensively study the gravitational wave signatures\nof the domain wall network during this decay phase. The domain walls are\ninitially formed through spontaneous breaking of a $\\mathbb{Z}_2$ symmetry, and\nsubsequently decay through the action of a small bias term which causes regions\nof false vacuum to collapse. We find that gravitational waves are produced in\nabundance throughout this collapsing phase, leading to a shift in the peak\nfrequency and increase in the overall amplitude of the spectrum by an\n$\\mathcal{O}(100)$ factor when compared against simple analytic arguments.\nImportantly, we also find that the characteristic frequency of emitted\ngravitational waves increases as the network decays, which leads to a softening\nof the high frequency spectral index. This high frequency spectrum therefore\ncarries key information related to the dynamics of the collapsing phase, and\ncan be used to discriminate between different domain wall scenarios using\nupcoming data.",
        "published": "2025-04-02T19:23:20+00:00"
    },
    {
        "title": "Stellar tidal streams around nearby spiral galaxies with deep imaging from amateur telescopes",
        "authors": [
            "David Martinez-Delgado",
            "Michael Stein",
            "Joanna D. Sakowska",
            "M. Maurice Weigelt",
            "Javier Roman",
            "Giuseppe Donatiello",
            "Santi Roca-Fabrega",
            "Mischa Schirmer",
            "Eva K. Grebel",
            "Teymoor Saifollahi",
            "Jeff Kanipe",
            "M. Angeles Gomez-Flechoso",
            "Mohammad Akhlaghi",
            "Behnam Javanmardi",
            "Gang Wu",
            "Sepideh Eskandarlou",
            "Dominik J. Bomans",
            "Cristian Henkel",
            "Adam Block",
            "Mark Hanson",
            "Johannes Schedler",
            "Karel Teuwen",
            "R. Jay GaBany",
            "Alvaro Iba\u00f1ez Perez",
            "Ken Crawford",
            "Wolfgang Promper",
            "Manuel Jimenez",
            "Silvia Farras-Aloy",
            "Juan Miro-Carretero"
        ],
        "summary": "Tidal interactions between massive galaxies and their satellites are\nfundamental processes in a Universe with L-Cold Dark Matter cosmology,\nredistributing material into faint features that preserve records of past\ngalactic interactions. While stellar streams in the Local Group impressively\ndemonstrate satellite disruption, they do not constitute a statistically\nsignificant sample. Constructing a substantial catalog of stellar streams\nbeyond the Local Group remains challenging due to the difficulties in obtaining\ndeep, wide-field images of galaxies. Despite their potential to illuminate dark\nmatter distribution and galaxy formation processes, stellar streams remain\nunderutilized as cosmological probes. The Stellar Tidal Stream Survey (STSS)\naddresses this observational gap by leveraging amateur telescopes to obtain\ndeep, scientific-grade images of galactic outskirts, capable of building a more\nstatistically meaningful sample of stellar streams. Over the last decade, the\nSTSS has acquired deep (up to surface brightness limit 28.3 mag/arcsec^2 in the\nr-band), wide-field images of 15 nearby Milky Way analog galaxies using a\nnetwork of robotic amateur telescopes, avoiding the issues associated with\n\"mosaicing\" smaller images taken with a professional telescope. Our survey has\nrevealed a diverse range of previously unreported faint features related to\ndwarf satellite accretion - including stellar streams, shells, and\numbrella-like structures. We discover an ultra-diffuse galaxy (NGC150-UDG1),\nwhich shows hints of tidal tails. The STSS demonstrates the suitability of\nmodern amateur telescopes to detect and study faint, diffuse structures in\nlarge fields around nearby spiral galaxies. Their economic and accessibility\nadvantages enable larger samples with deep imaging, essential for testing\ngalaxy formation models and constraining the properties of minor merger events\nin the local Universe.",
        "published": "2025-04-02T19:02:42+00:00"
    },
    {
        "title": "RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics",
        "authors": [
            "Zhiyuan Zhang",
            "Yuxin He",
            "Yong Sun",
            "Junyu Shi",
            "Lijiang Liu",
            "Qiang Nie"
        ],
        "summary": "Visual Language Models (VLMs) have emerged as pivotal tools for robotic\nsystems, enabling cross-task generalization, dynamic environmental interaction,\nand long-horizon planning through multimodal perception and semantic reasoning.\nHowever, existing open-source VLMs predominantly trained for generic\nvision-language alignment tasks fail to model temporally correlated action\nsemantics that are crucial for robotic manipulation effectively. While current\nimage-based fine-tuning methods partially adapt VLMs to robotic applications,\nthey fundamentally disregard temporal evolution patterns in video sequences and\nsuffer from visual feature entanglement between robotic agents, manipulated\nobjects, and environmental contexts, thereby limiting semantic decoupling\ncapability for atomic actions and compromising model generalizability.To\novercome these challenges, this work presents RoboAct-CLIP with dual technical\ncontributions: 1) A dataset reconstruction framework that performs\nsemantic-constrained action unit segmentation and re-annotation on open-source\nrobotic videos, constructing purified training sets containing singular atomic\nactions (e.g., \"grasp\"); 2) A temporal-decoupling fine-tuning strategy based on\nContrastive Language-Image Pretraining (CLIP) architecture, which disentangles\ntemporal action features across video frames from object-centric\ncharacteristics to achieve hierarchical representation learning of robotic\natomic actions.Experimental results in simulated environments demonstrate that\nthe RoboAct-CLIP pretrained model achieves a 12% higher success rate than\nbaseline VLMs, along with superior generalization in multi-object manipulation\ntasks.",
        "published": "2025-04-02T19:02:08+00:00"
    },
    {
        "title": "From Text to Graph: Leveraging Graph Neural Networks for Enhanced Explainability in NLP",
        "authors": [
            "Fabio Y\u00e1\u00f1ez-Romero",
            "Andr\u00e9s Montoyo",
            "Armando Su\u00e1rez",
            "Yoan Guti\u00e9rrez",
            "Ruslan Mitkov"
        ],
        "summary": "Researchers have relegated natural language processing tasks to\nTransformer-type models, particularly generative models, because these models\nexhibit high versatility when performing generation and classification tasks.\nAs the size of these models increases, they achieve outstanding results. Given\ntheir widespread use, many explainability techniques are developed based on\nthese models. However, this process becomes computationally expensive due to\nthe large size of the models. Additionally, transformers interpret input\ninformation through tokens that fragment input words into sequences lacking\ninherent semantic meaning, complicating the explanation of the model from the\nvery beginning. This study proposes a novel methodology to achieve\nexplainability in natural language processing tasks by automatically converting\nsentences into graphs and maintaining semantics through nodes and relations\nthat express fundamental linguistic concepts. It also allows the subsequent\nexploitation of this knowledge in subsequent tasks, making it possible to\nobtain trends and understand how the model associates the different elements\ninside the text with the explained task. The experiments delivered promising\nresults in determining the most critical components within the text structure\nfor a given classification.",
        "published": "2025-04-02T18:55:58+00:00"
    },
    {
        "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
        "authors": [
            "Yuxin Guo",
            "Shuailei Ma",
            "Shijie Ma",
            "Xiaoyi Bao",
            "Chen-Wei Xie",
            "Kecheng Zheng",
            "Tingyu Weng",
            "Siyang Sun",
            "Yun Zheng",
            "Wei Zou"
        ],
        "summary": "Audio is essential for multimodal video understanding. On the one hand, video\ninherently contains audio, which supplies complementary information to vision.\nBesides, video large language models (Video-LLMs) can encounter many\naudio-centric settings. However, existing Video-LLMs and Audio-Visual Large\nLanguage Models (AV-LLMs) exhibit deficiencies in exploiting audio information,\nleading to weak understanding and hallucinations. To solve the issues, we delve\ninto the model architecture and dataset. (1) From the architectural\nperspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent\nalignment of audio and visual modalities in both temporal and spatial\ndimensions ensures a comprehensive and accurate understanding of videos.\nSpecifically, we devise an audio-visual multi-scale adapter for multi-scale\ninformation aggregation, which achieves spatial alignment. For temporal\nalignment, we propose audio-visual interleaved merging. (2) From the dataset\nperspective, we curate an audio-visual caption and instruction-tuning dataset,\ncalled AVU. It comprises 5.2 million diverse, open-ended data tuples (video,\naudio, question, answer) and introduces a novel data partitioning strategy.\nExtensive experiments show our model not only achieves remarkable performance\nin audio-visual understanding, but also mitigates potential hallucinations.",
        "published": "2025-04-02T18:47:09+00:00"
    },
    {
        "title": "Negative and positive anisotropic thermal expansion in 2D fullerene networks",
        "authors": [
            "Armaan Shaikh",
            "Bo Peng"
        ],
        "summary": "We find a design principle for tailoring thermal expansion properties in\nmolecular networks. Using 2D fullerene networks as a representative system, we\nrealize positive thermal expansion along intermolecular [2+2] cycloaddition\nbonds and negative thermal expansion along intermolecular C$-$C single bonds by\nvarying the structural frameworks of molecules. The microscopic mechanism\noriginates from a combination of the framework's geometric flexibility and its\ntransverse vibrational characteristics. Based on this insight, we find\nmolecular networks beyond C$_{60}$ with tunable thermal expansion. These\nfindings shed light on the fundamental mechanisms governing thermal expansion\nin molecular networks towards rational materials design.",
        "published": "2025-04-02T18:00:05+00:00"
    },
    {
        "title": "Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis",
        "authors": [
            "Niluthpol Chowdhury Mithun",
            "Tuan Pham",
            "Qiao Wang",
            "Ben Southall",
            "Kshitij Minhas",
            "Bogdan Matei",
            "Stephan Mandt",
            "Supun Samarasekera",
            "Rakesh Kumar"
        ],
        "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance\nFields (NeRF) have achieved impressive results in real-time 3D reconstruction\nand novel view synthesis. However, these methods struggle in large-scale,\nunconstrained environments where sparse and uneven input coverage, transient\nocclusions, appearance variability, and inconsistent camera settings lead to\ndegraded quality. We propose GS-Diff, a novel 3DGS framework guided by a\nmulti-view diffusion model to address these limitations. By generating\npseudo-observations conditioned on multi-view inputs, our method transforms\nunder-constrained 3D reconstruction problems into well-posed ones, enabling\nrobust optimization even with sparse data. GS-Diff further integrates several\nenhancements, including appearance embedding, monocular depth priors, dynamic\nobject modeling, anisotropy regularization, and advanced rasterization\ntechniques, to tackle geometric and photometric challenges in real-world\nsettings. Experiments on four benchmarks demonstrate that GS-Diff consistently\noutperforms state-of-the-art baselines by significant margins.",
        "published": "2025-04-02T17:59:46+00:00"
    },
    {
        "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step",
        "authors": [
            "Hanyang Wang",
            "Fangfu Liu",
            "Jiawei Chi",
            "Yueqi Duan"
        ],
        "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene",
        "published": "2025-04-02T17:59:21+00:00"
    },
    {
        "title": "Towards Unified Referring Expression Segmentation Across Omni-Level Visual Target Granularities",
        "authors": [
            "Jing Liu",
            "Wenxuan Wang",
            "Yisi Zhang",
            "Yepeng Tang",
            "Xingjian He",
            "Longteng Guo",
            "Tongtian Yue",
            "Xinlong Wang"
        ],
        "summary": "Referring expression segmentation (RES) aims at segmenting the entities'\nmasks that match the descriptive language expression. While traditional RES\nmethods primarily address object-level grounding, real-world scenarios demand a\nmore versatile framework that can handle multiple levels of target granularity,\nsuch as multi-object, single object or part-level references. This introduces\ngreat challenges due to the diverse and nuanced ways users describe targets.\nHowever, existing datasets and models mainly focus on designing grounding\nspecialists for object-level target localization, lacking the necessary data\nresources and unified frameworks for the more practical multi-grained RES. In\nthis paper, we take a step further towards visual granularity unified RES task.\nTo overcome the limitation of data scarcity, we introduce a new\nmulti-granularity referring expression segmentation (MRES) task, alongside the\nRefCOCOm benchmark, which includes part-level annotations for advancing\nfiner-grained visual understanding. In addition, we create MRES-32M, the\nlargest visual grounding dataset, comprising over 32.2M masks and captions\nacross 1M images, specifically designed for part-level vision-language\ngrounding. To tackle the challenges of multi-granularity RES, we propose\nUniRES++, a unified multimodal large language model that integrates\nobject-level and part-level RES tasks. UniRES++ incorporates targeted designs\nfor fine-grained visual feature exploration. With the joint model architecture\nand parameters, UniRES++ achieves state-of-the-art performance across multiple\nbenchmarks, including RefCOCOm for MRES, gRefCOCO for generalized RES, and\nRefCOCO, RefCOCO+, RefCOCOg for classic RES. To foster future research into\nmulti-grained visual grounding, our RefCOCOm benchmark, MRES-32M dataset and\nmodel UniRES++ will be publicly available at\nhttps://github.com/Rubics-Xuan/MRES.",
        "published": "2025-04-02T17:58:05+00:00"
    },
    {
        "title": "Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging",
        "authors": [
            "Mohini Anand",
            "Xavier Tricoche"
        ],
        "summary": "Understanding the complex myocardial architecture is critical for diagnosing\nand treating heart disease. However, existing methods often struggle to\naccurately capture this intricate structure from Diffusion Tensor Imaging (DTI)\ndata, particularly due to the lack of ground truth labels and the ambiguous,\nintertwined nature of fiber trajectories. We present a novel deep learning\nframework for unsupervised clustering of myocardial fibers, providing a\ndata-driven approach to identifying distinct fiber bundles. We uniquely combine\na Bidirectional Long Short-Term Memory network to capture local sequential\ninformation along fibers, with a Transformer autoencoder to learn global shape\nfeatures, with pointwise incorporation of essential anatomical context.\nClustering these representations using a density-based algorithm identifies 33\nto 62 robust clusters, successfully capturing the subtle distinctions in fiber\ntrajectories with varying levels of granularity. Our framework offers a new,\nflexible, and quantitative way to analyze myocardial structure, achieving a\nlevel of delineation that, to our knowledge, has not been previously achieved,\nwith potential applications in improving surgical planning, characterizing\ndisease-related remodeling, and ultimately, advancing personalized cardiac\ncare.",
        "published": "2025-04-02T17:56:57+00:00"
    },
    {
        "title": "PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System",
        "authors": [
            "Manos Frouzakis",
            "Juan G\u00f3mez-Luna",
            "Geraldo F. Oliveira",
            "Mohammad Sadrosadati",
            "Onur Mutlu"
        ],
        "summary": "Database Management Systems (DBMSs) are crucial for efficient data management\nand analytics, and are used in several different application domains. Due to\nthe increasing volume of data a DBMS deals with, current processor-centric\narchitectures (e.g., CPUs, GPUs) suffer from data movement bottlenecks when\nexecuting key DBMS operations (e.g., selection, aggregation, ordering, and\njoin). This happens mostly due to the limited memory bandwidth between compute\nand memory resources. Data-centric architectures like Processing-in-Memory\n(PIM) are a promising alternative for applications bottlenecked by data,\nplacing compute resources close to where data resides. Previous works have\nevaluated using PIM for data analytics. However, they either do not use\nreal-world architectures or they consider only a subset of the operators used\nin analytical queries. This work aims to fully evaluate a data-centric approach\nto data analytics, by using the real-world UPMEM PIM system. To this end we\nfirst present the PIM Data Analytics Library (PIMDAL), which implements four\nmajor DB operators: selection, aggregation, ordering and join. Second, we use\nhardware performance metrics to understand which properties of a PIM system are\nimportant for a high-performance implementation. Third, we compare PIMDAL to\nreference implementations on high-end CPU and GPU systems. Fourth, we use\nPIMDAL to implement five TPC-H queries to gain insights into analytical\nqueries. We analyze and show how to overcome the three main limitations of the\nUPMEM system when implementing DB operators: (I) low arithmetic performance,\n(II) explicit memory management and (III) limited communication between compute\nunits. Our evaluation shows PIMDAL achieves 3.9x the performance of a high-end\nCPU, on average across the five TPC-H queries.",
        "published": "2025-04-02T17:54:33+00:00"
    },
    {
        "title": "Efficient Federated Learning Tiny Language Models for Mobile Network Feature Prediction",
        "authors": [
            "Daniel Becking",
            "Ingo Friese",
            "Karsten M\u00fcller",
            "Thomas Buchholz",
            "Mandy Galkow-Schneider",
            "Wojciech Samek",
            "Detlev Marpe"
        ],
        "summary": "In telecommunications, Autonomous Networks (ANs) automatically adjust\nconfigurations based on specific requirements (e.g., bandwidth) and available\nresources. These networks rely on continuous monitoring and intelligent\nmechanisms for self-optimization, self-repair, and self-protection, nowadays\nenhanced by Neural Networks (NNs) to enable predictive modeling and pattern\nrecognition. Here, Federated Learning (FL) allows multiple AN cells - each\nequipped with NNs - to collaboratively train models while preserving data\nprivacy. However, FL requires frequent transmission of large neural data and\nthus an efficient, standardized compression strategy for reliable\ncommunication. To address this, we investigate NNCodec, a Fraunhofer\nimplementation of the ISO/IEC Neural Network Coding (NNC) standard, within a\nnovel FL framework that integrates tiny language models (TLMs) for various\nmobile network feature prediction (e.g., ping, SNR or band frequency). Our\nexperimental results on the Berlin V2X dataset demonstrate that NNCodec\nachieves transparent compression (i.e., negligible performance loss) while\nreducing communication overhead to below 1%, showing the effectiveness of\ncombining NNC with FL in collaboratively learned autonomous mobile networks.",
        "published": "2025-04-02T17:54:06+00:00"
    },
    {
        "title": "Asynchronous Traffic Shaping and Redundancy: Avoiding Unbounded Latencies in In-Car Networks",
        "authors": [
            "Teresa L\u00fcbeck",
            "Philipp Meyer",
            "Timo H\u00e4ckel",
            "Franz Korf",
            "Thomas C. Schmidt"
        ],
        "summary": "Time-Sensitive Networking (TSN) enhances Ethernet based In-Vehicle Networks\n(IVNs) with real-time capabilities. Different traffic shaping algorithms have\nbeen proposed for time-critical communication, of which the Asynchronous\nTraffic Shaper (ATS) is an upcoming candidate. However, recent research has\nshown that ATS can introduce unbounded latencies when shaping traffic from\nnon-FIFO systems. This impacts the applicability of ATS in IVNs, as these\nnetworks often use redundancy mechanisms that can cause non-FIFO behavior. In\nthis paper, we approach the problem of accumulated delays from ATS by analyzing\nthe scenarios that generate latency and by devising placement and\nconfigurations of ATS schedulers to prevent this behavior. Our solution\nsuccessfully mitigates problematic preconditions that lead to unbounded delays,\nwhich we evaluate in simulations. Through a realistic IVN simulation case\nstudy, we demonstrate the occurrence of unbounded latencies and validate the\neffectiveness of our approach in avoiding them.",
        "published": "2025-04-02T17:53:37+00:00"
    },
    {
        "title": "Graphon games and an idealized limit of large network games",
        "authors": [
            "Motoki Otsuka"
        ],
        "summary": "Graphon games are a class of games with a continuum of agents, introduced to\napproximate the strategic interactions in large network games. The first result\nof this study is an equilibrium existence theorem in graphon games, under the\nsame conditions as those in network games. We prove the existence of an\nequilibrium in a graphon game with an infinite-dimensional strategy space,\nunder the continuity and quasi-concavity of the utility functions. The second\nresult characterizes Nash equilibria in graphon games as the limit points of\nasymptotic Nash equilibria in large network games. If a sequence of large\nnetwork games converges to a graphon game, any convergent sequence of\nasymptotic Nash equilibria in these large network games also converges to a\nNash equilibrium of the graphon game. In addition, for any graphon game and its\nequilibrium, there exists a sequence of large network games that converges to\nthe graphon game and has asymptotic Nash equilibria converging to the\nequilibrium. These results suggest that the concept of a graphon game is an\nidealized limit of large network games as the number of players tends to\ninfinity.",
        "published": "2025-04-02T17:51:15+00:00"
    },
    {
        "title": "Strengthening Multi-Robot Systems for SAR: Co-Designing Robotics and Communication Towards 6G",
        "authors": [
            "Juan Bravo-Arrabal",
            "Ricardo V\u00e1zquez-Mart\u00edn",
            "J. J. Fern\u00e1ndez-Lozano",
            "Alfonso Garc\u00eda-Cerezo"
        ],
        "summary": "This paper presents field-tested use cases from Search and Rescue (SAR)\nmissions, highlighting the co-design of mobile robots and communication systems\nto support Edge-Cloud architectures based on 5G Standalone (SA). The main goal\nis to contribute to the effective cooperation of multiple robots and first\nresponders. Our field experience includes the development of Hybrid Wireless\nSensor Networks (H-WSNs) for risk and victim detection, smartphones integrated\ninto the Robot Operating System (ROS) as Edge devices for mission requests and\npath planning, real-time Simultaneous Localization and Mapping (SLAM) via\nMulti-Access Edge Computing (MEC), and implementation of Uncrewed Ground\nVehicles (UGVs) for victim evacuation in different navigation modes. These\nexperiments, conducted in collaboration with actual first responders,\nunderscore the need for intelligent network resource management, balancing\nlow-latency and high-bandwidth demands. Network slicing is key to ensuring\ncritical emergency services are performed despite challenging communication\nconditions. The paper identifies architectural needs, lessons learned, and\nchallenges to be addressed by 6G technologies to enhance emergency response\ncapabilities.",
        "published": "2025-04-02T17:47:11+00:00"
    },
    {
        "title": "Fermionic Averaged Circuit Eigenvalue Sampling",
        "authors": [
            "Adrian Chapman",
            "Steven T. Flammia"
        ],
        "summary": "Fermionic averaged circuit eigenvalue sampling (FACES) is a protocol to\nsimultaneously learn the averaged error rates of many fermionic linear optical\n(FLO) gates simultaneously and self-consistently from a suitable collection of\nFLO circuits. It is highly flexible, allowing for the in situ characterization\nof FLO-averaged gate-dependent noise under natural assumptions on a family of\ncontinuously parameterized one- and two-qubit gates. We rigorously show that\nour protocol has an efficient sampling complexity, owing in-part to useful\nproperties of the Kravchuk transformations that feature in our analysis. We\nsupport our conclusions with numerical results. As FLO circuits become\nuniversal with access to certain resource states, we expect our results to\ninform noise characterization and error mitigation techniques on universal\nquantum computing architectures which naturally admit a fermionic description.",
        "published": "2025-04-02T17:46:16+00:00"
    },
    {
        "title": "Hessian-aware Training for Enhancing DNNs Resilience to Parameter Corruptions",
        "authors": [
            "Tahmid Hasan Prato",
            "Seijoon Kim",
            "Lizhong Chen",
            "Sanghyun Hong"
        ],
        "summary": "Deep neural networks are not resilient to parameter corruptions: even a\nsingle-bitwise error in their parameters in memory can cause an accuracy drop\nof over 10%, and in the worst cases, up to 99%. This susceptibility poses great\nchallenges in deploying models on computing platforms, where adversaries can\ninduce bit-flips through software or bitwise corruptions may occur naturally.\nMost prior work addresses this issue with hardware or system-level approaches,\nsuch as integrating additional hardware components to verify a model's\nintegrity at inference. However, these methods have not been widely deployed as\nthey require infrastructure or platform-wide modifications.\n  In this paper, we propose a new approach to addressing this issue: training\nmodels to be more resilient to bitwise corruptions to their parameters. Our\napproach, Hessian-aware training, promotes models with $flatter$ loss surfaces.\nWe show that, while there have been training methods, designed to improve\ngeneralization through Hessian-based approaches, they do not enhance resilience\nto parameter corruptions. In contrast, models trained with our method\ndemonstrate increased resilience to parameter corruptions, particularly with a\n20$-$50% reduction in the number of bits whose individual flipping leads to a\n90$-$100% accuracy drop. Moreover, we show the synergy between ours and\nexisting hardware and system-level defenses.",
        "published": "2025-04-02T17:42:31+00:00"
    },
    {
        "title": "A thorough benchmark of automatic text classification: From traditional approaches to large language models",
        "authors": [
            "Washington Cunha",
            "Leonardo Rocha",
            "Marcos Andr\u00e9 Gon\u00e7alves"
        ],
        "summary": "Automatic text classification (ATC) has experienced remarkable advancements\nin the past decade, best exemplified by recent small and large language models\n(SLMs and LLMs), leveraged by Transformer architectures. Despite recent\neffectiveness improvements, a comprehensive cost-benefit analysis investigating\nwhether the effectiveness gains of these recent approaches compensate their\nmuch higher costs when compared to more traditional text classification\napproaches such as SVMs and Logistic Regression is still missing in the\nliterature. In this context, this work's main contributions are twofold: (i) we\nprovide a scientifically sound comparative analysis of the cost-benefit of\ntwelve traditional and recent ATC solutions including five open LLMs, and (ii)\na large benchmark comprising {22 datasets}, including sentiment analysis and\ntopic classification, with their (train-validation-test) partitions based on\nfolded cross-validation procedures, along with documentation, and code. The\nrelease of code, data, and documentation enables the community to replicate\nexperiments and advance the field in a more scientifically sound manner. Our\ncomparative experimental results indicate that LLMs outperform traditional\napproaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in\nterms of effectiveness. However, LLMs incur significantly higher computational\ncosts due to fine-tuning, being, on average 590x and 8.5x slower than\ntraditional methods and SLMs, respectively. Results suggests the following\nrecommendations: (1) LLMs for applications that require the best possible\neffectiveness and can afford the costs; (2) traditional methods such as\nLogistic Regression and SVM for resource-limited applications or those that\ncannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for\nnear-optimal effectiveness-efficiency trade-off.",
        "published": "2025-04-02T17:40:08+00:00"
    },
    {
        "title": "Is the Reversal Curse a Binding Problem? Uncovering Limitations of Transformers from a Basic Generalization Failure",
        "authors": [
            "Boshi Wang",
            "Huan Sun"
        ],
        "summary": "Despite their impressive capabilities, LLMs exhibit a basic generalization\nfailure known as the Reversal Curse, where they struggle to learn reversible\nfactual associations. Understanding why this occurs could help identify\nweaknesses in current models and advance their generalization and robustness.\nIn this paper, we conjecture that the Reversal Curse in LLMs is a manifestation\nof the long-standing binding problem in cognitive science, neuroscience and AI.\nSpecifically, we identify two primary causes of the Reversal Curse stemming\nfrom transformers' limitations in conceptual binding: the inconsistency and\nentanglements of concept representations. We perform a series of experiments\nthat support these conjectures. Our exploration leads to a model design based\non JEPA (Joint-Embedding Predictive Architecture) that for the first time\nbreaks the Reversal Curse without side-stepping it with specialized data\naugmentation or non-causal masking, and moreover, generalization could be\nfurther improved by incorporating special memory layers that support\ndisentangled concept representations. We demonstrate that the skill of reversal\nunlocks a new kind of memory integration that enables models to solve\nlarge-scale arithmetic reasoning problems via parametric forward-chaining,\noutperforming frontier LLMs based on non-parametric memory and prolonged\nexplicit reasoning.",
        "published": "2025-04-02T17:38:03+00:00"
    },
    {
        "title": "Equivariant Spherical CNNs for Accurate Fiber Orientation Distribution Estimation in Neonatal Diffusion MRI with Reduced Acquisition Time",
        "authors": [
            "Haykel Snoussi",
            "Davood Karimi"
        ],
        "summary": "Early and accurate assessment of brain microstructure using diffusion\nMagnetic Resonance Imaging (dMRI) is crucial for identifying neurodevelopmental\ndisorders in neonates, but remains challenging due to low signal-to-noise ratio\n(SNR), motion artifacts, and ongoing myelination. In this study, we propose a\nrotationally equivariant Spherical Convolutional Neural Network (sCNN)\nframework tailored for neonatal dMRI. We predict the Fiber Orientation\nDistribution (FOD) from multi-shell dMRI signals acquired with a reduced set of\ngradient directions (30% of the full protocol), enabling faster and more\ncost-effective acquisitions. We train and evaluate the performance of our sCNN\nusing real data from 43 neonatal dMRI datasets provided by the Developing Human\nConnectome Project (dHCP). Our results demonstrate that the sCNN achieves\nsignificantly lower mean squared error (MSE) and higher angular correlation\ncoefficient (ACC) compared to a Multi-Layer Perceptron (MLP) baseline,\nindicating improved accuracy in FOD estimation. Furthermore, tractography\nresults based on the sCNN-predicted FODs show improved anatomical plausibility,\ncoverage, and coherence compared to those from the MLP. These findings\nhighlight that sCNNs, with their inherent rotational equivariance, offer a\npromising approach for accurate and clinically efficient dMRI analysis, paving\nthe way for improved diagnostic capabilities and characterization of early\nbrain development.",
        "published": "2025-04-02T17:36:51+00:00"
    },
    {
        "title": "Gen-C: Populating Virtual Worlds with Generative Crowds",
        "authors": [
            "Andreas Panayiotou",
            "Panayiotis Charalambous",
            "Ioannis Karamouzas"
        ],
        "summary": "Over the past two decades, researchers have made significant advancements in\nsimulating human crowds, yet these efforts largely focus on low-level tasks\nlike collision avoidance and a narrow range of behaviors such as path following\nand flocking. However, creating compelling crowd scenes demands more than just\nfunctional movement-it requires capturing high-level interactions between\nagents, their environment, and each other over time. To address this issue, we\nintroduce Gen-C, a generative model to automate the task of authoring\nhigh-level crowd behaviors. Gen-C bypasses the labor-intensive and challenging\ntask of collecting and annotating real crowd video data by leveraging a large\nlanguage model (LLM) to generate a limited set of crowd scenarios, which are\nsubsequently expanded and generalized through simulations to construct\ntime-expanded graphs that model the actions and interactions of virtual agents.\nOur method employs two Variational Graph Auto-Encoders guided by a condition\nprior network: one dedicated to learning a latent space for graph structures\n(agent interactions) and the other for node features (agent actions and\nnavigation). This setup enables the flexible generation of dynamic crowd\ninteractions. The trained model can be conditioned on natural language,\nempowering users to synthesize novel crowd behaviors from text descriptions. We\ndemonstrate the effectiveness of our approach in two scenarios, a University\nCampus and a Train Station, showcasing its potential for populating diverse\nvirtual environments with agents exhibiting varied and dynamic behaviors that\nreflect complex interactions and high-level decision-making patterns.",
        "published": "2025-04-02T17:33:53+00:00"
    },
    {
        "title": "Client Selection in Federated Learning with Data Heterogeneity and Network Latencies",
        "authors": [
            "Harsh Vardhan",
            "Xiaofan Yu",
            "Tajana Rosing",
            "Arya Mazumdar"
        ],
        "summary": "Federated learning (FL) is a distributed machine learning paradigm where\nmultiple clients conduct local training based on their private data, then the\nupdated models are sent to a central server for global aggregation. The\npractical convergence of FL is challenged by multiple factors, with the primary\nhurdle being the heterogeneity among clients. This heterogeneity manifests as\ndata heterogeneity concerning local data distribution and latency heterogeneity\nduring model transmission to the server. While prior research has introduced\nvarious efficient client selection methods to alleviate the negative impacts of\neither of these heterogeneities individually, efficient methods to handle\nreal-world settings where both these heterogeneities exist simultaneously do\nnot exist. In this paper, we propose two novel theoretically optimal client\nselection schemes that can handle both these heterogeneities. Our methods\ninvolve solving simple optimization problems every round obtained by minimizing\nthe theoretical runtime to convergence. Empirical evaluations on 9 datasets\nwith non-iid data distributions, 2 practical delay distributions, and\nnon-convex neural network models demonstrate that our algorithms are at least\ncompetitive to and at most 20 times better than best existing baselines.",
        "published": "2025-04-02T17:31:15+00:00"
    },
    {
        "title": "Overcoming Deceptiveness in Fitness Optimization with Unsupervised Quality-Diversity",
        "authors": [
            "Lisa Coiffard",
            "Paul Templier",
            "Antoine Cully"
        ],
        "summary": "Policy optimization seeks the best solution to a control problem according to\nan objective or fitness function, serving as a fundamental field of engineering\nand research with applications in robotics. Traditional optimization methods\nlike reinforcement learning and evolutionary algorithms struggle with deceptive\nfitness landscapes, where following immediate improvements leads to suboptimal\nsolutions. Quality-diversity (QD) algorithms offer a promising approach by\nmaintaining diverse intermediate solutions as stepping stones for escaping\nlocal optima. However, QD algorithms require domain expertise to define\nhand-crafted features, limiting their applicability where characterizing\nsolution diversity remains unclear. In this paper, we show that unsupervised QD\nalgorithms - specifically the AURORA framework, which learns features from\nsensory data - efficiently solve deceptive optimization problems without domain\nexpertise. By enhancing AURORA with contrastive learning and periodic\nextinction events, we propose AURORA-XCon, which outperforms all traditional\noptimization baselines and matches, in some cases even improving by up to 34%,\nthe best QD baseline with domain-specific hand-crafted features. This work\nestablishes a novel application of unsupervised QD algorithms, shifting their\nfocus from discovering novel solutions toward traditional optimization and\nexpanding their potential to domains where defining feature spaces poses\nchallenges.",
        "published": "2025-04-02T17:18:21+00:00"
    },
    {
        "title": "Representing Flow Fields with Divergence-Free Kernels for Reconstruction",
        "authors": [
            "Xingyu Ni",
            "Jingrui Xing",
            "Xingqiao Li",
            "Bin Wang",
            "Baoquan Chen"
        ],
        "summary": "Accurately reconstructing continuous flow fields from sparse or indirect\nmeasurements remains an open challenge, as existing techniques often suffer\nfrom oversmoothing artifacts, reliance on heterogeneous architectures, and the\ncomputational burden of enforcing physics-informed losses in implicit neural\nrepresentations (INRs). In this paper, we introduce a novel flow field\nreconstruction framework based on divergence-free kernels (DFKs), which\ninherently enforce incompressibility while capturing fine structures without\nrelying on hierarchical or heterogeneous representations. Through qualitative\nanalysis and quantitative ablation studies, we identify the matrix-valued\nradial basis functions derived from Wendland's $\\mathcal{C}^4$ polynomial\n(DFKs-Wen4) as the optimal form of analytically divergence-free approximation\nfor velocity fields, owing to their favorable numerical properties, including\ncompact support, positive definiteness, and second-order differentiablility.\nExperiments across various reconstruction tasks, spanning data compression,\ninpainting, super-resolution, and time-continuous flow inference, has\ndemonstrated that DFKs-Wen4 outperform INRs and other divergence-free\nrepresentations in both reconstruction accuracy and computational efficiency\nwhile requiring the fewest trainable parameters.",
        "published": "2025-04-02T17:13:59+00:00"
    },
    {
        "title": "Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based ML Libraries",
        "authors": [
            "Furkan \u00c7olhak",
            "Hasan Co\u015fkun",
            "Tsafac Nkombong Regine Cyrille",
            "Tedi Hoxa",
            "Mert \u0130lhan Ecevit",
            "Mehmet Nafiz Ayd\u0131n"
        ],
        "summary": "The Internet of Vehicles (IoV) may face challenging cybersecurity attacks\nthat may require sophisticated intrusion detection systems, necessitating a\nrapid development and response system. This research investigates the\nperformance advantages of GPU-accelerated libraries (cuML) compared to\ntraditional CPU-based implementations (scikit-learn), focusing on the speed and\nefficiency required for machine learning models used in IoV threat detection\nenvironments. The comprehensive evaluations conducted employ four machine\nlearning approaches (Random Forest, KNN, Logistic Regression, XGBoost) across\nthree distinct IoV security datasets (OTIDS, GIDS, CICIoV2024). Our findings\ndemonstrate that GPU-accelerated implementations dramatically improved\ncomputational efficiency, with training times reduced by a factor of up to 159\nand prediction speeds accelerated by up to 95 times compared to traditional CPU\nprocessing, all while preserving detection accuracy. This remarkable\nperformance breakthrough empowers researchers and security specialists to\nharness GPU acceleration for creating faster, more effective threat detection\nsystems that meet the urgent real-time security demands of today's connected\nvehicle networks.",
        "published": "2025-04-02T17:04:53+00:00"
    },
    {
        "title": "Graphically Speaking: Unmasking Abuse in Social Media with Conversation Insights",
        "authors": [
            "C\u00e9lia Nouri",
            "Jean-Philippe Cointet",
            "Chlo\u00e9 Clavel"
        ],
        "summary": "Detecting abusive language in social media conversations poses significant\nchallenges, as identifying abusiveness often depends on the conversational\ncontext, characterized by the content and topology of preceding comments.\nTraditional Abusive Language Detection (ALD) models often overlook this\ncontext, which can lead to unreliable performance metrics. Recent Natural\nLanguage Processing (NLP) methods that integrate conversational context often\ndepend on limited and simplified representations, and report inconsistent\nresults. In this paper, we propose a novel approach that utilize graph neural\nnetworks (GNNs) to model social media conversations as graphs, where nodes\nrepresent comments, and edges capture reply structures. We systematically\ninvestigate various graph representations and context windows to identify the\noptimal configuration for ALD. Our GNN model outperform both context-agnostic\nbaselines and linear context-aware methods, achieving significant improvements\nin F1 scores. These findings demonstrate the critical role of structured\nconversational context and establish GNNs as a robust framework for advancing\ncontext-aware abusive language detection.",
        "published": "2025-04-02T17:03:37+00:00"
    },
    {
        "title": "Multi-stream Physics Hybrid Networks for solving Navier-Stokes equations",
        "authors": [
            "Tatjana Protasevich",
            "Mikhail Surmach",
            "Aleksandr Sedykh",
            "Olga Tsurkan",
            "Matvei Anoshin",
            "Vadim Lopatkin",
            "Leonid Fedichkin"
        ],
        "summary": "Understanding and solving fluid dynamics equations efficiently remains a\nfundamental challenge in computational physics. Traditional numerical solvers\nand physics-informed neural networks struggle to capture the full range of\nfrequency components in partial differential equation solutions, limiting their\naccuracy and efficiency. Here, we propose the Multi-stream Physics Hybrid\nNetwork, a novel neural architecture that integrates quantum and classical\nlayers in parallel to improve the accuracy of solving fluid dynamics equations,\nnamely Kovasznay flow problem. This approach decomposes the solution into\nseparate frequency components, each predicted by independent Parallel Hybrid\nNetworks, simplifying the training process and enhancing performance. We\nevaluated the proposed model against a comparable classical neural network, the\nMulti-stream Physics Classical Network, in both data-driven and physics-driven\nscenarios. Our results show that the Multi-stream Physics Hybrid Network\nachieves a reduction in root mean square error by 36% for velocity components\nand 41% for pressure prediction compared to the classical model, while using\n24% fewer trainable parameters. These findings highlight the potential of\nhybrid quantum-classical architectures for advancing computational fluid\ndynamics.",
        "published": "2025-04-02T16:50:54+00:00"
    },
    {
        "title": "Is Temporal Prompting All We Need For Limited Labeled Action Recognition?",
        "authors": [
            "Shreyank N Gowda",
            "Boyan Gao",
            "Xiao Gu",
            "Xiaobo Jin"
        ],
        "summary": "Video understanding has shown remarkable improvements in recent years,\nlargely dependent on the availability of large scaled labeled datasets. Recent\nadvancements in visual-language models, especially based on contrastive\npretraining, have shown remarkable generalization in zero-shot tasks, helping\nto overcome this dependence on labeled datasets. Adaptations of such models for\nvideos, typically involve modifying the architecture of vision-language models\nto cater to video data. However, this is not trivial, since such adaptations\nare mostly computationally intensive and struggle with temporal modeling. We\npresent TP-CLIP, an adaptation of CLIP that leverages temporal visual prompting\nfor temporal adaptation without modifying the core CLIP architecture. This\npreserves its generalization abilities. TP-CLIP efficiently integrates into the\nCLIP architecture, leveraging its pre-trained capabilities for video data.\nExtensive experiments across various datasets demonstrate its efficacy in\nzero-shot and few-shot learning, outperforming existing approaches with fewer\nparameters and computational efficiency. In particular, we use just 1/3 the\nGFLOPs and 1/28 the number of tuneable parameters in comparison to recent\nstate-of-the-art and still outperform it by up to 15.8% depending on the task\nand dataset.",
        "published": "2025-04-02T16:50:28+00:00"
    },
    {
        "title": "CO-DEFEND: Continuous Decentralized Federated Learning for Secure DoH-Based Threat Detection",
        "authors": [
            "Diego Cajaraville-Aboy",
            "Marta Moure-Garrido",
            "Carlos Beis-Penedo",
            "Carlos Garcia-Rubio",
            "Rebeca P. D\u00edaz-Redondo",
            "Celeste Campo",
            "Ana Fern\u00e1ndez-Vilas",
            "Manuel Fern\u00e1ndez-Veiga"
        ],
        "summary": "The use of DNS over HTTPS (DoH) tunneling by an attacker to hide malicious\nactivity within encrypted DNS traffic poses a serious threat to network\nsecurity, as it allows malicious actors to bypass traditional monitoring and\nintrusion detection systems while evading detection by conventional traffic\nanalysis techniques. Machine Learning (ML) techniques can be used to detect DoH\ntunnels; however, their effectiveness relies on large datasets containing both\nbenign and malicious traffic. Sharing such datasets across entities is\nchallenging due to privacy concerns. In this work, we propose CO-DEFEND\n(Continuous Decentralized Federated Learning for Secure DoH-Based Threat\nDetection), a Decentralized Federated Learning (DFL) framework that enables\nmultiple entities to collaboratively train a classification machine learning\nmodel while preserving data privacy and enhancing resilience against single\npoints of failure. The proposed DFL framework, which is scalable and\nprivacy-preserving, is based on a federation process that allows multiple\nentities to train online their local models using incoming DoH flows in real\ntime as they are processed by the entity. In addition, we adapt four classical\nmachine learning algorithms, Support Vector Machines (SVM), Logistic Regression\n(LR), Decision Trees (DT), and Random Forest (RF), for federated scenarios,\ncomparing their results with more computationally complex alternatives such as\nneural networks. We compare our proposed method by using the dataset\nCIRA-CIC-DoHBrw-2020 with existing machine learning approaches to demonstrate\nits effectiveness in detecting malicious DoH tunnels and the benefits it\nbrings.",
        "published": "2025-04-02T16:40:01+00:00"
    },
    {
        "title": "Effects of Dynamic Bonds on the Kinetic Pathways of Supramolecular Diblock Copolymers Disorder-Order Transition",
        "authors": [
            "Xiangyu Zhang",
            "Dong Meng"
        ],
        "summary": "Supramolecular block copolymers (SBC) consist of covalent polymer building\nblocks that are connected into well-defined architectures via supramolecular\nbonds. Assisted by the dynamic and reversible supramolecular interactions, it\nis envisaged that SBC self-assemblies may exhibit more diverse morphologies,\nstimuli-responsivity comparing to their covalent analogues. At the fundamental\nlevel, these features are related to the free-energy landscape of\nself-assemblies. It is therefore of central importance to understand the impact\nof dynamic/reversible bonds on the free energy landscape during structure\ntransitions. In this study, we first conduct smart Monte Carlo simulations to\ncompare the kinetics of the disorder-order transition (DOT) of supramolecular\ndiblock copolymers (SDBC) to that of covalent diblock copolymers (CDBC). The\nstructural order parameter for CDBC exhibits a fast and smooth transition\nprocess across different random number seeds and initial configurations. In\ncontrast, the SDBC system shows more diverse transition pathways, which can be\nclassified into three types. These results suggest that reversible\nsupramolecular interactions complicate the pathways, and bring about various\nintermediate structures. Next, we apply the string method to construct the\nminimum free energy path of the transition, from which the transition state and\nthe free energy barrier are evaluated. It is found that the transition free\nenergy barrier strongly correlates with the fraction of supramolecules. By\ndecomposing the free energy into A-B interaction energy and association energy,\nwe found that the interplay of both two effects decide the kinetic pathway and\nthe final equilibrium structures.",
        "published": "2025-04-02T16:33:17+00:00"
    },
    {
        "title": "Architect Your Landscape Approach (AYLA) for Optimizations in Deep Learning",
        "authors": [
            "Ben Keslaki"
        ],
        "summary": "Stochastic Gradient Descent (SGD) and its variants, such as ADAM, are\nfoundational to deep learning optimization, adjusting model parameters using\nfixed or adaptive learning rates based on loss function gradients. However,\nthese methods often face challenges in balancing adaptability and efficiency in\nnon-convex, high-dimensional settings. This paper introduces AYLA, a novel\noptimization technique that enhances training dynamics through loss function\ntransformations. By applying a tunable power-law transformation, AYLA preserves\ncritical points while scaling loss values to amplify gradient sensitivity,\naccelerating convergence. We further propose a dynamic (effective) learning\nrate that adapts to the transformed loss, improving optimization efficiency.\nEmpirical tests on finding minimum of a synthetic non-convex polynomial, a\nnon-convex curve-fitting dataset, and digit classification (MNIST) demonstrate\nthat AYLA surpasses SGD and ADAM in convergence speed and stability. This\napproach redefines the loss landscape for better optimization outcomes,\noffering a promising advancement for deep neural networks and can be applied to\nany optimization method and potentially improve the performance of it.",
        "published": "2025-04-02T16:31:39+00:00"
    },
    {
        "title": "Corner-Grasp: Multi-Action Grasp Detection and Active Gripper Adaptation for Grasping in Cluttered Environments",
        "authors": [
            "Yeong Gwang Son",
            "Seunghwan Um",
            "Juyong Hong",
            "Tat Hieu Bui",
            "Hyouk Ryeol Choi"
        ],
        "summary": "Robotic grasping is an essential capability, playing a critical role in\nenabling robots to physically interact with their surroundings. Despite\nextensive research, challenges remain due to the diverse shapes and properties\nof target objects, inaccuracies in sensing, and potential collisions with the\nenvironment. In this work, we propose a method for effectively grasping in\ncluttered bin-picking environments where these challenges intersect. We utilize\na multi-functional gripper that combines both suction and finger grasping to\nhandle a wide range of objects. We also present an active gripper adaptation\nstrategy to minimize collisions between the gripper hardware and the\nsurrounding environment by actively leveraging the reciprocating suction cup\nand reconfigurable finger motion. To fully utilize the gripper's capabilities,\nwe built a neural network that detects suction and finger grasp points from a\nsingle input RGB-D image. This network is trained using a larger-scale\nsynthetic dataset generated from simulation. In addition to this, we propose an\nefficient approach to constructing a real-world dataset that facilitates grasp\npoint detection on various objects with diverse characteristics. Experiment\nresults show that the proposed method can grasp objects in cluttered\nbin-picking scenarios and prevent collisions with environmental constraints\nsuch as a corner of the bin. Our proposed method demonstrated its effectiveness\nin the 9th Robotic Grasping and Manipulation Competition (RGMC) held at ICRA\n2024.",
        "published": "2025-04-02T16:12:28+00:00"
    },
    {
        "title": "Code Red! On the Harmfulness of Applying Off-the-shelf Large Language Models to Programming Tasks",
        "authors": [
            "Ali Al-Kaswan",
            "Sebastian Deatc",
            "Beg\u00fcm Ko\u00e7",
            "Arie van Deursen",
            "Maliheh Izadi"
        ],
        "summary": "Nowadays, developers increasingly rely on solutions powered by Large Language\nModels (LLM) to assist them with their coding tasks. This makes it crucial to\nalign these tools with human values to prevent malicious misuse. In this paper,\nwe propose a comprehensive framework for assessing the potential harmfulness of\nLLMs within the software engineering domain. We begin by developing a taxonomy\nof potentially harmful software engineering scenarios and subsequently, create\na dataset of prompts based on this taxonomy. To systematically assess the\nresponses, we design and validate an automatic evaluator that classifies the\noutputs of a variety of LLMs both open-source and closed-source models, as well\nas general-purpose and code-specific LLMs. Furthermore, we investigate the\nimpact of models size, architecture family, and alignment strategies on their\ntendency to generate harmful content. The results show significant disparities\nin the alignment of various LLMs for harmlessness. We find that some models and\nmodel families, such as Openhermes, are more harmful than others and that\ncode-specific models do not perform better than their general-purpose\ncounterparts. Notably, some fine-tuned models perform significantly worse than\ntheir base-models due to their design choices. On the other side, we find that\nlarger models tend to be more helpful and are less likely to respond with\nharmful information. These results highlight the importance of targeted\nalignment strategies tailored to the unique challenges of software engineering\ntasks and provide a foundation for future work in this critical area.",
        "published": "2025-04-02T16:00:14+00:00"
    },
    {
        "title": "BOGausS: Better Optimized Gaussian Splatting",
        "authors": [
            "St\u00e9phane Pateux",
            "Matthieu Gendrin",
            "Luce Morin",
            "Th\u00e9o Ladune",
            "Xiaoran Jiang"
        ],
        "summary": "3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view\nsynthesis. Its framework provides fast and high-fidelity rendering. Although\nless complex than other solutions such as Neural Radiance Fields (NeRF), there\nare still some challenges building smaller models without sacrificing quality.\nIn this study, we perform a careful analysis of 3DGS training process and\npropose a new optimization methodology. Our Better Optimized Gaussian Splatting\n(BOGausS) solution is able to generate models up to ten times lighter than the\noriginal 3DGS with no quality degradation, thus significantly boosting the\nperformance of Gaussian Splatting compared to the state of the art.",
        "published": "2025-04-02T15:49:23+00:00"
    },
    {
        "title": "Efficient Constant-Space Multi-Vector Retrieval",
        "authors": [
            "Sean MacAvaney",
            "Antonio Mallia",
            "Nicola Tonellotto"
        ],
        "summary": "Multi-vector retrieval methods, exemplified by the ColBERT architecture, have\nshown substantial promise for retrieval by providing strong trade-offs in terms\nof retrieval latency and effectiveness. However, they come at a high cost in\nterms of storage since a (potentially compressed) vector needs to be stored for\nevery token in the input collection. To overcome this issue, we propose\nencoding documents to a fixed number of vectors, which are no longer\nnecessarily tied to the input tokens. Beyond reducing the storage costs, our\napproach has the advantage that document representations become of a fixed size\non disk, allowing for better OS paging management. Through experiments using\nthe MSMARCO passage corpus and BEIR with the ColBERT-v2 architecture, a\nrepresentative multi-vector ranking model architecture, we find that passages\ncan be effectively encoded into a fixed number of vectors while retaining most\nof the original effectiveness.",
        "published": "2025-04-02T15:22:23+00:00"
    },
    {
        "title": "Multiplexed Control at Scale for Electrode Arrays in Trapped-Ion Quantum Processors",
        "authors": [
            "Ryutaro Ohira",
            "Shinichi Morisaka",
            "Ippei Nakamura",
            "Atsushi Noguchi",
            "Takefumi Miyoshi"
        ],
        "summary": "The scaling up of trapped-ion quantum processors based on the quantum\ncharge-coupled device (QCCD) architecture is difficult owing to the extensive\nelectronics and high-density wiring required to control numerous trap\nelectrodes. In conventional QCCD architectures, each trap electrode is\ncontrolled via a dedicated digital-to-analog converter (DAC). The conventional\napproach places an overwhelming demand on electronic resources and wiring\ncomplexity. This is because the number of trap electrodes typically exceeds the\nnumber of trapped-ion qubits. This study proposes a method that leverages a\nhigh-speed DAC to generate time-division multiplexed signals to control a\nlarge-scale QCCD trapped-ion quantum processor. The proposed method replaces\nconventional DACs with a single high-speed DAC that generates the complete\nvoltage waveforms required to control the trap electrodes, thereby\nsignificantly reducing the wiring complexity and overall resource requirements.\nBased on realistic parameters and commercially available electronics, our\nanalysis demonstrates that a QCCD trapped-ion quantum computer with 10,000 trap\nelectrodes can be controlled using only 13 field-programmable gate arrays and\n104 high-speed DACs. This is in stark contrast to the 10,000 dedicated DACs\nrequired by conventional control methods. Consequently, employing this\napproach, we developed a proof-of-concept electronic system and evaluated its\nanalog output performance.",
        "published": "2025-04-02T15:21:31+00:00"
    },
    {
        "title": "Inference of hidden common driver dynamics by anisotropic self-organizing neural networks",
        "authors": [
            "Zsigmond Benk\u0151",
            "Marcell Stippinger",
            "Zolt\u00e1n Somogyv\u00e1ri"
        ],
        "summary": "We are introducing a novel approach to infer the underlying dynamics of\nhidden common drivers, based on analyzing time series data from two driven\ndynamical systems. The inference relies on time-delay embedding, estimation of\nthe intrinsic dimension of the observed systems, and their mutual dimension. A\nkey component of our approach is a new anisotropic training technique applied\nto Kohonen's self-organizing map, which effectively learns the attractor of the\ndriven system and separates it into submanifolds corresponding to the\nself-dynamics and shared dynamics.\n  To demonstrate the effectiveness of our method, we conducted simulated\nexperiments using different chaotic maps in a setup, where two chaotic maps\nwere driven by a third map with nonlinear coupling. The inferred time series\nexhibited high correlation with the time series of the actual hidden common\ndriver, in contrast to the observed systems. The quality of our reconstruction\nwere compared and shown to be superior to several other methods that are\nintended to find the common features behind the observed time series, including\nlinear methods like PCA and ICA as well as nonlinear methods like dynamical\ncomponent analysis, canonical correlation analysis and even deep canonical\ncorrelation analysis.",
        "published": "2025-04-02T15:17:23+00:00"
    },
    {
        "title": "Detector Response to Gravitational Wave Polarizations in Gravitational Quantum Field Theory",
        "authors": [
            "Cong Xu",
            "Hong-Bo Jin",
            "Yue-Liang Wu"
        ],
        "summary": "We present an analysis of gravitational wave polarization modes within\nGravitational Quantum Field Theory (GQFT), a unified theoretical framework\nreconciling general relativity and quantum field theory. Our study focuses on\nfive fundamental polarization states predicted in GQFT: two tensor ($+,\n\\times$), two vector (x, y), and one scalar (breathing) mode, focusing on their\ndistinctive detection signatures in space-based interferometers like LISA and\nTaiji. Using first-order orbital dynamics in the Solar System Barycenter frame,\nwe identify three novel observational features: (1) characteristic interference\npatterns between polarization modes, (2) distinctive null-point signatures\nenabling mode discrimination, and (3) sky-position-dependent optimal detection\nwindows. Our approach provides complete sky coverage through polarization\nmapping while remaining fully compatible with existing mission designs, notably\navoiding the need for challenging direct breathing-mode measurements. The\nresults are presented through comprehensive sky maps, offering both theoretical\ninsights into gravitational wave polarization and practical tools for future\ndetector networks. This work establishes a new paradigm for testing fundamental\ngravity theories through their unique polarization fingerprints, with\nparticular relevance for upcoming multi-messenger gravitational wave astronomy.",
        "published": "2025-04-02T15:15:45+00:00"
    },
    {
        "title": "Distributed Triangle Detection is Hard in Few Rounds",
        "authors": [
            "Sepehr Assadi",
            "Janani Sundaresan"
        ],
        "summary": "In the distributed triangle detection problem, we have an $n$-vertex network\n$G=(V,E)$ with one player for each vertex of the graph who sees the edges\nincident on the vertex. The players communicate in synchronous rounds using the\nedges of this network and have a limited bandwidth of $O(\\log{n})$ bits over\neach edge. The goal is to detect whether or not $G$ contains a triangle as a\nsubgraph in a minimal number of rounds.\n  We prove that any protocol (deterministic or randomized) for distributed\ntriangle detection requires $\\Omega(\\log\\log{n})$ rounds of communication.\nPrior to our work, only one-round lower bounds were known for this problem.\n  The primary technique for proving these types of distributed lower bounds is\nvia reductions from two-party communication complexity. However, it has been\nknown for a while that this approach is provably incapable of establishing any\nmeaningful lower bounds for distributed triangle detection. Our main technical\ncontribution is a new information theoretic argument which combines recent\nadvances on multi-pass graph streaming lower bounds with the point-to-point\ncommunication aspects of distributed models, and can be of independent\ninterest.",
        "published": "2025-04-02T15:10:21+00:00"
    },
    {
        "title": "A Novel Approach To Implementing Knowledge Distillation In Tsetlin Machines",
        "authors": [
            "Calvin Kinateder"
        ],
        "summary": "The Tsetlin Machine (TM) is a propositional logic based model that uses\nconjunctive clauses to learn patterns from data. As with typical neural\nnetworks, the performance of a Tsetlin Machine is largely dependent on its\nparameter count, with a larger number of parameters producing higher accuracy\nbut slower execution. Knowledge distillation in neural networks transfers\ninformation from an already-trained teacher model to a smaller student model to\nincrease accuracy in the student without increasing execution time. We propose\na novel approach to implementing knowledge distillation in Tsetlin Machines by\nutilizing the probability distributions of each output sample in the teacher to\nprovide additional context to the student. Additionally, we propose a novel\nclause-transfer algorithm that weighs the importance of each clause in the\nteacher and initializes the student with only the most essential data. We find\nthat our algorithm can significantly improve performance in the student model\nwithout negatively impacting latency in the tested domains of image recognition\nand text classification.",
        "published": "2025-04-02T15:06:27+00:00"
    },
    {
        "title": "UniViTAR: Unified Vision Transformer with Native Resolution",
        "authors": [
            "Limeng Qiao",
            "Yiyang Gan",
            "Bairui Wang",
            "Jie Qin",
            "Shuang Xu",
            "Siqi Yang",
            "Lin Ma"
        ],
        "summary": "Conventional Vision Transformer simplifies visual modeling by standardizing\ninput resolutions, often disregarding the variability of natural visual data\nand compromising spatial-contextual fidelity. While preliminary explorations\nhave superficially investigated native resolution modeling, existing approaches\nstill lack systematic analysis from a visual representation perspective. To\nbridge this gap, we introduce UniViTAR, a family of homogeneous vision\nfoundation models tailored for unified visual modality and native resolution\nscenario in the era of multimodal. Our framework first conducts architectural\nupgrades to the vanilla paradigm by integrating multiple advanced components.\nBuilding upon these improvements, a progressive training paradigm is\nintroduced, which strategically combines two core mechanisms: (1) resolution\ncurriculum learning, transitioning from fixed-resolution pretraining to native\nresolution tuning, thereby leveraging ViT's inherent adaptability to\nvariable-length sequences, and (2) visual modality adaptation via inter-batch\nimage-video switching, which balances computational efficiency with enhanced\ntemporal reasoning. In parallel, a hybrid training framework further synergizes\nsigmoid-based contrastive loss with feature distillation from a frozen teacher\nmodel, thereby accelerating early-stage convergence. Finally, trained\nexclusively on public datasets, externsive experiments across multiple model\nscales from 0.3B to 1B demonstrate its effectiveness.",
        "published": "2025-04-02T14:59:39+00:00"
    },
    {
        "title": "Embedding Method for Knowledge Graph with Densely Defined Ontology",
        "authors": [
            "Takanori Ugai"
        ],
        "summary": "Knowledge graph embedding (KGE) is a technique that enhances knowledge graphs\nby addressing incompleteness and improving knowledge retrieval. A limitation of\nthe existing KGE models is their underutilization of ontologies, specifically\nthe relationships between properties. This study proposes a KGE model, TransU,\ndesigned for knowledge graphs with well-defined ontologies that incorporate\nrelationships between properties. The model treats properties as a subset of\nentities, enabling a unified representation. We present experimental results\nusing a standard dataset and a practical dataset.",
        "published": "2025-04-02T14:43:47+00:00"
    },
    {
        "title": "Leveraging Embedding Techniques in Multimodal Machine Learning for Mental Illness Assessment",
        "authors": [
            "Abdelrahaman A. Hassan",
            "Abdelrahman A. Ali",
            "Aya E. Fouda",
            "Radwa J. Hanafy",
            "Mohammed E. Fouda"
        ],
        "summary": "The increasing global prevalence of mental disorders, such as depression and\nPTSD, requires objective and scalable diagnostic tools. Traditional clinical\nassessments often face limitations in accessibility, objectivity, and\nconsistency. This paper investigates the potential of multimodal machine\nlearning to address these challenges, leveraging the complementary information\navailable in text, audio, and video data. Our approach involves a comprehensive\nanalysis of various data preprocessing techniques, including novel chunking and\nutterance-based formatting strategies. We systematically evaluate a range of\nstate-of-the-art embedding models for each modality and employ Convolutional\nNeural Networks (CNNs) and Bidirectional LSTM Networks (BiLSTMs) for feature\nextraction. We explore data-level, feature-level, and decision-level fusion\ntechniques, including a novel integration of Large Language Model (LLM)\npredictions. We also investigate the impact of replacing Multilayer Perceptron\nclassifiers with Support Vector Machines. We extend our analysis to severity\nprediction using PHQ-8 and PCL-C scores and multi-class classification\n(considering co-occurring conditions). Our results demonstrate that\nutterance-based chunking significantly improves performance, particularly for\ntext and audio modalities. Decision-level fusion, incorporating LLM\npredictions, achieves the highest accuracy, with a balanced accuracy of 94.8%\nfor depression and 96.2% for PTSD detection. The combination of CNN-BiLSTM\narchitectures with utterance-level chunking, coupled with the integration of\nexternal LLM, provides a powerful and nuanced approach to the detection and\nassessment of mental health conditions. Our findings highlight the potential of\nMMML for developing more accurate, accessible, and personalized mental\nhealthcare tools.",
        "published": "2025-04-02T14:19:06+00:00"
    },
    {
        "title": "KD$^{2}$M: An unifying framework for feature knowledge distillation",
        "authors": [
            "Eduardo Fernandes Montesuma"
        ],
        "summary": "Knowledge Distillation (KD) seeks to transfer the knowledge of a teacher,\ntowards a student neural net. This process is often done by matching the\nnetworks' predictions (i.e., their output), but, recently several works have\nproposed to match the distributions of neural nets' activations (i.e., their\nfeatures), a process known as \\emph{distribution matching}. In this paper, we\npropose an unifying framework, Knowledge Distillation through Distribution\nMatching (KD$^{2}$M), which formalizes this strategy. Our contributions are\nthreefold. We i) provide an overview of distribution metrics used in\ndistribution matching, ii) benchmark on computer vision datasets, and iii)\nderive new theoretical results for KD.",
        "published": "2025-04-02T14:14:46+00:00"
    },
    {
        "title": "Geometric Reasoning in the Embedding Space",
        "authors": [
            "Jan H\u016fla",
            "David Moj\u017e\u00ed\u0161ek",
            "Ji\u0159\u00ed Jane\u010dek",
            "David Herel",
            "Mikol\u00e1\u0161 Janota"
        ],
        "summary": "In this contribution, we demonstrate that Graph Neural Networks and\nTransformers can learn to reason about geometric constraints. We train them to\npredict spatial position of points in a discrete 2D grid from a set of\nconstraints that uniquely describe hidden figures containing these points. Both\nmodels are able to predict the position of points and interestingly, they form\nthe hidden figures described by the input constraints in the embedding space\nduring the reasoning process. Our analysis shows that both models recover the\ngrid structure during training so that the embeddings corresponding to the\npoints within the grid organize themselves in a 2D subspace and reflect the\nneighborhood structure of the grid. We also show that the Graph Neural Network\nwe design for the task performs significantly better than the Transformer and\nis also easier to scale.",
        "published": "2025-04-02T14:13:52+00:00"
    },
    {
        "title": "Bridge the Gap between SNN and ANN for Image Restoration",
        "authors": [
            "Xin Su",
            "Chen Wu",
            "Zhuoran Zheng"
        ],
        "summary": "Models of dense prediction based on traditional Artificial Neural Networks\n(ANNs) require a lot of energy, especially for image restoration tasks.\nCurrently, neural networks based on the SNN (Spiking Neural Network) framework\nare beginning to make their mark in the field of image restoration, especially\nas they typically use less than 10\\% of the energy of ANNs with the same\narchitecture. However, training an SNN is much more expensive than training an\nANN, due to the use of the heuristic gradient descent strategy. In other words,\nthe process of SNN's potential membrane signal changing from sparse to dense is\nvery slow, which affects the convergence of the whole model.To tackle this\nproblem, we propose a novel distillation technique, called asymmetric framework\n(ANN-SNN) distillation, in which the teacher is an ANN and the student is an\nSNN. Specifically, we leverage the intermediate features (feature maps) learned\nby the ANN as hints to guide the training process of the SNN. This approach not\nonly accelerates the convergence of the SNN but also improves its final\nperformance, effectively bridging the gap between the efficiency of the SNN and\nthe superior learning capabilities of ANN. Extensive experimental results show\nthat our designed SNN-based image restoration model, which has only 1/300 the\nnumber of parameters of the teacher network and 1/50 the energy consumption of\nthe teacher network, is as good as the teacher network in some denoising tasks.",
        "published": "2025-04-02T14:12:06+00:00"
    },
    {
        "title": "High-Yield Assembly of Plasmon-Coupled Nanodiamonds \\textit{via} DNA Origami for Tailored Emission",
        "authors": [
            "Niklas Hansen",
            "Jakub Copak",
            "Marek Kindermann",
            "David Roesel",
            "Federica Scollo",
            "Ilko Bald",
            "Petr Cigler",
            "Vladimira Petrakova"
        ],
        "summary": "Controlling the spatial arrangement of optically active elements is crucial\nfor the advancement of engineered photonic systems. Color centers in\nnanodiamond offer unique advantages for quantum sensing and information\nprocessing; however, their integration into complex optical architectures is\nlimited by challenges in precise and reproducible positioning, as well as\nefficient coupling. DNA origami provides an elegant solution, as demonstrated\nby recent studies showcasing nanoscale positioning of fluorescent nanodiamonds\nand plasmonic gold nanoparticles. Here, we present a scalable and robust method\nfor covalently functionalizing nanodiamonds with DNA, enabling high-yield,\nspatially controlled assembly of diamond and gold nanoparticles onto DNA\norigami. By precisely controlling the interparticle spacing, we reveal\ndistance-dependent modulation of NV center photoluminescence with a 10-fold\nincrease in the fastest decay pathway at short interparticle distances. Our\nfindings indicate selective plasmon-driven effects and interplay between\nradiative and non-radiative processes. This work overcomes key limitations in\ncurrent nanodiamond assembly strategies and provides insights into engineering\nNV photoluminescence by plasmonic coupling that advance toward quantum photonic\nand sensing applications.",
        "published": "2025-04-02T14:02:04+00:00"
    },
    {
        "title": "Stable Structure Learning with HC-Stable and Tabu-Stable Algorithms",
        "authors": [
            "Neville K. Kitson",
            "Anthony C. Constantinou"
        ],
        "summary": "Many Bayesian Network structure learning algorithms are unstable, with the\nlearned graph sensitive to arbitrary dataset artifacts, such as the ordering of\ncolumns (i.e., variable order). PC-Stable attempts to address this issue for\nthe widely-used PC algorithm, prompting researchers to use the \"stable\" version\ninstead. However, this problem seems to have been overlooked for score-based\nalgorithms. In this study, we show that some widely-used score-based\nalgorithms, as well as hybrid and constraint-based algorithms, including\nPC-Stable, suffer from the same issue. We propose a novel solution for\nscore-based greedy hill-climbing that eliminates instability by determining a\nstable node order, leading to consistent results regardless of variable\nordering. Two implementations, HC-Stable and Tabu-Stable, are introduced.\nTabu-Stable achieves the highest BIC scores across all networks, and the\nhighest accuracy for categorical networks. These results highlight the\nimportance of addressing instability in structure learning and provide a robust\nand practical approach for future applications. This extends the scope and\nimpact of our previous work presented at Probabilistic Graphical Models 2024 by\nincorporating continuous variables. The implementation, along with usage\ninstructions, is freely available on GitHub at\nhttps://github.com/causal-iq/discovery.",
        "published": "2025-04-02T13:51:44+00:00"
    },
    {
        "title": "Understanding Cross-Model Perceptual Invariances Through Ensemble Metamers",
        "authors": [
            "Lukas Boehm",
            "Jonas Leo Mueller",
            "Christoffer Loeffler",
            "Leo Schwinn",
            "Bjoern Eskofier",
            "Dario Zanca"
        ],
        "summary": "Understanding the perceptual invariances of artificial neural networks is\nessential for improving explainability and aligning models with human vision.\nMetamers - stimuli that are physically distinct yet produce identical neural\nactivations - serve as a valuable tool for investigating these invariances. We\nintroduce a novel approach to metamer generation by leveraging ensembles of\nartificial neural networks, capturing shared representational subspaces across\ndiverse architectures, including convolutional neural networks and vision\ntransformers. To characterize the properties of the generated metamers, we\nemploy a suite of image-based metrics that assess factors such as semantic\nfidelity and naturalness. Our findings show that convolutional neural networks\ngenerate more recognizable and human-like metamers, while vision transformers\nproduce realistic but less transferable metamers, highlighting the impact of\narchitectural biases on representational invariances.",
        "published": "2025-04-02T13:51:19+00:00"
    },
    {
        "title": "Enlightenment Period Improving DNN Performance",
        "authors": [
            "Tiantian Liu",
            "Weishi Xu",
            "Meng Wan",
            "Jue Wang"
        ],
        "summary": "In the early stage of deep neural network training, the loss decreases\nrapidly before gradually leveling off. Extensive research has shown that during\nthis stage, the model parameters undergo significant changes and their\ndistribution is largely established. Existing studies suggest that the\nintroduction of noise during early training can degrade model performance. We\nidentify a critical \"enlightenment period\" encompassing up to the first 4% of\nthe training cycle (1--20 epochs for 500-epoch training schedules), a phase\ncharacterized by intense parameter fluctuations and heightened noise\nsensitivity. Our findings reveal that strategically reducing noise during this\nbrief phase--by disabling data augmentation techniques such as Mixup or\nremoving high-loss samples--leads to statistically significant improvements in\nmodel performance. This work opens new avenues for exploring the relationship\nbetween the enlightenment period and network training dynamics across diverse\nmodel architectures and tasks.",
        "published": "2025-04-02T13:49:31+00:00"
    },
    {
        "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization",
        "authors": [
            "Chaohu Liu",
            "Tianyi Gui",
            "Yu Liu",
            "Linli Xu"
        ],
        "summary": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research.",
        "published": "2025-04-02T13:43:21+00:00"
    },
    {
        "title": "AI-Driven Framework for Multi-Service Multi-Modal Devices in NextG ORAN Systems",
        "authors": [
            "Mrityunjoy Gain",
            "Kitae Kim",
            "Avi Deb Raha",
            "Apurba Adhikary",
            "Walid Saad",
            "Zhu Han",
            "Choong Seon Hong"
        ],
        "summary": "In this paper, an artificial intelligence (AI)-driven efficient RAN\nmanagement framework is proposed. This framework introduces the concept of the\nmulti-service-modal UE (MSMU) system, which allows a single UE to handle both\neMBB and uRLLC services. The proposed framework integrates traffic demand\nprediction, route optimization, RAN slicing, service identification, and radio\nresource management under uncertainty. The challenge of dynamic environments in\nsuch a system is addressed by decomposing the optimization problem into\nlong-term (L-SP) and short-term (S-SP) subproblems. Using a long short-term\nmemory (LSTM) model, the proposed approach allows the prediction of eMBB and\nuRLLC traffic demands and optimal routes for RAN slicing in the L-SP. For the\nS-SP, another LSTM model is employed to handle real-time service type\nidentification and resource management based on long-term predictions. To\nsupport continuous adaptation, continual learning is incorporated into the S-SP\nframework, allowing the model to learn new service types while retaining prior\nknowledge. Experimental results show that the proposed framework efficiently\nmanages dual-mode UEs, achieving low mean square error for traffic demand\n(0.003), resource block prediction (0.003), and power prediction (0.002), with\n99\\% accuracy in service type and route selection and over 95\\% average\naccuracy for continual service adaptation across seven tasks.",
        "published": "2025-04-02T13:40:36+00:00"
    },
    {
        "title": "An Adaptive Proximal Inexact Gradient Framework and Its Application to Per-Antenna Constrained Joint Beamforming and Compression Design",
        "authors": [
            "Xilai Fan",
            "Bo Jiang",
            "Ya-Feng Liu"
        ],
        "summary": "In this paper, we propose an adaptive proximal inexact gradient (APIG)\nframework for solving a class of nonsmooth composite optimization problems\ninvolving function and gradient errors. Unlike existing inexact proximal\ngradient methods, the proposed framework introduces a new line search condition\nthat jointly adapts to function and gradient errors, enabling adaptive stepsize\nselection while maintaining theoretical guarantees. Specifically, we prove that\nthe proposed framework achieves an $\\epsilon$-stationary point within\n$\\mathcal{O}(\\epsilon^{-2})$ iterations for nonconvex objectives and an\n$\\epsilon$-optimal solution within $\\mathcal{O}(\\epsilon^{-1})$ iterations for\nconvex cases, matching the best-known complexity in this context. We then\ncustom-apply the APIG framework to an important signal processing problem: the\njoint beamforming and compression problem (JBCP) with per-antenna power\nconstraints (PAPCs) in cooperative cellular networks. This customized\napplication requires careful exploitation of the problem's special structure\nsuch as the tightness of the semidefinite relaxation (SDR) and the\ndifferentiability of the dual. Numerical experiments demonstrate the superior\nperformance of our custom-application over state-of-the-art benchmarks for the\nJBCP.",
        "published": "2025-04-02T13:28:15+00:00"
    },
    {
        "title": "A Novel Dynamic Epidemic Model for Successive Opinion Diffusion in Social Networks",
        "authors": [
            "Bin Han",
            "Fabienne Renckens",
            "C. Clark Cao",
            "Hans D. Schotten"
        ],
        "summary": "This paper proposes a dynamic epidemic model for successive opinion diffusion\nin social networks, extending the SHIMR model. It incorporates dynamic\ndecision-making influenced by social distances and captures accumulative\nopinion diffusion caused by interrelated rumors. The model reflects the impact\nof rumor spread on social network structures. Simulations validate its\neffectiveness in explaining phenomena like the echo chamber effect and provide\ninsights into opinion diffusion dynamics, with implications for understanding\nsocial polarization and network evolution.",
        "published": "2025-04-02T13:26:30+00:00"
    },
    {
        "title": "Method for Mitigating Attention to Inappropriate Content Based on Attention Dynamics Model",
        "authors": [
            "Naoki Hirakura"
        ],
        "summary": "The expansion of the attention economy has led to the growing issue of\ninappropriate content being posted by profit-driven users. Previous\ncountermeasures against inappropriate content have relied on moderation, which\nraises ethical concerns, or information diffusion control, which requires\nconsidering larger scale networks, including general users. This study proposes\nan imitation strategy as an intervention method that does not rely on\nmoderation and focuses on a relatively smaller scale competitive network of\ninformation disseminators rather than the entire social network. The imitation\nstrategy is a novel approach that utilizes increased competition among\ninformation disseminators through imitation to reduce attention to\ninappropriate content. Through theoretical analysis and numerical simulations,\nI demonstrate that the imitation strategy is more effective when nodes with\nhigher eigenvector centrality are selected as targets and nodes with lower\neigenvector centrality are chosen as imitators.",
        "published": "2025-04-02T13:20:25+00:00"
    },
    {
        "title": "Fourier Feature Attribution: A New Efficiency Attribution Method",
        "authors": [
            "Zechen Liu",
            "Feiyang Zhang",
            "Wei Song",
            "Xiang Li",
            "Wei Wei"
        ],
        "summary": "The study of neural networks from the perspective of Fourier features has\ngarnered significant attention. While existing analytical research suggests\nthat neural networks tend to learn low-frequency features, a clear attribution\nmethod for identifying the specific learned Fourier features has remained\nelusive. To bridge this gap, we propose a novel Fourier feature attribution\nmethod grounded in signal decomposition theory. Additionally, we analyze the\ndifferences between game-theoretic attribution metrics for Fourier and spatial\ndomain features, demonstrating that game-theoretic evaluation metrics are\nbetter suited for Fourier-based feature attribution.\n  Our experiments show that Fourier feature attribution exhibits superior\nfeature selection capabilities compared to spatial domain attribution methods.\nFor instance, in the case of Vision Transformers (ViTs) on the ImageNet\ndataset, only $8\\%$ of the Fourier features are required to maintain the\noriginal predictions for $80\\%$ of the samples. Furthermore, we compare the\nspecificity of features identified by our method against traditional spatial\ndomain attribution methods. Results reveal that Fourier features exhibit\ngreater intra-class concentration and inter-class distinctiveness, indicating\ntheir potential for more efficient classification and explainable AI\nalgorithms.",
        "published": "2025-04-02T13:20:19+00:00"
    },
    {
        "title": "Sky of Unlearning (SoUL): Rewiring Federated Machine Unlearning via Selective Pruning",
        "authors": [
            "Md Mahabub Uz Zaman",
            "Xiang Sun",
            "Jingjing Yao"
        ],
        "summary": "The Internet of Drones (IoD), where drones collaborate in data collection and\nanalysis, has become essential for applications such as surveillance and\nenvironmental monitoring. Federated learning (FL) enables drones to train\nmachine learning models in a decentralized manner while preserving data\nprivacy. However, FL in IoD networks is susceptible to attacks like data\npoisoning and model inversion. Federated unlearning (FU) mitigates these risks\nby eliminating adversarial data contributions, preventing their influence on\nthe model. This paper proposes sky of unlearning (SoUL), a federated unlearning\nframework that efficiently removes the influence of unlearned data while\nmaintaining model performance. A selective pruning algorithm is designed to\nidentify and remove neurons influential in unlearning but minimally impact the\noverall performance of the model. Simulations demonstrate that SoUL outperforms\nexisting unlearning methods, achieves accuracy comparable to full retraining,\nand reduces computation and communication overhead, making it a scalable and\nefficient solution for resource-constrained IoD networks.",
        "published": "2025-04-02T13:07:30+00:00"
    },
    {
        "title": "Reasoning LLMs for User-Aware Multimodal Conversational Agents",
        "authors": [
            "Hamed Rahimi",
            "Jeanne Cattoni",
            "Meriem Beghili",
            "Mouad Abrini",
            "Mahdi Khoramshahi",
            "Maribel Pino",
            "Mohamed Chetouani"
        ],
        "summary": "Personalization in social robotics is critical for fostering effective\nhuman-robot interactions, yet systems often face the cold start problem, where\ninitial user preferences or characteristics are unavailable. This paper\nproposes a novel framework called USER-LLM R1 for a user-aware conversational\nagent that addresses this challenge through dynamic user profiling and model\ninitiation. Our approach integrates chain-of-thought (CoT) reasoning models to\niteratively infer user preferences and vision-language models (VLMs) to\ninitialize user profiles from multimodal inputs, enabling personalized\ninteractions from the first encounter. Leveraging a Retrieval-Augmented\nGeneration (RAG) architecture, the system dynamically refines user\nrepresentations within an inherent CoT process, ensuring contextually relevant\nand adaptive responses. Evaluations on the ElderlyTech-VQA Bench demonstrate\nsignificant improvements in ROUGE-1 (+23.2%), ROUGE-2 (+0.6%), and ROUGE-L\n(+8%) F1 scores over state-of-the-art baselines, with ablation studies\nunderscoring the impact of reasoning model size on performance. Human\nevaluations further validate the framework's efficacy, particularly for elderly\nusers, where tailored responses enhance engagement and trust. Ethical\nconsiderations, including privacy preservation and bias mitigation, are\nrigorously discussed and addressed to ensure responsible deployment.",
        "published": "2025-04-02T13:00:17+00:00"
    },
    {
        "title": "InvFussion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems",
        "authors": [
            "Noam Elata",
            "Hyungjin Chung",
            "Jong Chul Ye",
            "Tomer Michaeli",
            "Michael Elad"
        ],
        "summary": "Diffusion Models have demonstrated remarkable capabilities in handling\ninverse problems, offering high-quality posterior-sampling-based solutions.\nDespite significant advances, a fundamental trade-off persists, regarding the\nway the conditioned synthesis is employed: Training-based methods achieve high\nquality results, while zero-shot approaches trade this with flexibility. This\nwork introduces a framework that combines the best of both worlds -- the strong\nperformance of supervised approaches and the flexibility of zero-shot methods.\nThis is achieved through a novel architectural design that seamlessly\nintegrates the degradation operator directly into the denoiser. In each block,\nour proposed architecture applies the degradation operator on the network\nactivations and conditions the output using the attention mechanism, enabling\nadaptation to diverse degradation scenarios while maintaining high performance.\nOur work demonstrates the versatility of the proposed architecture, operating\nas a general MMSE estimator, a posterior sampler, or a Neural Posterior\nPrincipal Component estimator. This flexibility enables a wide range of\ndownstream tasks, highlighting the broad applicability of our framework. The\nproposed modification of the denoiser network offers a versatile, accurate, and\ncomputationally efficient solution, demonstrating the advantages of dedicated\nnetwork architectures for complex inverse problems. Experimental results on the\nFFHQ and ImageNet datasets demonstrate state-of-the-art posterior-sampling\nperformance, surpassing both training-based and zero-shot alternatives.",
        "published": "2025-04-02T12:40:57+00:00"
    },
    {
        "title": "Enhancing European Cooperation in the Search for Dark Matter",
        "authors": [
            "Bernard Andrieu",
            "Ties Behnke",
            "Philip Bechtle",
            "Xavier Bertou",
            "Jose Busto",
            "Susana Cebrian",
            "Marco Cirelli",
            "Laurent Derome",
            "Cristinel Diaconu",
            "Caterina Doglioni",
            "Guiliana Fiorillo",
            "Davide Franco",
            "Juan Fuster",
            "Romain Gaior",
            "Erika Garutti",
            "Claudio Gatti",
            "B. Gimeno-Martinez",
            "Frederic Girard",
            "Roxanne Guenette",
            "Matthias Hamer",
            "Sophie Henrot-Versille",
            "Thibaut Houdy",
            "Fabrice Hubaut",
            "Adrian Irles",
            "Yoann Kermaidic",
            "Marcin Kuzniak",
            "Axel Lindner",
            "Julien Masbou",
            "Giovanni Mazzitelli",
            "Akira Miyasaki",
            "Enrique Minaya",
            "Konstantinos Nikolopoulos",
            "Federica Petricca",
            "Roman P\u00f6schl",
            "Pascal Pralavorio",
            "Florian Reindl",
            "Leszek Roszkowski",
            "Daniel Santos",
            "Jochen Schieck",
            "Thomas Schoerner",
            "Silvia Scorza",
            "Luca Scotto Lavina",
            "Steinar Stapnes",
            "Achille Stocchi",
            "Maxim Titov",
            "Julia K. Vogel",
            "Masayuki Wada",
            "Jonathan Wilson",
            "Yajing Xing andDirk Zerwas"
        ],
        "summary": "The search for dark matter is an exciting topic that is pursued in different\ncommunities over a wide range of masses and using a variety of experimental\napproaches. The result is a strongly correlated matrix of activities across\nEurope and beyond, both on the experimental and the theoretical side. We\nsuggest to encourage and foster the collaboration of the involved institutions\non technical, scientific and organisational level, in order to realise the\nsynergies that are required to increase the impact of dark matter research and\nto cope with the increasing experiment sizes. The suggested network -- loosely\ntitled \"DMInfraNet\" -- could be realised as a new initiative of the European\nstrategy or be based on existing structures like iDMEu or DRD. The network can\nalso serve as a nucleus for future joint funding proposals.",
        "published": "2025-04-02T12:36:28+00:00"
    },
    {
        "title": "Satellite Edge Artificial Intelligence with Large Models: Architectures and Technologies",
        "authors": [
            "Yuanming Shi",
            "Jingyang Zhu",
            "Chunxiao Jiang",
            "Linling Kuang",
            "Khaled B. Letaief"
        ],
        "summary": "Driven by the growing demand for intelligent remote sensing applications,\nlarge artificial intelligence (AI) models pre-trained on large-scale unlabeled\ndatasets and fine-tuned for downstream tasks have significantly improved\nlearning performance for various downstream tasks due to their generalization\ncapabilities. However, many specific downstream tasks, such as extreme weather\nnowcasting (e.g., downburst and tornado), disaster monitoring, and battlefield\nsurveillance, require real-time data processing. Traditional methods via\ntransferring raw data to ground stations for processing often cause significant\nissues in terms of latency and trustworthiness. To address these challenges,\nsatellite edge AI provides a paradigm shift from ground-based to on-board data\nprocessing by leveraging the integrated communication-and-computation\ncapabilities in space computing power networks (Space-CPN), thereby enhancing\nthe timeliness, effectiveness, and trustworthiness for remote sensing\ndownstream tasks. Moreover, satellite edge large AI model (LAM) involves both\nthe training (i.e., fine-tuning) and inference phases, where a key challenge\nlies in developing computation task decomposition principles to support\nscalable LAM deployment in resource-constrained space networks with\ntime-varying topologies. In this article, we first propose a satellite\nfederated fine-tuning architecture to split and deploy the modules of LAM over\nspace and ground networks for efficient LAM fine-tuning. We then introduce a\nmicroservice-empowered satellite edge LAM inference architecture that\nvirtualizes LAM components into lightweight microservices tailored for\nmulti-task multimodal inference. Finally, we discuss the future directions for\nenhancing the efficiency and scalability of satellite edge LAM, including\ntask-oriented communication, brain-inspired computing, and satellite edge AI\nnetwork optimization.",
        "published": "2025-04-02T12:25:57+00:00"
    },
    {
        "title": "K-P Quantum Neural Networks",
        "authors": [
            "Elija Perrier"
        ],
        "summary": "We present an extension of K-P time-optimal quantum control solutions using\nglobal Cartan $KAK$ decompositions for geodesic-based solutions. Extending\nrecent time-optimal \\emph{constant-$\\theta$} control results, we integrate\nCartan methods into equivariant quantum neural network (EQNN) for quantum\ncontrol tasks. We show that a finite-depth limited EQNN ansatz equipped with\nCartan layers can replicate the constant-$\\theta$ sub-Riemannian geodesics for\nK-P problems. We demonstrate how for certain classes of control problem on\nRiemannian symmetric spaces, gradient-based training using an appropriate cost\nfunction converges to certain global time-optimal solutions when satisfying\nsimple regularity conditions. This generalises prior geometric control theory\nmethods and clarifies how optimal geodesic estimation can be performed in\nquantum machine learning contexts.",
        "published": "2025-04-02T12:22:18+00:00"
    },
    {
        "title": "A flexible framework for early power and timing comparison of time-multiplexed CGRA kernel executions",
        "authors": [
            "Maxime Henri Aspros",
            "Juan Sapriza",
            "Giovanni Ansaloni",
            "David Atienza"
        ],
        "summary": "At the intersection between traditional CPU architectures and more\nspecialized options such as FPGAs or ASICs lies the family of reconfigurable\nhardware architectures, termed Coarse-Grained Reconfigurable Arrays (CGRAs).\nCGRAs are composed of a 2-dimensional array of processing elements (PE),\ntightly integrated with each other, each capable of performing arithmetic and\nlogic operations. The vast design space of CGRA implementations poses a\nchallenge, which calls for fast exploration tools to prune it in advance of\ntime-consuming syntheses. The proposed tool aims to simplify this process by\nsimulating kernel execution and providing a characterization framework. The\nestimator returns energy and latency values otherwise only available through a\ntime-consuming post-synthesis simulation, allowing for instantaneous\ncomparative analysis between different kernels and hardware configurations.",
        "published": "2025-04-02T12:21:09+00:00"
    },
    {
        "title": "Recovering Small Communities in the Planted Partition Model",
        "authors": [
            "Martijn G\u00f6sgens",
            "Maximilien Dreveton"
        ],
        "summary": "We analyze community recovery in the planted partition model (PPM) in regimes\nwhere the number of communities is arbitrarily large. We examine the three\nstandard recovery regimes: exact recovery, almost exact recovery, and weak\nrecovery. When communities vary in size, traditional accuracy- or\nalignment-based metrics become unsuitable for assessing the correctness of a\npredicted partition. To address this, we redefine these recovery regimes using\nthe correlation coefficient, a more versatile metric for comparing partitions.\nWe then demonstrate that \\emph{Diamond Percolation}, an algorithm based on\ncommon-neighbors, successfully recovers communities under mild assumptions on\nedge probabilities, with minimal restrictions on the number and sizes of\ncommunities. As a key application, we consider the case where community sizes\nfollow a power-law distribution, a characteristic frequently found in\nreal-world networks. To the best of our knowledge, we provide the first\nrecovery results for such unbalanced partitions.",
        "published": "2025-04-02T12:14:57+00:00"
    },
    {
        "title": "BioAtt: Anatomical Prior Driven Low-Dose CT Denoising",
        "authors": [
            "Namhun Kim",
            "UiHyun Cho"
        ],
        "summary": "Deep-learning-based denoising methods have significantly improved Low-Dose CT\n(LDCT) image quality. However, existing models often over-smooth important\nanatomical details due to their purely data-driven attention mechanisms. To\naddress this challenge, we propose a novel LDCT denoising framework, BioAtt.\nThe key innovation lies in attending anatomical prior distributions extracted\nfrom the pretrained vision-language model BiomedCLIP. These priors guide the\ndenoising model to focus on anatomically relevant regions to suppress noise\nwhile preserving clinically relevant structures. We highlight three main\ncontributions: BioAtt outperforms baseline and attention-based models in SSIM,\nPSNR, and RMSE across multiple anatomical regions. The framework introduces a\nnew architectural paradigm by embedding anatomic priors directly into spatial\nattention. Finally, BioAtt attention maps provide visual confirmation that the\nimprovements stem from anatomical guidance rather than increased model\ncomplexity.",
        "published": "2025-04-02T12:14:04+00:00"
    },
    {
        "title": "Market-Oriented Flow Allocation for Thermal Solar Plants: An Auction-Based Methodology with Artificial Intelligence",
        "authors": [
            "Sara Ruiz-Moreno",
            "Antonio J. Gallego",
            "Manuel Mac\u00edas",
            "Eduardo F. Camacho"
        ],
        "summary": "This paper presents a novel method to optimize thermal balance in parabolic\ntrough collector (PTC) plants. It uses a market-based system to distribute flow\namong loops combined with an artificial neural network (ANN) to reduce\ncomputation and data requirements. This auction-based approach balances loop\ntemperatures, accommodating varying thermal losses and collector efficiencies.\nValidation across different thermal losses, optical efficiencies, and\nirradiance conditions-sunny, partially cloudy, and cloudy-show improved thermal\npower output and intercept factors compared to a no-allocation system. It\ndemonstrates scalability and practicality for large solar thermal plants,\nenhancing overall performance. The method was first validated through\nsimulations on a realistic solar plant model, then adapted and successfully\ntested in a 50 MW solar trough plant, demonstrating its advantages.\nFurthermore, the algorithms have been implemented, commissioned, and are\ncurrently operating in 13 commercial solar trough plants.",
        "published": "2025-04-02T12:01:41+00:00"
    },
    {
        "title": "Sparse Gaussian Neural Processes",
        "authors": [
            "Tommy Rochussen",
            "Vincent Fortuin"
        ],
        "summary": "Despite significant recent advances in probabilistic meta-learning, it is\ncommon for practitioners to avoid using deep learning models due to a\ncomparative lack of interpretability. Instead, many practitioners simply use\nnon-meta-models such as Gaussian processes with interpretable priors, and\nconduct the tedious procedure of training their model from scratch for each\ntask they encounter. While this is justifiable for tasks with a limited number\nof data points, the cubic computational cost of exact Gaussian process\ninference renders this prohibitive when each task has many observations. To\nremedy this, we introduce a family of models that meta-learn sparse Gaussian\nprocess inference. Not only does this enable rapid prediction on new tasks with\nsparse Gaussian processes, but since our models have clear interpretations as\nmembers of the neural process family, it also allows manual elicitation of\npriors in a neural process for the first time. In meta-learning regimes for\nwhich the number of observed tasks is small or for which expert domain\nknowledge is available, this offers a crucial advantage.",
        "published": "2025-04-02T12:00:09+00:00"
    },
    {
        "title": "Efficient Calibration for RRAM-based In-Memory Computing using DoRA",
        "authors": [
            "Weirong Dong",
            "Kai Zhou",
            "Zhen Kong",
            "Quan Cheng",
            "Junkai Huang",
            "Zhengke Yang",
            "Masanori Hashimoto",
            "Longyang Lin"
        ],
        "summary": "Resistive In-Memory Computing (RIMC) offers ultra-efficient computation for\nedge AI but faces accuracy degradation due to RRAM conductance drift over time.\nTraditional retraining methods are limited by RRAM's high energy consumption,\nwrite latency, and endurance constraints. We propose a DoRA-based calibration\nframework that restores accuracy by compensating influential weights with\nminimal calibration parameters stored in SRAM, leaving RRAM weights untouched.\nThis eliminates in-field RRAM writes, ensuring energy-efficient, fast, and\nreliable calibration. Experiments on RIMC-based ResNet50 (ImageNet-1K)\ndemonstrate 69.53% accuracy restoration using just 10 calibration samples while\nupdating only 2.34% of parameters.",
        "published": "2025-04-02T11:58:08+00:00"
    },
    {
        "title": "Proposition of Affordance-Driven Environment Recognition Framework Using Symbol Networks in Large Language Models",
        "authors": [
            "Kazuma Arii",
            "Satoshi Kurihara"
        ],
        "summary": "In the quest to enable robots to coexist with humans, understanding dynamic\nsituations and selecting appropriate actions based on common sense and\naffordances are essential. Conventional AI systems face challenges in applying\naffordance, as it represents implicit knowledge derived from common sense.\nHowever, large language models (LLMs) offer new opportunities due to their\nability to process extensive human knowledge. This study proposes a method for\nautomatic affordance acquisition by leveraging LLM outputs. The process\ninvolves generating text using LLMs, reconstructing the output into a symbol\nnetwork using morphological and dependency analysis, and calculating\naffordances based on network distances. Experiments using ``apple'' as an\nexample demonstrated the method's ability to extract context-dependent\naffordances with high explainability. The results suggest that the proposed\nsymbol network, reconstructed from LLM outputs, enables robots to interpret\naffordances effectively, bridging the gap between symbolized data and\nhuman-like situational understanding.",
        "published": "2025-04-02T11:48:44+00:00"
    },
    {
        "title": "Bridge 2D-3D: Uncertainty-aware Hierarchical Registration Network with Domain Alignment",
        "authors": [
            "Zhixin Cheng",
            "Jiacheng Deng",
            "Xinjun Li",
            "Baoqun Yin",
            "Tianzhu Zhang"
        ],
        "summary": "The method for image-to-point cloud registration typically determines the\nrigid transformation using a coarse-to-fine pipeline. However, directly and\nuniformly matching image patches with point cloud patches may lead to focusing\non incorrect noise patches during matching while ignoring key ones. Moreover,\ndue to the significant differences between image and point cloud modalities, it\nmay be challenging to bridge the domain gap without specific improvements in\ndesign. To address the above issues, we innovatively propose the\nUncertainty-aware Hierarchical Matching Module (UHMM) and the Adversarial Modal\nAlignment Module (AMAM). Within the UHMM, we model the uncertainty of critical\ninformation in image patches and facilitate multi-level fusion interactions\nbetween image and point cloud features. In the AMAM, we design an adversarial\napproach to reduce the domain gap between image and point cloud. Extensive\nexperiments and ablation studies on RGB-D Scene V2 and 7-Scenes benchmarks\ndemonstrate the superiority of our method, making it a state-of-the-art\napproach for image-to-point cloud registration tasks.",
        "published": "2025-04-02T11:43:55+00:00"
    },
    {
        "title": "LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach",
        "authors": [
            "Reo Abe",
            "Akifumi Ito",
            "Kanata Takayasu",
            "Satoshi Kurihara"
        ],
        "summary": "Planning methods with high adaptability to dynamic environments are crucial\nfor the development of autonomous and versatile robots. We propose a method for\nleveraging a large language model (GPT-4o) to automatically generate networks\ncapable of adapting to dynamic environments. The proposed method collects\nenvironmental \"status,\" representing conditions and goals, and uses them to\ngenerate agents. These agents are interconnected on the basis of specific\nconditions, resulting in networks that combine flexibility and generality. We\nconducted evaluation experiments to compare the networks automatically\ngenerated with the proposed method with manually constructed ones, confirming\nthe comprehensiveness of the proposed method's networks and their higher\ngenerality. This research marks a significant advancement toward the\ndevelopment of versatile planning methods applicable to robotics, autonomous\nvehicles, smart systems, and other complex environments.",
        "published": "2025-04-02T11:42:49+00:00"
    },
    {
        "title": "Dataset and Methodology for Material Identification Using AFM Phase Approach Curves",
        "authors": [
            "Stefan R. Anton",
            "Denis E. Tranca",
            "Stefan G. Stanciu",
            "Adrian M. Ionescu",
            "George A. Stanciu"
        ],
        "summary": "Atomic force microscopy (AFM) phase approach-curves have significant\npotential for nanoscale material characterization, however, the availability of\nrobust datasets and automated analysis tools has been limited. In this paper,\nwe introduce a novel methodology for material identification using a\nhigh-dimensional dataset consisting of AFM phase approach-curves collected from\nfive distinct materials: silicon, silicon dioxide, platinum, silver, and gold.\nEach measurement comprises 50 phase values obtained at progressively increasing\ntip-sample distances, resulting in 50x50x50 voxel images that represent phase\nvariations at different depths. Using this dataset, we compare k-nearest\nneighbors (KNN), random forest (RF), and feedforward neural network (FNN)\nmethods for material segmentation. Our results indicate that the FNN provides\nthe highest accuracy and F1 score, outperforming more traditional approaches.\nFinally, we demonstrate the practical value of these segmented maps by\ngenerating simulated scattering-type scanning near-field optical microscopy\n(s-SNOM) images, highlighting how AFM phase approach-curves can be leveraged to\nproduce detailed, predictive tools for nanoscale optical analysis.",
        "published": "2025-04-02T11:42:03+00:00"
    },
    {
        "title": "Horizon Scans can be accelerated using novel information retrieval and artificial intelligence tools",
        "authors": [
            "Lena Schmidt",
            "Oshin Sharma",
            "Chris Marshall",
            "Sonia Garcia Gonzalez Moral"
        ],
        "summary": "Introduction: Horizon scanning in healthcare assesses early signals of\ninnovation, crucial for timely adoption. Current horizon scanning faces\nchallenges in efficient information retrieval and analysis, especially from\nunstructured sources like news, presenting a need for innovative tools.\nMethodology: The study introduces SCANAR and AIDOC, open-source Python-based\ntools designed to improve horizon scanning. SCANAR automates the retrieval and\nprocessing of news articles, offering functionalities such as de-duplication\nand unsupervised relevancy ranking. AIDOC aids filtration by leveraging AI to\nreorder textual data based on relevancy, employing neural networks for semantic\nsimilarity, and subsequently prioritizing likely relevant entries for human\nreview. Results: Twelve internal datasets from horizon scans and four external\nbenchmarking datasets were used. SCANAR improved retrieval efficiency by\nautomating processes previously dependent on manual labour. AIDOC displayed\nwork-saving potential, achieving around 62% reduction in manual review efforts\nat 95% recall. Comparative analysis with benchmarking data showed AIDOC's\nperformance was similar to existing systematic review automation tools, though\nperformance varied depending on dataset characteristics. A smaller case-study\non our news datasets shows the potential of ensembling large language models\nwithin the active-learning process for faster detection of relevant articles\nacross news datasets. Conclusion: The validation indicates that SCANAR and\nAIDOC show potential to enhance horizon scanning efficiency by streamlining\ndata retrieval and prioritisation. These tools may alleviate methodological\nlimitations and allow broader, swifter horizon scans. Further studies are\nsuggested to optimize these models and to design new workflows and validation\nprocesses that integrate large language models.",
        "published": "2025-04-02T11:33:08+00:00"
    },
    {
        "title": "The Mini-SiTian Array: Optical design",
        "authors": [
            "Zi-Jian Han",
            "Zheng-Yang Li",
            "Chao Chen",
            "Jia-Nan Cong",
            "Ting-Ting Liu",
            "Yi-Ming Zhang",
            "Qing-Shan Li",
            "Liang Chen",
            "Wei-Bin Kong"
        ],
        "summary": "Time-domain astronomy is one of the most important areas. Large sky area,\ndeep-field, and short timescale are the priority of time-domain observations.\nSiTian is an ambitious ground-based project processing all sky optical\nmonitoring, aiming for sky-survey timescale of less than 1 day. It is developed\nby the Chinese Academy of Sciences, an integrated network of dozens of\n1-m-class telescopes deployed worldwide. The Mini-SiTian Telescope Array is\ncarried out for demonstrations on optical design, group scheduling, and\nsoftware pipeline developments, to overcome the high technical and financial\ndifficulties of SiTian project. One array contains three 300 mm F/3 telescope,\nwith FOV of 5 degrees over 400-1000 nm wavelength range. The Mini-SiTian\nTelescope Array is now under commissioning in Xinglong Observatory, and a\nperfect platform for technical research and educational purposes.",
        "published": "2025-04-02T11:26:36+00:00"
    },
    {
        "title": "The Mini-SiTian Array: Design and application of Master Control System",
        "authors": [
            "Zheng Wang",
            "Jin-hang Zou",
            "Liang Ge",
            "Min He",
            "Jian Li",
            "Yi Hu",
            "Jianfeng Tian"
        ],
        "summary": "The SiTian Project represents a groundbreaking initiative in astronomy,\naiming to deploy a global network of telescopes, each with a 1-meter aperture,\nfor comprehensive time-domain sky surveys. The network's innovative\narchitecture features multiple observational nodes, each comprising three\nstrategically aligned telescopes equipped with filters. This design enables\nthree-color (g, r, i) channel imaging within each node, facilitating precise\nand coordinated observations. As a pathfinder to the full-scale project, the\nMini-SiTian Project serves as the scientific and technological validation\nplatform, utilizing three 30-centimeter aperture telescopes to validate the\nmethodologies and technologies planned for the broader SiTian network. This\npaper focuses on the development and implementation of the Master Control\nSystem (MCS),and the central command hub for the Mini-SiTian array. The MCS is\ndesigned to facilitate seamless communication with the SiTian Brain, the\nproject's central processing and decision-making unit, while ensuring accurate\ntask allocation, real-time status monitoring, and optimized observational\nworkflows. The system adopts a robust architecture that separates front-end and\nback-end functionalities.A key innovation of the MCS is its ability to\ndynamically adjust observation plans in response to transient source alerts,\nenabling rapid and coordinated scans of target sky regions...(abridged)",
        "published": "2025-04-02T11:26:11+00:00"
    },
    {
        "title": "The Mini-SiTian Array: real-bogus classification using deep learning",
        "authors": [
            "Jing-Hang Shi",
            "Hong-Rui Gu",
            "Yang Huang",
            "Yan-Xia Zhang",
            "Peng-Liang Du"
        ],
        "summary": "The Mini-SiTian (MST) project is a pathfinder for China's next-generation\nlarge-scale time-domain survey, SiTian, aimed at discovering variable stars,\ntransients, and explosive events. MST generates hundreds of thousands of\ntransient alerts every night, approximately 99\\% of which are false alarms,\nposing a significant challenge to its scientific goals. To mitigate the impact\nof false positives, we propose a deep learning-based solution and\nsystematically evaluate thirteen convolutional neural networks. The results\nshow that ResNet achieves exceptional specificity (99.70\\%),\n  EfficientNet achieves the highest recall rate (98.68\\%), and DenseNet\nprovides balanced performance with a recall rate of 94.55\\% and specificity of\n98.66\\%. Leveraging these complementary strengths, we developed a bagging-based\nensemble classifier that integrates ResNet18, DenseNet121, and EfficientNet\\_B0\nusing a soft voting strategy. This classifier achieved the best AUC value\n(0.9961) among all models, with a recall rate of 95.37\\% and specificity of\n99.25\\%. It has now been successfully deployed in the MST real-time data\nprocessing pipeline. Validation using 5,000 practically processed samples with\na classification threshold of 0.798 showed that the classifier achieved 88.31\\%\naccuracy, 91.89\\% recall rate, and 99.82\\% specificity, confirming its\neffectiveness and robustness under real application conditions.",
        "published": "2025-04-02T11:25:59+00:00"
    },
    {
        "title": "Multi-Relation Graph-Kernel Strengthen Network for Graph-Level Clustering",
        "authors": [
            "Renda Han",
            "Guangzhen Yao",
            "Wenxin Zhang",
            "Yu Li",
            "Wen Xin",
            "Huajie Lei",
            "Mengfei Li",
            "Zeyu Zhang",
            "Chengze Du",
            "Yahe Tian"
        ],
        "summary": "Graph-level clustering is a fundamental task of data mining, aiming at\ndividing unlabeled graphs into distinct groups. However, existing deep methods\nthat are limited by pooling have difficulty extracting diverse and complex\ngraph structure features, while traditional graph kernel methods rely on\nexhaustive substructure search, unable to adaptive handle multi-relational\ndata. This limitation hampers producing robust and representative graph-level\nembeddings. To address this issue, we propose a novel Multi-Relation\nGraph-Kernel Strengthen Network for Graph-Level Clustering (MGSN), which\nintegrates multi-relation modeling with graph kernel techniques to fully\nleverage their respective advantages. Specifically, MGSN constructs\nmulti-relation graphs to capture diverse semantic relationships between nodes\nand graphs, which employ graph kernel methods to extract graph similarity\nfeatures, enriching the representation space. Moreover, a relation-aware\nrepresentation refinement strategy is designed, which adaptively aligns\nmulti-relation information across views while enhancing graph-level features\nthrough a progressive fusion process. Extensive experiments on multiple\nbenchmark datasets demonstrate the superiority of MGSN over state-of-the-art\nmethods. The results highlight its ability to leverage multi-relation\nstructures and graph kernel features, establishing a new paradigm for robust\ngraph-level clustering.",
        "published": "2025-04-02T11:17:15+00:00"
    },
    {
        "title": "A topology-preserving three-stage framework for fully-connected coronary artery extraction",
        "authors": [
            "Yuehui Qiu",
            "Dandan Shan",
            "Yining Wang",
            "Pei Dong",
            "Dijia Wu",
            "Xinnian Yang",
            "Qingqi Hong",
            "Dinggang Shen"
        ],
        "summary": "Coronary artery extraction is a crucial prerequisite for computer-aided\ndiagnosis of coronary artery disease. Accurately extracting the complete\ncoronary tree remains challenging due to several factors, including presence of\nthin distal vessels, tortuous topological structures, and insufficient\ncontrast. These issues often result in over-segmentation and under-segmentation\nin current segmentation methods. To address these challenges, we propose a\ntopology-preserving three-stage framework for fully-connected coronary artery\nextraction. This framework includes vessel segmentation, centerline\nreconnection, and missing vessel reconstruction. First, we introduce a new\ncenterline enhanced loss in the segmentation process. Second, for the broken\nvessel segments, we further propose a regularized walk algorithm to integrate\ndistance, probabilities predicted by a centerline classifier, and directional\ncosine similarity, for reconnecting the centerlines. Third, we apply implicit\nneural representation and implicit modeling, to reconstruct the geometric model\nof the missing vessels. Experimental results show that our proposed framework\noutperforms existing methods, achieving Dice scores of 88.53\\% and 85.07\\%,\nwith Hausdorff Distances (HD) of 1.07mm and 1.63mm on ASOCA and PDSCA datasets,\nrespectively. Code will be available at https://github.com/YH-Qiu/CorSegRec.",
        "published": "2025-04-02T11:04:44+00:00"
    },
    {
        "title": "DEPTHOR: Depth Enhancement from a Practical Light-Weight dToF Sensor and RGB Image",
        "authors": [
            "Jijun Xiang",
            "Xuan Zhu",
            "Xianqi Wang",
            "Yu Wang",
            "Hong Zhang",
            "Fei Guo",
            "Xin Yang"
        ],
        "summary": "Depth enhancement, which uses RGB images as guidance to convert raw signals\nfrom dToF into high-precision, dense depth maps, is a critical task in computer\nvision. Although existing super-resolution-based methods show promising results\non public datasets, they often rely on idealized assumptions like accurate\nregion correspondences and reliable dToF inputs, overlooking calibration errors\nthat cause misalignment and anomaly signals inherent to dToF imaging, limiting\nreal-world applicability. To address these challenges, we propose a novel\ncompletion-based method, named DEPTHOR, featuring advances in both the training\nstrategy and model architecture. First, we propose a method to simulate\nreal-world dToF data from the accurate ground truth in synthetic datasets to\nenable noise-robust training. Second, we design a novel network that\nincorporates monocular depth estimation (MDE), leveraging global depth\nrelationships and contextual information to improve prediction in challenging\nregions. On the ZJU-L5 dataset, our training strategy significantly enhances\ndepth completion models, achieving results comparable to depth super-resolution\nmethods, while our model achieves state-of-the-art results, improving Rel and\nRMSE by 27% and 18%, respectively. On a more challenging set of dToF samples we\ncollected, our method outperforms SOTA methods on preliminary stereo-based GT,\nimproving Rel and RMSE by 23% and 22%, respectively. Our Code is available at\nhttps://github.com/ShadowBbBb/Depthor",
        "published": "2025-04-02T11:02:21+00:00"
    },
    {
        "title": "Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models",
        "authors": [
            "Zhaochen Wang",
            "Bryan Hooi",
            "Yiwei Wang",
            "Ming-Hsuan Yang",
            "Zi Huang",
            "Yujun Cai"
        ],
        "summary": "Vision-language models (VLMs) have advanced rapidly in processing multimodal\ninformation, but their ability to reconcile conflicting signals across\nmodalities remains underexplored. This work investigates how VLMs process ASCII\nart, a unique medium where textual elements collectively form visual patterns,\npotentially creating semantic-visual conflicts. We introduce a novel evaluation\nframework that systematically challenges five state-of-the-art models\n(including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where\ncharacter-level semantics deliberately contradict global visual patterns. Our\nexperiments reveal a strong text-priority bias: VLMs consistently prioritize\ntextual information over visual patterns, with visual recognition ability\ndeclining dramatically as semantic complexity increases. Various mitigation\nattempts through visual parameter tuning and prompt engineering yielded only\nmodest improvements, suggesting that this limitation requires\narchitectural-level solutions. These findings uncover fundamental flaws in how\ncurrent VLMs integrate multimodal information, providing important guidance for\nfuture model development while highlighting significant implications for\ncontent moderation systems vulnerable to adversarial examples.",
        "published": "2025-04-02T10:47:07+00:00"
    },
    {
        "title": "Building Knowledge from Interactions: An LLM-Based Architecture for Adaptive Tutoring and Social Reasoning",
        "authors": [
            "Luca Garello",
            "Giulia Belgiovine",
            "Gabriele Russo",
            "Francesco Rea",
            "Alessandra Sciutti"
        ],
        "summary": "Integrating robotics into everyday scenarios like tutoring or physical\ntraining requires robots capable of adaptive, socially engaging, and\ngoal-oriented interactions. While Large Language Models show promise in\nhuman-like communication, their standalone use is hindered by memory\nconstraints and contextual incoherence. This work presents a multimodal,\ncognitively inspired framework that enhances LLM-based autonomous\ndecision-making in social and task-oriented Human-Robot Interaction.\nSpecifically, we develop an LLM-based agent for a robot trainer, balancing\nsocial conversation with task guidance and goal-driven motivation. To further\nenhance autonomy and personalization, we introduce a memory system for\nselecting, storing and retrieving experiences, facilitating generalized\nreasoning based on knowledge built across different interactions. A preliminary\nHRI user study and offline experiments with a synthetic dataset validate our\napproach, demonstrating the system's ability to manage complex interactions,\nautonomously drive training tasks, and build and retrieve contextual memories,\nadvancing socially intelligent robotics.",
        "published": "2025-04-02T10:45:41+00:00"
    },
    {
        "title": "Modeling Urban Population Dynamics and City-to-City Migration",
        "authors": [
            "Rafael Prieto-Curiel",
            "Carmen Cabrera-Arnau"
        ],
        "summary": "Migration plays a crucial role in urban growth. Over time, individuals opting\nto relocate led to vast metropolises like London and Paris during the\nIndustrial Revolution, Shanghai and Karachi during the last decades and\nthousands of smaller settlements. Here, we analyze the impact that migration\nhas on population redistribution. We use a model of city-to-city migration as a\nprocess that occurs within a network, where the nodes represent cities, and the\nedges correspond to the flux of individuals. We analyze metrics characterizing\nthe urban distribution and show how a slight preference for some destinations\nmight result in the observed distribution of the population.",
        "published": "2025-04-02T10:45:14+00:00"
    },
    {
        "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in Embedded Processors",
        "authors": [
            "Dean You",
            "Jieyu Jiang",
            "Xiaoxuan Wang",
            "Yushu Du",
            "Zhihang Tan",
            "Wenbo Xu",
            "Hui Wang",
            "Jiapeng Guan",
            "Zhenyuan Wang",
            "Ran Wei",
            "Shuai Zhao",
            "Zhe Jiang"
        ],
        "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
        "published": "2025-04-02T10:38:25+00:00"
    },
    {
        "title": "Decoding Covert Speech from EEG Using a Functional Areas Spatio-Temporal Transformer",
        "authors": [
            "Muyun Jiang",
            "Yi Ding",
            "Wei Zhang",
            "Kok Ann Colin Teo",
            "LaiGuan Fong",
            "Shuailei Zhang",
            "Zhiwei Guo",
            "Chenyu Liu",
            "Raghavan Bhuvanakantham",
            "Wei Khang Jeremy Sim",
            "Chuan Huat Vince Foo",
            "Rong Hui Jonathan Chua",
            "Parasuraman Padmanabhan",
            "Victoria Leong",
            "Jia Lu",
            "Balazs Gulyas",
            "Cuntai Guan"
        ],
        "summary": "Covert speech involves imagining speaking without audible sound or any\nmovements. Decoding covert speech from electroencephalogram (EEG) is\nchallenging due to a limited understanding of neural pronunciation mapping and\nthe low signal-to-noise ratio of the signal. In this study, we developed a\nlarge-scale multi-utterance speech EEG dataset from 57 right-handed native\nEnglish-speaking subjects, each performing covert and overt speech tasks by\nrepeating the same word in five utterances within a ten-second duration. Given\nthe spatio-temporal nature of the neural activation process during speech\npronunciation, we developed a Functional Areas Spatio-temporal Transformer\n(FAST), an effective framework for converting EEG signals into tokens and\nutilizing transformer architecture for sequence encoding. Our results reveal\ndistinct and interpretable speech neural features by the visualization of\nFAST-generated activation maps across frontal and temporal brain regions with\neach word being covertly spoken, providing new insights into the discriminative\nfeatures of the neural representation of covert speech. This is the first\nreport of such a study, which provides interpretable evidence for speech\ndecoding from EEG. The code for this work has been made public at\nhttps://github.com/Jiang-Muyun/FAST",
        "published": "2025-04-02T10:38:08+00:00"
    },
    {
        "title": "Pro-DG: Procedural Diffusion Guidance for Architectural Facade Generation",
        "authors": [
            "Aleksander Plocharski",
            "Jan Swidzinski",
            "Przemyslaw Musialski"
        ],
        "summary": "We present Pro-DG, a framework for procedurally controllable photo-realistic\nfacade generation that combines a procedural shape grammar with diffusion-based\nimage synthesis. Starting from a single input image, we reconstruct its facade\nlayout using grammar rules, then edit that structure through user-defined\ntransformations. As facades are inherently multi-hierarchical structures, we\nintroduce hierarchical matching procedure that aligns facade structures at\ndifferent levels which is used to introduce control maps to guide a generative\ndiffusion pipeline. This approach retains local appearance fidelity while\naccommodating large-scale edits such as floor duplication or window\nrearrangement. We provide a thorough evaluation, comparing Pro-DG against\ninpainting-based baselines and synthetic ground truths. Our user study and\nquantitative measurements indicate improved preservation of architectural\nidentity and higher edit accuracy. Our novel method is the first to integrate\nneuro-symbolically derived shape-grammars for modeling with modern generative\nmodel and highlights the broader potential of such approaches for precise and\ncontrollable image manipulation.",
        "published": "2025-04-02T10:16:19+00:00"
    },
    {
        "title": "Density estimation via mixture discrepancy and moments",
        "authors": [
            "Zhengyang Lei",
            "Sihong Shao"
        ],
        "summary": "With the aim of generalizing histogram statistics to higher dimensional\ncases, density estimation via discrepancy based sequential partition (DSP) has\nbeen proposed [D. Li, K. Yang, W. Wong, Advances in Neural Information\nProcessing Systems (2016) 1099-1107] to learn an adaptive piecewise constant\napproximation defined on a binary sequential partition of the underlying\ndomain, where the star discrepancy is adopted to measure the uniformity of\nparticle distribution. However, the calculation of the star discrepancy is\nNP-hard and it does not satisfy the reflection invariance and rotation\ninvariance either. To this end, we use the mixture discrepancy and the\ncomparison of moments as a replacement of the star discrepancy, leading to the\ndensity estimation via mixture discrepancy based sequential partition (DSP-mix)\nand density estimation via moments based sequential partition (MSP),\nrespectively. Both DSP-mix and MSP are computationally tractable and exhibit\nthe reflection and rotation invariance. Numerical experiments in reconstructing\nthe $d$-D mixture of Gaussians and Betas with $d=2, 3, \\dots, 6$ demonstrate\nthat DSP-mix and MSP both run approximately ten times faster than DSP while\nmaintaining the same accuracy.",
        "published": "2025-04-02T10:15:03+00:00"
    },
    {
        "title": "Learning and criticality in a self-organizing model of connectome growth",
        "authors": [
            "Michelle T. Cirunay",
            "Rene C. Batac",
            "G\u00e9za Od\u00f3r"
        ],
        "summary": "The exploration of brain networks has reached an important milestone as\nrelatively large and reliable information has been gathered for connectomes of\ndifferent species. Analyses of connectome data sets reveal that the structural\nlength and the distributions of in- and out-node strengths follow heavy-tailed\nlognormal statistics, while the functional network properties exhibit powerlaw\ntails, suggesting that the brain operates close to a critical point where\ncomputational capabilities and sensitivity to stimulus is optimal. Because\nthese universal network features emerge from bottom-up (self-)organization, one\ncan pose the question of whether they can be modeled via a common framework,\nparticularly through the lens of criticality of statistical physical systems.\nHere, we simultaneously reproduce the powerlaw statistics of connectome edge\nweights and the lognormal distributions of node strengths from an\navalanche-type model with learning that operates on baseline networks that\nmimic the neuronal circuitry. We observe that the avalanches created by a\nsandpile-like model on simulated neurons connected by a hierarchical modular\nnetwork (HMN) produce robust powerlaw avalanche size distributions with\ncritical exponents of 3/2 characteristic of neuronal systems. Introducing\nHebbian learning, wherein neurons that `fire together, wire together,' recovers\nthe powerlaw distribution of edge weights and the lognormal distributions of\nnode degrees, comparable to those obtained from connectome data. Our results\nstrengthen the notion of a critical brain, one whose local interactions drive\nconnectivity and learning without a need for external intervention and precise\ntuning.",
        "published": "2025-04-02T10:07:43+00:00"
    },
    {
        "title": "STPNet: Scale-aware Text Prompt Network for Medical Image Segmentation",
        "authors": [
            "Dandan Shan",
            "Zihan Li",
            "Yunxiang Li",
            "Qingde Li",
            "Jie Tian",
            "Qingqi Hong"
        ],
        "summary": "Accurate segmentation of lesions plays a critical role in medical image\nanalysis and diagnosis. Traditional segmentation approaches that rely solely on\nvisual features often struggle with the inherent uncertainty in lesion\ndistribution and size. To address these issues, we propose STPNet, a\nScale-aware Text Prompt Network that leverages vision-language modeling to\nenhance medical image segmentation. Our approach utilizes multi-scale textual\ndescriptions to guide lesion localization and employs retrieval-segmentation\njoint learning to bridge the semantic gap between visual and linguistic\nmodalities. Crucially, STPNet retrieves relevant textual information from a\nspecialized medical text repository during training, eliminating the need for\ntext input during inference while retaining the benefits of cross-modal\nlearning. We evaluate STPNet on three datasets: COVID-Xray, COVID-CT, and\nKvasir-SEG. Experimental results show that our vision-language approach\noutperforms state-of-the-art segmentation methods, demonstrating the\neffectiveness of incorporating textual semantic knowledge into medical image\nanalysis. The code has been made publicly on\nhttps://github.com/HUANGLIZI/STPNet.",
        "published": "2025-04-02T10:01:42+00:00"
    },
    {
        "title": "RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable 3D Gaussian Avatars",
        "authors": [
            "Yahui Li",
            "Zhi Zeng",
            "Liming Pang",
            "Guixuan Zhang",
            "Shuwu Zhang"
        ],
        "summary": "Modeling animatable human avatars from monocular or multi-view videos has\nbeen widely studied, with recent approaches leveraging neural radiance fields\n(NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in\nnovel-view and novel-pose synthesis. However, existing methods often struggle\nto accurately capture the dynamics of loose clothing, as they primarily rely on\nglobal pose conditioning or static per-frame representations, leading to\noversmoothing and temporal inconsistencies in non-rigid regions. To address\nthis, We propose RealityAvatar, an efficient framework for high-fidelity\ndigital human modeling, specifically targeting loosely dressed avatars. Our\nmethod leverages 3D Gaussian Splatting to capture complex clothing deformations\nand motion dynamics while ensuring geometric consistency. By incorporating a\nmotion trend module and a latentbone encoder, we explicitly model\npose-dependent deformations and temporal variations in clothing behavior.\nExtensive experiments on benchmark datasets demonstrate the effectiveness of\nour approach in capturing fine-grained clothing deformations and motion-driven\nshape variations. Our method significantly enhances structural fidelity and\nperceptual quality in dynamic human reconstruction, particularly in non-rigid\nregions, while achieving better consistency across temporal frames.",
        "published": "2025-04-02T09:59:12+00:00"
    },
    {
        "title": "Semi-Supervised Biomedical Image Segmentation via Diffusion Models and Teacher-Student Co-Training",
        "authors": [
            "Luca Ciampi",
            "Gabriele Lagani",
            "Giuseppe Amato",
            "Fabrizio Falchi"
        ],
        "summary": "Supervised deep learning for semantic segmentation has achieved excellent\nresults in accurately identifying anatomical and pathological structures in\nmedical images. However, it often requires large annotated training datasets,\nwhich limits its scalability in clinical settings. To address this challenge,\nsemi-supervised learning is a well-established approach that leverages both\nlabeled and unlabeled data. In this paper, we introduce a novel semi-supervised\nteacher-student framework for biomedical image segmentation, inspired by the\nrecent success of generative models. Our approach leverages denoising diffusion\nprobabilistic models (DDPMs) to generate segmentation masks by progressively\nrefining noisy inputs conditioned on the corresponding images. The teacher\nmodel is first trained in an unsupervised manner using a cycle-consistency\nconstraint based on noise-corrupted image reconstruction, enabling it to\ngenerate informative semantic masks. Subsequently, the teacher is integrated\ninto a co-training process with a twin-student network. The student learns from\nground-truth labels when available and from teacher-generated pseudo-labels\notherwise, while the teacher continuously improves its pseudo-labeling\ncapabilities. Finally, to further enhance performance, we introduce a\nmulti-round pseudo-label generation strategy that iteratively improves the\npseudo-labeling process. We evaluate our approach on multiple biomedical\nimaging benchmarks, spanning multiple imaging modalities and segmentation\ntasks. Experimental results show that our method consistently outperforms\nstate-of-the-art semi-supervised techniques, highlighting its effectiveness in\nscenarios with limited annotated data. The code to replicate our experiments\ncan be found at\nhttps://github.com/ciampluca/diffusion_semi_supervised_biomedical_image_segmentation",
        "published": "2025-04-02T09:41:43+00:00"
    },
    {
        "title": "Incorporating Coupling Knowledge into Echo State Networks for Learning Spatiotemporally Chaotic Dynamics",
        "authors": [
            "Kuei-Jan Chu",
            "Nozomi Akashi",
            "Akihiro Yamamoto"
        ],
        "summary": "Machine learning methods have shown promise in learning chaotic dynamical\nsystems, enabling model-free short-term prediction and attractor\nreconstruction. However, when applied to large-scale, spatiotemporally chaotic\nsystems, purely data-driven machine learning methods often suffer from\ninefficiencies, as they require a large learning model size and a massive\namount of training data to achieve acceptable performance. To address this\nchallenge, we incorporate the spatial coupling structure of the target system\nas an inductive bias in the network design. Specifically, we introduce\nphysics-guided clustered echo state networks, leveraging the efficiency of the\necho state networks as a base model. Experimental results on benchmark chaotic\nsystems demonstrate that our physics-informed method outperforms existing echo\nstate network models in learning the target chaotic systems. Additionally, our\nmodels exhibit robustness to noise in training data and remain effective even\nwhen prior coupling knowledge is imperfect. This approach has the potential to\nenhance other machine learning methods.",
        "published": "2025-04-02T09:19:33+00:00"
    },
    {
        "title": "DRAN: A Distribution and Relation Adaptive Network for Spatio-temporal Forecasting",
        "authors": [
            "Xiaobei Zou",
            "Luolin Xiong",
            "Kexuan Zhang",
            "Cesare Alippi",
            "Yang Tang"
        ],
        "summary": "Accurate predictions of spatio-temporal systems' states are crucial for tasks\nsuch as system management, control, and crisis prevention. However, the\ninherent time variance of spatio-temporal systems poses challenges to achieving\naccurate predictions whenever stationarity is not granted. To address\nnon-stationarity frameworks, we propose a Distribution and Relation Adaptive\nNetwork (DRAN) capable of dynamically adapting to relation and distribution\nchanges over time. While temporal normalization and de-normalization are\nfrequently used techniques to adapt to distribution shifts, this operation is\nnot suitable for the spatio-temporal context as temporal normalization scales\nthe time series of nodes and possibly disrupts the spatial relations among\nnodes. In order to address this problem, we develop a Spatial Factor Learner\n(SFL) module that enables the normalization and de-normalization process in\nspatio-temporal systems. To adapt to dynamic changes in spatial relationships\namong sensors, we propose a Dynamic-Static Fusion Learner (DSFL) module that\neffectively integrates features learned from both dynamic and static relations\nthrough an adaptive fusion ratio mechanism. Furthermore, we introduce a\nStochastic Learner to capture the noisy components of spatio-temporal\nrepresentations. Our approach outperforms state of the art methods in weather\nprediction and traffic flows forecasting tasks. Experimental results show that\nour SFL efficiently preserves spatial relationships across various temporal\nnormalization operations. Visualizations of the learned dynamic and static\nrelations demonstrate that DSFL can capture both local and distant\nrelationships between nodes. Moreover, ablation studies confirm the\neffectiveness of each component.",
        "published": "2025-04-02T09:18:43+00:00"
    },
    {
        "title": "Improvement of fully-implicit two-phase pore-network models by employing generalized flux functions with additional throat variables",
        "authors": [
            "Martin Schneider",
            "Hanchuan Wu",
            "Maziar Veyskarami",
            "Sorin Pop",
            "Rainer Helmig"
        ],
        "summary": "In fully-implicit two-phase pore-network models, developing a well-converged\nscheme remains a major challenge, primarily due to the discontinuities in the\nphase conductivities. This paper addresses these numerical issues by proposing\na generalized flux function that establishes a continuous flux expression for\ntwo-phase flows by introducing an additional throat variable $\\Theta$. Two\napproaches for expressing this additional throat variable are introduced: the\nfirst applies regularization strategies, while the second constructs an\nadditional residual constraint equation. It is shown that this approach\nsignificantly improves accuracy and ensures the temporal convergence, as\ndemonstrated through various numerical examples.",
        "published": "2025-04-02T09:14:42+00:00"
    },
    {
        "title": "Beyond Nearest Neighbor Interpolation in Data Augmentation",
        "authors": [
            "Olivier Rukundo"
        ],
        "summary": "Avoiding the risk of undefined categorical labels using nearest neighbor\ninterpolation overlooks the risk of exacerbating pixel level annotation errors\nin data augmentation. To simultaneously avoid these risks, the author modified\nconvolutional neural networks data transformation functions by incorporating a\nmodified geometric transformation function to improve the quality of augmented\ndata by removing the reliance on nearest neighbor interpolation and integrating\na mean based class filtering mechanism to handle undefined categorical labels\nwith alternative interpolation algorithms. Experiments on semantic segmentation\ntasks using three medical image datasets demonstrated both qualitative and\nquantitative improvements with alternative interpolation algorithms.",
        "published": "2025-04-02T09:13:18+00:00"
    },
    {
        "title": "Approximate Agreement Algorithms for Byzantine Collaborative Learning",
        "authors": [
            "M\u00e9lanie Cambus",
            "Darya Melnyk",
            "Tijana Milentijevi\u0107",
            "Stefan Schmid"
        ],
        "summary": "In Byzantine collaborative learning, $n$ clients in a peer-to-peer network\ncollectively learn a model without sharing their data by exchanging and\naggregating stochastic gradient estimates. Byzantine clients can prevent others\nfrom collecting identical sets of gradient estimates. The aggregation step thus\nneeds to be combined with an efficient (approximate) agreement subroutine to\nensure convergence of the training process.\n  In this work, we study the geometric median aggregation rule for Byzantine\ncollaborative learning. We show that known approaches do not provide\ntheoretical guarantees on convergence or gradient quality in the agreement\nsubroutine. To satisfy these theoretical guarantees, we present a hyperbox\nalgorithm for geometric median aggregation.\n  We practically evaluate our algorithm in both centralized and decentralized\nsettings under Byzantine attacks on non-i.i.d. data. We show that our geometric\nmedian-based approaches can tolerate sign-flip attacks better than known\nmean-based approaches from the literature.",
        "published": "2025-04-02T08:55:24+00:00"
    },
    {
        "title": "Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment",
        "authors": [
            "Ziteng Cui",
            "Xuangeng Chu",
            "Tatsuya Harada"
        ],
        "summary": "Capturing high-quality photographs under diverse real-world lighting\nconditions is challenging, as both natural lighting (e.g., low-light) and\ncamera exposure settings (e.g., exposure time) significantly impact image\nquality. This challenge becomes more pronounced in multi-view scenarios, where\nvariations in lighting and image signal processor (ISP) settings across\nviewpoints introduce photometric inconsistencies. Such lighting degradations\nand view-dependent variations pose substantial challenges to novel view\nsynthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel\napproach to achieving high-quality novel view synthesis results under diverse\nchallenging lighting conditions using 3DGS. By adopting per-view color matrix\nmapping and view-adaptive curve adjustments, Luminance-GS achieves\nstate-of-the-art (SOTA) results across various lighting conditions -- including\nlow-light, overexposure, and varying exposure -- while not altering the\noriginal 3DGS explicit representation. Compared to previous NeRF- and\n3DGS-based baselines, Luminance-GS provides real-time rendering speed with\nimproved reconstruction quality.",
        "published": "2025-04-02T08:54:57+00:00"
    },
    {
        "title": "Deep Learning-Driven Protein Structure Prediction and Design: Key Model Developments by Nobel Laureates and Multi-Domain Applications",
        "authors": [
            "Wanqing Yang",
            "Yanwei Wang",
            "Yang Wang"
        ],
        "summary": "This systematic review outlines pivotal advancements in deep learning-driven\nprotein structure prediction and design, focusing on four core\nmodels-AlphaFold, RoseTTAFold, RFDiffusion, and ProteinMPNN-developed by 2024\nNobel Laureates in Chemistry: David Baker, Demis Hassabis, and John Jumper. We\nanalyze their technological iterations and collaborative design paradigms,\nemphasizing breakthroughs in atomic-level structural accuracy, functional\nprotein engineering, and multi-component biomolecular interaction modeling. Key\ninnovations include AlphaFold3's diffusion-based framework for unified\nbiomolecular prediction, RoseTTAFold's three-track architecture integrating\nsequence and spatial constraints, RFDiffusion's denoising diffusion for de novo\nprotein generation, and ProteinMPNN's inverse folding for sequence-structure\nco-optimization. Despite transformative progress in applications such as binder\ndesign, nanomaterials, and enzyme engineering, challenges persist in dynamic\nconformational sampling, multimodal data integration, and generalization to\nnon-canonical targets. We propose future directions, including hybrid\nphysics-AI frameworks and multimodal learning, to bridge gaps between\ncomputational design and functional validation in cellular environments.",
        "published": "2025-04-02T08:44:10+00:00"
    },
    {
        "title": "Diameter Shortcut Sets on Temporal Graphs",
        "authors": [
            "Gerome Quantmeyer"
        ],
        "summary": "Shortcut sets are a vital instrument for reducing the diameter of a static\ngraph and, consequently, its shortest path complexity, which is relevant in\nnumerous subfields of graph theory. We explore the notion of shortcut sets in\ntemporal graphs, which incorporate a discrete time model into the graph,\nrendering each edge accessible exclusively at specific points in time. This not\nonly alters the underlying assumptions of regular graphs but also substantially\nincreases the complexity of path problems and reachability. In turn, a temporal\ngraph is often a much more realistic and accurate representation of a\nreal-world network. In this thesis we provide a definition for a shortcut set\nin a temporal graph and explore differences to classic shortcut sets. Utilizing\nthis definition, we show that temporal and regular shortcut sets yield the same\nresults on temporal paths, enabling the application of existing construction\nalgorithms for static shortcut sets on paths. The primary contribution of this\nthesis is a translation approach for general temporal graphs that utilizes the\nstatic expansion of a temporal graph, allowing the conversion of static\nshortcut sets into temporal shortcut sets, yielding similar results.",
        "published": "2025-04-02T08:38:21+00:00"
    },
    {
        "title": "GarmageNet: A Dataset and Scalable Representation for Generic Garment Modeling",
        "authors": [
            "Siran Li",
            "Ruiyang Liu",
            "Chen Liu",
            "Zhendong Wang",
            "Gaofeng He",
            "Yong-Lu Li",
            "Xiaogang Jin",
            "Huamin Wang"
        ],
        "summary": "High-fidelity garment modeling remains challenging due to the lack of\nlarge-scale, high-quality datasets and efficient representations capable of\nhandling non-watertight, multi-layer geometries. In this work, we introduce\nGarmage, a neural-network-and-CG-friendly garment representation that\nseamlessly encodes the accurate geometry and sewing pattern of complex\nmulti-layered garments as a structured set of per-panel geometry images. As a\ndual-2D-3D representation, Garmage achieves an unprecedented integration of 2D\nimage-based algorithms with 3D modeling workflows, enabling high fidelity,\nnon-watertight, multi-layered garment geometries with direct compatibility for\nindustrial-grade simulations.Built upon this representation, we present\nGarmageNet, a novel generation framework capable of producing detailed\nmulti-layered garments with body-conforming initial geometries and intricate\nsewing patterns, based on user prompts or existing in-the-wild sewing patterns.\nFurthermore, we introduce a robust stitching algorithm that recovers per-vertex\nstitches, ensuring seamless integration into flexible simulation pipelines for\ndownstream editing of sewing patterns, material properties, and dynamic\nsimulations. Finally, we release an industrial-standard, large-scale,\nhigh-fidelity garment dataset featuring detailed annotations, vertex-wise\ncorrespondences, and a robust pipeline for converting unstructured production\nsewing patterns into GarmageNet standard structural assets, paving the way for\nlarge-scale, industrial-grade garment generation systems.",
        "published": "2025-04-02T08:37:32+00:00"
    },
    {
        "title": "Identifying Obfuscated Code through Graph-Based Semantic Analysis of Binary Code",
        "authors": [
            "Roxane Cohen",
            "Robin David",
            "Florian Yger",
            "Fabrice Rossi"
        ],
        "summary": "Protecting sensitive program content is a critical issue in various\nsituations, ranging from legitimate use cases to unethical contexts.\nObfuscation is one of the most used techniques to ensure such protection.\nConsequently, attackers must first detect and characterize obfuscation before\nlaunching any attack against it. This paper investigates the problem of\nfunction-level obfuscation detection using graph-based approaches, comparing\nalgorithms, from elementary baselines to promising techniques like GNN (Graph\nNeural Networks), on different feature choices. We consider various obfuscation\ntypes and obfuscators, resulting in two complex datasets. Our findings\ndemonstrate that GNNs need meaningful features that capture aspects of function\nsemantics to outperform baselines. Our approach shows satisfactory results,\nespecially in a challenging 11-class classification task and in a practical\nmalware analysis example.",
        "published": "2025-04-02T08:36:27+00:00"
    },
    {
        "title": "A microscopic traffic flow model on network with destination-aware V2V communications and rational decision-making",
        "authors": [
            "Emiliano Cristiani",
            "Francesca L. Ignoto"
        ],
        "summary": "In this paper we carry out a computational study of a novel microscopic\nfollow-the-leader model for traffic flow on road networks. We assume that each\ndriver has its own origin and destination, and wants to complete its journey in\nminimal time. We also assume that each driver is able to take rational\ndecisions at junctions and can change route while moving depending on the\ntraffic conditions. The main novelty of the model is that vehicles can\nautomatically and anonymously share information about their position,\ndestination, and planned path when they are close to each other within a\ncertain distance. The pieces of information acquired during the journey are\nused to optimize the route itself. In the limit case of a infinite\ncommunication range, we recover the classical Reactive User Equilibrium and\nDynamic User Equilibrium.",
        "published": "2025-04-02T08:35:37+00:00"
    },
    {
        "title": "EEG-EyeTrack: A Benchmark for Time Series and Functional Data Analysis with Open Challenges and Baselines",
        "authors": [
            "Tiago Vasconcelos Afonso",
            "Florian Heinrichs"
        ],
        "summary": "A new benchmark dataset for functional data analysis (FDA) is presented,\nfocusing on the reconstruction of eye movements from EEG data. The contribution\nis twofold: first, open challenges and evaluation metrics tailored to FDA\napplications are proposed. Second, functional neural networks are used to\nestablish baseline results for the primary regression task of reconstructing\neye movements from EEG signals. Baseline results are reported for the new\ndataset, based on consumer-grade hardware, and the EEGEyeNet dataset, based on\nresearch-grade hardware.",
        "published": "2025-04-02T08:33:38+00:00"
    },
    {
        "title": "Fault injection analysis of Real NVP normalising flow model for satellite anomaly detection",
        "authors": [
            "Gabriele Greco",
            "Carlo Cena",
            "Umberto Albertin",
            "Mauro Martini",
            "Marcello Chiaberge"
        ],
        "summary": "Satellites are used for a multitude of applications, including\ncommunications, Earth observation, and space science. Neural networks and deep\nlearning-based approaches now represent the state-of-the-art to enhance the\nperformance and efficiency of these tasks. Given that satellites are\nsusceptible to various faults, one critical application of Artificial\nIntelligence (AI) is fault detection. However, despite the advantages of neural\nnetworks, these systems are vulnerable to radiation errors, which can\nsignificantly impact their reliability. Ensuring the dependability of these\nsolutions requires extensive testing and validation, particularly using fault\ninjection methods. This study analyses a physics-informed (PI) real-valued\nnon-volume preserving (Real NVP) normalizing flow model for fault detection in\nspace systems, with a focus on resilience to Single-Event Upsets (SEUs). We\npresent a customized fault injection framework in TensorFlow to assess neural\nnetwork resilience. Fault injections are applied through two primary methods:\nLayer State injection, targeting internal network components such as weights\nand biases, and Layer Output injection, which modifies layer outputs across\nvarious activations. Fault types include zeros, random values, and bit-flip\noperations, applied at varying levels and across different network layers. Our\nfindings reveal several critical insights, such as the significance of bit-flip\nerrors in critical bits, that can lead to substantial performance degradation\nor even system failure. With this work, we aim to exhaustively study the\nresilience of Real NVP models against errors due to radiation, providing a\nmeans to guide the implementation of fault tolerance measures.",
        "published": "2025-04-02T08:32:59+00:00"
    },
    {
        "title": "HH-PIM: Dynamic Optimization of Power and Performance with Heterogeneous-Hybrid PIM for Edge AI Devices",
        "authors": [
            "Sangmin Jeon",
            "Kangju Lee",
            "Kyeongwon Lee",
            "Woojoo Lee"
        ],
        "summary": "Processing-in-Memory (PIM) architectures offer promising solutions for\nefficiently handling AI applications in energy-constrained edge environments.\nWhile traditional PIM designs enhance performance and energy efficiency by\nreducing data movement between memory and processing units, they are limited in\nedge devices due to continuous power demands and the storage requirements of\nlarge neural network weights in SRAM and DRAM. Hybrid PIM architectures,\nincorporating non-volatile memories like MRAM and ReRAM, mitigate these\nlimitations but struggle with a mismatch between fixed computing resources and\ndynamically changing inference workloads. To address these challenges, this\nstudy introduces a Heterogeneous-Hybrid PIM (HH-PIM) architecture, comprising\nhigh-performance MRAM-SRAM PIM modules and low-power MRAM-SRAM PIM modules. We\nfurther propose a data placement optimization algorithm that dynamically\nallocates data based on computational demand, maximizing energy efficiency.\nFPGA prototyping and power simulations with processors featuring HH-PIM and\nother PIM types demonstrate that the proposed HH-PIM achieves up to $60.43$\npercent average energy savings over conventional PIMs while meeting application\nlatency requirements. These results confirm the suitability of HH-PIM for\nadaptive, energy-efficient AI processing in edge devices.",
        "published": "2025-04-02T08:22:32+00:00"
    },
    {
        "title": "Versatile silicon integrated photonic processor: a reconfigurable solution for netx-generation AI clusters",
        "authors": [
            "Ying Zhu",
            "Yifan Liu",
            "Xinyu Yang",
            "Kailai Liu",
            "Xin Hua",
            "Ming Luo",
            "Jia Liu",
            "Siyao Chang",
            "Shengxiang Zhang",
            "Miao Wu",
            "Zhicheng Wang",
            "Hongguang Zhang",
            "Daigao Chen",
            "Xi Xiao",
            "Shaohua Yu"
        ],
        "summary": "The Artificial Intelligence models pose serious challenges in intensive\ncomputing and high-bandwidth communication for conventional electronic\ncircuit-based computing clusters. Silicon photonic technologies, owing to their\nhigh speed, low latency, large bandwidth, and complementary\nmetal-oxide-semiconductor compatibility, have been widely implemented for data\ntransfer and actively explored as photonic neural networks in AI clusters.\nHowever, current silicon photonic integrated chips lack adaptability for\nmultifuncional use and hardware-software systematic coordination. Here, we\ndevelop a reconfigurable silicon photonic processor with $40$ programmable unit\ncells integrating over $160$ component, which, to the best of our knowledge, is\nthe first to realize diverse functions with a chip for AI clusters, from\ncomputing acceleration and signal processing to network swtiching and secure\nencryption. Through a self-developed automated testing, compilation, and tuning\nframework to the processor without in-network monitoring photodetectors, we\nimplement $4\\times4$ dual-direction unitary and $3\\times3$ uni-direction\nnon-unitary matrix multiplications, neural networks for image recognition,\nmicro-ring modulator wavelength locking, $4\\times4$ photonic channel switching\n, and silicon photonic physical unclonable functions. This optoelectronic\nprocessing system, incorporating the photonic processor and its software stack,\npaves the way for both advanced photonic system-on-chip design and the\nconstruction of photo-electronic AI clusters.",
        "published": "2025-04-02T08:20:04+00:00"
    },
    {
        "title": "Quantum Key Distribution with Efficient Post-Quantum Cryptography-Secured Trusted Node on a Quantum Network",
        "authors": [
            "Yoann Pi\u00e9tri",
            "Pierre-Enguerrand Verdier",
            "Baptiste Lacour",
            "Maxime Gautier",
            "Heming Huang",
            "Thomas Camus",
            "Jean-S\u00e9bastien Pegon",
            "Martin Zuber",
            "Jean-Charles Faug\u00e8re",
            "Matteo Schiavon",
            "Amine Rhouni",
            "Yves Jaou\u00ebn",
            "Nicolas Fabre",
            "Romain All\u00e9aume",
            "Thomas Rivera",
            "Eleni Diamanti"
        ],
        "summary": "Quantum Key Distribution (QKD) enables two distant users to exchange a secret\nkey with information-theoretic security, based on the fundamental laws of\nquantum physics. While it is arguably the most mature application of quantum\ncryptography, it has inherent limitations in the achievable distance and the\nscalability to large-scale infrastructures. While the applicability of QKD can\nbe readily increased with the use of intermediary trusted nodes, this adds\nadditional privacy requirements on third parties. In this work, we present an\nefficient scheme leveraging a trusted node with lower privacy requirements\nthanks to the use of post-quantum cryptographic techniques, and implement it on\na deployed fiber optic quantum communication network in the Paris area.",
        "published": "2025-04-02T08:06:16+00:00"
    },
    {
        "title": "BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models",
        "authors": [
            "Encheng Su",
            "Hu Cao",
            "Alois Knoll"
        ],
        "summary": "Accurate segmentation of polyps and skin lesions is essential for diagnosing\ncolorectal and skin cancers. While various segmentation methods for polyps and\nskin lesions using fully supervised deep learning techniques have been\ndeveloped, the pixel-level annotation of medical images by doctors is both\ntime-consuming and costly. Foundational vision models like the Segment Anything\nModel (SAM) have demonstrated superior performance; however, directly applying\nSAM to medical segmentation may not yield satisfactory results due to the lack\nof domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a\nSAM-guided weakly supervised prompting and boundary refinement network for the\nsegmentation of polyps and skin lesions. Specifically, we fine-tune SAM\ncombined with a CNN module to learn local features. We introduce a WeakBox with\ntwo functions: automatically generating box prompts for the SAM model and using\nour proposed Multi-choice Mask-to-Box (MM2B) transformation for rough\nmask-to-box conversion, addressing the mismatch between coarse labels and\nprecise predictions. Additionally, we apply scale consistency (SC) loss for\nprediction scale alignment. Our DetailRefine module enhances boundary precision\nand segmentation accuracy by refining coarse predictions using a limited amount\nof ground truth labels. This comprehensive approach enables BiSeg-SAM to\nachieve excellent multi-task segmentation performance. Our method demonstrates\nsignificant superiority over state-of-the-art (SOTA) methods when tested on\nfive polyp datasets and one skin cancer dataset.",
        "published": "2025-04-02T08:04:37+00:00"
    },
    {
        "title": "LLM-VPRF: Large Language Model Based Vector Pseudo Relevance Feedback",
        "authors": [
            "Hang Li",
            "Shengyao Zhuang",
            "Bevan Koopman",
            "Guido Zuccon"
        ],
        "summary": "Vector Pseudo Relevance Feedback (VPRF) has shown promising results in\nimproving BERT-based dense retrieval systems through iterative refinement of\nquery representations. This paper investigates the generalizability of VPRF to\nLarge Language Model (LLM) based dense retrievers. We introduce LLM-VPRF and\nevaluate its effectiveness across multiple benchmark datasets, analyzing how\ndifferent LLMs impact the feedback mechanism. Our results demonstrate that\nVPRF's benefits successfully extend to LLM architectures, establishing it as a\nrobust technique for enhancing dense retrieval performance regardless of the\nunderlying models. This work bridges the gap between VPRF with traditional\nBERT-based dense retrievers and modern LLMs, while providing insights into\ntheir future directions.",
        "published": "2025-04-02T08:02:01+00:00"
    },
    {
        "title": "Deep Graph Reinforcement Learning for UAV-Enabled Multi-User Secure Communications",
        "authors": [
            "Xiao Tang",
            "Kexin Zhao",
            "Chao Shen",
            "Qinghe Du",
            "Yichen Wang",
            "Dusit Niyato",
            "Zhu Han"
        ],
        "summary": "While unmanned aerial vehicles (UAVs) with flexible mobility are envisioned\nto enhance physical layer security in wireless communications, the efficient\nsecurity design that adapts to such high network dynamics is rather\nchallenging. The conventional approaches extended from optimization\nperspectives are usually quite involved, especially when jointly considering\nfactors in different scales such as deployment and transmission in UAV-related\nscenarios. In this paper, we address the UAV-enabled multi-user secure\ncommunications by proposing a deep graph reinforcement learning framework.\nSpecifically, we reinterpret the security beamforming as a graph neural network\n(GNN) learning task, where mutual interference among users is managed through\nthe message-passing mechanism. Then, the UAV deployment is obtained through\nsoft actor-critic reinforcement learning, where the GNN-based security\nbeamforming is exploited to guide the deployment strategy update. Simulation\nresults demonstrate that the proposed approach achieves near-optimal security\nperformance and significantly enhances the efficiency of strategy\ndetermination. Moreover, the deep graph reinforcement learning framework offers\na scalable solution, adaptable to various network scenarios and configurations,\nestablishing a robust basis for information security in UAV-enabled\ncommunications.",
        "published": "2025-04-02T07:57:33+00:00"
    },
    {
        "title": "Enabling Systematic Generalization in Abstract Spatial Reasoning through Meta-Learning for Compositionality",
        "authors": [
            "Philipp Mondorf",
            "Shijia Zhou",
            "Monica Riedler",
            "Barbara Plank"
        ],
        "summary": "Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend the\napproach of meta-learning for compositionality to the domain of abstract\nspatial reasoning. To this end, we introduce $\\textit{SYGAR}$-a dataset\ndesigned to evaluate the capacity of models to systematically generalize from\nknown geometric transformations (e.g., translation, rotation) of\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions,\nsignificantly outperforming state-of-the-art LLMs, including o3-mini, GPT-4o,\nand Gemini 2.0 Flash, which fail to exhibit similar systematic behavior. Our\nfindings highlight the effectiveness of meta-learning in promoting\nsystematicity beyond linguistic tasks, suggesting a promising direction toward\nmore robust and generalizable models.",
        "published": "2025-04-02T07:56:39+00:00"
    },
    {
        "title": "Split Federated Learning for UAV-Enabled Integrated Sensing, Computation, and Communication",
        "authors": [
            "Xiangwang Hou",
            "Jingjing Wang",
            "Zekai Zhang",
            "Jiacheng Wang",
            "Lei Liu",
            "Yong Ren"
        ],
        "summary": "Unmanned aerial vehicles (UAVs) with integrated sensing, computation, and\ncommunication (ISCC) capabilities have become key enablers of next-generation\nwireless networks. Federated edge learning (FEL) leverages UAVs as mobile\nlearning agents to collect data, perform local model updates, and contribute to\nglobal model aggregation. However, existing UAV-assisted FEL systems face\ncritical challenges, including excessive computational demands, privacy risks,\nand inefficient communication, primarily due to the requirement for full-model\ntraining on resource-constrained UAVs. To deal with aforementioned challenges,\nwe propose Split Federated Learning for UAV-Enabled ISCC (SFLSCC), a novel\nframework that integrates split federated learning (SFL) into UAV-assisted FEL.\nSFLSCC optimally partitions model training between UAVs and edge servers,\nsignificantly reducing UAVs' computational burden while preserving data\nprivacy. We conduct a theoretical analysis of UAV deployment, split point\nselection, data sensing volume, and client-side aggregation frequency, deriving\nclosed-form upper bounds for the convergence gap. Based on these insights, we\nconceive a joint optimization problem to minimize the energy consumption\nrequired to achieve a target model accuracy. Given the non-convex nature of the\nproblem, we develop a low-complexity algorithm to efficiently determine UAV\ndeployment, split point selection, and communication frequency. Extensive\nsimulations on a target motion recognition task validate the effectiveness of\nSFLSCC, demonstrating superior convergence performance and energy efficiency\ncompared to baseline methods.",
        "published": "2025-04-02T07:53:24+00:00"
    },
    {
        "title": "Solving Time-Fractional Partial Integro-Differential Equations Using Tensor Neural Network",
        "authors": [
            "Zhongshuo Lin",
            "Qingkui Ma",
            "Hehu Xie",
            "Xiaobo Yin"
        ],
        "summary": "In this paper, we propose a novel machine learning method based on adaptive\ntensor neural network subspace to solve linear time-fractional diffusion-wave\nequations and nonlinear time-fractional partial integro-differential equations.\nIn this framework, the tensor neural network and Gauss-Jacobi quadrature are\neffectively combined to construct a universal numerical scheme for the temporal\nCaputo derivative with orders spanning $ (0,1)$ and $(1,2)$. Specifically, in\norder to effectively utilize Gauss-Jacobi quadrature to discretize Caputo\nderivatives, we design the tensor neural network function multiplied by the\nfunction $t^{\\mu}$ where the power $\\mu$ is selected according to the\nparameters of the equations at hand. Finally, some numerical examples are\nprovided to validate the efficiency and accuracy of the proposed tensor neural\nnetwork based machine learning method.",
        "published": "2025-04-02T07:50:23+00:00"
    },
    {
        "title": "EEG2GAIT: A Hierarchical Graph Convolutional Network for EEG-based Gait Decoding",
        "authors": [
            "Xi Fu",
            "Rui Liu",
            "Aung Aung Phyo Wai",
            "Hannah Pulferer",
            "Neethu Robinson",
            "Gernot R M\u00fcller-Putz",
            "Cuntai Guan"
        ],
        "summary": "Decoding gait dynamics from EEG signals presents significant challenges due\nto the complex spatial dependencies of motor processes, the need for accurate\ntemporal and spectral feature extraction, and the scarcity of high-quality gait\nEEG datasets. To address these issues, we propose EEG2GAIT, a novel\nhierarchical graph-based model that captures multi-level spatial embeddings of\nEEG channels using a Hierarchical Graph Convolutional Network (GCN) Pyramid. To\nfurther improve decoding accuracy, we introduce a Hybrid Temporal-Spectral\nReward (HTSR) loss function, which combines time-domain, frequency-domain, and\nreward-based loss components. Moreover, we contribute a new Gait-EEG Dataset\n(GED), consisting of synchronized EEG and lower-limb joint angle data collected\nfrom 50 participants over two lab visits. Validation experiments on both the\nGED and the publicly available Mobile Brain-body imaging (MoBI) dataset\ndemonstrate that EEG2GAIT outperforms state-of-the-art methods and achieves the\nbest joint angle prediction. Ablation studies validate the contributions of the\nhierarchical GCN modules and HTSR Loss, while saliency maps reveal the\nsignificance of motor-related brain regions in decoding tasks. These findings\nunderscore EEG2GAIT's potential for advancing brain-computer interface\napplications, particularly in lower-limb rehabilitation and assistive\ntechnologies.",
        "published": "2025-04-02T07:48:21+00:00"
    },
    {
        "title": "Refining Interactions: Enhancing Anisotropy in Graph Neural Networks with Language Semantics",
        "authors": [
            "Zhaoxing Li",
            "Xiaoming Zhang",
            "Haifeng Zhang",
            "Chengxiang Liu"
        ],
        "summary": "The integration of Large Language Models (LLMs) with Graph Neural Networks\n(GNNs) has recently been explored to enhance the capabilities of Text Attribute\nGraphs (TAGs). Most existing methods feed textual descriptions of the graph\nstructure or neighbouring nodes' text directly into LLMs. However, these\napproaches often cause LLMs to treat structural information simply as general\ncontextual text, thus limiting their effectiveness in graph-related tasks. In\nthis paper, we introduce LanSAGNN (Language Semantic Anisotropic Graph Neural\nNetwork), a framework that extends the concept of anisotropic GNNs to the\nnatural language level. This model leverages LLMs to extract tailor-made\nsemantic information for node pairs, effectively capturing the unique\ninteractions within node relationships. In addition, we propose an efficient\ndual-layer LLMs finetuning architecture to better align LLMs' outputs with\ngraph tasks. Experimental results demonstrate that LanSAGNN significantly\nenhances existing LLM-based methods without increasing complexity while also\nexhibiting strong robustness against interference.",
        "published": "2025-04-02T07:32:45+00:00"
    },
    {
        "title": "MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation",
        "authors": [
            "Zhuangzhuang Chen",
            "Hualiang Wang",
            "Chubin Ou",
            "Xiaomeng Li"
        ],
        "summary": "Optical coherence tomography angiography (OCTA) shows its great importance in\nimaging microvascular networks by providing accurate 3D imaging of blood\nvessels, but it relies upon specialized sensors and expensive devices. For this\nreason, previous works show the potential to translate the readily available 3D\nOptical Coherence Tomography (OCT) images into 3D OCTA images. However,\nexisting OCTA translation methods directly learn the mapping from the OCT\ndomain to the OCTA domain in continuous and infinite space with guidance from\nonly a single view, i.e., the OCTA project map, resulting in suboptimal\nresults. To this end, we propose the multi-view Tri-alignment framework for OCT\nto OCTA 3D image translation in discrete and finite space, named MuTri. In the\nfirst stage, we pre-train two vector-quantized variational auto-encoder (VQ-\nVAE) by reconstructing 3D OCT and 3D OCTA data, providing semantic prior for\nsubsequent multi-view guidances. In the second stage, our multi-view\ntri-alignment facilitates another VQVAE model to learn the mapping from the OCT\ndomain to the OCTA domain in discrete and finite space. Specifically, a\ncontrastive-inspired semantic alignment is proposed to maximize the mutual\ninformation with the pre-trained models from OCT and OCTA views, to facilitate\ncodebook learning. Meanwhile, a vessel structure alignment is proposed to\nminimize the structure discrepancy with the pre-trained models from the OCTA\nproject map view, benefiting from learning the detailed vessel structure\ninformation. We also collect the first large-scale dataset, namely, OCTA2024,\nwhich contains a pair of OCT and OCTA volumes from 846 subjects.",
        "published": "2025-04-02T07:28:09+00:00"
    },
    {
        "title": "Enhancing Traffic Sign Recognition On The Performance Based On Yolov8",
        "authors": [
            "Baba Ibrahim",
            "Zhou Kui"
        ],
        "summary": "This paper Traffic sign recognition plays a crucial role in the development\nof autonomous vehicles and advanced driver-assistance systems (ADAS). Despite\nsignificant advances in deep learning and object detection, accurately\ndetecting and classifying traffic signs remains challenging due to their small\nsizes, variable environmental conditions, occlusion, and class imbalance. This\nthesis presents an enhanced YOLOv8-based detection system that integrates\nadvanced data augmentation techniques, novel architectural enhancements\nincluding Coordinate Attention (CA), Bidirectional Feature Pyramid Network\n(BiFPN), and dynamic modules such as ODConv and LSKA, along with refined loss\nfunctions (EIoU and WIoU combined with Focal Loss). Extensive experiments\nconducted on datasets including GTSRB, TT100K, and GTSDB demonstrate marked\nimprovements in detection accuracy, robustness under adverse conditions, and\nreal-time inference on edge devices. The findings contribute actionable\ninsights for deploying reliable traffic sign recognition systems in real-world\nautonomous driving scenarios.",
        "published": "2025-04-02T07:28:05+00:00"
    },
    {
        "title": "Dynamic Incentive Strategies for Smart EV Charging Stations: An LLM-Driven User Digital Twin Approach",
        "authors": [
            "Yichen Sun",
            "Chenggang Cui",
            "Chuanlin Zhang",
            "Chunyang Gong"
        ],
        "summary": "This paper presents an enhanced electric vehicle demand response system based\non large language models, aimed at optimizing the application of\nvehicle-to-grid technology. By leveraging an large language models-driven\nmulti-agent framework to construct user digital twins integrated with\nmultidimensional user profile features, it enables deep simulation and precise\nprediction of users' charging and discharging decision-making patterns.\nAdditionally, a data- and knowledge-driven dynamic incentive mechanism is\nproposed, combining a distributed optimization model under network constraints\nto optimize the grid-user interaction while ensuring both economic viability\nand security. Simulation results demonstrate that the approach significantly\nimproves load peak-valley regulation and charging/discharging strategies.\nExperimental validation highlights the system's substantial advantages in load\nbalancing, user satisfaction and grid stability, providing decision-makers with\na scalable V2G management tool that promotes the sustainable, synergistic\ndevelopment of vehicle-grid integration.",
        "published": "2025-04-02T07:16:06+00:00"
    },
    {
        "title": "Optimization of BLE Broadcast Mode in Offline Finding Network",
        "authors": [
            "Li Zhang",
            "Cheng Feng",
            "Tian Xia"
        ],
        "summary": "In the Offline Finding Network(OFN), offline Bluetooth tags broadcast to the\nsurrounding area, the finder devices receiving the broadcast signal and upload\nlocation information to the IoT(Internet of Things) cloud servers, thereby\nachieving offline finding of lost items. This process is essentially a\nBluetooth low energy (BLE) neighbor discovery process(NDP). In the process, the\nvariety of Bluetooth scan modes caused by the scan interval and scan window\nsettings affects the discovery latency of finder devices finding the tag\nbroadcast packets. To optimize the experience of searching for lost devices, we\npropose the CPBIS-mechanism, a certain proportion broadcast-intervals screening\nmechanism that calculates the most suitable two broadcast intervals and their\nproportion for offline tags. This reduces discovery latency in the BLE NDP,\nimproves the discovery success rate, further enhances the user experience. To\nour knowledge, we are the first to propose a comprehensive solution for\nconfiguring the broadcast interval parameters of advertisers in BLE NDP,\nparticularly for configurations involving two or more broadcast intervals. We\nevaluated the results obtained by CPBIS on the nRF52832 chip. The data shows\nthat the CPBIS-mechanism achieves relatively low discovery latencies for\nmultiple scan modes.",
        "published": "2025-04-02T07:13:22+00:00"
    },
    {
        "title": "Balancing Subjectivity and Objectivity in Network Selection: A Decision-Making Framework Towards Digital Twins",
        "authors": [
            "Brahim Mefgouda",
            "Hanen Idoudi",
            "Mohammad Al-Quraan",
            "Ismail Lotfi",
            "Omar Alhussein",
            "Lina Mohjazi",
            "Sami Muhaidat"
        ],
        "summary": "Selecting the optimal radio access technology (RAT) during vertical handovers\n(VHO) in heterogeneous wireless networks (HWNs) is critical. Multi-attribute\ndecision-making (MADM) is the most common approach used for network selection\n(NS) in HWNs. However, existing MADM-NS methods face two major challenges: the\nrank reversal problem (RRP), where the relative ranking of alternatives changes\nunexpectedly, and inefficient handling of user and/or service requirements.\nThese limitations result in suboptimal RAT selection and diminished quality of\nservice, which becomes particularly critical for time-sensitive applications.\nTo address these issues, we introduce in this work a novel weighting assignment\ntechnique called BWM-GWO, which integrates the Best-Worst Method (BWM) with the\nGrey Wolf Optimization (GWO) algorithm through a convex linear combination. The\nproposed framework achieves a balanced decision-making process by using BWM to\ncompute subjective weights that capture user/service preferences, while\nemploying GWO to derive objective weights aimed at minimizing RRP. The\ndevelopment and validation of this framework establish a digital model for NS\nin HWNs, marking the initial step toward realizing a digital twin (DT).\nExperimental results show that integrating the proposed BWM-GWO technique with\nMADM-NS reduces RRP occurrence by up to 71.3% while significantly improving\nuser and service satisfaction compared to benchmark approaches.",
        "published": "2025-04-02T07:06:58+00:00"
    },
    {
        "title": "HCAF-DTA: drug-target binding affinity prediction with cross-attention fused hypergraph neural networks",
        "authors": [
            "Jiannuo Li",
            "Lan Yao"
        ],
        "summary": "Accurate prediction of the binding affinity between drugs and target proteins\nis a core task in computer-aided drug design. Existing deep learning methods\ntend to ignore the information of internal sub-structural features of drug\nmolecules and drug-target interactions, resulting in limited prediction\nperformance. In this paper, we propose a drug-target association prediction\nmodel HCAF-DTA based on cross-attention fusion hypergraph neural network. The\nmodel innovatively introduces hypergraph representation in the feature\nextraction stage: drug molecule hypergraphs are constructed based on the tree\ndecomposition algorithm, and the sub-structural and global features extracted\nby fusing the hypergraph neural network with the graphical neural network\nthrough hopping connections, in which the hyper edges can efficiently\ncharacterise the functional functional groups and other key chemical features;\nfor the protein feature extraction, a weighted graph is constructed based on\nthe residues predicted by the ESM model contact maps to construct weighted\ngraphs, and multilayer graph neural networks were used to capture spatial\ndependencies. In the prediction stage, a bidirectional multi-head\ncross-attention mechanism is designed to model intermolecular interactions from\nthe dual viewpoints of atoms and amino acids, and cross-modal features with\ncorrelated information are fused by attention. Experiments on benchmark\ndatasets such as Davis and KIBA show that HCAF-DTA outperforms state of the\narts in all three performance evaluation metrics, with the MSE metrics reaching\n0.198 and 0.122, respectively, with an improvement of up to 4% from the optimal\nbaseline.",
        "published": "2025-04-02T06:46:28+00:00"
    },
    {
        "title": "Semi-Self Representation Learning for Crowdsourced WiFi Trajectories",
        "authors": [
            "Yu-Lin Kuo",
            "Yu-Chee Tseng",
            "Ting-Hui Chiang",
            "Yan-Ann Chen"
        ],
        "summary": "WiFi fingerprint-based localization has been studied intensively. Point-based\nsolutions rely on position annotations of WiFi fingerprints. Trajectory-based\nsolutions, however, require end-position annotations of WiFi trajectories,\nwhere a WiFi trajectory is a multivariate time series of signal features. A\ntrajectory dataset is much larger than a pointwise dataset as the number of\npotential trajectories in a field may grow exponentially with respect to the\nsize of the field. This work presents a semi-self representation learning\nsolution, where a large dataset $C$ of crowdsourced unlabeled WiFi trajectories\ncan be automatically labeled by a much smaller dataset $\\tilde C$ of labeled\nWiFi trajectories. The size of $\\tilde C$ only needs to be proportional to the\nsize of the physical field, while the unlabeled $C$ could be much larger. This\nis made possible through a novel ``cut-and-flip'' augmentation scheme based on\nthe meet-in-the-middle paradigm. A two-stage learning consisting of trajectory\nembedding followed by endpoint embedding is proposed for the unlabeled $C$.\nThen the learned representations are labeled by $\\tilde C$ and connected to a\nneural-based localization network. The result, while delivering promising\naccuracy, significantly relieves the burden of human annotations for\ntrajectory-based localization.",
        "published": "2025-04-02T06:19:43+00:00"
    },
    {
        "title": "Spatial-Filter-Bank-Based Neural Method for Multichannel Speech Enhancement",
        "authors": [
            "Tianqin Zheng",
            "Jilu Jin",
            "Hanchen Pei",
            "Gongping Huang",
            "Jingdong Chen",
            "Jacob Benesty"
        ],
        "summary": "The performance of deep learning-based multi-channel speech enhancement\nmethods often deteriorates when the geometric parameters of the microphone\narray change. Traditional approaches to mitigate this issue typically involve\ntraining on multiple microphone arrays, which can be costly. To address this\nchallenge, we focus on uniform circular arrays and propose the use of a spatial\nfilter bank to extract features that are approximately invariant to geometric\nparameters. These features are then processed by a two-stage conformer-based\nmodel (TSCBM) to enhance speech quality. Experimental results demonstrate that\nour proposed method can be trained on a fixed microphone array while\nmaintaining effective performance across uniform circular arrays with unseen\ngeometric configurations during applications.",
        "published": "2025-04-02T06:13:37+00:00"
    },
    {
        "title": "v-CLR: View-Consistent Learning for Open-World Instance Segmentation",
        "authors": [
            "Chang-Bin Zhang",
            "Jinhong Ni",
            "Yujie Zhong",
            "Kai Han"
        ],
        "summary": "In this paper, we address the challenging problem of open-world instance\nsegmentation. Existing works have shown that vanilla visual networks are biased\ntoward learning appearance information, \\eg texture, to recognize objects. This\nimplicit bias causes the model to fail in detecting novel objects with unseen\ntextures in the open-world setting. To address this challenge, we propose a\nlearning framework, called view-Consistent LeaRning (v-CLR), which aims to\nenforce the model to learn appearance-invariant representations for robust\ninstance segmentation. In v-CLR, we first introduce additional views for each\nimage, where the texture undergoes significant alterations while preserving the\nimage's underlying structure. We then encourage the model to learn the\nappearance-invariant representation by enforcing the consistency between object\nfeatures across different views, for which we obtain class-agnostic object\nproposals using off-the-shelf unsupervised models that possess strong\nobject-awareness. These proposals enable cross-view object feature matching,\ngreatly reducing the appearance dependency while enhancing the\nobject-awareness. We thoroughly evaluate our method on public benchmarks under\nboth cross-class and cross-dataset settings, achieving state-of-the-art\nperformance. Project page: https://visual-ai.github.io/vclr",
        "published": "2025-04-02T05:52:30+00:00"
    },
    {
        "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
        "authors": [
            "Soro Bedionita",
            "Bruno Andreis",
            "Song Chong",
            "Sung Ju Hwang"
        ],
        "summary": "Learning to generate neural network parameters conditioned on task\ndescriptions and architecture specifications is pivotal for advancing model\nadaptability and transfer learning. Existing methods especially those based on\ndiffusion models suffer from limited scalability to large architectures,\nrigidity in handling varying network depths, and disjointed parameter\ngeneration that undermines inter-layer coherence. In this work, we propose IGPG\n(Instruction Guided Parameter Generation), an autoregressive framework that\nunifies parameter synthesis across diverse tasks and architectures. IGPG\nleverages a VQ-VAE and an autoregressive model to generate neural network\nparameters, conditioned on task instructions, dataset, and architecture\ndetails. By autoregressively generating neural network weights' tokens, IGPG\nensures inter-layer coherence and enables efficient adaptation across models\nand datasets. Operating at the token level, IGPG effectively captures complex\nparameter distributions aggregated from a broad spectrum of pretrained models.\nExtensive experiments on multiple vision datasets demonstrate that IGPG\nconsolidates diverse pretrained models into a single, flexible generative\nframework. The synthesized parameters achieve competitive or superior\nperformance relative to state-of-the-art methods, especially in terms of\nscalability and efficiency when applied to large architectures. These results\nunderscore ICPG potential as a powerful tool for pretrained weight retrieval,\nmodel selection, and rapid task-specific fine-tuning.",
        "published": "2025-04-02T05:50:19+00:00"
    },
    {
        "title": "FireGuard: A Generalized Microarchitecture for Fine-Grained Security Analysis on OoO Superscalar Cores",
        "authors": [
            "Zhe Jiang",
            "Sam Ainsworth",
            "Timothy Jones"
        ],
        "summary": "High-performance security guarantees rely on hardware support. Generic\nprogrammable support for fine-grained instruction analysis has gained broad\ninterest in the literature as a fundamental building block for the security of\nfuture processors. Yet, implementation in real out-of-order (OoO) superscalar\nprocessors presents tough challenges that cannot be explored in highly abstract\nsimulators. We detail the challenges of implementing complex programmable\npathways without critical paths or contention. We then introduce FireGuard, the\nfirst implementation of fine-grained instruction analysis on a real OoO\nsuperscalar processor. We establish an end-to-end system, including\nmicroarchitecture, SoC, ISA and programming model. Experiments show that our\nsolution simultaneously ensures both security and performance of the system,\nwith parallel scalability. We examine the feasibility of building FireGuard\ninto modern SoCs: Apple's M1-Pro, Huawei's Kirin-960, and Intel's i7-12700F,\nwhere less than 1% silicon area is introduced. The Repo. of FireGuard's source\ncode: https://github.com/SEU-ACAL/reproduce-FireGuard-DAC-25.",
        "published": "2025-04-02T05:49:22+00:00"
    },
    {
        "title": "The Multifractal IP Address Structure: Physical Explanation and Implications",
        "authors": [
            "Chris Misa",
            "Ram Durairajan",
            "Arpit Gupta",
            "Reza Rejaie",
            "Walter Willinger"
        ],
        "summary": "The structure of IP addresses observed in Internet traffic plays a critical\nrole for a wide range of networking problems of current interest. For example,\nmodern network telemetry systems that take advantage of existing data plane\ntechnologies for line rate traffic monitoring and processing cannot afford to\nwaste precious data plane resources on traffic that comes from \"uninteresting\"\nregions of the IP address space. However, there is currently no\nwell-established structural model or analysis toolbox that enables a\nfirst-principles approach to the specific problem of identifying\n\"uninteresting\" regions of the address space or the myriad of other networking\nproblems that prominently feature IP addresses.\n  To address this key missing piece, we present in this paper a\nfirst-of-its-kind empirically validated physical explanation for why the\nobserved IP address structure in measured Internet traffic is multifractal in\nnature. Our root cause analysis overcomes key limitations of mostly forgotten\nfindings from ~20 years ago and demonstrates that the Internet processes and\nmechanisms responsible for how IP addresses are allocated, assigned, and used\nin today's Internet are consistent with and well modeled by a class of\nevocative mathematical models called conservative cascades. We complement this\nroot cause analysis with the development of an improved toolbox that is\ntailor-made for analyzing finite and discrete sets of IP addresses and includes\nstatistical estimators that engender high confidence in the inferences they\nproduce. We illustrate the use of this toolbox in the context of a novel\naddress structure anomaly detection method we designed and conclude with a\ndiscussion of a range of challenging open networking problems that are\nmotivated or inspired by our findings.",
        "published": "2025-04-02T05:36:10+00:00"
    },
    {
        "title": "UniFault: A Fault Diagnosis Foundation Model from Bearing Data",
        "authors": [
            "Emadeldeen Eldele",
            "Mohamed Ragab",
            "Xu Qing",
            "Edward",
            "Zhenghua Chen",
            "Min Wu",
            "Xiaoli Li",
            "Jay Lee"
        ],
        "summary": "Machine fault diagnosis (FD) is a critical task for predictive maintenance,\nenabling early fault detection and preventing unexpected failures. Despite its\nimportance, existing FD models are operation-specific with limited\ngeneralization across diverse datasets. Foundation models (FM) have\ndemonstrated remarkable potential in both visual and language domains,\nachieving impressive generalization capabilities even with minimal data through\nfew-shot or zero-shot learning. However, translating these advances to FD\npresents unique hurdles. Unlike the large-scale, cohesive datasets available\nfor images and text, FD datasets are typically smaller and more heterogeneous,\nwith significant variations in sampling frequencies and the number of channels\nacross different systems and applications. This heterogeneity complicates the\ndesign of a universal architecture capable of effectively processing such\ndiverse data while maintaining robust feature extraction and learning\ncapabilities. In this paper, we introduce UniFault, a foundation model for\nfault diagnosis that systematically addresses these issues. Specifically, the\nmodel incorporates a comprehensive data harmonization pipeline featuring two\nkey innovations. First, a unification scheme transforms multivariate inputs\ninto standardized univariate sequences while retaining local inter-channel\nrelationships. Second, a novel cross-domain temporal fusion strategy mitigates\ndistribution shifts and enriches sample diversity and count, improving the\nmodel generalization across varying conditions. UniFault is pretrained on over\n9 billion data points spanning diverse FD datasets, enabling superior few-shot\nperformance. Extensive experiments on real-world FD datasets demonstrate that\nUniFault achieves SoTA performance, setting a new benchmark for fault diagnosis\nmodels and paving the way for more scalable and robust predictive maintenance\nsolutions.",
        "published": "2025-04-02T05:34:27+00:00"
    },
    {
        "title": "SCNR Maximization for MIMO ISAC Assisted by Fluid Antenna System",
        "authors": [
            "Yuqi Ye",
            "Li You",
            "Hao Xu",
            "Ahmed Elzanaty",
            "Kai-Kit Wong",
            "Xiqi Gao"
        ],
        "summary": "The integrated sensing and communication (ISAC) technology has been\nextensively researched to enhance communication rates and radar sensing\ncapabilities. Additionally, a new technology known as fluid antenna system\n(FAS) has recently been proposed to obtain higher communication rates for\nfuture wireless networks by dynamically altering the antenna position to obtain\na more favorable channel condition. The application of the FAS technology in\nISAC scenarios holds significant research potential. In this paper, we\ninvestigate a FAS-assisted multiple-input multiple-output (MIMO) ISAC system\nfor maximizing the radar sensing signal-clutter-noise ratio (SCNR) under\ncommunication signal-to-interference-plus-noise ratio (SINR) and antenna\nposition constraints. We devise an iterative algorithm that tackles the\noptimization problem by maximizing a lower bound of SCNR with respect to the\ntransmit precoding matrix and the antenna position. By addressing the\nnon-convexity of the problem through this iterative approach, our method\nsignificantly improves the SCNR. Our simulation results demonstrate that the\nproposed scheme achieves a higher SCNR compared to the baselines.",
        "published": "2025-04-02T05:34:18+00:00"
    },
    {
        "title": "Accelerating Blockchain Scalability: New Models for Parallel Transaction Execution in the EVM",
        "authors": [
            "Souradeep Das",
            "Konpat Preechakul",
            "Jonas B\u00e4umer",
            "Riddhi Patel",
            "Jefferson Jinchuan Li"
        ],
        "summary": "As the number of decentralized applications and users on Ethereum grows, the\nability of the blockchain to efficiently handle a growing number of\ntransactions becomes increasingly strained. Ethereums current execution model\nrelies heavily on sequential processing, meaning that operations are processed\none after the other, which creates significant bottlenecks to future\nscalability demands. While scalability solutions for Ethereum exist, they\ninherit the limitations of the EVM, restricting the extent to which they can\nscale. This paper proposes a novel solution to enable maximally parallelizable\nexecutions within Ethereum, built out of three self-sufficient approaches.\nThese approaches include strategies in which Ethereum transaction state\naccesses could be strategically and efficiently predetermined, and further\npropose how the incorporation of gas based incentivization mechanisms could\nenforce a maximally parallelizable network.",
        "published": "2025-04-02T05:33:42+00:00"
    },
    {
        "title": "MEEK: Re-thinking Heterogeneous Parallel Error Detection Architecture for Real-World OoO Superscalar Processors",
        "authors": [
            "Zhe Jiang",
            "Minli Liao",
            "Sam Ainsworth",
            "Dean You",
            "Timothy Jones"
        ],
        "summary": "Heterogeneous parallel error detection is an approach to achieving\nfault-tolerant processors, leveraging multiple power-efficient cores to\nre-execute software originally run on a high-performance core. Yet, its complex\ncomponents, gathering data cross-chip from many parts of the core, raise\nquestions of how to build it into commodity cores without heavy design invasion\nand extensive re-engineering.\n  We build the first full-RTL design, MEEK, into an open-source SoC, from\nmicroarchitecture and ISA to the OS and programming model. We identify and\nsolve bottlenecks and bugs overlooked in previous work, and demonstrate that\nMEEK offers microsecond-level detection capacity with affordable overheads. By\ntrading off architectural functionalities across codesigned hardware-software\nlayers, MEEK features only light changes to a mature out-of-order superscalar\ncore, simple coordinating software layers, and a few lines of operating-system\ncode. The Repo. of MEEK's source code:\nhttps://github.com/SEU-ACAL/reproduce-MEEK-DAC-25.",
        "published": "2025-04-02T04:32:49+00:00"
    },
    {
        "title": "IRS Assisted Decentralized Learning for Wideband Spectrum Sensing",
        "authors": [
            "Sicheng Liu",
            "Qun Wang",
            "Zhuwei Qin",
            "Weishan Zhang",
            "Jingyi Wang",
            "Xiang Ma"
        ],
        "summary": "The increasing demand for reliable connectivity in industrial environments\nnecessitates effective spectrum utilization strategies, especially in the\ncontext of shared spectrum bands.\n  However, the dynamic spectrum-sharing mechanisms often lead to significant\ninterference and critical failures, creating a trade-off between spectrum\nscarcity and under-utilization.\n  This paper addresses these challenges by proposing a novel Intelligent\nReflecting Surface (IRS)-assisted spectrum sensing framework integrated with\ndecentralized deep learning.\n  The proposed model overcomes partial observation constraints and minimizes\ncommunication overhead while leveraging IRS technology to enhance spectrum\nsensing accuracy.\n  Through comprehensive simulations, the framework demonstrates its ability to\nmonitor wideband spectrum occupancy effectively, even under challenging\nsignal-to-noise ratio (SNR) conditions.\n  This approach offers a scalable and robust solution for spectrum management\nin next-generation wireless networks.",
        "published": "2025-04-02T04:18:41+00:00"
    },
    {
        "title": "Global Rice Multi-Class Segmentation Dataset (RiceSEG): A Comprehensive and Diverse High-Resolution RGB-Annotated Images for the Development and Benchmarking of Rice Segmentation Algorithms",
        "authors": [
            "Junchi Zhou",
            "Haozhou Wang",
            "Yoichiro Kato",
            "Tejasri Nampally",
            "P. Rajalakshmi",
            "M. Balram",
            "Keisuke Katsura",
            "Hao Lu",
            "Yue Mu",
            "Wanneng Yang",
            "Yangmingrui Gao",
            "Feng Xiao",
            "Hongtao Chen",
            "Yuhao Chen",
            "Wenjuan Li",
            "Jingwen Wang",
            "Fenghua Yu",
            "Jian Zhou",
            "Wensheng Wang",
            "Xiaochun Hu",
            "Yuanzhu Yang",
            "Yanfeng Ding",
            "Wei Guo",
            "Shouyang Liu"
        ],
        "summary": "Developing computer vision-based rice phenotyping techniques is crucial for\nprecision field management and accelerating breeding, thereby continuously\nadvancing rice production. Among phenotyping tasks, distinguishing image\ncomponents is a key prerequisite for characterizing plant growth and\ndevelopment at the organ scale, enabling deeper insights into eco-physiological\nprocesses. However, due to the fine structure of rice organs and complex\nillumination within the canopy, this task remains highly challenging,\nunderscoring the need for a high-quality training dataset. Such datasets are\nscarce, both due to a lack of large, representative collections of rice field\nimages and the time-intensive nature of annotation. To address this gap, we\nestablished the first comprehensive multi-class rice semantic segmentation\ndataset, RiceSEG. We gathered nearly 50,000 high-resolution, ground-based\nimages from five major rice-growing countries (China, Japan, India, the\nPhilippines, and Tanzania), encompassing over 6,000 genotypes across all growth\nstages. From these original images, 3,078 representative samples were selected\nand annotated with six classes (background, green vegetation, senescent\nvegetation, panicle, weeds, and duckweed) to form the RiceSEG dataset. Notably,\nthe sub-dataset from China spans all major genotypes and rice-growing\nenvironments from the northeast to the south. Both state-of-the-art\nconvolutional neural networks and transformer-based semantic segmentation\nmodels were used as baselines. While these models perform reasonably well in\nsegmenting background and green vegetation, they face difficulties during the\nreproductive stage, when canopy structures are more complex and multiple\nclasses are involved. These findings highlight the importance of our dataset\nfor developing specialized segmentation models for rice and other crops.",
        "published": "2025-04-02T04:03:23+00:00"
    },
    {
        "title": "Computing Time-varying Network Reliability using Binary Decision Diagrams",
        "authors": [
            "Yu Nakahata",
            "Shun Arizono",
            "Shoji Kasahara"
        ],
        "summary": "Computing the reliability of a time-varying network, taking into account its\ndynamic nature, is crucial for networks that change over time, such as space\nnetworks, vehicular ad-hoc networks, and drone networks. These networks are\nmodeled using temporal graphs, in which each edge is labeled with a time\nindicating its existence at a specific point in time. The time-varying network\nreliability is defined as the probability that a data packet from the source\nvertex can reach the terminal vertex, following links with increasing time\nlabels (i.e., a journey), while taking into account the possibility of network\nlink failures. Currently, the existing method for calculating this reliability\ninvolves explicitly enumerating all possible journeys between the source and\nterminal vertices and then calculating the reliability using the sum of\ndisjoint products method. However, this method has high computational\ncomplexity. In contrast, there is an efficient algorithm that uses binary\ndecision diagrams (BDDs) to evaluate the reliability of a network whose\ntopology does not change over time. This paper presents an efficient exact\nalgorithm that utilizes BDDs for computing the time-varying network\nreliability. Experimental results show that the proposed method runs faster\nthan the existing method up to four orders of magnitude.",
        "published": "2025-04-02T03:58:50+00:00"
    },
    {
        "title": "Robust AI-Synthesized Image Detection via Multi-feature Frequency-aware Learning",
        "authors": [
            "Hongfei Cai",
            "Chi Liu",
            "Sheng Shen",
            "Youyang Qu",
            "Peng Gui"
        ],
        "summary": "The rapid progression of generative AI (GenAI) technologies has heightened\nconcerns regarding the misuse of AI-generated imagery. To address this issue,\nrobust detection methods have emerged as particularly compelling, especially in\nchallenging conditions where the targeted GenAI models are out-of-distribution\nor the generated images have been subjected to perturbations during\ntransmission. This paper introduces a multi-feature fusion framework designed\nto enhance spatial forensic feature representations with incorporating three\ncomplementary components, namely noise correlation analysis, image gradient\ninformation, and pretrained vision encoder knowledge, using a cross-source\nattention mechanism. Furthermore, to identify spectral abnormality in synthetic\nimages, we propose a frequency-aware architecture that employs the\nFrequency-Adaptive Dilated Convolution, enabling the joint modeling of spatial\nand spectral features while maintaining low computational complexity. Our\nframework exhibits exceptional generalization performance across fourteen\ndiverse GenAI systems, including text-to-image diffusion models, autoregressive\napproaches, and post-processed deepfake methods. Notably, it achieves\nsignificantly higher mean accuracy in cross-model detection tasks when compared\nto existing state-of-the-art techniques. Additionally, the proposed method\ndemonstrates resilience against various types of real-world image noise\nperturbations such as compression and blurring. Extensive ablation studies\nfurther corroborate the synergistic benefits of fusing multi-model forensic\nfeatures with frequency-aware learning, underscoring the efficacy of our\napproach.",
        "published": "2025-04-02T03:57:12+00:00"
    },
    {
        "title": "Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design",
        "authors": [
            "Mohan Zhang",
            "Pingzhi Li",
            "Jie Peng",
            "Mufan Qiu",
            "Tianlong Chen"
        ],
        "summary": "Mixture-of-Experts (MoE) has successfully scaled up models while maintaining\nnearly constant computing costs. By employing a gating network to route input\ntokens, it selectively activates a subset of expert networks to process the\ncorresponding token embeddings. However, in practice, the efficiency of MoE is\nchallenging to achieve due to two key reasons: imbalanced expert activation,\nwhich leads to substantial idle time during model or expert parallelism, and\ninsufficient capacity utilization; massive communication overhead, induced by\nnumerous expert routing combinations in expert parallelism at the system level.\nPrevious works typically formulate it as the load imbalance issue characterized\nby the gating network favoring certain experts over others or attribute it to\nstatic execution which fails to adapt to the dynamic expert workload at\nruntime. In this paper, we exploit it from a brand new perspective, a\nhigher-order view and analysis of MoE routing policies: expert collaboration\nand specialization where some experts tend to activate broadly with others\n(collaborative), while others are more likely to activate only with a specific\nsubset of experts (specialized). Our experiments reveal that most experts tend\nto be overly collaborative, leading to increased communication overhead from\nrepeatedly sending tokens to different accelerators. To this end, we propose a\nnovel collaboration-constrained routing (C2R) strategy to encourage more\nspecialized expert groups, as well as to improve expert utilization, and\npresent an efficient implementation of MoE that further leverages expert\nspecialization. We achieve an average performance improvement of 0.51% and\n0.33% on LLaMA-MoE and Qwen-MoE respectively across ten downstream NLP\nbenchmarks, and reduce the all2all communication costs between GPUs, bringing\nan extra 20%-30% total running time savings on top of the existing SoTA, i.e.\nMegaBlocks.",
        "published": "2025-04-02T03:51:59+00:00"
    },
    {
        "title": "Inverse RL Scene Dynamics Learning for Nonlinear Predictive Control in Autonomous Vehicles",
        "authors": [
            "Sorin Grigorescu",
            "Mihai Zaha"
        ],
        "summary": "This paper introduces the Deep Learning-based Nonlinear Model Predictive\nController with Scene Dynamics (DL-NMPC-SD) method for autonomous navigation.\nDL-NMPC-SD uses an a-priori nominal vehicle model in combination with a scene\ndynamics model learned from temporal range sensing information. The scene\ndynamics model is responsible for estimating the desired vehicle trajectory, as\nwell as to adjust the true system model used by the underlying model predictive\ncontroller. We propose to encode the scene dynamics model within the layers of\na deep neural network, which acts as a nonlinear approximator for the high\norder state-space of the operating conditions. The model is learned based on\ntemporal sequences of range sensing observations and system states, both\nintegrated by an Augmented Memory component. We use Inverse Reinforcement\nLearning and the Bellman optimality principle to train our learning controller\nwith a modified version of the Deep Q-Learning algorithm, enabling us to\nestimate the desired state trajectory as an optimal action-value function. We\nhave evaluated DL-NMPC-SD against the baseline Dynamic Window Approach (DWA),\nas well as against two state-of-the-art End2End and reinforcement learning\nmethods, respectively. The performance has been measured in three experiments:\ni) in our GridSim virtual environment, ii) on indoor and outdoor navigation\ntasks using our RovisLab AMTU (Autonomous Mobile Test Unit) platform and iii)\non a full scale autonomous test vehicle driving on public roads.",
        "published": "2025-04-02T03:46:37+00:00"
    },
    {
        "title": "Reconfigurable Codebook-Based Beamforming for RDARS-Aided mmWave MU-MIMO Systems",
        "authors": [
            "Chengwang Ji",
            "Qing Xue",
            "Haiquan Lu",
            "Jintao Wang",
            "Qiaoyan Peng",
            "Shaodan Ma",
            "Wei Zhang"
        ],
        "summary": "Reconfigurable distributed antenna and reflecting surface (RDARS) is a new\narchitecture for the sixth-generation (6G) millimeter wave (mmWave)\ncommunications. In RDARS-aided mmWave systems, the active and passive\nbeamforming design and working mode configuration for reconfigurable elements\nare crucial for system performance. In this paper, we aim to maximize the\nweighted sum rate (WSR) in the RDARS-aided mmWave system. To take advantage of\nRDARS, we first design a reconfigurable codebook (RCB) in which the number and\ndimension of the codeword can be flexibly adjusted. Then, a low overhead beam\ntraining scheme based on hierarchical search is proposed. Accordingly, the\nactive and passive beamforming for data transmission is designed to achieve the\nmaximum WSR for both space-division multiple access (SDMA) and time-division\nmultiple access (TDMA) schemes. For the TDMA scheme, the optimal number of\nRDARS transmit elements and the allocated power budget for WSR maximization are\nderived in closed form. Besides, the superiority of the RDARS is verified and\nthe conditions under which RDARS outperforms RIS and DAS are given. For the\nSDMA scheme, we characterize the relationship between the number of RDARS\nconnected elements and the user distribution, followed by the derivation of the\noptimal placement positions of the RDARS transmit elements. High-quality\nbeamforming design solutions are derived to minimize the inter-user\ninterference (IUI) at the base station and RDARS side respectively, which\nnearly leads to the maximal WSR. Finally, simulation results confirm our\ntheoretical findings and the superiority of the proposed schemes.",
        "published": "2025-04-02T03:40:37+00:00"
    },
    {
        "title": "When to Truncate the Archive? On the Effect of the Truncation Frequency in Multi-Objective Optimisation",
        "authors": [
            "Zhiji Cui",
            "Zimin Liang",
            "Lie Meng Pang",
            "Hisao Ishibuchi",
            "Miqing Li"
        ],
        "summary": "Using an archive to store nondominated solutions found during the search of a\nmulti-objective evolutionary algorithm (MOEA) is a useful practice. However, as\nnondominated solutions of a multi-objective optimisation problem can be\nenormous or infinitely many, it is desirable to provide the decision-maker with\nonly a small, representative portion of all the nondominated solutions in the\narchive, thus entailing a truncation operation. Then, an important issue is\nwhen to truncate the archive. This can be done once a new solution generated, a\nbatch of new solutions generated, or even using an unbounded archive to keep\nall nondominated solutions generated and truncate it later. Intuitively, the\nlast approach may lead to a better result since we have all the information in\nhand before performing the truncation. In this paper, we study this issue and\ninvestigate the effect of the timing of truncating the archive. We apply\nwell-established truncation criteria that are commonly used in the population\nmaintenance procedure of MOEAs (e.g., crowding distance, hypervolume indicator,\nand decomposition). We show that, interestingly, truncating the archive once a\nnew solution generated tends to be the best, whereas considering an unbounded\narchive is often the worst. We analyse and discuss this phenomenon. Our results\nhighlight the importance of developing effective subset selection techniques\n(rather than employing the population maintenance methods in MOEAs) when using\na large archive.",
        "published": "2025-04-02T03:33:49+00:00"
    },
    {
        "title": "An Explainable Reconfiguration-Based Optimization Algorithm for Industrial and Reliability-Redundancy Allocation Problems",
        "authors": [
            "Dikshit Chauhan",
            "Nitin Gupta",
            "Anupam Yadav"
        ],
        "summary": "Industrial and reliability optimization problems often involve complex\nconstraints and require efficient, interpretable solutions. This paper presents\nAI-AEFA, an advanced parameter reconfiguration-based metaheuristic algorithm\ndesigned to address large-scale industrial and reliability-redundancy\nallocation problems. AI-AEFA enhances search space exploration and convergence\nefficiency through a novel log-sigmoid-based parameter adaptation and chaotic\nmapping mechanism. The algorithm is validated across twenty-eight IEEE CEC 2017\nconstrained benchmark problems, fifteen large-scale industrial optimization\nproblems, and seven reliability-redundancy allocation problems, consistently\noutperforming state-of-the-art optimization techniques in terms of feasibility,\ncomputational efficiency, and convergence speed. The additional key\ncontribution of this work is the integration of SHAP (Shapley Additive\nExplanations) to enhance the interpretability of AI-AEFA, providing insights\ninto the impact of key parameters such as Coulomb's constant, charge,\nacceleration, and electrostatic force. This explainability feature enables a\ndeeper understanding of decision-making within the AI-AEFA framework during the\noptimization processes. The findings confirm AI-AEFA as a robust, scalable, and\ninterpretable optimization tool with significant real-world applications.",
        "published": "2025-04-02T03:33:48+00:00"
    },
    {
        "title": "Flexible and Explainable Graph Analysis for EEG-based Alzheimer's Disease Classification",
        "authors": [
            "Jing Wang",
            "Jun-En Ding",
            "Feng Liu",
            "Elisa Kallioniemi",
            "Shuqiang Wang",
            "Wen-Xiang Tsai",
            "Albert C. Yang"
        ],
        "summary": "Alzheimer's Disease is a progressive neurological disorder that is one of the\nmost common forms of dementia. It leads to a decline in memory, reasoning\nability, and behavior, especially in older people. The cause of Alzheimer's\nDisease is still under exploration and there is no all-inclusive theory that\ncan explain the pathologies in each individual patient. Nevertheless, early\nintervention has been found to be effective in managing symptoms and slowing\ndown the disease's progression. Recent research has utilized\nelectroencephalography (EEG) data to identify biomarkers that distinguish\nAlzheimer's Disease patients from healthy individuals. Prior studies have used\nvarious machine learning methods, including deep learning and graph neural\nnetworks, to examine electroencephalography-based signals for identifying\nAlzheimer's Disease patients. In our research, we proposed a Flexible and\nExplainable Gated Graph Convolutional Network (GGCN) with Multi-Objective\nTree-Structured Parzen Estimator (MOTPE) hyperparameter tuning. This provides a\nflexible solution that efficiently identifies the optimal number of GGCN blocks\nto achieve the optimized precision, specificity, and recall outcomes, as well\nas the optimized area under the Receiver Operating Characteristic (AUC). Our\nfindings demonstrated a high efficacy with an over 0.9 Receiver Operating\nCharacteristic score, alongside precision, specificity, and recall scores in\ndistinguishing health control with Alzheimer's Disease patients in Moderate to\nSevere Dementia using the power spectrum density (PSD) of\nelectroencephalography signals across various frequency bands. Moreover, our\nresearch enhanced the interpretability of the embedded adjacency matrices,\nrevealing connectivity differences in frontal and parietal brain regions\nbetween Alzheimer's patients and healthy individuals.",
        "published": "2025-04-02T03:29:12+00:00"
    },
    {
        "title": "Slow-Fast Architecture for Video Multi-Modal Large Language Models",
        "authors": [
            "Min Shi",
            "Shihao Wang",
            "Chieh-Yun Chen",
            "Jitesh Jain",
            "Kai Wang",
            "Junjun Xiong",
            "Guilin Liu",
            "Zhiding Yu",
            "Humphrey Shi"
        ],
        "summary": "Balancing temporal resolution and spatial detail under limited compute budget\nremains a key challenge for video-based multi-modal large language models\n(MLLMs). Existing methods typically compress video representations using\npredefined rules before feeding them into the LLM, resulting in irreversible\ninformation loss and often ignoring input instructions. To address this, we\npropose a novel slow-fast architecture that naturally circumvents this\ntrade-off, enabling the use of more input frames while preserving spatial\ndetails. Inspired by how humans first skim a video before focusing on relevant\nparts, our slow-fast design employs a dual-token strategy: 1) \"fast\" visual\ntokens -- a compact set of compressed video features -- are fed into the LLM\nalongside text embeddings to provide a quick overview; 2) \"slow\" visual tokens\n-- uncompressed video features -- are cross-attended by text embeddings through\nspecially designed hybrid decoder layers, enabling instruction-aware extraction\nof relevant visual details with linear complexity. We conduct systematic\nexploration to optimize both the overall architecture and key components.\nExperiments show that our model significantly outperforms self-attention-only\nbaselines, extending the input capacity from 16 to 128 frames with just a 3%\nincrease in computation, and achieving a 16% average performance improvement\nacross five video understanding benchmarks. Our 7B model achieves\nstate-of-the-art performance among models of similar size. Furthermore, our\nslow-fast architecture is a plug-and-play design that can be integrated into\nother video MLLMs to improve efficiency and scalability.",
        "published": "2025-04-02T03:24:58+00:00"
    },
    {
        "title": "CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection",
        "authors": [
            "Jin Lian",
            "Zhongyu Wan",
            "Ming Gao",
            "JunFeng Chen"
        ],
        "summary": "Cross-layer feature pyramid networks (CFPNs) have achieved notable progress\nin multi-scale feature fusion and boundary detail preservation for salient\nobject detection. However, traditional CFPNs still suffer from two core\nlimitations: (1) a computational bottleneck caused by complex feature weighting\noperations, and (2) degraded boundary accuracy due to feature blurring in the\nupsampling process. To address these challenges, we propose CFMD, a novel\ncross-layer feature pyramid network that introduces two key innovations. First,\nwe design a context-aware feature aggregation module (CFLMA), which\nincorporates the state-of-the-art Mamba architecture to construct a dynamic\nweight distribution mechanism. This module adaptively adjusts feature\nimportance based on image context, significantly improving both representation\nefficiency and generalization. Second, we introduce an adaptive dynamic\nupsampling unit (CFLMD) that preserves spatial details during resolution\nrecovery. By adjusting the upsampling range dynamically and initializing with a\nbilinear strategy, the module effectively reduces feature overlap and maintains\nfine-grained boundary structures. Extensive experiments on three standard\nbenchmarks using three mainstream backbone networks demonstrate that CFMD\nachieves substantial improvements in pixel-level accuracy and boundary\nsegmentation quality, especially in complex scenes. The results validate the\neffectiveness of CFMD in jointly enhancing computational efficiency and\nsegmentation performance, highlighting its strong potential in salient object\ndetection tasks.",
        "published": "2025-04-02T03:22:36+00:00"
    },
    {
        "title": "Biological network dynamics: Poincar\u00e9-Lindstedt series and the effect of delays",
        "authors": [
            "Renato Calleja",
            "Pablo Padilla-Longoria",
            "Edgar Rodr\u00edguez-Mendieta"
        ],
        "summary": "This paper focuses on the Hopf bifurcation in an activator-inhibitor system\nwithout diffusion which can be modeled as a delay differential equation. The\nmain result of this paper is the existence of the Poincar\\'e-Lindstedt series\nto all orders for the bifurcating periodic solutions. The model has a\nnon-linearity which is non-polynomial, and yet this allows us to exploit the\nuse of Fourier-Taylor series to develop order-by-order calculations that lead\nto linear recurrence equations for the coefficients of the Poincar\\'e-Lindstedt\nseries. As applications, we implement the computation of the coefficients of\nthese series for any finite order, and use a pseudo-arclength continuation to\ncompute branches of periodic solutions.",
        "published": "2025-04-02T03:13:38+00:00"
    },
    {
        "title": "Decoupled anisotropic Charge-Phonon Transport Enables Exceptional n-Type Thermoelectric Performance in CuBiSCl$_2$",
        "authors": [
            "Yu Wu",
            "Ying Chen",
            "Shuming Zeng",
            "Liujiang Zhou",
            "Chenhan Liu"
        ],
        "summary": "First-principles calculations demonstrate an exceptional decoupling of charge\nand thermal transport along the \\textit{a}-axis in CuBiSCl$_2$. The material\nachieves superior electron mobility (138 cm$^2$/V$\\cdot$s at 300 K) through\ndelocalized Bi-6\\textit{p}/S-3\\textit{p} networks while maintaining ultralow\nlattice thermal conductivity (0.40 W/mK at 300 K) via Cu-dominated anharmonic\nphonon scattering - both optimized along the same crystallographic direction.\nThis simultaneous optimization originates from the anisotropic bonding\nhierarchy where [BiSCl$_2$]$_n$ ribbons enable efficient charge transport along\n\\textit{a}-axis, while the soft vibrational modes associated with Cu atoms\nstrongly scatter heat-carrying phonons. The resulting high power factor (1.71\nmW/mK$^2$ at 700 K) and peak \\textit{ZT} of 1.57 establish CuBiSCl$_2$ as a\nmodel system that realizes the long-sought \"phonon glass-electron crystal\"\nparadigm through crystallographically engineered transport channels.",
        "published": "2025-04-02T03:10:17+00:00"
    },
    {
        "title": "Test-time Adaptation for Foundation Medical Segmentation Model without Parametric Updates",
        "authors": [
            "Kecheng Chen",
            "Xinyu Luo",
            "Tiexin Qin",
            "Jie Liu",
            "Hui Liu",
            "Victor Ho Fun Lee",
            "Hong Yan",
            "Haoliang Li"
        ],
        "summary": "Foundation medical segmentation models, with MedSAM being the most popular,\nhave achieved promising performance across organs and lesions. However, MedSAM\nstill suffers from compromised performance on specific lesions with intricate\nstructures and appearance, as well as bounding box prompt-induced\nperturbations. Although current test-time adaptation (TTA) methods for medical\nimage segmentation may tackle this issue, partial (e.g., batch normalization)\nor whole parametric updates restrict their effectiveness due to limited update\nsignals or catastrophic forgetting in large models. Meanwhile, these approaches\nignore the computational complexity during adaptation, which is particularly\nsignificant for modern foundation models. To this end, our theoretical analyses\nreveal that directly refining image embeddings is feasible to approach the same\ngoal as parametric updates under the MedSAM architecture, which enables us to\nrealize high computational efficiency and segmentation performance without the\nrisk of catastrophic forgetting. Under this framework, we propose to encourage\nmaximizing factorized conditional probabilities of the posterior prediction\nprobability using a proposed distribution-approximated latent conditional\nrandom field loss combined with an entropy minimization loss. Experiments show\nthat we achieve about 3\\% Dice score improvements across three datasets while\nreducing computational complexity by over 7 times.",
        "published": "2025-04-02T03:03:34+00:00"
    },
    {
        "title": "Low-Complexity Channel Estimation for RIS-Assisted ISAC System",
        "authors": [
            "Chen Zhen",
            "Li Jianqing",
            "Zhang Haijun",
            "Zhang Wei"
        ],
        "summary": "Integrated sensing and communication (ISAC), assisted by reconfigurable\nintelligent surface (RIS) has emerged as a breakthrough technology to improve\nthe capacity and reliability of 6G wireless network. However, a significant\nchallenge in RIS-ISAC systems is the acquisition of channel state information\n(CSI), largely due to co-channel interference, which hinders meeting the\nrequired reliability standards. To address this issue, a minimax-concave\npenalty (MCP)-based CSI refinement scheme is proposed. This approach utilizes\nan element-grouping strategy to jointly estimate the ISAC channel and the RIS\nphase shift matrix. Unlike previous methods, our scheme exploits the inherent\nsparsity in RIS-assisted ISAC channels to reduce training overhead, and the\nnear-optimal solution is derived for our studied RIS-ISAC scheme. The\neffectiveness of the element-grouping strategy is validated through simulation\nexperiments, demonstrating superior channel estimation results when compared to\nexisting benchmarks.",
        "published": "2025-04-02T02:57:05+00:00"
    },
    {
        "title": "Revisiting Funnel Transformers for Modern LLM Architectures with Comprehensive Ablations in Training and Inference Configurations",
        "authors": [
            "DongHyun Choi",
            "Lucas Spangher",
            "Chris Hidey",
            "Peter Grabowski",
            "Ramy Eskander"
        ],
        "summary": "Transformer-based Large Language Models, which suffer from high computational\ncosts, advance so quickly that techniques proposed to streamline earlier\niterations are not guaranteed to benefit more modern models. Building upon the\nFunnel Transformer proposed by Dai and Le (2020), which progressively\ncompresses intermediate representations, we investigate the impact of funneling\nin contemporary Gemma2 Transformer architectures. We systematically evaluate\nvarious funnel configurations and recovery methods, comparing: (1) standard\npretraining to funnel-aware pretraining strategies, (2) the impact of\nfunnel-aware fine-tuning, and (3) the type of sequence recovery operation. Our\nresults demonstrate that funneling creates information bottlenecks that\npropagate through deeper network layers, particularly in larger models (e.g.,\nGemma 7B), leading to at times unmanageable performance lost. However,\ncarefully selecting the funneling layer and employing effective recovery\nstrategies, can substantially mitigate performance losses, achieving up to a\n44\\% reduction in latency. Our findings highlight key trade-offs between\ncomputational efficiency and model accuracy, providing practical guidance for\ndeploying funnel-based approaches in large-scale natural language applications.",
        "published": "2025-04-02T02:09:17+00:00"
    },
    {
        "title": "Cross-Validating Quantum Network Simulators",
        "authors": [
            "Joaquin Chung",
            "Michal Hajdu\u0161ek",
            "Naphan Benchasattabuse",
            "Alexander Kolar",
            "Ansh Singal",
            "Kento Samuel Soon",
            "Kentaro Teramoto",
            "Allen Zang",
            "Raj Kettimuthu",
            "Rodney Van Meter"
        ],
        "summary": "We present a first cross-validation of two open-source quantum network\nsimulators, QuISP and SeQUeNCe, focusing on basic networking tasks to ensure\nconsistency and accuracy in simulation outputs. Despite very similar design\nobjectives of both simulators, their differing underlying assumptions can lead\nto variations in simulation results. We highlight the discrepancies in how the\ntwo simulators handle connections, internal network node processing time, and\nclassical communication, resulting in significant differences in the time\nrequired to perform basic network tasks such as elementary link generation and\nentanglement swapping. We devise common ground scenarios to compare both the\ntime to complete resource distribution and the fidelity of the distributed\nresources. Our findings indicate that while the simulators differ in the time\nrequired to complete network tasks, a constant factor difference attributable\nto their respective connection models, they agree on the fidelity of the\ndistributed resources under identical error parameters. This work demonstrates\na crucial first step towards enhancing the reliability and reproducibility of\nquantum network simulations, as well as leading to full protocol development.\nFurthermore, our benchmarking methodology establishes a foundational set of\ntasks for the cross-validation of simulators to study future quantum networks.",
        "published": "2025-04-02T01:48:37+00:00"
    },
    {
        "title": "BOLDSimNet: Examining Brain Network Similarity between Task and Resting-State fMRI",
        "authors": [
            "Boseong Kim",
            "Debashis Das Chakladar",
            "Haejun Chung",
            "Ikbeom Jang"
        ],
        "summary": "Traditional causal connectivity methods in task-based and resting-state\nfunctional magnetic resonance imaging (fMRI) face challenges in accurately\ncapturing directed information flow due to their sensitivity to noise and\ninability to model multivariate dependencies. These limitations hinder the\neffective comparison of brain networks between cognitive states, making it\ndifficult to analyze network reconfiguration during task and resting states. To\naddress these issues, we propose BOLDSimNet, a novel framework utilizing\nMultivariate Transfer Entropy (MTE) to measure causal connectivity and network\nsimilarity across different cognitive states. Our method groups functionally\nsimilar regions of interest (ROIs) rather than spatially adjacent nodes,\nimproving accuracy in network alignment. We applied BOLDSimNet to fMRI data\nfrom 40 healthy controls and found that children exhibited higher similarity\nscores between task and resting states compared to adolescents, indicating\nreduced variability in attention shifts. In contrast, adolescents showed more\ndifferences between task and resting states in the Dorsal Attention Network\n(DAN) and the Default Mode Network (DMN), reflecting enhanced network\nadaptability. These findings emphasize developmental variations in the\nreconfiguration of the causal brain network, showcasing BOLDSimNet's ability to\nquantify network similarity and identify attentional fluctuations between\ndifferent cognitive states.",
        "published": "2025-04-02T00:54:55+00:00"
    },
    {
        "title": "GigaAPI for GPU Parallelization",
        "authors": [
            "M. Suvarna",
            "O. Tehrani"
        ],
        "summary": "GigaAPI is a user-space API that simplifies multi-GPU programming, bridging\nthe gap between the capabilities of parallel GPU systems and the ability of\ndevelopers to harness their full potential. The API offers a comprehensive set\nof functionalities, including fundamental GPU operations, image processing, and\ncomplex GPU tasks, abstracting away the intricacies of low-level CUDA and C++\nprogramming. GigaAPI's modular design aims to inspire future NVIDIA researchers\nto create a generalized, dynamic, extensible, and cross-GPU\narchitecture-compatible API. Through experiments and simulations, we\ndemonstrate the general efficiency gains achieved by leveraging GigaAPI's\nsimplified multi-GPU programming model and showcase our learning experience\nthrough setup and other aspects, as we were interested in learning complex CUDA\nprogramming and parallelism. We hope that this contributes to the\ndemocratization of parallel GPU computing, enabling researchers and\npractitioners to unlock new possibilities across diverse domains.",
        "published": "2025-04-02T00:30:31+00:00"
    },
    {
        "title": "Bayesian critical points in classical lattice models",
        "authors": [
            "Adam Nahum",
            "Jesper Lykke Jacobsen"
        ],
        "summary": "The Boltzmann distribution encodes our subjective knowledge of the\nconfiguration in a classical lattice model, given only its Hamiltonian. If we\nacquire further information about the configuration from measurement, our\nknowledge is updated according to Bayes' theorem. We examine the resulting\n\"conditioned ensembles\", finding that they show many new phase transitions and\nnew renormalization-group fixed points. (Similar conditioned ensembles also\ndescribe \"partial quenches\" in which some of the system's degrees of freedom\nare instantaneously frozen, while the others continue to evolve.) After\ndescribing general features of the replica field theories for these problems,\nwe analyze the effect of measurement on illustrative critical systems,\nincluding: critical Ising and Potts models, which show surprisingly rich phase\ndiagrams, with RG fixed points at weak, intermediate, and infinite measurement\nstrength; various models involving free fields, XY spins, or flux lines in 2D\nor 3D; and geometrical models such as polymers or clusters. We make connections\nwith quantum dynamics, in particular with \"charge sharpening\" in 1D, by giving\na formalism for measurement of classical stochastic processes: e.g. we give a\npurely hydrodynamic derivation of the known effective field theory for charge\nsharpening. We discuss qualitative differences between RG flows for the above\nmeasured systems, described by $N\\to 1$ replica limits, and those for\ndisordered systems, described by $N\\to 0$ limits. In addition to discussing\nmeasurement of critical states, we give a unifying treatment of a family of\ninference problems for non-critical states. These are related to the Nishimori\nline in the phase diagram of the random-bond Ising model, and are relevant to\nvarious quantum error correction problems. We describe distinct physical\ninterpretations of conditioned ensembles and note interesting open questions.",
        "published": "2025-04-02T00:25:27+00:00"
    },
    {
        "title": "The Social Life of Industrial Arms: How Arousal and Attention Shape Human-Robot Interaction",
        "authors": [
            "Roy El-Helou",
            "Matthew K. X. J Pan"
        ],
        "summary": "This study explores how human perceptions of a non-anthropomorphic robotic\nmanipulator are shaped by two key dimensions of behaviour: arousal, defined as\nthe robot's movement energy and expressiveness, and attention, defined as the\nrobot's capacity to selectively orient toward and engage with a user. We\nintroduce a novel control architecture that integrates a gaze-like attention\nengine with an arousal-modulated motion system to generate socially meaningful\nbehaviours. In a user study, we find that robots exhibiting high attention --\nactively directing their focus toward users -- are perceived as warmer and more\ncompetent, intentional, and lifelike. In contrast, high arousal --\ncharacterized by fast, expansive, and energetic motions -- increases\nperceptions of discomfort and disturbance. Importantly, a combination of\nfocused attention and moderate arousal yields the highest ratings of trust and\nsociability, while excessive arousal diminishes social engagement. These\nfindings offer design insights for endowing non-humanoid robots with\nexpressive, intuitive behaviours that support more natural human-robot\ninteraction.",
        "published": "2025-04-02T00:17:24+00:00"
    },
    {
        "title": "Multicriticality in stochastic dynamics protected by self-duality",
        "authors": [
            "Konstantinos Sfairopoulos",
            "Luke Causer",
            "Juan P. Garrahan"
        ],
        "summary": "We study the dynamical large deviations (LD) of a class of one-dimensional\nkinetically constrained models whose (tilted) generators can be mapped into\nthemselves via duality transformations. We consider four representative models\nin detail: the domain-wall (DW) Fredrickson-Andersen (FA), the DW East, the\nZZZ-FA, and the XOR-FA models. Using numerical tensor networks, we build the LD\nphase diagrams of these models in terms of the softness of the constraint and\nthe counting field conjugate to the dynamical activity. In all cases, we find\ndistinct dynamical phases separated by phase transitions along the self-dual\nlines, revealing the presence of multi-critical points that delimit first-order\nfrom continuous active-inactive transitions. We discuss connections to\nsupersymmetry and possible extensions to higher spin and space dimensions.",
        "published": "2025-04-02T00:10:57+00:00"
    },
    {
        "title": "Proof of Humanity: A Multi-Layer Network Framework for Certifying Human-Originated Content in an AI-Dominated Internet",
        "authors": [
            "Sebastian Barros"
        ],
        "summary": "The rapid proliferation of generative AI has led to an internet increasingly\npopulated with synthetic content-text, images, audio, and video generated\nwithout human intervention. As the distinction between human and AI-generated\ndata blurs, the ability to verify content origin becomes critical for\napplications ranging from social media and journalism to legal and financial\nsystems.\n  In this paper, we propose a conceptual, multi-layer architectural framework\nthat enables telecommunications networks to act as infrastructure level\ncertifiers of human-originated content. By leveraging identity anchoring at the\nphysical layer, metadata propagation at the network and transport layers, and\ncryptographic attestations at the session and application layers, Telcos can\nprovide an end-to-end Proof of Humanity for data traversing their networks.\n  We outline how each OSI layer can contribute to this trust fabric using\ntechnical primitives such as SIM/eSIM identity, digital signatures,\nbehavior-based ML heuristics, and edge-validated APIs. The framework is\npresented as a foundation for future implementation, highlighting monetization\npathways for telcos such as trust-as-a-service APIs, origin-certified traffic\ntiers, and regulatory compliance tools.\n  The paper does not present implementation or benchmarking results but offers\na technically detailed roadmap and strategic rationale for transforming Telcos\ninto validators of digital authenticity in an AI-dominated internet. Security,\nprivacy, and adversarial considerations are discussed as directions for future\nwork.",
        "published": "2025-04-02T00:02:51+00:00"
    },
    {
        "title": "Real Time Animator: High-Quality Cartoon Style Transfer in 6 Animation Styles on Images and Videos",
        "authors": [
            "Liuxin Yang",
            "Priyanka Ladha"
        ],
        "summary": "This paper presents a comprehensive pipeline that integrates state-of-the-art\ntechniques to achieve high-quality cartoon style transfer for educational\nimages and videos. The proposed approach combines the Inversion-based Style\nTransfer (InST) framework for both image and video style stylization, the\nPre-Trained Image Processing Transformer (IPT) for post-denoising, and the\nDomain-Calibrated Translation Network (DCT-Net) for more consistent video style\ntransfer. By fine-tuning InST with specific cartoon styles, applying IPT for\nartifact reduction, and leveraging DCT-Net for temporal consistency, the\npipeline generates visually appealing and educationally effective stylized\ncontent. Extensive experiments and evaluations using the scenery and monuments\ndataset demonstrate the superiority of the proposed approach in terms of style\ntransfer accuracy, content preservation, and visual quality compared to the\nbaseline method, AdaAttN. The CLIP similarity scores further validate the\neffectiveness of InST in capturing style attributes while maintaining semantic\ncontent. The proposed pipeline streamlines the creation of engaging educational\ncontent, empowering educators and content creators to produce visually\ncaptivating and informative materials efficiently.",
        "published": "2025-04-01T23:56:11+00:00"
    },
    {
        "title": "OccludeNeRF: Geometric-aware 3D Scene Inpainting with Collaborative Score Distillation in NeRF",
        "authors": [
            "Jingyu Shi",
            "Achleshwar Luthra",
            "Jiazhi Li",
            "Xiang Gao",
            "Xiyun Song",
            "Zongfang Lin",
            "David Gu",
            "Heather Yu"
        ],
        "summary": "With Neural Radiance Fields (NeRFs) arising as a powerful 3D representation,\nresearch has investigated its various downstream tasks, including inpainting\nNeRFs with 2D images. Despite successful efforts addressing the view\nconsistency and geometry quality, prior methods yet suffer from occlusion in\nNeRF inpainting tasks, where 2D prior is severely limited in forming a faithful\nreconstruction of the scene to inpaint.\n  To address this, we propose a novel approach that enables cross-view\ninformation sharing during knowledge distillation from a diffusion model,\neffectively propagating occluded information across limited views.\nAdditionally, to align the distillation direction across multiple sampled\nviews, we apply a grid-based denoising strategy and incorporate additional\nrendered views to enhance cross-view consistency. To assess our approach's\ncapability of handling occlusion cases, we construct a dataset consisting of\nchallenging scenes with severe occlusion, in addition to existing datasets.\nCompared with baseline methods, our method demonstrates better performance in\ncross-view consistency and faithfulness in reconstruction, while preserving\nhigh rendering quality and fidelity.",
        "published": "2025-04-01T23:51:46+00:00"
    },
    {
        "title": "R2DN: Scalable Parameterization of Contracting and Lipschitz Recurrent Deep Networks",
        "authors": [
            "Nicholas H. Barbara",
            "Ruigang Wang",
            "Ian R. Manchester"
        ],
        "summary": "This paper presents the Robust Recurrent Deep Network (R2DN), a scalable\nparameterization of robust recurrent neural networks for machine learning and\ndata-driven control. We construct R2DNs as a feedback interconnection of a\nlinear time-invariant system and a 1-Lipschitz deep feedforward network, and\ndirectly parameterize the weights so that our models are stable (contracting)\nand robust to small input perturbations (Lipschitz) by design. Our\nparameterization uses a structure similar to the previously-proposed recurrent\nequilibrium networks (RENs), but without the requirement to iteratively solve\nan equilibrium layer at each time-step. This speeds up model evaluation and\nbackpropagation on GPUs, and makes it computationally feasible to scale up the\nnetwork size, batch size, and input sequence length in comparison to RENs. We\ncompare R2DNs to RENs on three representative problems in nonlinear system\nidentification, observer design, and learning-based feedback control and find\nthat training and inference are both up to an order of magnitude faster with\nsimilar test set performance, and that training/inference times scale more\nfavorably with respect to model expressivity.",
        "published": "2025-04-01T23:37:43+00:00"
    },
    {
        "title": "Dynamic Graph Structure Estimation for Learning Multivariate Point Process using Spiking Neural Networks",
        "authors": [
            "Biswadeep Chakraborty",
            "Hemant Kumawat",
            "Beomseok Kang",
            "Saibal Mukhopadhyay"
        ],
        "summary": "Modeling and predicting temporal point processes (TPPs) is critical in\ndomains such as neuroscience, epidemiology, finance, and social sciences. We\nintroduce the Spiking Dynamic Graph Network (SDGN), a novel framework that\nleverages the temporal processing capabilities of spiking neural networks\n(SNNs) and spike-timing-dependent plasticity (STDP) to dynamically estimate\nunderlying spatio-temporal functional graphs. Unlike existing methods that rely\non predefined or static graph structures, SDGN adapts to any dataset by\nlearning dynamic spatio-temporal dependencies directly from the event data,\nenhancing generalizability and robustness. While SDGN offers significant\nimprovements over prior methods, we acknowledge its limitations in handling\ndense graphs and certain non-Gaussian dependencies, providing opportunities for\nfuture refinement. Our evaluations, conducted on both synthetic and real-world\ndatasets including NYC Taxi, 911, Reddit, and Stack Overflow, demonstrate that\nSDGN achieves superior predictive accuracy while maintaining computational\nefficiency. Furthermore, we include ablation studies to highlight the\ncontributions of its core components.",
        "published": "2025-04-01T23:23:10+00:00"
    },
    {
        "title": "Towards Resilient Federated Learning in CyberEdge Networks: Recent Advances and Future Trends",
        "authors": [
            "Kai Li",
            "Zhengyang Zhang",
            "Azadeh Pourkabirian",
            "Wei Ni",
            "Falko Dressler",
            "Ozgur B. Akan"
        ],
        "summary": "In this survey, we investigate the most recent techniques of resilient\nfederated learning (ResFL) in CyberEdge networks, focusing on joint training\nwith agglomerative deduction and feature-oriented security mechanisms. We\nexplore adaptive hierarchical learning strategies to tackle non-IID data\nchallenges, improving scalability and reducing communication overhead. Fault\ntolerance techniques and agglomerative deduction mechanisms are studied to\ndetect unreliable devices, refine model updates, and enhance convergence\nstability. Unlike existing FL security research, we comprehensively analyze\nfeature-oriented threats, such as poisoning, inference, and reconstruction\nattacks that exploit model features. Moreover, we examine resilient aggregation\ntechniques, anomaly detection, and cryptographic defenses, including\ndifferential privacy and secure multi-party computation, to strengthen FL\nsecurity. In addition, we discuss the integration of 6G, large language models\n(LLMs), and interoperable learning frameworks to enhance privacy-preserving and\ndecentralized cross-domain training. These advancements offer ultra-low\nlatency, artificial intelligence (AI)-driven network management, and improved\nresilience against adversarial attacks, fostering the deployment of secure\nResFL in CyberEdge networks.",
        "published": "2025-04-01T23:06:45+00:00"
    },
    {
        "title": "First Field-Trial Demonstration of L4 Autonomous Optical Network for Distributed AI Training Communication: An LLM-Powered Multi-AI-Agent Solution",
        "authors": [
            "Yihao Zhang",
            "Qizhi Qiu",
            "Xiaomin Liu",
            "Dianxuan Fu",
            "Xingyu Liu",
            "Leyan Fei",
            "Yuming Cheng",
            "Lilin Yi",
            "Weisheng Hu",
            "Qunbi Zhuge"
        ],
        "summary": "We demonstrate the first cross-domain cross-layer level-4 autonomous optical\nnetwork via a multi-AI-agent system. Field trials show 98 percent task\ncompletion rate across the distributed AI training lifecycle-3.2x higher than\nsingle agents using state-of-the-art LLMs.",
        "published": "2025-04-01T22:48:22+00:00"
    },
    {
        "title": "Computational Study of Density Fluctuation-Induced Shear Bands Formation in Bulk Metallic Glasses",
        "authors": [
            "Siya Zhu",
            "Hagen Eckert",
            "Stefano Curtarolo",
            "Jan Schroers",
            "Axel van de Walle"
        ],
        "summary": "Seemingly identical Bulk Metallic Glasses (BMG) often exhibit strikingly\ndifferent mechanical properties despite having the same composition and fictive\ntemperature. A postulated mechanism underlying these differences is the\npresence of \"defects\". Here we investigate this hypothesis through the study of\nthe effect of density fluctuations on shear band formation under an applied\nstress. We find that the critical shear stress is strongly dependent on the\nmagnitude and size of the fluctuations. This finding also elucidates why,\nhistorically, critical shear stresses obtained in simulations have differed so\nmuch from those found experimentally, as typical simulations setups might favor\nunrealistically uniform geometries.",
        "published": "2025-04-01T22:36:18+00:00"
    },
    {
        "title": "rPPG-SysDiaGAN: Systolic-Diastolic Feature Localization in rPPG Using Generative Adversarial Network with Multi-Domain Discriminator",
        "authors": [
            "Banafsheh Adami",
            "Nima Karimian"
        ],
        "summary": "Remote photoplethysmography (rPPG) offers a novel approach to noninvasive\nmonitoring of vital signs, such as respiratory rate, utilizing a camera.\nAlthough several supervised and self-supervised methods have been proposed,\nthey often fail to accurately reconstruct the PPG signal, particularly in\ndistinguishing between systolic and diastolic components. Their primary focus\ntends to be solely on extracting heart rate, which may not accurately represent\nthe complete PPG signal. To address this limitation, this paper proposes a\nnovel deep learning architecture using Generative Adversarial Networks by\nintroducing multi-discriminators to extract rPPG signals from facial videos.\nThese discriminators focus on the time domain, the frequency domain, and the\nsecond derivative of the original time domain signal. The discriminator\nintegrates four loss functions: variance loss to mitigate local minima caused\nby noise; dynamic time warping loss to address local minima induced by\nalignment and sequences of variable lengths; Sparsity Loss for heart rate\nadjustment, and Variance Loss to ensure a uniform distribution across the\ndesired frequency domain and time interval between systolic and diastolic\nphases of the PPG signal.",
        "published": "2025-04-01T22:20:27+00:00"
    },
    {
        "title": "Gradient-free Continual Learning",
        "authors": [
            "Grzegorz Rype\u015b\u0107"
        ],
        "summary": "Continual learning (CL) presents a fundamental challenge in training neural\nnetworks on sequential tasks without experiencing catastrophic forgetting.\nTraditionally, the dominant approach in CL has been gradient-based\noptimization, where updates to the network parameters are performed using\nstochastic gradient descent (SGD) or its variants. However, a major limitation\narises when previous data is no longer accessible, as is often assumed in CL\nsettings. In such cases, there is no gradient information available for past\ndata, leading to uncontrolled parameter changes and consequently severe\nforgetting of previously learned tasks. By shifting focus from data\navailability to gradient availability, this work opens up new avenues for\naddressing forgetting in CL. We explore the hypothesis that gradient-free\noptimization methods can provide a robust alternative to conventional\ngradient-based continual learning approaches. We discuss the theoretical\nunderpinnings of such method, analyze their potential advantages and\nlimitations, and present empirical evidence supporting their effectiveness. By\nreconsidering the fundamental cause of forgetting, this work aims to contribute\na fresh perspective to the field of continual learning and inspire novel\nresearch directions.",
        "published": "2025-04-01T22:18:59+00:00"
    },
    {
        "title": "Prompting Forgetting: Unlearning in GANs via Textual Guidance",
        "authors": [
            "Piyush Nagasubramaniam",
            "Neeraj Karamchandani",
            "Chen Wu",
            "Sencun Zhu"
        ],
        "summary": "State-of-the-art generative models exhibit powerful image-generation\ncapabilities, introducing various ethical and legal challenges to service\nproviders hosting these models. Consequently, Content Removal Techniques (CRTs)\nhave emerged as a growing area of research to control outputs without\nfull-scale retraining. Recent work has explored the use of Machine Unlearning\nin generative models to address content removal. However, the focus of such\nresearch has been on diffusion models, and unlearning in Generative Adversarial\nNetworks (GANs) has remained largely unexplored. We address this gap by\nproposing Text-to-Unlearn, a novel framework that selectively unlearns concepts\nfrom pre-trained GANs using only text prompts, enabling feature unlearning,\nidentity unlearning, and fine-grained tasks like expression and multi-attribute\nremoval in models trained on human faces. Leveraging natural language\ndescriptions, our approach guides the unlearning process without requiring\nadditional datasets or supervised fine-tuning, offering a scalable and\nefficient solution. To evaluate its effectiveness, we introduce an automatic\nunlearning assessment method adapted from state-of-the-art image-text alignment\nmetrics, providing a comprehensive analysis of the unlearning methodology. To\nour knowledge, Text-to-Unlearn is the first cross-modal unlearning framework\nfor GANs, representing a flexible and efficient advancement in managing\ngenerative model behavior.",
        "published": "2025-04-01T22:18:40+00:00"
    },
    {
        "title": "Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models",
        "authors": [
            "Feng Chen",
            "Dror Ben-Zeev",
            "Gillian Sparks",
            "Arya Kadakia",
            "Trevor Cohen"
        ],
        "summary": "Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical\nsettings, presenting opportunities for automated detection to identify\npatients. This study evaluates natural language processing approaches for\ndetecting PTSD from clinical interview transcripts. We compared general and\nmental health-specific transformer models (BERT/RoBERTa), embedding-based\nmethods (SentenceBERT/LLaMA), and large language model prompting strategies\n(zero-shot/few-shot/chain-of-thought) using the DAIC-WOZ dataset.\nDomain-specific models significantly outperformed general models\n(Mental-RoBERTa F1=0.643 vs. RoBERTa-base 0.485). LLaMA embeddings with neural\nnetworks achieved the highest performance (F1=0.700). Zero-shot prompting using\nDSM-5 criteria yielded competitive results without training data (F1=0.657).\nPerformance varied significantly across symptom severity and comorbidity\nstatus, with higher accuracy for severe PTSD cases and patients with comorbid\ndepression. Our findings highlight the potential of domain-adapted embeddings\nand LLMs for scalable screening while underscoring the need for improved\ndetection of nuanced presentations and offering insights for developing\nclinically viable AI tools for PTSD assessment.",
        "published": "2025-04-01T22:06:28+00:00"
    },
    {
        "title": "GRU-AUNet: A Domain Adaptation Framework for Contactless Fingerprint Presentation Attack Detection",
        "authors": [
            "Banafsheh Adami",
            "Nima Karimian"
        ],
        "summary": "Although contactless fingerprints offer user comfort, they are more\nvulnerable to spoofing. The current solution for anti-spoofing in the area of\ncontactless fingerprints relies on domain adaptation learning, limiting their\ngeneralization and scalability. To address these limitations, we introduce\nGRU-AUNet, a domain adaptation approach that integrates a Swin\nTransformer-based UNet architecture with GRU-enhanced attention mechanisms, a\nDynamic Filter Network in the bottleneck, and a combined Focal and Contrastive\nLoss function. Trained in both genuine and spoof fingerprint images, GRU-AUNet\ndemonstrates robust resilience against presentation attacks, achieving an\naverage BPCER of 0.09\\% and APCER of 1.2\\% in the CLARKSON, COLFISPOOF, and\nIIITD datasets, outperforming state-of-the-art domain adaptation methods.",
        "published": "2025-04-01T22:02:41+00:00"
    },
    {
        "title": "Cooper: A Library for Constrained Optimization in Deep Learning",
        "authors": [
            "Jose Gallego-Posada",
            "Juan Ramirez",
            "Meraj Hashemizadeh",
            "Simon Lacoste-Julien"
        ],
        "summary": "Cooper is an open-source package for solving constrained optimization\nproblems involving deep learning models. Cooper implements several\nLagrangian-based first-order update schemes, making it easy to combine\nconstrained optimization algorithms with high-level features of PyTorch such as\nautomatic differentiation, and specialized deep learning architectures and\noptimizers. Although Cooper is specifically designed for deep learning\napplications where gradients are estimated based on mini-batches, it is\nsuitable for general non-convex continuous constrained optimization. Cooper's\nsource code is available at https://github.com/cooper-org/cooper.",
        "published": "2025-04-01T21:52:53+00:00"
    },
    {
        "title": "Lightweight Deep Models for Dermatological Disease Detection: A Study on Instance Selection and Channel Optimization",
        "authors": [
            "Ian Mateos Gonzalez",
            "Estefani Jaramilla Nava",
            "Abraham S\u00e1nchez Morales",
            "Jes\u00fas Garc\u00eda-Ram\u00edrez",
            "Ricardo Ramos-Aguilar"
        ],
        "summary": "The identification of dermatological disease is an important problem in\nMexico according with different studies. Several works in literature use the\ndatasets of different repositories without applying a study of the data\nbehavior, especially in medical images domain. In this work, we propose a\nmethodology to preprocess dermaMNIST dataset in order to improve its quality\nfor the classification stage, where we use lightweight convolutional neural\nnetworks. In our results, we reduce the number of instances for the neural\nnetwork training obtaining a similar performance of models as ResNet.",
        "published": "2025-04-01T21:47:57+00:00"
    },
    {
        "title": "Articulated Kinematics Distillation from Video Diffusion Models",
        "authors": [
            "Xuan Li",
            "Qianli Ma",
            "Tsung-Yi Lin",
            "Yongxin Chen",
            "Chenfanfu Jiang",
            "Ming-Yu Liu",
            "Donglai Xiang"
        ],
        "summary": "We present Articulated Kinematics Distillation (AKD), a framework for\ngenerating high-fidelity character animations by merging the strengths of\nskeleton-based animation and modern generative models. AKD uses a\nskeleton-based representation for rigged 3D assets, drastically reducing the\nDegrees of Freedom (DoFs) by focusing on joint-level control, which allows for\nefficient, consistent motion synthesis. Through Score Distillation Sampling\n(SDS) with pre-trained video diffusion models, AKD distills complex,\narticulated motions while maintaining structural integrity, overcoming\nchallenges faced by 4D neural deformation fields in preserving shape\nconsistency. This approach is naturally compatible with physics-based\nsimulation, ensuring physically plausible interactions. Experiments show that\nAKD achieves superior 3D consistency and motion quality compared with existing\nworks on text-to-4D generation. Project page:\nhttps://research.nvidia.com/labs/dir/akd/",
        "published": "2025-04-01T21:37:57+00:00"
    },
    {
        "title": "Global explainability of a deep abstaining classifier",
        "authors": [
            "Sayera Dhaubhadel",
            "Jamaludin Mohd-Yusof",
            "Benjamin H. McMahon",
            "Trilce Estrada",
            "Kumkum Ganguly",
            "Adam Spannaus",
            "John P. Gounley",
            "Xiao-Cheng Wu",
            "Eric B. Durbin",
            "Heidi A. Hanson",
            "Tanmoy Bhattacharya"
        ],
        "summary": "We present a global explainability method to characterize sources of errors\nin the histology prediction task of our real-world multitask convolutional\nneural network (MTCNN)-based deep abstaining classifier (DAC), for automated\nannotation of cancer pathology reports from NCI-SEER registries. Our classifier\nwas trained and evaluated on 1.04 million hand-annotated samples and makes\nsimultaneous predictions of cancer site, subsite, histology, laterality, and\nbehavior for each report. The DAC framework enables the model to abstain on\nambiguous reports and/or confusing classes to achieve a target accuracy on the\nretained (non-abstained) samples, but at the cost of decreased coverage.\nRequiring 97% accuracy on the histology task caused our model to retain only\n22% of all samples, mostly the less ambiguous and common classes. Local\nexplainability with the GradInp technique provided a computationally efficient\nway of obtaining contextual reasoning for thousands of individual predictions.\nOur method, involving dimensionality reduction of approximately 13000\naggregated local explanations, enabled global identification of sources of\nerrors as hierarchical complexity among classes, label noise, insufficient\ninformation, and conflicting evidence. This suggests several strategies such as\nexclusion criteria, focused annotation, and reduced penalties for errors\ninvolving hierarchically related classes to iteratively improve our DAC in this\ncomplex real-world implementation.",
        "published": "2025-04-01T21:34:10+00:00"
    },
    {
        "title": "A Virtual Laboratory for Managing Computational Experiments",
        "authors": [
            "Eleni Adamidi",
            "Panayiotis Deligiannis",
            "Nikos Foutris",
            "Thanasis Vergoulis"
        ],
        "summary": "Computational experiments have become essential for scientific discovery,\nallowing researchers to test hypotheses, analyze complex datasets, and validate\nfindings. However, as computational experiments grow in scale and complexity,\nensuring reproducibility and managing detailed metadata becomes increasingly\nchallenging, especially when orchestrating complex sequence of computational\ntasks. To address these challenges we have developed a virtual laboratory\ncalled SCHEMA lab, focusing on capturing rich metadata such as experiment\nconfigurations and performance metrics, to support computational\nreproducibility. SCHEMA lab enables researchers to create experiments by\ngrouping together multiple executions and manage them throughout their life\ncycle. In this demonstration paper, we present the SCHEMA lab architecture,\ncore functionalities, and implementation, emphasizing its potential to\nsignificantly enhance reproducibility and efficiency in computational research.",
        "published": "2025-04-01T21:25:23+00:00"
    },
    {
        "title": "Towards Sign Distance Function based Metamaterial Design: Neural Operator Transformer for Forward Prediction and Diffusion Models for Inverse Design",
        "authors": [
            "Qibang Liu",
            "Seid Koric",
            "Diab Abueidda",
            "Hadi Meidani",
            "Philippe Geubelle"
        ],
        "summary": "The inverse design of metamaterial architectures presents a significant\nchallenge, particularly for nonlinear mechanical properties involving large\ndeformations, buckling, contact, and plasticity. Traditional methods, such as\ngradient-based optimization, and recent generative deep-learning approaches\noften rely on binary pixel-based representations, which introduce jagged edges\nthat hinder finite element (FE) simulations and 3D printing. To overcome these\nchallenges, we propose an inverse design framework that utilizes a signed\ndistance function (SDF) representation combined with a conditional diffusion\nmodel. The SDF provides a smooth boundary representation, eliminating the need\nfor post-processing and ensuring compatibility with FE simulations and\nmanufacturing methods. A classifier-free guided diffusion model is trained to\ngenerate SDFs conditioned on target macroscopic stress-strain curves, enabling\nefficient one-shot design synthesis. To assess the mechanical response of the\ngenerated designs, we introduce a forward prediction model based on Neural\nOperator Transformers (NOT), which accurately predicts homogenized\nstress-strain curves and local solution fields for arbitrary geometries with\nirregular query meshes. This approach enables a closed-loop process for general\nmetamaterial design, offering a pathway for the development of advanced\nfunctional materials.",
        "published": "2025-04-01T21:17:41+00:00"
    },
    {
        "title": "Revolutionizing Fractional Calculus with Neural Networks: Voronovskaya-Damasclin Theory for Next-Generation AI Systems",
        "authors": [
            "R\u00f4mulo Damasclin Chaves dos Santos",
            "Jorge Henrique de Oliveira Sales"
        ],
        "summary": "This work introduces rigorous convergence rates for neural network operators\nactivated by symmetrized and perturbed hyperbolic tangent functions, utilizing\nnovel Voronovskaya-Damasclin asymptotic expansions. We analyze basic,\nKantorovich, and quadrature-type operators over infinite domains, extending\nclassical approximation theory to fractional calculus via Caputo derivatives.\nKey innovations include parameterized activation functions with asymmetry\ncontrol, symmetrized density operators, and fractional Taylor expansions for\nerror analysis. The main theorem demonstrates that Kantorovich operators\nachieve \\(o(n^{-\\beta(N-\\varepsilon)})\\) convergence rates, while basic\noperators exhibit \\(\\mathcal{O}(n^{-\\beta N})\\) error decay. For deep networks,\nwe prove \\(\\mathcal{O}(L^{-\\beta(N-\\varepsilon)})\\) approximation bounds.\nStability results under parameter perturbations highlight operator robustness.\nBy integrating neural approximation theory with fractional calculus, this work\nprovides foundational mathematical insights and deployable engineering\nsolutions, with potential applications in complex system modeling and signal\nprocessing.",
        "published": "2025-04-01T21:03:00+00:00"
    },
    {
        "title": "How to Maximize Efficiency in Systems with Exhausted Workers",
        "authors": [
            "Elif Beray Sariisik",
            "Melih Bastopcu",
            "Nail Akar",
            "Sennur Ulukus"
        ],
        "summary": "We consider the problem of assigning tasks efficiently to a set of workers\nthat can exhaust themselves as a result of processing tasks. If a worker is\nexhausted, it will take a longer time to recover. To model efficiency of\nworkers with exhaustion, we use a continuous-time Markov chain (CTMC). By\ntaking samples from the internal states of the workers, the source assigns\ntasks to the workers when they are found to be in their efficient states. We\nconsider two different settings where (i) the source can assign tasks to the\nworkers only when they are in their most efficient state, and (ii) it can\nassign tasks to workers when they are also moderately efficient in spite of a\npotentially reduced success probability. In the former case, we find the\noptimal policy to be a threshold-based sampling policy where the thresholds\ndepend on the workers' recovery and exhaustion rates. In the latter case, we\nsolve a non-convex sum-of-ratios problem using a branch-and-bound approach\nwhich performs well compared with the globally optimal solution.",
        "published": "2025-04-01T21:01:45+00:00"
    },
    {
        "title": "Detecting Financial Fraud with Hybrid Deep Learning: A Mix-of-Experts Approach to Sequential and Anomalous Patterns",
        "authors": [
            "Diego Vallarino"
        ],
        "summary": "Financial fraud detection remains a critical challenge due to the dynamic and\nadversarial nature of fraudulent behavior. As fraudsters evolve their tactics,\ndetection systems must combine robustness, adaptability, and precision. This\nstudy presents a hybrid architecture for credit card fraud detection that\nintegrates a Mixture of Experts (MoE) framework with Recurrent Neural Networks\n(RNNs), Transformer encoders, and Autoencoders. Each expert module contributes\na specialized capability: RNNs capture sequential behavior, Transformers\nextract high-order feature interactions, and Autoencoders detect anomalies\nthrough reconstruction loss. The MoE framework dynamically assigns predictive\nresponsibility among the experts, enabling adaptive and context-sensitive\ndecision-making.\n  Trained on a high-fidelity synthetic dataset that simulates real-world\ntransaction patterns and fraud typologies, the hybrid model achieved 98.7\npercent accuracy, 94.3 percent precision, and 91.5 percent recall,\noutperforming standalone models and classical machine learning baselines. The\nAutoencoder component significantly enhanced the system's ability to identify\nemerging fraud strategies and atypical behaviors.\n  Beyond technical performance, the model contributes to broader efforts in\nfinancial governance and crime prevention. It supports regulatory compliance\nwith Anti-Money Laundering (AML) and Know Your Customer (KYC) protocols and\naligns with routine activity theory by operationalizing AI as a capable\nguardian within financial ecosystems. The proposed hybrid system offers a\nscalable, modular, and regulation-aware approach to detecting increasingly\nsophisticated fraud patterns, contributing both to the advancement of\nintelligent systems and to the strengthening of institutional fraud defense\ninfrastructures.",
        "published": "2025-04-01T20:47:18+00:00"
    },
    {
        "title": "Neural Approaches to SAT Solving: Design Choices and Interpretability",
        "authors": [
            "David Moj\u017e\u00ed\u0161ek",
            "Jan H\u016fla",
            "Ziwei Li",
            "Ziyu Zhou",
            "Mikol\u00e1\u0161 Janota"
        ],
        "summary": "In this contribution, we provide a comprehensive evaluation of graph neural\nnetworks applied to Boolean satisfiability problems, accompanied by an\nintuitive explanation of the mechanisms enabling the model to generalize to\ndifferent instances. We introduce several training improvements, particularly a\nnovel closest assignment supervision method that dynamically adapts to the\nmodel's current state, significantly enhancing performance on problems with\nlarger solution spaces. Our experiments demonstrate the suitability of\nvariable-clause graph representations with recurrent neural network updates,\nwhich achieve good accuracy on SAT assignment prediction while reducing\ncomputational demands. We extend the base graph neural network into a diffusion\nmodel that facilitates incremental sampling and can be effectively combined\nwith classical techniques like unit propagation. Through analysis of embedding\nspace patterns and optimization trajectories, we show how these networks\nimplicitly perform a process very similar to continuous relaxations of MaxSAT,\noffering an interpretable view of their reasoning process. This understanding\nguides our design choices and explains the ability of recurrent architectures\nto scale effectively at inference time beyond their training distribution,\nwhich we demonstrate with test-time scaling experiments.",
        "published": "2025-04-01T20:31:01+00:00"
    },
    {
        "title": "Estimating Hourly Neighborhood Population Using Mobile Phone Data in the United States",
        "authors": [
            "Huan Ning",
            "Zhenlong Li",
            "Manzhu Yu",
            "Shiyan Zhang",
            "Shan Qiao"
        ],
        "summary": "Traditional population estimation techniques often fail to capture the\ndynamic fluctuations inherent in urban and rural population movements.\nRecognizing the need for a high spatiotemporal dynamic population dataset, we\npropose a method using smartphone-based human mobility data to reconstruct the\nhourly population for each neighborhood across the US. We quantify population\nfluctuations on an hourly, diurnal, daily, and seasonal basis, and compare\nthese with static population data to highlight the limitations of traditional\nmodels in capturing temporal dynamics. This study is one of the first hourly\npopulation products at a large geographic extent (US), contributing to various\nstudies that involve dynamic populations with high spatiotemporal resolution,\nsuch as air pollution exposure analysis and emergency response.",
        "published": "2025-04-01T20:25:32+00:00"
    },
    {
        "title": "Efficient n-body simulations using physics informed graph neural networks",
        "authors": [
            "V\u00edctor Ramos-Osuna",
            "Alberto D\u00edaz-\u00c1lvarez",
            "Ra\u00fal Lara-Cabrera"
        ],
        "summary": "This paper presents a novel approach for accelerating n-body simulations by\nintegrating a physics-informed graph neural networks (GNN) with traditional\nnumerical methods. Our method implements a leapfrog-based simulation engine to\ngenerate datasets from diverse astrophysical scenarios which are then\ntransformed into graph representations. A custom-designed GNN is trained to\npredict particle accelerations with high precision. Experiments, conducted on\n60 training and 6 testing simulations spanning from 3 to 500 bodies over 1000\ntime steps, demonstrate that the proposed model achieves extremely low\nprediction errors-loss values while maintaining robust long-term stability,\nwith accumulated errors in position, velocity, and acceleration remaining\ninsignificant. Furthermore, our method yields a modest speedup of approximately\n17% over conventional simulation techniques. These results indicate that the\nintegration of deep learning with traditional physical simulation methods\noffers a promising pathway to significantly enhance computational efficiency\nwithout compromising accuracy.",
        "published": "2025-04-01T20:23:34+00:00"
    },
    {
        "title": "LimTDD: A Compact Decision Diagram Integrating Tensor and Local Invertible Map Representations",
        "authors": [
            "Xin Hong",
            "Aochu Dai",
            "Dingchao Gao",
            "Sanjiang Li",
            "Zhengfeng Ji",
            "Mingsheng Ying"
        ],
        "summary": "Tensor Decision Diagrams (TDDs) provide an efficient structure for\nrepresenting tensors by combining techniques from both tensor networks and\ndecision diagrams, demonstrating competitive performance in quantum circuit\nsimulation and verification. However, existing decision diagrams, including\nTDDs, fail to exploit isomorphisms within tensors, limiting their compression\nefficiency. This paper introduces Local Invertible Map Tensor Decision Diagrams\n(LimTDDs), an extension of TDD that integrates local invertible maps (LIMs) to\nachieve more compact representations. Unlike LIMDD, which applies Pauli\noperators to quantum states, LimTDD generalizes this approach using the\nXP-stabilizer group, enabling broader applicability. We develop efficient\nalgorithms for normalization and key tensor operations, including slicing,\naddition, and contraction, essential for quantum circuit simulation and\nverification. Theoretical analysis shows that LimTDD surpasses TDD in\ncompactness while maintaining its generality and offers exponential advantages\nover both TDD and LIMDD in the best-case scenarios. Experimental results\nvalidate these improvements, demonstrating LimTDD's superior efficiency in\nquantum circuit simulation and functionality computation.",
        "published": "2025-04-01T20:23:33+00:00"
    },
    {
        "title": "Diversity Methods for Improving Convergence and Accuracy of Quantum Error Correction Decoders Through Hardware Emulation",
        "authors": [
            "Francisco Garcia-Herrero",
            "Javier Valls",
            "Llanos Vergara-Picazo",
            "Vicente Torres"
        ],
        "summary": "Understanding the impact of accuracy and speed when quantum error correction\n(QEC) decoders transition from floating-point software implementations to\nfinite-precision hardware architectures is crucial for resource estimation on\nboth classical and quantum sides. The final performance of the hardware\nimplementation influences the code distance, affecting the number of physical\nqubits needed, and defines connectivity between quantum and classical control\nunits, among other factors like refrigeration systems.\n  This paper introduces a hardware emulator to evaluate QEC decoders using real\nhardware instead of software models. The emulator can explore $10^{13}$\ndifferent error patterns in 20 days with a single FPGA device running at 150\nMHz, guaranteeing the decoder's performance at logical rates of $10^{-12}$, the\nrequirement for most quantum algorithms. In contrast, an optimized C++ software\non an Intel Core i9 with 128 GB RAM would take over a year to achieve similar\nresults. The emulator also enables storing patterns that generate logical\nerrors for offline analysis and to design new decoders.\n  Using results from the emulator, we propose a diversity-based method\ncombining several belief propagation (BP) decoders with different quantization\nlevels. Individually, these decoders may show subpar error correction, but\ntogether they outperform the floating-point version of BP for quantum\nlow-density parity-check (QLDPC) codes like hypergraph or lifted product.\nPreliminary results with circuit-level noise and bivariate bicycle codes\nsuggest hardware insights can also improve software. Our diversity-based\nproposal achieves a similar logical error rate as BP with ordered statistics\ndecoding, with average speed improvements ranging from 30% to 80%, and 10% to\n120% in worst-case scenarios, while reducing post-processing algorithm\nactivation by 47% to 96.93%, maintaining the same accuracy.",
        "published": "2025-04-01T20:04:27+00:00"
    },
    {
        "title": "Input Resolution Downsizing as a Compression Technique for Vision Deep Learning Systems",
        "authors": [
            "Jeremy Morlier",
            "Mathieu Leonardon",
            "Vincent Gripon"
        ],
        "summary": "Model compression is a critical area of research in deep learning, in\nparticular in vision, driven by the need to lighten models memory or\ncomputational footprints. While numerous methods for model compression have\nbeen proposed, most focus on pruning, quantization, or knowledge distillation.\nIn this work, we delve into an under-explored avenue: reducing the resolution\nof the input image as a complementary approach to other types of compression.\nBy systematically investigating the impact of input resolution reduction, on\nboth tasks of classification and semantic segmentation, and on convnets and\ntransformer-based architectures, we demonstrate that this strategy provides an\ninteresting alternative for model compression. Our experimental results on\nstandard benchmarks highlight the potential of this method, achieving\ncompetitive performance while significantly reducing computational and memory\nrequirements. This study establishes input resolution reduction as a viable and\npromising direction in the broader landscape of model compression techniques\nfor vision applications.",
        "published": "2025-04-01T19:34:57+00:00"
    },
    {
        "title": "Performative Drift Resistant Classification Using Generative Domain Adversarial Networks",
        "authors": [
            "Maciej Makowski",
            "Brandon Gower-Winter",
            "Georg Krempl"
        ],
        "summary": "Performative Drift is a special type of Concept Drift that occurs when a\nmodel's predictions influence the future instances the model will encounter. In\nthese settings, retraining is not always feasible. In this work, we instead\nfocus on drift understanding as a method for creating drift-resistant\nclassifiers. To achieve this, we introduce the Generative Domain Adversarial\nNetwork (GDAN) which combines both Domain and Generative Adversarial Networks.\nUsing GDAN, domain-invariant representations of incoming data are created and a\ngenerative network is used to reverse the effects of performative drift. Using\nsemi-real and synthetic data generators, we empirically evaluate GDAN's ability\nto provide drift-resistant classification. Initial results are promising with\nGDAN limiting performance degradation over several timesteps. Additionally,\nGDAN's generative network can be used in tandem with other models to limit\ntheir performance degradation in the presence of performative drift. Lastly, we\nhighlight the relationship between model retraining and the unpredictability of\nperformative drift, providing deeper insights into the challenges faced when\nusing traditional Concept Drift mitigation strategies in the performative\nsetting.",
        "published": "2025-04-01T19:12:34+00:00"
    },
    {
        "title": "A widely applicable Galaxy Group finder Using Machine Learning",
        "authors": [
            "Juntao Ma",
            "Jie Wang",
            "Tianxiang Mao",
            "Hongxiang Chen",
            "Yuxi Meng",
            "Xiaohu Yang",
            "Qingyang Li"
        ],
        "summary": "Galaxy groups are essential for studying the distribution of matter on a\nlarge scale in redshift surveys and for deciphering the link between galaxy\ntraits and their associated halos. In this work, we propose a widely applicable\nmethod for identifying groups through machine learning techniques in real space\ntaking into account the impact of redshift distortion. Our methodology involves\ntwo neural networks: one is a classification model for identifying central\ngalaxy groups, and the other is a regression model for predicting the mass of\nthese groups. Both models input observable galaxy traits, allowing future\napplicability to real survey data. Testing on simulated datasets indicates our\nmethod accurately identifies over $92\\%$ of groups with $\\mathrm{M}_{vir} \\geq\n10^{11}h^{-1}\\mathrm{M}_\\odot$, with $80\\%$ achieving a membership completeness\nof at least $80\\%$. The predicted group masses vary by less than 0.3 dex across\ndifferent mass scales, even in the absence of a priori data. Our network adapts\nseamlessly to expand to sparse samples with a flux limit of $m_{r} < 14$, to\nhigh redshift samples at $z=1.08$, and to galaxy samples from the TNG300\nhydrodynamical simulation without further training. Furthermore, the framework\ncan easily adjust to real surveys by training on redshift distorted samples\nwithout needing parameter changes. Careful consideration of different\nobservational effects in redshift space makes it promising that this method\nwill be applicable to real galaxy surveys.",
        "published": "2025-04-01T19:01:45+00:00"
    }
]