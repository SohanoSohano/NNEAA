{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network RAG Advisor - Workflow Notebook\n",
    "\n",
    "This notebook orchestrates the steps for the RAG system:\n",
    "1.  **Setup:** Installs dependencies and imports necessary modules.\n",
    "2.  **Configuration:** Sets up LlamaIndex global settings (Embedding Model, LLM).\n",
    "3.  **(Optional) Data Collection:** Runs the arXiv scraper.\n",
    "4.  **Index Building:** Creates or updates the FAISS vector index.\n",
    "5.  **Querying:** Loads the index and answers questions using the configured LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Working Directory\n",
    "\n",
    "Ensure this notebook is running with the project's root directory (`V1`) as the working directory so imports from `src` work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "print(f\"Current Working Directory: {cwd}\")\n",
    "# Verify 'src' directory exists - adjust path if notebook is not in V1\n",
    "src_path = os.path.join(cwd, 'src')\n",
    "if not os.path.isdir(src_path):\n",
    "    print(\"\\nERROR: 'src' directory not found.\")\n",
    "    print(\"Please ensure you are running this notebook from the project root ('V1') directory.\")\n",
    "else:\n",
    "    print(\"'src' directory found. Imports should work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dependencies\n",
    "\n",
    "Install the required packages using pip. Ensure your virtual environment (`venv`) is activated if running locally outside of an integrated environment like Colab/Paperspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall numpy==1.26.4 scipy==1.10.1 protobuf==3.20.3 fsspec==2024.6.1\n",
    "%pip install --upgrade llama-index-core llama-index-vector-stores-faiss llama-index-readers-file llama-index-embeddings-huggingface llama-index-llms-huggingface sentence-transformers faiss-cpu accelerate bitsandbytes transformers huggingface_hub arxiv\n",
    "# Install specific PyTorch version for your CUDA (Example: CUDA 12.1 Stable)\n",
    "# Adjust 'cu121' if you have a different CUDA version (e.g., 'cu118', or use nightly '--pre --index-url .../nightly/cu128' for CUDA 12.8)\n",
    "%pip uninstall torch torchvision torchaudio -y\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Optional: Install CUDA-enabled bitsandbytes if you have a compatible GPU and want 4-bit quantization\n",
    "# %pip uninstall bitsandbytes -y\n",
    "# %pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n",
    "\n",
    "# Verify PyTorch CUDA\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA Available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA Version detected by PyTorch: {torch.version.cuda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Imports & Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import faiss # Ensure faiss is imported if needed elsewhere\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    PromptTemplate,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "# Import functions from our .py files\n",
    "from src.arxiv_scraper import scrape_arxiv\n",
    "from src.rag_pipeline import build_faiss_index, load_faiss_index\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.getLogger().setLevel(logging.INFO) # Ensure root logger level is INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration (Global Settings)\n",
    "\n",
    "Configure the Embedding Model and LLM globally using `llama_index.core.Settings`. This ensures consistency across index loading and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Configuring global LlamaIndex settings...\")\n",
    "\n",
    "# 1. Configure Embedding Model\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "logging.info(f\"Using embedding model: {Settings.embed_model.model_name}\")\n",
    "\n",
    "# 2. Configure LLM (Llama 3.1 8B Instruct - No Quantization based on qa_system.py)\n",
    "llm_model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "logging.info(f\"Setting up LLM: {llm_model_name}\")\n",
    "\n",
    "# --- RAG Prompt Template for Llama 3 Instruct ---\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    \"You are an expert Q&A assistant specialized in neural network architectures. \"\n",
    "    \"Your goal is to answer the user's query accurately based *only* on the provided context information. \"\n",
    "    \"If the context does not contain the information needed to answer the query, \"\n",
    "    \"state that the answer is not found in the context. Do not add information \"\n",
    "    \"that is not present in the context. Keep your answers concise and directly relevant to the query.\"\n",
    "    \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    \"Context information:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, answer the query.\\n\"\n",
    "    \"Query: {query_str}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    ")\n",
    "\n",
    "# --- LLM Initialization (No Quantization, using float16) ---\n",
    "# Ensure you have enough VRAM (>16GB) for float16, otherwise remove torch_dtype\n",
    "use_quantization = False # Set to True if you successfully installed CUDA bitsandbytes and want to try it\n",
    "quantization_config = None\n",
    "model_kwargs = {\"torch_dtype\": torch.float16} # Use float16 by default\n",
    "\n",
    "if use_quantization:\n",
    "    try:\n",
    "        # Define 4-bit quantization config\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "           load_in_4bit=True,\n",
    "           bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        model_kwargs[\"quantization_config\"] = quantization_config\n",
    "        # If using quantization, might not need torch_dtype explicitly\n",
    "        if \"torch_dtype\" in model_kwargs: del model_kwargs[\"torch_dtype\"]\n",
    "        logging.info(\"Using 4-bit quantization configuration.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to configure quantization: {e}. Disabling quantization.\", exc_info=True)\n",
    "        use_quantization = False\n",
    "        quantization_config = None\n",
    "        if \"quantization_config\" in model_kwargs: del model_kwargs[\"quantization_config\"]\n",
    "        # Fallback to float16 if quantization fails\n",
    "        if \"torch_dtype\" not in model_kwargs:\n",
    "            model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "\n",
    "if not use_quantization:\n",
    "     logging.info(\"Quantization disabled. Using torch_dtype: %s\", model_kwargs.get(\"torch_dtype\", \"Default (likely float32)\"))\n",
    "\n",
    "try:\n",
    "    # Log in to Hugging Face Hub (required for Llama 3 models)\n",
    "    # Ensure you have run `huggingface-cli login` in your terminal previously,\n",
    "    # OR set the HF_TOKEN environment variable.\n",
    "    from huggingface_hub import login\n",
    "    # login() # Call this if needed, or rely on CLI login / env var\n",
    "\n",
    "    Settings.llm = HuggingFaceLLM(\n",
    "        model_name=llm_model_name,\n",
    "        tokenizer_name=llm_model_name,\n",
    "        query_wrapper_prompt=query_wrapper_prompt,\n",
    "        context_window=131072,\n",
    "        max_new_tokens=512,\n",
    "        model_kwargs=model_kwargs,\n",
    "        generate_kwargs={\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True,\n",
    "        },\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    logging.info(f\"LLM '{llm_model_name}' configured successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to initialize LLM: {e}\", exc_info=True)\n",
    "    logging.error(\"Ensure you have accepted Llama 3 terms and logged into Hugging Face.\")\n",
    "    # Set LLM to None to potentially allow other parts of the notebook to run\n",
    "    Settings.llm = None\n",
    "    print(\"\\n!!! LLM INITIALIZATION FAILED. Querying will not work. Please check errors above. !!!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (Optional) Data Collection\n",
    "\n",
    "Run the arXiv scraper to fetch research papers. This only needs to be done once or when you want to update the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scraper = False # Set to True to run the scraper\n",
    "\n",
    "if run_scraper:\n",
    "    logging.info(\"Running arXiv scraper...\")\n",
    "    try:\n",
    "        # You can customize the query and max_results here\n",
    "        scrape_arxiv(query=\"neural network architecture OR large language model\", max_results=500)\n",
    "        logging.info(\"arXiv scraper finished.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"arXiv scraper failed: {e}\", exc_info=True)\n",
    "else:\n",
    "    logging.info(\"Skipping arXiv scraper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Index Building\n",
    "\n",
    "Create the FAISS vector index from the downloaded documents. This uses the `build_faiss_index` function from `src/rag_pipeline.py` and relies on the global `Settings.embed_model` configured earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_index_build = True # Set to True to build/rebuild the index\n",
    "data_directory = \"data/research_papers\"\n",
    "persist_directory = \"storage\"\n",
    "\n",
    "# Optional: Check if index already exists to avoid rebuilding\n",
    "faiss_binary_path = os.path.join(persist_directory, \"vector_store.faiss\")\n",
    "if os.path.exists(faiss_binary_path):\n",
    "    logging.info(f\"Index already exists at {persist_directory}. Set run_index_build=True to force rebuild.\")\n",
    "    run_index_build = False # Avoid accidental rebuild\n",
    "\n",
    "if run_index_build:\n",
    "    logging.info(\"Building FAISS index...\")\n",
    "    # Make sure the data directory exists\n",
    "    if not os.path.isdir(data_directory) or not os.listdir(data_directory):\n",
    "        logging.error(f\"Data directory '{data_directory}' is empty or does not exist. Cannot build index.\")\n",
    "        logging.error(\"Please run the scraper (Step 3) or place documents in the directory first.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Note: build_faiss_index in rag_pipeline.py might try to set Settings.llm = None.\n",
    "            # Our global Settings configuration in this notebook should take precedence during execution.\n",
    "            build_faiss_index(data_dir=data_directory, persist_dir=persist_directory)\n",
    "            logging.info(\"Index building process finished.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Index building failed: {e}\", exc_info=True)\n",
    "else:\n",
    "    logging.info(\"Skipping index build.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Querying\n",
    "\n",
    "Load the index and ask questions. This uses the `load_faiss_index` function and the globally configured `Settings` (including the LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query function specific to the notebook\n",
    "def run_query(question: str, index_persist_dir=\"storage\"):\n",
    "    \"\"\"Loads index, creates engine, and runs query using global Settings.\"\"\"\n",
    "    if Settings.llm is None:\n",
    "        logging.error(\"LLM is not configured (Settings.llm is None). Cannot run query.\")\n",
    "        return \"Error: LLM not initialized.\"\n",
    "\n",
    "    logging.info(f\"Loading index from '{index_persist_dir}' for querying...\")\n",
    "    index = load_faiss_index(persist_dir=index_persist_dir)\n",
    "\n",
    "    if index is None:\n",
    "        logging.error(\"Index loading failed.\")\n",
    "        return \"Error: Could not load the index. Please build it first.\"\n",
    "    logging.info(\"Index loaded successfully.\")\n",
    "\n",
    "    logging.info(\"Creating query engine...\")\n",
    "    try:\n",
    "        # Create engine using global Settings (LLM + Embed Model)\n",
    "        query_engine = index.as_query_engine()\n",
    "        logging.info(\"Query engine ready.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create query engine: {e}\", exc_info=True)\n",
    "        return f\"Error creating query engine: {e}\"\n",
    "\n",
    "    logging.info(f\"Sending query to LLM: '{question}'\")\n",
    "    try:\n",
    "        response = query_engine.query(question)\n",
    "        logging.info(\"LLM processing finished.\")\n",
    "        answer_text = str(response.response).strip()\n",
    "        # Clean up potential end token\n",
    "        if answer_text.endswith(\"<|eot_id|>\"):\n",
    "              answer_text = answer_text[:-len(\"<|eot_id|>\")].strip()\n",
    "        return answer_text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during querying: {e}\", exc_info=True)\n",
    "        return f\"Error during query: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Sample Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define your question here ---\n",
    "my_question = \"I have a finance advisor ai model. I want to evolve it using genetic algorithms. What must i do?\"\n",
    "#my_question = \"What is ResNet?\"\n",
    "#my_question = \"Summarize recent advancements in transformer architectures.\"\n",
    "\n",
    "print(f\"Asking: {my_question}\\n\")\n",
    "\n",
    "# Run the query function\n",
    "answer = run_query(my_question)\n",
    "\n",
    "print(\"\\nSynthesized Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "*   Experiment with different questions in the cell above.\n",
    "*   Modify the LLM configuration (e.g., try quantization if you fix `bitsandbytes`, try different `temperature` values).\n",
    "*   Update the scraper query and rebuild the index with different data.\n",
    "*   Integrate the `data_structures.py` content if needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
